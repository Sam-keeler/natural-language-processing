{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was acquired using web scraping techniques to pull from github based on search results from the word \"repository\". After obtaining the repo name, primary coding language, and readme contents, I processed the readme data into a form that would be easier to use natural language methods on. The goal here being to predict the primary coding language based on the contents of the readme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It was found that Python, Java, and C++ were the most common coding languages\n",
    "- Only about a third of the repositories had specific coding languages mentioned in their readmes\n",
    "- The common words found in each coding language group varied from group to group\n",
    "- The best performing model (KNN) beat my baseline by 19% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/samkeeler/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samkeeler/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "import prepare\n",
    "import acquire\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from env import github_token, github_username\n",
    "\n",
    "headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Acquires the data via a saved csv, or if that is not present runs the scrape_github_data function '''\n",
    "\n",
    "df = acquire.get_repo_data(cached = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Takes in readme contents and applies the make_prepped_columns() function. Drops altered readme \n",
    "columns with the exception of the lemmatized one (as that's what I'll be working with) and the original.\n",
    "Also gets rid of rows containing null readme or null language columns. Drops rows if their respective language \n",
    "appears less than twice (can't split it). Removes stopwords.\n",
    "'''\n",
    "\n",
    "df = prepare.prep_repos(df)\n",
    "\n",
    "'''\n",
    "Adds a feature that searches the readme for mentions of a specific coding language and extracts it, then puts that \n",
    "language into the \"languages_in_readme\" column. Dummies are then created for each language found. Also adds a \n",
    "feature for \"readme_length\"\n",
    "'''\n",
    "\n",
    "df = prepare.add_language_dummies_and_length_feature(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the data into train, validate, and test\n",
    "\n",
    "train, validate, test = prepare.split(df, stratify_by = 'language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing how often each language appears in the dataset\n",
    "\n",
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I visualize the language distributions\n",
    "\n",
    "plt.subplots(figsize = (20, 6))\n",
    "plt.title('Coding Language Distribution')\n",
    "sns.countplot(x=\"language\", data=df,\n",
    "                 palette=\"Blues_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the top words from all of the readme's combined\n",
    "\n",
    "the_words = ' '.join(df.readme_contents_clean)\n",
    "all_freq = pd.Series(the_words.split(' ')).value_counts()\n",
    "\n",
    "all_word_freq = pd.DataFrame(all_freq, columns =['Count'])\n",
    "all_word_freq = all_word_freq.reset_index()\n",
    "all_word_freq = all_word_freq.rename(columns = {'index':'Top Words'})\n",
    "\n",
    "plt.subplots(figsize = (20, 6))\n",
    "plt.title('All Words Distribution')\n",
    "sns.barplot(x=\"Top Words\", y='Count', data=all_word_freq.head(10),\n",
    "                 palette=\"Purples_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the top words of the three most frequent coding languages in the dataset\n",
    "\n",
    "python_words = ' '.join(train[train.language == 'Python'].readme_contents_clean)\n",
    "java_words = ' '.join(train[train.language == 'Java'].readme_contents_clean)\n",
    "cplusplus_words = ' '.join(train[train.language == 'C++'].readme_contents_clean)\n",
    "\n",
    "python_freq = pd.Series(python_words.split(' ')).value_counts()\n",
    "java_freq = pd.Series(java_words.split(' ')).value_counts()\n",
    "cplusplus_freq = pd.Series(cplusplus_words.split(' ')).value_counts()\n",
    "\n",
    "print('Popular Words in Python'), print(python_freq.head()), print(f\"\\n\"), print('Popular Words in Java'),\n",
    "print(java_freq.head()), print(\"\\n\"),print('Popular Words in C++'), print(cplusplus_freq.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing a distribution of the top words used in readmes where the repo's main language was python\n",
    "\n",
    "py_word_freq = pd.DataFrame(python_freq, columns =['Count'])\n",
    "py_word_freq = py_word_freq.reset_index()\n",
    "py_word_freq = py_word_freq.rename(columns = {'index':'Top Words'})\n",
    "\n",
    "plt.subplots(figsize = (20, 6))\n",
    "plt.title('Python Word Distribution')\n",
    "sns.barplot(x=\"Top Words\", y='Count', data=py_word_freq.head(10),\n",
    "                 palette=\"Reds_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing a distribution of the top words used in readmes where the repo's main language was java\n",
    "\n",
    "java_word_freq = pd.DataFrame(java_freq, columns =['Count'])\n",
    "java_word_freq = java_word_freq.reset_index()\n",
    "java_word_freq = java_word_freq.rename(columns = {'index':'Top Words'})\n",
    "\n",
    "plt.subplots(figsize = (20, 6))\n",
    "plt.title('Java Word Distribution')\n",
    "sns.barplot(x=\"Top Words\", y='Count', data=java_word_freq.head(10),\n",
    "                 palette=\"Oranges_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing a distribution of the top words used in readmes where the repo's main language was C++\n",
    "\n",
    "cpl_word_freq = pd.DataFrame(cplusplus_freq, columns =['Count'])\n",
    "cpl_word_freq = cpl_word_freq.reset_index()\n",
    "cpl_word_freq = cpl_word_freq.rename(columns = {'index':'Top Words'})\n",
    "\n",
    "plt.subplots(figsize = (20, 6))\n",
    "plt.title('C++ Word Distribution')\n",
    "sns.barplot(x=\"Top Words\", y='Count', data=cpl_word_freq.head(10),\n",
    "                 palette=\"Greens_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprisingly there was a significant difference in the words used from coding language to coding language. \n",
    "# Among the top 3 coding language's top 10 words, only two words were present in more than one language (project \n",
    "# and module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the average readme length grouped by coding language. There is a large amount of variance in the\n",
    "# readme lengths\n",
    "\n",
    "language_lengths = train.groupby('language').readme_length.mean().sort_values(ascending = False)\n",
    "language_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I'm gonna visualize it\n",
    "\n",
    "lang_length_df = pd.DataFrame(language_lengths)\n",
    "lang_length_df = lang_length_df.reset_index()\n",
    "lang_length_df = lang_length_df.rename(columns = {'language': 'Programming Language',\n",
    "                                                  'readme_length': 'Readme Length (characters)'})\n",
    "plt.subplots(figsize = (15, 7.5))\n",
    "plt.title('Mean Readme Length By Language')\n",
    "sns.barplot(x='Programming Language', y='Readme Length (characters)', data=lang_length_df,\n",
    "                 palette=\"Greens_d\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample of repos that had languages listed in the readme\n",
    "\n",
    "df[df['languages_in_readme'].notnull()].head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the data into train, validate, and test\n",
    "df.drop(columns = ['languages_in_readme', 'repo'], inplace = True)\n",
    "train, validate, test = prepare.split(df, stratify_by = 'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting from target variable for creating models\n",
    "\n",
    "X_train = train.drop(columns = ['language'])\n",
    "X_validate = validate.drop(columns = ['language'])\n",
    "X_test = test.drop(columns = ['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating target variable groups for creating models\n",
    "\n",
    "y_train = train.language\n",
    "y_validate = validate.language\n",
    "y_test = test.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vectorizer object \n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fitting that object onto the train data\n",
    "\n",
    "tfidf.fit(X_train.readme_contents_clean)\n",
    "\n",
    "# Applying the vector transformer to each data set\n",
    "\n",
    "X_train_vectorized = tfidf.transform(X_train.readme_contents_clean)\n",
    "X_validate_vectorized = tfidf.transform(X_validate.readme_contents_clean)\n",
    "X_test_vectorized = tfidf.transform(X_test.readme_contents_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe that will hold predicted and actual values for evaluation metrics\n",
    "\n",
    "train = pd.DataFrame(dict(actual=y_train))\n",
    "validate = pd.DataFrame(dict(actual=y_validate))\n",
    "test = pd.DataFrame(dict(actual=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing a baseline\n",
    "\n",
    "print('Baseline Accuracy:', round((21/len(df)), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and fitting the logistic regression model\n",
    "\n",
    "lm = LogisticRegression()\n",
    "lm.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying and evaluating the logistic regression model\n",
    "\n",
    "train['predicted_logreg'] = lm.predict(X_train_vectorized)\n",
    "validate[\"predicted_logreg\"] = lm.predict(X_validate_vectorized)\n",
    "print('Train:', (train.actual == train.predicted_logreg).mean()), print('Validate:', (validate.actual == validate.predicted_logreg).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and fitting the naive bayes model\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_vectorized.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying and evaluating the naive bayes model\n",
    "\n",
    "train['predicted_gnb'] = gnb.predict(X_train_vectorized.toarray())\n",
    "validate['predicted_gnb'] = gnb.predict(X_validate_vectorized.toarray())\n",
    "print('Train:', (train.actual == train.predicted_gnb).mean()), print('Validate:', (validate.actual == validate.predicted_gnb).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and fitting the random forest model\n",
    "\n",
    "rf = RandomForestClassifier(max_depth = 8, min_samples_leaf = 3, random_state=123)\n",
    "rf.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying and evaluating the random forest model\n",
    "\n",
    "train['predicted_rf'] = rf.predict(X_train_vectorized.toarray())\n",
    "validate['predicted_rf'] = rf.predict(X_validate_vectorized.toarray())\n",
    "print('Train:', (train.actual == train.predicted_rf).mean()), print('Validate:', (validate.actual == validate.predicted_rf).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and fitting the KNN model\n",
    "\n",
    "kn = KNeighborsClassifier(n_neighbors=6, weights='uniform')\n",
    "kn = kn.fit(X_train_vectorized.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying and evaluating the KNN model\n",
    "\n",
    "train['predicted_knn'] = kn.predict(X_train_vectorized.toarray())\n",
    "validate['predicted_knn'] = kn.predict(X_validate_vectorized.toarray())\n",
    "print('Train:', (train.actual == train.predicted_knn).mean()), print('Validate:', (validate.actual == validate.predicted_knn).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since KNN is my best performing model on validate, I'm gonna go ahead and test on it\n",
    "\n",
    "test['predicted_knn'] = kn.predict(X_test_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Performance:', (test.actual == test.predicted_knn).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A performance more than twice as good as the baseline is a great improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
