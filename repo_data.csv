,repo,language,readme_contents
0,zq99299/repository-summary,,"<p align=""center"">
    <a href=""https://github.com/zq99299/repository-summary"" target=""_blank"">
     <img width=""300"" src=""./BIGMLOGO.gif"" alt=""logo"">
    </a>
    <br/>
    <span>希望聚集一些有自己精彩笔记分享的人</span>
    <br/>
    <a href=""https://jq.qq.com/?_wv=1027&k=RE3803x2"" target=""_blank"">点击链接加入群聊【技术笔记分享】</a>
</p>


# 仓库汇总精选 😎

本账户下的笔记项目越来越多，比较凌乱，特将一些比较可以的仓库分类成导航目录，方便查阅

> 如果你打不开 GitHub 相关网页，可以安装 **[ dev-sidecar](https://github.com/docmirror/dev-sidecar)** 工具，应该就可以访问了，也能在线阅读本笔记了

> 🎉 由于笔记太多太分散，需要笔记的时候往往很难找到所需要的笔记，决定以后的新增笔记将会在 [语雀](https://www.yuque.com/mrcode.cn) 上更新

[![QQ](https://img.shields.io/badge/QQ-99299684-67ab82?logo=Tencent-QQ)](http://wpa.qq.com/msgrd?v=3&uin=99299684&site=qq&menu=yes) 
[![CSDN](https://img.shields.io/badge/CSDN-%20-67ab82?logo=bloglovin)](https://blog.csdn.net/mr_zhuqiang) 
[![GitHub](https://img.shields.io/badge/GitHub-%20-67ab82?logo=github)](https://github.com/zq99299) 
[![语雀](https://img.shields.io/badge/yuque-语雀-67ab82?logo=)](https://www.yuque.com/mrcode.cn) 

## 仓库类 🎉
- [essay-note](https://github.com/zq99299/essay-note) &nbsp; &nbsp; 
![GitHub stars](https://img.shields.io/github/stars/zq99299/essay-note)
![GitHub forks](https://img.shields.io/github/forks/zq99299/essay-note)
  
  老笔记本，是以 GitBook 为蓝本写的，GitBook 被抛弃后，该仓库内容很多了，主要是排版质量不行，就暂停更新了
  
  成套内容包含：
  
  - 某课 SpringSecurity 视频教程笔记 
  - 某课 Spring Cloud 微服务实战笔记
  - CSS 深入理解-张星旭系列
  
   自己原创内容包含：  
  
    - spring-restdocs-asciidoctor
    - Spring-WebSocket 
  
    另外还有一些零散的知识
  
- [note-book](https://github.com/zq99299/note-book) &nbsp; &nbsp; 
  ![GitHub stars](https://img.shields.io/github/stars/zq99299/note-book)
  ![GitHub forks](https://img.shields.io/github/forks/zq99299/note-book)

  新笔记本，以后都使用 vuepress 为蓝本编写的，排版质量比较高

  成套内容包含：
  - elasticsearch 初级、高级
  - 缓存架构-亿级流量电商详情页系统实战
  - 设计模式（某课）、研磨设计模式
  - Spring Cloud Config 官网教程阅读笔记
  - 正则入门
  - CSS 深入理解-张星旭（重学版）
  - 后端存储实战
  - 说透中台
  - Oath 2.0 实战
  - Git 系统学习笔记

  另外包含一些零散的知识：mycat、gradle、npm、k8s、docker、virtualbox 、vuepress 等内容

- [note-book2](https://github.com/zq99299/note-book2) &nbsp; &nbsp; 
  ![GitHub stars](https://img.shields.io/github/stars/zq99299/note-book2)
  ![GitHub forks](https://img.shields.io/github/forks/zq99299/note-book2)

  note-book 分仓，主仓由于笔记文件太多，导致 vuepress 构建吃力。

  成套内容包含：

  - JAVA 生产环境下性能监控与调优详解
  - 透视 HTTP 协议
  - DDD 实战课

- [java-tutorial](https://github.com/zq99299/java-tutorial) &nbsp; &nbsp; 
  ![GitHub stars](https://img.shields.io/github/stars/zq99299/java-tutorial)
  ![GitHub forks](https://img.shields.io/github/forks/zq99299/java-tutorial)

  Java8 官网教程文档，机翻 + 经验实践

  「基础篇」已完成，「高级篇」已更新至「JDBC 数据库访问」，由于最近都在学习其他方面的知识，该项目可能很长一段时间都不会继续更新了

- [linux-tutorial](https://github.com/zq99299/linux-tutorial) &nbsp; &nbsp; 
  ![GitHub stars](https://img.shields.io/github/stars/zq99299/linux-tutorial)
  ![GitHub forks](https://img.shields.io/github/forks/zq99299/linux-tutorial)

  linux 系列教程笔记，目前有《Linux 基础篇（鸟哥私房菜）- 第四版》 

- [mysql-tutorial](https://github.com/zq99299/mysql-tutorial) &nbsp; &nbsp; 
  ![GitHub stars](https://img.shields.io/github/stars/zq99299/mysql-tutorial)
  ![GitHub forks](https://img.shields.io/github/forks/zq99299/mysql-tutorial)

  mysql 系列知识，成套内容包含：

  - 某课 给程序员的 MySQL 必知必会
  - 阿里新零售数据库设计与实战
  - 高性能 MySQL 第三版（太难，只学习了其中一部分，实在看不懂，看不下去了）

- [mq-tutorial](https://github.com/zq99299/mq-tutorial) &nbsp; &nbsp; 
  ![GitHub stars](https://img.shields.io/github/stars/zq99299/mq-tutorial)
  ![GitHub forks](https://img.shields.io/github/forks/zq99299/mq-tutorial)

  mq 系列知识，成套内容包含：RabbitMq 实战指南

- [dsalg-tutorial](https://github.com/zq99299/dsalg-tutorial) &nbsp; &nbsp; 
  ![GitHub stars](https://img.shields.io/github/stars/zq99299/dsalg-tutorial)
  ![GitHub forks](https://img.shields.io/github/forks/zq99299/dsalg-tutorial)

  数据结构与算法 系列知识，成套内容包含 ：图解 Java 数据结构和算法

- [note-combat](https://github.com/zq99299/note-combat)&nbsp; &nbsp; 
  ![GitHub stars](https://img.shields.io/github/stars/zq99299/note-combat)
  ![GitHub forks](https://img.shields.io/github/forks/zq99299/note-combat)

  实战笔记：主要定位是在工作中实际遇到的难题、没有接触过、或稍有难度的真实场景解决记录；

  此类笔记由于便捷性较高，转而使用语雀来记录，[语雀地址](https://www.yuque.com/mrcode.cn/note-combat)

- [note-architect](https://github.com/zq99299/note-architect) &nbsp; &nbsp; 
  ![GitHub stars](https://img.shields.io/github/stars/zq99299/note-architect)
  ![GitHub forks](https://img.shields.io/github/forks/zq99299/note-architect)

  架构师笔记：主要定位是对架构师系统学习相关笔记；

  成套内容包含：

  - 高并发系统设计 40 问
  - JAVA 架构师直通车（已转到语雀继续学习记录）

## 笔记系列 📖 
由于某些笔记是放在一个大仓库的笔记本中的，某些笔记还不错，这里做一部分的汇总；

优先查看前面的「仓库类」，因为提供了良好的在线阅读体验

### 新笔记本精彩知识精选（良好的在线阅读）
- [elasticsearch 核心知识篇](https://zq99299.github.io/note-book/elasticsearch-core/)
- [elasticsearch 高级篇](https://zq99299.github.io/note-book/elasticsearch-senior/)
- [亿级流量电商详情页系统实战（第二版）：缓存架构+高可用服务架构+微服务架构](https://zq99299.github.io/note-book/cache-pdp/)
- [亿级流量电商详情页系统实战（第二版）-> 之第一版练习项目](https://github.com/zq99299/cache-pdp)
- [亿级流量电商详情页系统实战（第二版）-> 之第二版练习项目](https://github.com/zq99299/cache-eshop)
：[第一版和第二版区别请阅读该说明](https://github.com/zq99299/cache-pdp#%E7%AC%AC%E4%B8%80%E7%89%88%E4%B8%8E%E7%AC%AC%E4%BA%8C%E7%89%88%E7%9A%84%E5%8C%BA%E5%88%AB)
- [某课 JAVA 设计模式精讲 Debug 方式+内存分析 笔记](https://zq99299.github.io/note-book/imocc/design_pattern/)
- [研磨设计模式（李兴华）](https://zq99299.github.io/note-book/design_pattern/)
- [MyCat1 前端连接交互探索源码级 + 手写基础版 MySQL 协议代理登录转发](https://zq99299.github.io/note-book/mycat/frontend_connection_interaction.html)
- [spring cloud config](https://zq99299.github.io/note-book/spring-cloud-tutorial/config/)
- [CSS 深入理解 张鑫旭](https://zq99299.github.io/note-book/css-zxx/)
：重新学习，记录的笔记更多更完整，可以吧它当成工具书来搜藏

### 老笔记本精彩知识精选（GitHub 仓库阅读）
- [某课 SpringSecurity 视频教程笔记](https://github.com/zq99299/essay-note/blob/master/chapter/imooc/spring_security/index.md)
：写得不错，笔记使用 boot2 完成
- [某课 SpringSecurity 视频教程笔记 -> 配套练习项目](https://github.com/zq99299/spring-security)
- [某课 Spring Cloud 微服务实战](https://github.com/zq99299/essay-note/blob/master/chapter/imooc/spring_cloud/index.md)
：内容不太行，没有接触过的可以让你有一个大概的了解
- [某课 Spring Cloud 微服务实战 -> 配套练习项目](https://github.com/zq99299/immoc-spring-cloud)
- [CSS 深入理解-张星旭系列](https://github.com/zq99299/essay-note/blob/master/chapter/htmlcss/README.md)
：后端程序员不会 CSS 样式，学完可以搞定简单的效果，不至于啥都看不懂
- [HTML/CSS 响应式（小部分）](https://github.com/zq99299/essay-note/blob/master/chapter/htmlcss_responsive/index.md)
：后端程序员不会 CSS 样式，补充学习
- [spring-restdocs-asciidoctor](https://github.com/zq99299/essay-note/blob/master/chapter/spring/spring_restdocs_asciidoctor/index.md)
：笔者原创笔记内容，可以让你快速的入门
- [spring-restdocs-asciidoctor -> 配套练习项目](https://github.com/zq99299/spring-restdocs-example)
- [Spring-WebSocket](https://github.com/zq99299/essay-note/blob/master/chapter/websocket/index.md)
：笔者原创笔记内容，Spring-WebSocket 的一篇很好的入门开发篇
- [Spring-WebSocket -> 配套练习项目](https://github.com/zq99299/java-websocket-demo)
- [GitBook 使用教程](https://github.com/zq99299/gitbook-guide/blob/master/SUMMARY.md)
- [JAVA 高端基础](https://github.com/zq99299/hp-note/blob/master/SUMMARY.md)
：一小部分，请参阅 高端JAVA基础 部分
- [vue 学习笔记](https://github.com/zq99299/vue-note/blob/master/SUMMARY.md)
：包含了 vue2-搭建开发环境、某课 vue 移动端开发(音乐 app)、vue 组件库开发
- [某课 vue 移动端开发(音乐 app) -> 配套练习项目](https://github.com/zq99299/vue-music)
：vue 移动端开发(音乐 app)
- [Gradle 早期简单学习笔记](https://github.com/zq99299/gradle-note)


### CSDN 笔记精选
- [spring cloud stream 基础使用](https://blog.csdn.net/mr_zhuqiang/article/details/84820076)
- [Gradle 发布 JAR 包到 maven 中央仓库(sonatype )](https://blog.csdn.net/mr_zhuqiang/article/details/84564256)
- [Quartz 表达式相关工具类与 VUE 实现的表达式选择器](https://blog.csdn.net/mr_zhuqiang/article/details/92572042)

## 其他

- [IDEA 常用知识点分享](https://github.com/zq99299/idea)
    - [IDEA 常用默认快捷键（官方） - 汉化版](https://github.com/zq99299/idea/blob/master/shortcut/README.md)
    - [IDEA 常用插件推荐](https://github.com/zq99299/idea/blob/master/plugins/README.md)

## 项目类

- [vuepress-plugin-baidu-tongji-analytics](https://github.com/zq99299/vuepress-plugin/tree/master/vuepress-plugin-baidu-tongji-analytics)
：vuepress 百度统计插件
- [vuepress-plugin-toolbar](https://github.com/zq99299/vuepress-plugin/tree/master/vuepress-plugin-toolbar)
：vuepress 侧边工具栏插件
- [vuepress-plugin-tags](https://github.com/zq99299/vuepress-plugin/tree/master/vuepress-plugin-tags)
：vuepress 在主标题下方生成标签列表

---

- [gitbook-plugin-anchor-navigation-ex](https://github.com/zq99299/gitbook-plugin-anchor-navigation-ex) 
&nbsp; &nbsp; 
![GitHub stars](https://img.shields.io/github/stars/zq99299/gitbook-plugin-anchor-navigation-ex)
![GitHub forks](https://img.shields.io/github/forks/zq99299/gitbook-plugin-anchor-navigation-ex)

    gitbook 侧边栏+页面导航综合类插件

- [gitbook-plugin-page-footer-ex](https://github.com/zq99299/gitbook-plugin-page-footer-ex)
&nbsp; &nbsp; 
![GitHub stars](https://img.shields.io/github/stars/zq99299/gitbook-plugin-page-footer-ex)
![GitHub forks](https://img.shields.io/github/forks/zq99299/gitbook-plugin-page-footer-ex)
  
    gitbook 定制每篇文章的页脚，可以添加版权和显示文件修改时间和新增时间 的插件

- [fast-csv](https://github.com/zq99299/fast-csv)
：使用 NIO + csv 协议手写 csv 读写功能； 对 CSV 协议解析有一点问题，暂时就这样了
- [wx_sdk](https://gitee.com/zhuqiang/wx_sdk)
: 早期的 wx 功能，从接入公众号，到开发调用微信 api ，搭成了一个架子，包含了一部分常用的 API
"
1,CloudCompare/CloudCompare,C++,"CloudCompare
============

Homepage: https://cloudcompare.org

[![GitHub release](https://img.shields.io/github/release/cloudcompare/trunk.svg)](https://github.com/cloudcompare/trunk/releases)

- [![Build Status](https://travis-ci.org/CloudCompare/CloudCompare.svg?branch=master)](https://travis-ci.org/CloudCompare/CloudCompare) Linux
- [![Build](https://github.com/CloudCompare/CloudCompare/workflows/Build/badge.svg?branch=master)](https://github.com/CloudCompare/CloudCompare/actions?query=workflow%3ABuild+branch%3Amaster) Windows & MacOS
- [![Releases](https://coderelease.io/badge/CloudCompare/CloudCompare)](https://coderelease.io/github/repository/CloudCompare/CloudCompare)

Introduction
------------

CloudCompare is a 3D point cloud (and triangular mesh) processing software.
It was originally designed to perform comparison between two 3D points clouds
(such as the ones obtained with a laser scanner) or between a point cloud and a
triangular mesh. It relies on an octree structure that is highly optimized for
this particular use-case. It was also meant to deal with huge point
clouds (typically more than 10 million points, and up to 120 million with 2 GB
of memory).

More on CloudCompare [here](http://en.wikipedia.org/wiki/CloudCompare)

Compilation
-----------

Supports: Windows, Linux, and macOS

Refer to the [BUILD.md file](BUILD.md) for up-to-date information.

Basically, you have to:
- clone this repository
- install mandatory dependencies (OpenGL,  etc.) and optional ones if you really need them
(mainly to support particular file formats, or for some plugins)
- launch CMake (from the trunk root)
- enjoy!


Contributing to CloudCompare
----------------------------

If you want to help us improve CloudCompare or create a new plugin you can start by reading this [guide](CONTRIBUTING.md)

Supporting the project
----------------------

If you want to help us in another way, you can make donations via [donorbox](https://donorbox.org/support-cloudcompare)
Thanks!

<a href='https://donorbox.org/support-cloudcompare' target=""_blank""><img src=""https://d1iczxrky3cnb2.cloudfront.net/button-medium-blue.png""></a>
"
2,vispy/vispy,Python,"VisPy: interactive scientific visualization in Python
-----------------------------------------------------

Main website: http://vispy.org

|Build Status| |Coverage Status| |Zenodo Link|

----

VisPy is a **high-performance interactive 2D/3D data visualization
library**. VisPy leverages the computational power of modern **Graphics
Processing Units (GPUs)** through the **OpenGL** library to display very
large datasets. Applications of VisPy include:

-  High-quality interactive scientific plots with millions of points.
-  Direct visualization of real-time data.
-  Fast interactive visualization of 3D models (meshes, volume
   rendering).
-  OpenGL visualization demos.
-  Scientific GUIs with fast, scalable visualization widgets (`Qt <http://www.qt.io>`__ or
   `IPython notebook <http://ipython.org/notebook.html>`__ with WebGL).


Announcements
-------------

- **Release!** Version 0.6.5, September 24, 2020
- **Release!** Version 0.6.4, December 13, 2019
- **Release!** Version 0.6.3, November 27, 2019
- **Release!** Version 0.6.2, November 4, 2019
- **Release!** Version 0.6.1, July 28, 2019
- **Release!** Version 0.6.0, July 11, 2019
- **Release!** Version 0.5.3, March 28, 2018
- **Release!** Version 0.5.2, December 11, 2017
- **Release!** Version 0.5.1, November 4, 2017
- **Release!** Version 0.5, October 24, 2017
- **Release!** Version 0.4, May 22, 2015
- `VisPy tutorial in the IPython Cookbook <https://github.com/ipython-books/cookbook-code/blob/master/featured/06_vispy.ipynb>`__
- **Release!** Version 0.3, August 29, 2014
- **EuroSciPy 2014**: talk at Saturday 30, and sprint at Sunday 31, August 2014
- `Article in Linux Magazine, French Edition <https://github.com/vispy/linuxmag-article>`__, July 2014
- **GSoC 2014**: `two GSoC students are currently working on VisPy under the PSF umbrella <https://github.com/vispy/vispy/wiki/Project.%20GSoC-2014>`__
- **Release!**, Version 0.2.1 04-11-2013
- **Presentation at BI forum**, Budapest, 6 November 2013
- **Presentation at Euroscipy**, Belgium, August 2013
- **EuroSciPy Sprint**, Belgium, August 2013
- **Release!** Version 0.1.0 14-08-2013


Using VisPy
-----------

VisPy is a young library under heavy development at this time. It
targets two categories of users:

1. **Users knowing OpenGL**, or willing to learn OpenGL, who want to
   create beautiful and fast interactive 2D/3D visualizations in Python
   as easily as possible.
2. **Scientists without any knowledge of OpenGL**, who are seeking a
   high-level, high-performance plotting toolkit.

If you're in the first category, you can already start using VisPy.
VisPy offers a Pythonic, NumPy-aware, user-friendly interface for OpenGL
ES 2.0 called **gloo**. You can focus on writing your GLSL code instead
of dealing with the complicated OpenGL API - VisPy takes care of that
automatically for you.

If you're in the second category, we're starting to build experimental
high-level plotting interfaces. Notably, VisPy now ships a very basic
and experimental OpenGL backend for matplotlib.


Installation
------------

Please follow the detailed
`installation instructions <http://vispy.org/installation.html>`_
on the VisPy website.

Structure of VisPy
------------------

Currently, the main subpackages are:

-  **app**: integrates an event system and offers a unified interface on
   top of many window backends (Qt4, wx, glfw, IPython notebook
   with/without WebGL, and others). Relatively stable API.
-  **gloo**: a Pythonic, object-oriented interface to OpenGL. Relatively
   stable API.
-  **scene**: this is the system underlying our upcoming high level
   visualization interfaces. Under heavy development and still
   experimental, it contains several modules.

   -  **Visuals** are graphical abstractions representing 2D shapes, 3D
      meshes, text, etc.
   -  **Transforms** implement 2D/3D transformations implemented on both
      CPU and GPU.
   -  **Shaders** implements a shader composition system for plumbing
      together snippets of GLSL code.
   -  The **scene graph** tracks all objects within a transformation
      graph.
-  **plot**: high-level plotting interfaces.

The API of all public interfaces are subject to change in the future,
although **app** and **gloo** are *relatively* stable at this point.


Genesis
-------

VisPy began when four developers with their own visualization libraries
decided to team up:
`Luke Campagnola <http://luke.campagnola.me/>`__ with `PyQtGraph <http://www.pyqtgraph.org/>`__,
`Almar Klein <http://www.almarklein.org/>`__ with `Visvis <https://github.com/almarklein/visvis>`__,
`Cyrille Rossant <http://cyrille.rossant.net>`__ with `Galry <https://github.com/rossant/galry>`__,
`Nicolas Rougier <http://www.loria.fr/~rougier/index.html>`__ with `Glumpy <https://github.com/rougier/Glumpy>`__.

Now VisPy looks to build on the expertise of these developers and the
broader open-source community to build a high-performance OpenGL library.

----

External links
--------------

-  `User mailing
   list <https://groups.google.com/forum/#!forum/vispy>`__
-  `Dev mailing
   list <https://groups.google.com/forum/#!forum/vispy-dev>`__
-  `Dev chat room <https://gitter.im/vispy/vispy>`__
-  `Wiki <http://github.com/vispy/vispy/wiki>`__
-  `Gallery <http://vispy.org/gallery.html>`__
-  `Documentation <http://vispy.readthedocs.org>`__

.. |Build Status| image:: https://github.com/vispy/vispy/workflows/CI/badge.svg
   :target: https://github.com/vispy/vispy/actions
.. |Coverage Status| image:: https://img.shields.io/coveralls/vispy/vispy/main.svg
   :target: https://coveralls.io/r/vispy/vispy?branch=main
.. |Zenodo Link| image:: https://zenodo.org/badge/5822/vispy/vispy.svg
   :target: http://dx.doi.org/10.5281/zenodo.17869
"
3,mootools/mootools-core,JavaScript,"# MooTools Core

[![Build Status](https://travis-ci.org/mootools/mootools-core.png?branch=master)](https://travis-ci.org/mootools/mootools-core)

[![Selenium Test Status](https://saucelabs.com/browser-matrix/MooTools-Core.svg)](https://saucelabs.com/u/MooTools-Core)

---

This repository is for MooTools developers; not users.
All users should download MooTools from [MooTools.net](http://mootools.net/download ""Download MooTools"")

---
## Contribute

You are welcome to contribute to MooTools! What we ask of you:

a. __To report a bug:__

   1. Create a [jsFiddle](http://jsfiddle.net/) with the minimal amount of code to reproduce the bug.
   2. Create a [GitHub Issue](https://github.com/mootools/mootools-core/issues), and link to the jsFiddle.

b. __To fix a bug:__

   1. Clone the repo.
   2. Add a [spec](https://github.com/Automattic/expect.js#api).
   3. Fix the bug.
   4. Build and run the specs.
   5. Push to your GitHub fork.
   6. Create Pull Request, and send Pull Request.


__Do try to contribute!__ This is a community project.


## Building & Testing

Current build process uses [Grunt](http://github.com/gruntjs), [Grunt MooTools Packager plugin](https://github.com/ibolmo/grunt-packager), and [Karma related repos](http://github.com/karma-runner/grunt-karma).

**By default**, the build process runs the tests (specs) relevant to the build. To build without testing see the `packager` build targets.

### Building MooTools _With_ Compatibility
This means `1.5.1` that is compatible with: `1.4.6`, `1.3.x`, `1.2.x`, and so on.

**Examples**

	grunt compat             # or
	grunt packager:compat    # to only build the source

### Building MooTools _Without_ Compatibility
This means `1.5.1` **without** deprecated code in `1.4.6`, `1.3.x`, `1.2.x`, and so on.

``` js
'Proceed at your own risk'
See the changelog or the blog related to each version for migrating your code.
```

**Examples**

	grunt nocompat           # or
	grunt packager:nocompat  # to only build the source


### Advanced Building and Testing
See the [Gruntfile](https://github.com/mootools/mootools-core/blob/master/Gruntfile.js) and [MooTools packager](https://github.com/ibolmo/grunt-mootools-packager) for further options.

**Examples**

	# with compat
	grunt compat --file=Function    # builds with only Core/Function and dependencies, then tests against specs in Specs/Core/Function
	grunt compat --module=Class     # tests against all specs in the Specs/Class *folder* (use --file to limit the build)

	# without compat
	grunt nocompat --file=Function  # builds with only Core/Function and dependencies, then tests against specs in Specs/Core/Function
	grunt nocompat --module=Class   # tests against all specs in the Specs/Class *folder* (use --file to limit the build)

#### Removing Other Packager Blocks
You'll need to add a specific task to the Gruntfile. See [packager's documentation](https://github.com/ibolmo/grunt-mootools-packager) for more examples.

### Testing locally

I you want to test your local repo you need just some small steps. Follow these in order:

    $ git clone https://github.com/mootools/mootools-core  # clone the MooTools repo
    $ cd mootools-core                                     # get into the directory
    $ npm install                                          # install de testing tools
    $ `npm bin`/grunt test                                 # run the specs!


To test a build in a local browser, you can run the `:dev` target of that build to start a test server at `http://localhost:9876/` and point your browser to it. When you're done testing, pressing `Ctrl+c` in the window running the grunt process should stop the server.

Example:

	$ `npm bin`/grunt compat:dev

If the log is too long, or if you want to store it in a file you can do:

    $ grunt > logs.txt   # This will create a new file called logs.txt in the local directory

### Testing on Travis & Sauce Labs

Every new Build and Pull Request is now tested on [Travis](https://travis-ci.org/) and [Sauce Labs](https://saucelabs.com/). You can also open your own free account on [Travis](https://travis-ci.org/) and [Sauce Labs](https://saucelabs.com/) to test new code ideas there.

[Travis](https://travis-ci.org/) testing uses [PhantomJS](http://phantomjs.org/) which is a headless browser. When connected to [Sauce Labs](https://saucelabs.com/) then it is possible to choose any number of [different Browsers and Platforms](https://saucelabs.com/platforms). You will need in this case to change the login key so it will match your account.

To add new Browsers in [Sauce Labs](https://saucelabs.com/) testing you can make changes to __[Grunt/options/browsers.json](Grunt/options/browsers.json)__:

 - add a new browser to the custom launchers already in the Gruntfile.

		...
		chrome: {
			base: 'SauceLabs',
			platform: 'Linux',
			browserName: 'chrome',
		},
		...


 - add the chosen browser, with the correct builds to .travis.yml:

		env:
			matrix:
				- BUILD='compat'     BROWSER='chrome'

#### Browsers, Platforms, and More

This test suite is ready for Travis & SauceLabs.
You can also run locally.

Support:

 - IE
 - Edge
 - Firefox
 - Safari
 - Chrome
 - Opera
 - PhantomJS (headless browser)

## More Information

[See the MooTools Wiki for more information](http://github.com/mootools/mootools-core/wikis)
"
4,CodeMazeBlog/async-repository-dotnetcore-webapi,C#,"# Async Repository in ASP.NET Core Web API  
https://code-maze.com/async-generic-repository-pattern/
"
5,AnemoneIndicum/taotao-repository,Java,高可用的综合电商服务平台，包含后台商品管理，会员管理，订单管理，前端购物平台，商品搜索，商品详情，订单物流中心等子系统（系统架构springmvc+mybatis+dubbo+zookeeper+redis+solr+activemq；zookeeper高可用集群，redis高可用集群，solr高可用集群，freemarker网页静态化，activemq消息队列，单点登录等）
6,gentoo/gentoo,Shell,
7,openstack/openstack,Python,"OpenStack
=========

OpenStack is a collection of interoperable components that can be deployed
to provide computing, networking and storage resources. Those infrastructure
resources can then be accessed by end users through programmable APIs.

This repository just represents OpenStack as a collection of git submodules.
You can find the repositories for individual components at:
https://opendev.org/openstack

You can learn more about the various components in OpenStack at:
https://openstack.org/software

To learn more about how to contribute to OpenStack, please head to our
Contributor portal: https://www.openstack.org/community/

To learn more about how OpenStack is governed, you can visit:
https://governance.openstack.org/


Why this repository ?
---------------------

Our continuous integration system, Zuul, gates all of the contained projects
in an effective single timeline. This means that OpenStack, across all of the
projects, does already have a sequence of combinations that have been
explicitly tested, but it's non-trivial to go from a single commit of a
particular project to the commits that were tested with it.

Gerrit's submodule tracking feature will update a super project every
time a subproject is updated, so the specific sequence created by zuul
will be captured by the super project commits.

This repo is intended to be used in a read-only manner. Any commit in this
repo will get a collection of commits in the other repos that have
explicitly been tested with each other, if that sort of thing is important
to you.
"
8,scummvm/scummvm,C++,"# [ScummVM README](https://www.scummvm.org/) · [![Build Status](https://travis-ci.org/scummvm/scummvm.svg?branch=master)](https://travis-ci.org/scummvm/scummvm) ![CI](https://github.com/scummvm/scummvm/workflows/CI/badge.svg) [![Translation status](https://translations.scummvm.org/widgets/scummvm/-/scummvm/svg-badge.svg)](https://translations.scummvm.org/engage/scummvm/?utm_source=widget) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md#pull-requests) [![Codacy Badge](https://app.codacy.com/project/badge/Grade/e06e5b18f8464fef859b5a7f78d10357)](https://www.codacy.com/gh/scummvm/scummvm/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=scummvm/scummvm&amp;utm_campaign=Badge_Grade)

## About ScummVM

ScummVM allows you to play classic graphic point-and-click adventure games, text adventure games, and RPGs, as long as you already have the game data files. ScummVM replaces the executable files shipped with the games, which means you can now play your favorite games on all your favorite devices.

So how did ScummVM get its name? Many of the famous LucasArts adventure games, such as Maniac Mansion and the Monkey Island series, were created using a utility called SCUMM (Script Creation Utility for Maniac Mansion). The ‘VM’ in ScummVM stands for Virtual Machine.

While ScummVM was originally designed to run LucasArts’ SCUMM games, over time support has been added for many other games: see the full list [on our wiki](https://wiki.scummvm.org/index.php?title=Category:Supported_Games). Noteworthy titles include Broken Sword, Myst and Blade Runner, although there are countless other hidden gems to explore.

For more information, compatibility lists, details on donating, the
latest release, progress reports and more, please visit the ScummVM [home
page](https://www.scummvm.org/).

## Quickstart

For the impatient among you, here is how to get ScummVM running in five simple steps.

1. Download ScummVM from [our website](https://www.scummvm.org/downloads/) and install it.

2. Create a directory on your hard drive and copy the game datafiles from the original media to this directory. Repeat this for every game you want to play.

3. Start ScummVM, choose 'Add game', select the directory containing the game datafiles (do not try to select the datafiles themselves!) and press Choose.

4. The Edit Game dialog opens to allow configuration of various settings for the game. These can be reconfigured at any time, but for now everything should be OK at the default settings.

5. Select the game you want to play in the list, and press Start. To play a game next time, skip to step 5, unless you want to add more games.

>
> Hint:
>
> To add multiple games in one go, press and hold the shift key, then click 'Add game' -- the label will change to 'Mass Add' and if you press it, you are again asked to select a directory, only this time ScummVM will search through all subdirectories for supported games.



## Reporting a bug

To report a bug, go to the ScummVM [Issue Tracker](https://bugs.scummvm.org/>) and log in with your GitHub account.

Please make sure the bug is reproducible, and still occurs in the latest git/Daily build version. Also check the [compatibility list](https://www.scummvm.org/compatibility/) for that game, to ensure the issue is not already known. Please do not report bugs for games that are not listed as completeable on the [Supported Games](https://wiki.scummvm.org/index.php?title=Category:Supported_Games>) wiki page, or on the compatibility list. We already know those games have bugs!

Please include the following information in the bug report:

- ScummVM version (test the latest git/Daily build)
- Bug details, including instructions for how to reproduce the bug. If possible, include log files, screenshots, and any other relevant information.
- Game language
- Game version (for example, talkie or floppy)
- Platform and Compiler (for example, Win32, Linux or FreeBSD)
- An attached saved game, if possible.
- If this bug only occurred recently, include the last version without the bug, and the first version with the bug. That way we can fix it quicker by looking at the changes made.

Finally, please report each issue separately; do not file multiple issues on the same ticket. It is difficult to track the status of each individual bug when they aren't on their own tickets.

## Documentation

### User documentation

For everything you need to know about how to use ScummVM, see our [user documentation](https://docs.scummvm.org/).

### The ScummVM Wiki

[The wiki](https://wiki.scummvm.org/) is the place to go for information about every game supported by ScummVM. If you're a developer, there's also some very handy information in the Developer section.

### Changelog

Our extensive change log is available [here](NEWS.md).

## Credits

A massive thank you to the entire team for making the ScummVM project possible. See the credits [here](AUTHORS)!

-----

> Good Luck and Happy Adventuring\!
> The ScummVM team.
> <https://www.scummvm.org/>
"
9,webpack/webpack.js.org,JavaScript,"<div align=""center"">
  <a href=""https://github.com/webpack/webpack"">
    <img width=""200"" height=""200"" src=""https://webpack.js.org/assets/icon-square-big.svg"" />
  </a>
  <h1>webpack.js.org</h1>

[![Build Status][build-status]][build-status-url]
[![Standard Version][release]][release-url]
[![chat on gitter][chat]][chat-url]

Guides, documentation, and all things webpack.

</div>

## Content Progress

Now that we've covered much of the backlog of _missing documentation_, we are
starting to heavily review each section of the site's content to sort and
structure it appropriately. The following issues should provide a pretty good
idea of where things are, and where they are going:

- [Guides - Review and Simplify][guides-url]
- [Concepts - Review and Organize][concepts-url]

We haven't created issues for the other sections yet, but they will be coming
soon. For dev-related work please see [General - Updates & Fixes][general-url].

## Translation

To help translate this documentation please jump to the [translation branch][translate-url].

## Versioning

Since webpack 4 we have created a subdomain-based archive for older states of documentation
matching older webpack version. webpack 4's documentation is available at
[https://v4.webpack.js.org/](https://v4.webpack.js.org/) and is deployed from [`gh-pages` branch of v4.webpack.js.org repository](https://github.com/webpack/v4.webpack.js.org/tree/gh-pages)

There are various known issues that need fixing ([#3366](https://github.com/webpack/webpack.js.org/issues/3366)).

## Contributing

Read through the [writer's guide][writer-guide-url] if you're interested in editing the
content on this site. See the [contributors page][contributing-url] to learn how to set up and
start working on the site locally.

## License

The content is available under the [Creative Commons BY 4.0][license-url] license.

## Special Thanks

_BrowserStack_ has graciously allowed us to do cross-browser and cross-os
testing of the site at no cost...

[![BrowserStackLogo][browserstack]][browserstack-url]

_Vercel_ has given us a Pro account.

[![VercelLogo][vercel]][vercel-url]

[webpack5-milestone-url]: https://github.com/webpack/webpack.js.org/issues?q=is%3Aopen+is%3Aissue+milestone%3A%22webpack+5%22
[build-status]: https://github.com/webpack/webpack.js.org/workflows/Deploy/badge.svg
[build-status-url]: https://github.com/webpack/webpack.js.org/actions
[browserstack]: ./browserstack-logo.png
[vercel]: ./src/assets/powered-by-vercel.svg
[browserstack-url]: http://browserstack.com/
[vercel-url]: https://vercel.com/?utm_source=webpackdocs
[chat]: https://badges.gitter.im/webpack/webpack.svg
[chat-url]: https://gitter.im/webpack/webpack
[concepts-url]: https://github.com/webpack/webpack.js.org/issues/1386
[contributing-url]: https://github.com/webpack/webpack.js.org/blob/master/.github/CONTRIBUTING.md
[general-url]: https://github.com/webpack/webpack.js.org/issues/1525
[guides-url]: https://github.com/webpack/webpack.js.org/issues/1258
[license-url]: https://creativecommons.org/licenses/by/4.0/
[release]: https://img.shields.io/badge/release-standard%20version-brightgreen.svg
[release-url]: https://github.com/conventional-changelog/standard-version
[translate-url]: https://github.com/webpack/webpack.js.org/tree/translation
[writer-guide-url]: https://webpack.js.org/contribute/writers-guide
"
10,magnayn/Jenkins-Repository,Java,
11,WPO-Foundation/webpagetest,PHP,"# WebPageTest

[![travis](https://img.shields.io/travis/WPO-Foundation/webpagetest.svg?label=travis)](http://travis-ci.org/WPO-Foundation/webpagetest)

This is the official repository for the [WebPageTest](https://www.webpagetest.org/) web-performance testing code.

If you are looking to install your own instance, I recommend grabbing the latest [private instance release](https://docs.webpagetest.org/private-instances/).

# Troubleshooting Private instances
If your instance is running, but you're having issues configuring agents, try navigating to {server_ip}/install and checking for a valid configuration.

# Agents
The cross-platform browser agent is [here](https://github.com/WPO-Foundation/wptagent).


# Documentation

[WebPageTest Documentation](https://github.com/WPO-Foundation/webpagetest-docs)

# API Examples
There are two examples using the [Restful API](https://docs.webpagetest.org/dev/api/):

* /bulktest - A php cli project that can submit a bulk set of tests, gather the results and aggregate analysis.
* /batchtool - A python project that can submit a bulk set of tests and gather the results.

# Contributing
There are 2 separate lies of development under different licenses and pull requests are accepted to either of them.  The master branch where most active development is occurring is under the [Polyform Shield 1.0.0 license](LICENSE.md) and there is an ""apache"" branch which is under the more permissive Apache 2.0 license.

# Change Log
View the [WebPageTest Change Log](https://docs.webpagetest.org/change-log)
"
12,octocat/Hello-World,,"Hello World!
"
13,Torann/laravel-repository,PHP,"# Laravel Repository

[![Build Status](https://travis-ci.org/Torann/laravel-repository.svg?branch=master)](https://travis-ci.org/Torann/laravel-repository)
[![Latest Stable Version](https://poser.pugx.org/torann/laravel-repository/v/stable.png)](https://packagist.org/packages/torann/laravel-repository)
[![Total Downloads](https://poser.pugx.org/torann/laravel-repository/downloads.png)](https://packagist.org/packages/torann/laravel-repository)
[![Patreon donate button](https://img.shields.io/badge/patreon-donate-yellow.svg)](https://www.patreon.com/torann)
[![Donate weekly to this project using Gratipay](https://img.shields.io/badge/gratipay-donate-yellow.svg)](https://gratipay.com/~torann)
[![Donate to this project using Flattr](https://img.shields.io/badge/flattr-donate-yellow.svg)](https://flattr.com/profile/torann)
[![Donate to this project using Paypal](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=4CJA2A97NPYVU)

The Laravel Repository package is meant to be a generic repository implementation for Laravel.

- [Laravel Repository on Packagist](https://packagist.org/packages/torann/laravel-repository)
- [Laravel Repository on GitHub](https://github.com/torann/laravel-repository)

## Official Documentation

Documentation for the package can be found on [Lyften.com](http://lyften.com/projects/laravel-repository/).
"
14,maduce/fosscad-repo,OpenSCAD,"fosscad-repo
============

This pack is a collection of the newest FOSSCAD CAD files including all previous Megapacks, i.e., Megapacks 4.2 (Saito), 4,3 (Tetsuo), 4.4 (Raiden), 4.5 (Otacon),  4.6 (Tachikoma) and 4.7 (Aramaki). This Megapack hence replaces all previous Megapacks. These files are not being released by Defense Distributed or DEFCAD Incorporated.

# License

* These files are released open source.
* GNU General Public License v3.0 (GPLv3).

# Version

* 4.9 Alpha (unnamed)

# Author
* FOSSCAD

# Websites 
* http://fosscad.org
* http://confab.fosscad.org

# IRC
* Server: irc.oftc.net 
* Channel: #fosscad

# Twitter
* http://twitter.com/fosscad

# Changelog 4.8

* Firearms/Parlor_Pistol-Caboose/
* Firearms/PM422_Songbird_22lr_Pistol_v2.1-JamesRPatrick/
* Firearms/PM522_Washbear_Revolver_v2.0-JamesRPatrick/
* Misc/AR-15_Pistol_Cheek_Rest-unnamed/
* Misc/FOSSCAD_AFG_GRIP-Warfairy/
* Misc/FOSSCAD_RVG_Vertical_Grip-WarFairy/
* Misc/Keymod_Hand_Stop-unnamed/
* Misc/Picatinny_Riser_and_Rail_Generator-RollaTroll/
* Misc/Rail_Mounted_Fore_Grip/
* Muzzle_Devices/FOSSCAD_Mystique_Suppressor-unnamed/
* Pistols/9mm_Gluty_AR-15_Pistol-unnamed/
* Pistols/9mm_HK_USP_10_Round_Magazine_Baseplate-Jelly/
* Pistols/9mm_Shuty_AR-15_Pistol_v2.0-derwood/
* Pistols/9mm_Shuty_AR-15_Pistol_v4.0_MP-1-unnamed/
* Pistols/Steyr_Pistol_Magazine_Extension-crysys/
* Rifles/10_22_Receiver-Unnamed/
* Rifles/10_22_Trigger_Housing-Unnamed/
* Rifles/AK-47_Grip-Anonymous/
* Rifles/AK-47_Standard_Upper_and_Lower_Handguards_1.0/
* Rifles/AR-10_Lower_Receiver_Abraxas_JT_v1.0-Warfairy/
* Rifles/AR-10_Lower_Receiver_Abraxas_v1.0-Warfairy/
* Rifles/AR-10_Lower_Receiver_Caleb_v1.0-Warfairy/
* Rifles/AR-10_Nephilim_Reinforced_Lower_Receiver_v2.0-WarFairy/
* Rifles/AR-15_Bolt_Lower_Receiver_v2.0-RollTroll/
* Rifles/AR-15_Bumpfire_Stock_v2-Disruptive_Solutions/
* Rifles/AR-15_FOSSCAD_Lower_Receiver_V5.1_NS-cb/
* Rifles/AR-15_FOSSCAD_MOE_Grip_v1.1-FP/
* Rifles/AR-15_Hanuman_Bullpup_v1.1-WarFairy/
* Rifles/AR-15_Lower_Receiver_Aliamanu-Phobos-ArmaDelite/
* Rifles/AR-15_Lower_Receiver_Atlas_v2.0-Warfairy/
* Rifles/AR-15_Lower_Receiver_Phobos_v1.2-Warfairy/
* Rifles/AR-15_Treillage_Stock-Warfairy/
* Rifles/AR-15_Vanguard_JT_Lower_Receiver_v1.1-JT/
* Rifles/AR-15_WarFairy_Charon_Lower_Receiver_v4_Warfairy/
* Rifles/AR-15_Warfairy_Hermes_Lower_Receiver_v1.4-Warfairy/
* Rifles/AR-15_Warfairy_Hermes_Zero_Lower_Receiver_v2.0-Warfairy/
* Rifles/AR_Ambidextrous_Safety_Selector-unamed/
* Rifles/P90_50rd_Spring_Plate-crysys/

======
* Current Megapack Categories
- Ammo; Firearms; Firearms/Liberators; Grenades; Misc; Muzzle_Devices; Pistols; Rifles
"
15,angular/material-start,JavaScript,"# AngularJS Material-Start (ES6)
[![Gitter](https://badges.gitter.im/angular/material2.svg)](https://gitter.im/angular/material?utm_source=badge&utm_medium=badge)

This branch contains the final/complete version (i.e. `step-10-finished`) of the
[Material Start ES6 Tutorial](https://github.com/angular/material-start/tree/es6-tutorial) branch
in this repository. 

You can see the [Live Demo here](https://angularjs-material-start.firebaseapp.com/).

There are many additional branches in this repository that you may find useful:

 - [`master`](https://github.com/angular/material-start/tree/master) (this branch) - A copy of the
 `es6` branch outlined below with additional notes about the available branches.
 - [`es5-tutorial`](https://github.com/angular/material-start/tree/es5-tutorial) - Step-by-step
 instructions that clearly demonstrate how the Starter application can be created in minutes using
 ES5. 
 - [`es5`](https://github.com/angular/material-start/tree/es5) - The final ES5 version which you
 complete in the last step of the tutorials above.
 - [`es6-tutorial`](https://github.com/angular/material-start/tree/es6-tutorial) - Step-by-step
 instructions that clearly demonstrate how the Starter application can be created in minutes using
 ES6. 
 - [`es6`](https://github.com/angular/material-start/tree/es6) - The final ES6 version which you
 complete in the last step of the tutorials above.
 - [`typescript`](https://github.com/angular/material-start/tree/typescript) - The final Starter
 Application built using Typescript.
 
> **Note:** We do not currently offer a `typescript-tutorial` branch as the steps are fairly similar
  to the existing `es6-tutorial` branch.

#### Purpose

This project uses the latest master branch of AngularJS Material to build the application outlined
below.

![material-starter-ux2](https://cloud.githubusercontent.com/assets/6004537/14996543/b588eb46-1137-11e6-803c-ce23996c9742.png)

Above is a snapshot of the Starter-App with a **Master-Detail** layout: showing a list of users
(left) and a user detail view (right).

Also shown is the user experience that will be displayed for smaller device sizes. The responsive
layout reveals the **menu** button that can be used to hide the user list. And the **share** button
can be used to show the Share bottom sheet view.

This Starter app demonstrates how:

*  AngularJS Material `layout` and `flex` options can easily configure HTML containers
*  AngularJS Material components `<md-toolbar>`, `<md-sidenav>`, and `<md-icon>` can quickly provide
   a base application structure
*  Custom controllers can be used and show `<md-bottomsheet>` with HTML templates
*  Custom controller can easily, and programmatically open/close the SideNav component
*  Responsive breakpoints and `$mdMedia` are used
*  Theming can be altered/configured using `$mdThemingProvider`


This sample application is purposed as both a learning tool and a skeleton application for a typical
[AngularJS Material](http://angularjs.org/) web app, comprised of a side navigation area and a
content area. You can use it to quickly bootstrap your angular webapp projects and dev environment
for these projects.

- - -

#### ""How to build an App""

Here are some generalized steps that may be used to conceptualize the application implementation
process:

1. Plan your layout and the components you want to use

2. Use hard-coded HTML and mock content to make sure the components appear as desired

3. Wire components to your application logic

   > Use the seamless integration possible with AngularJS directives and controllers.<br/>
   > This integration assumes that you have unit tested your application logic.

4. Add Responsive breakpoints

5. Add Theming support

6. Confirm ARIA compliance

7. Write End-to-end (e2e) Tests

   > It is important to validate your application logic with AngularJS Material UI components.

###### Wireframe

The illustration below shows how we planned the layout and identified the primary components that
will be used in the Starter app:

<br/>
<img src=""https://cloud.githubusercontent.com/assets/210413/6444676/c247c8f8-c0c4-11e4-8206-208f55cbceee.png"">

> **Note:** The container #2 (above) is a simple `<div>` container and not an AngularJS Material
  component.

- - -

##### Getting Started

This project uses [jspm.io](http://jspm.io), a package manager for SystemJS which is built on top
of the dynamic ES6 module loader. This allows developers to load any module format (ES6, CommonJS,
AMD, and globals).

###### Prerequisites

This project assumes that you have NodeJS and any relevant development tools (like XCode) already
installed.
 
###### Getting Started

Clone this repository and execute the following commands in a terminal:

* `git checkout master`
* `npm install`
* `npm run serve`

> **Note:** Open the dev console to see any warnings and browse the elements.

###### Layout

You will notice a few files/directories within this project:

 1. `app/src` - This is where all of your application files are stored.
 2. `app/assets` - This folder contains some tutorial-provided images and icons which are used by
    the application.
 3. `index.html` - The entry point to your application. This uses System.js to load the
    `app/src/boot/boot.js` bootstrap file which in turn loads the `app/src/app.js` file that imports
     all of your dependencies and declares them as AngularJS modules, and configures the icons and
     theming for the application.

#### Troubleshooting

If you have issues getting the application to run or work as expected:

1. Make sure you have installed JSPM and run the `jspm update` command.
2. Reach out on our [Forum](https://groups.google.com/forum/#!forum/ngmaterial) to see if any other
   developers have had the same issue.
3. This project is based against the `master` branch of AngularJS Material, so it is always showing
   the latest and greatest. You may want to update the `package.json` to use Version 1.1.0 or
   another stable release to make sure it isn't because of something we changed recently.
4. Search for the issue here on [GitHub](https://github.com/angular/material-start/issues?q=is%3Aissue+is%3Aopen).
5. If you don't see an existing issue, please open a new one with the relevant information and the
   details of the problem you are facing.
"
16,a34546/Wei.Repository,C#,"# Wei.Repository
基于EFCore3.0+Dapper 封装Repository，实现UnitOfWork,提供基本的CURD操作，可直接注入泛型Repository，也可以继承Repository，重写CURD操作

支持Mysql MsSqlServer

## 快速开始

> Nuget引用包：Wei.Repository

1. 实体对象需要继承Entity
```cs
public class User : Entity
{
public string UserName { get; set; }
public string Password { get; set; }
public string Mobile { get; set; }
}
```
2. 【可选】继承BaseDbContext,如果不需要DbContext，可以忽略该步骤
```cs
public class DemoDbContext : DbContext
{
    public DemoDbContext(DbContextOptions options) : base(options)
    {
    }
}
```
3. 注入服务,修改Startup.cs,添加AddRepository
```cs
public void ConfigureServices(IServiceCollection services)
{
    ...
    services.AddRepository<DemoDbContext>(ops =>
    {
	ops.UseMySql(""server = 127.0.0.1;database=demo;uid=root;password=root;"");
    });
    //services.AddRepository(ops =>
    //{
    //    ops.UseMySql(""server = 127.0.0.1;database=demo;uid=root;password=root;"");
    //});
    services.AddControllers();
    ...
}
```
4.  【可选】如果不用泛型Repository注入,可以自定义Repository,需要继承Repository,IRepository,可以重写基类CURD方法
```cs
public class UserRepository : Repository<User>, IUserRepository
{
    public UserRepository(DbContext dbDbContext) : base(dbDbContext)
    {
    }    
    public override Task<User> FirstOrDefaultAsync()
    {
        return default;
    }
}

public interface IUserRepository : IRepository<User>
{
}
```
4.  在Controller中使用
```cs
public class UserController : ControllerBase
{
    /// <summary>
    /// 泛型注入
    /// </summary>
    private readonly IRepository<User> _repository;
    
    /// <summary>
    /// 自定义UserRepository
    /// </summary>
    private readonly IUserRepository _userRepository;
    public UserController(IRepository<User> repository,
        IUserRepository userRepository)
    {
        _repository = repository;
        _userRepository = userRepository;
    }
    
    [HttpGet]
    public async Task<User> Get()
    {
        //泛型注入不会调用重写的方法
        return await _repository.FirstOrDefaultAsync();
    
        //会调用重写的FirstOrDefaultAsync()
        //return await _userRepository.FirstOrDefaultAsync();
    }
}
```

## 详细介绍
**1. ITrack接口**
```cs
public interface ITrack
{

    /// <summary>
    /// 创建时间
    /// </summary>
    DateTime CreateTime { get; set; }    
    /// <summary>
    /// 更新时间
    /// </summary>
    DateTime? UpdateTime { get; set; }    
    /// <summary>
    /// 是否删除
    /// </summary>
    bool IsDelete { get; set; }    
    /// <summary>
    /// 删除时间
    /// </summary>
    DateTime? DeleteTime { get; set; }
}
```
实体类继承Entity,Entity实现ITrack接口，用于记录CURD操作时间，
- 对实体进行Insert操作会自动记录CreateTime，
- 进行Update操作会自动记录UpdateTime，
- 进行Delete操作时，不会真正删除，会修改IsDelete字段，标记为1，并记录DeleteTime，如需彻底删除需要调用HardDelete方法

**2. IRepository接口**
```cs
 #region Query

/// <summary>
/// 查询
/// </summary>
IQueryable<TEntity> Query();
IQueryable<TEntity> Query(Expression<Func<TEntity, bool>> predicate);

/// <summary>
/// 查询不跟踪实体变化
/// </summary>
IQueryable<TEntity> QueryNoTracking();
IQueryable<TEntity> QueryNoTracking(Expression<Func<TEntity, bool>> predicate);

/// <summary>
/// 根据主键获取
/// </summary>
TEntity Get(TPrimaryKey id);
Task<TEntity> GetAsync(TPrimaryKey id);

/// <summary>
/// 获取所有,默认过滤IsDelete=1的
/// </summary>
List<TEntity> GetAll();
Task<List<TEntity>> GetAllAsync();
List<TEntity> GetAll(Expression<Func<TEntity, bool>> predicate);
Task<List<TEntity>> GetAllAsync(Expression<Func<TEntity, bool>> predicate);

/// <summary>
/// 获取第一个
/// </summary>
TEntity FirstOrDefault();
Task<TEntity> FirstOrDefaultAsync();
TEntity FirstOrDefault(Expression<Func<TEntity, bool>> predicate);
Task<TEntity> FirstOrDefaultAsync(Expression<Func<TEntity, bool>> predicate);

#endregion

#region Insert

/// <summary>
/// 新增
/// </summary>
TEntity Insert(TEntity entity);
Task<TEntity> InsertAsync(TEntity entity);

/// <summary>
/// 批量新增
/// </summary>
/// <param name=""entities""></param>
void Insert(List<TEntity> entities);
Task InsertAsync(List<TEntity> entities);

#endregion Insert

#region Update

/// <summary>
/// 更新
/// </summary>
TEntity Update(TEntity entity);
Task<TEntity> UpdateAsync(TEntity entity);

#endregion Update

#region Delete

/// <summary>
/// 逻辑删除，标记IsDelete = 1
/// </summary>
/// <param name=""entity""></param>
void Delete(TEntity entity);
Task DeleteAsync(TEntity entity);
void Delete(TPrimaryKey id);
Task DeleteAsync(TPrimaryKey id);
void Delete(Expression<Func<TEntity, bool>> predicate);
Task DeleteAsync(Expression<Func<TEntity, bool>> predicate);

#endregion

#region HardDelete

/// <summary>
/// 物理删除，从数据库中移除
/// </summary>
/// <param name=""entity""></param>
void HardDelete(TEntity entity);
Task HardDeleteAsync(TEntity entity);
void HardDelete(TPrimaryKey id);
Task HardDeleteAsync(TPrimaryKey id);
void HardDelete(Expression<Func<TEntity, bool>> predicate);
Task HardDeleteAsync(Expression<Func<TEntity, bool>> predicate);

#endregion

#region Aggregate

/// <summary>
/// 聚合操作
/// </summary>
bool Any(Expression<Func<TEntity, bool>> predicate);
Task<bool> AnyAsync(Expression<Func<TEntity, bool>> predicate);
int Count();
Task<int> CountAsync();
int Count(Expression<Func<TEntity, bool>> predicate);
Task<int> CountAsync(Expression<Func<TEntity, bool>> predicate);
long LongCount();
Task<long> LongCountAsync();
long LongCount(Expression<Func<TEntity, bool>> predicate);
Task<long> LongCountAsync(Expression<Func<TEntity, bool>> predicate);

#endregion
```
**3. EF工作单元事务**
```cs
await _userRepository.InsertAsync(user1);
await _userRepository.InsertAsync(user2);
await _unitOfWork.SaveChangesAsync();
```
**4. Dapper事务**
```cs
 using (var tran = _unitOfWork.BeginTransaction())
{
    try
    {
        await _unitOfWork.ExecuteAsync(""INSERT INTO `user` (`CreateTime`, `IsDelete`, `UserName`) VALUES (now(), 0, @UserName);"", user1, tran);
        await _unitOfWork.ExecuteAsync(""INSERT INTO `user` (`CreateTime`, `IsDelete`, `UserName`) VALUES (now(), 0, @UserName);"", user2, tran);
        throw new Exception();
        tran.Commit();
    }
    catch (Exception e)
    {
        tran.Rollback();
    }
}
```
**5. EF+Dapper混合事务**
```cs
using (var tran = _unitOfWork.BeginTransaction())
{
    try
    {
        await _userRepository.InsertAsync(user1);
        await _unitOfWork.ExecuteAsync(""INSERT INTO `user` (`CreateTime`, `IsDelete`, `UserName`) VALUES (now(), 0, @UserName);"", user2, tran);
        throw new Exception();
        tran.Commit();
    }
    catch (Exception e)
    {
        tran.Rollback();
    }
}
```
**6. 获GetConnection,使用更多dapper扩展的方法**
```cs
await _unitOfWork.GetConnection().QueryAsync(""select * from user"");
```
---
> 本项目有参考chimp
"
17,kaiyuanshe/repository,,"# 开源社成员项目列表

* [cube-ui](https://github.com/didi/cube-ui)
* [dragonfly](https://github.com/alibaba/dragonfly)
* [GoodERP](https://github.com/osbzr/gooderp_addons)
* [learn-with-open-source](https://github.com/zhuangbiaowei/learn-with-open-source)
* [LiteOS](https://github.com/LITEOS/LiteOS_Kernel)
* [neo-project](https://github.com/neo-project)
* [pouch](https://github.com/alibaba/pouch)
* [UKUI](https://github.com/ukui)

## 成员目录

开源社成员可以在 [成员](members) 目录下面创建一个自己的目录（建议以自己github账号名命名），
所有关于成员的介绍、成员项目的介绍都可以放到该目录下。
目录的内容可以参考[模板](members/template) .

## 如何添加自己的项目

**提交[PR](https://github.com/kaiyuanshe/pulls)**
- fork 本项目
- 创建一个`members/yourself`目录并把图片、文档放到里面
- 把项目的简要信息按照字母顺序排列添加到本文件开头的项目列表
```
* [你的项目名](你的项目地址)
```
- 提交 PR

*我们需要更好的模板、更好的项目展示方式，欢迎各种建议*

## 如何维护

维护者应当遵循[维护指南](MAINTAINERS_GUIDE.md),维护者的角色通过项目贡献完成。
初始的维护者来自开源社理事会。
"
18,wordpress-mobile/WordPress-iOS,Swift,"# WordPress for iOS #

[![CircleCI](https://circleci.com/gh/wordpress-mobile/WordPress-iOS.svg?style=svg)](https://circleci.com/gh/wordpress-mobile/WordPress-iOS)
[![Reviewed by Hound](https://img.shields.io/badge/Reviewed_by-Hound-8E64B0.svg)](https://houndci.com)

## Build Instructions

Please refer to the sections below for more detailed information. The instructions assume the work is performed from a command line.

> Please note – these setup instructions only apply to Intel-based machines. M1-based Mac support is coming, but isn't yet supported by our tooling.

### Getting Started

1. [Download](https://developer.apple.com/downloads/index.action) and install Xcode. *WordPress for iOS* requires Xcode 11.2.1 or newer.
1. From a command line, run `git clone git@github.com:wordpress-mobile/WordPress-iOS.git` in the folder of your preference.
1. Now, run `cd WordPress-iOS` to enter the working directory.

#### Create WordPress.com API Credentials

1. Create a WordPress.com account at https://wordpress.com/start/user (if you don't already have one).
1. Create an application at https://developer.wordpress.com/apps/.
1. Set ""Redirect URLs""= `https://localhost` and ""Type"" = `Native` and click ""Create"" then ""Update"".
1. Copy the `Client ID` and `Client Secret` from the OAuth Information.

#### Configure Your WordPress App Development Environment

1. Return to the command line and run `rake init:oss` to configure your computer and WordPress app to be able to run and login to WordPress.com
1. Once completed, run `rake xcode` to open the project in Xcode.

If all went well you can now compile to your iOS device or simulator, and log into the WordPress app.

Note: You can only log in with the WordPress.com account that you used to create the WordPress application.

## Configuration Details

The steps above will help you configure the WordPress app to run and compile.  But you may sometimes need to update or re-run specific parts of the initial setup (like updating the dependencies.)  To see how to do that, please check out the steps below.

### Third party tools

We use a few tools to help with development. Running `rake dependencies` will configure or update them for you.

#### CocoaPods

WordPress for iOS uses [CocoaPods](http://cocoapods.org/) to manage third party libraries.  
Third party libraries and resources managed by CocoaPods will be installed by the `rake dependencies` command above.

#### SwiftLint

We use [SwiftLint](https://github.com/realm/SwiftLint) to enforce a common style for Swift code. The app should build and work without it, but if you plan to write code, you are encouraged to install it. No commit should have lint warnings or errors.

You can set up a Git [pre-commit hook](https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks) to run SwiftLint automatically when committing by running:

`rake git:install_hooks`

This is the recommended way to include SwiftLint in your workflow, as it catches lint issues locally before your code makes its way to Github.

Alternately, a SwiftLint scheme is exposed within the project; Xcode will show a warning if you don't have SwiftLint installed.

Finally, you can also run SwiftLint manually from the command line with:

`rake lint`

If your code has any style violations, you can try to automatically correct them by running:

`rake lint:autocorrect`

Otherwise you have to fix them manually.

### Open Xcode

Launch the workspace by running the following from the command line:

`rake xcode`

This will ensure any dependencies are ready before launching Xcode.

You can also open the project by double clicking on `WordPress.xcworkspace` file, or launching Xcode and choose `File` > `Open` and browse to `WordPress.xcworkspace`.

### Setup Credentials

In order to login to WordPress.com with the app you need to create an account over at the [WordPress.com Developer Portal](https://developer.wordpress.com).

After you create an account you can create an application on the [WordPress.com applications manager](https://developer.wordpress.com/apps/).

When creating your application, select ""Native client"" for the application type. The applications manager currently requires a ""redirect URL"", but this isn't used for mobile apps. Just use ""https://localhost"".

Your new application will have an associated client ID and a client secret key. These are used to authenticate the API calls made by your application.

Next, run the command `rake credentials:setup` you will be prompted for your Client ID and your Client Secret.  Once added you will be able to log into the WordPress app

**Remember the only WordPress.com account you will be able to login in with is the one used to create your client ID and client secret.**

Read more about [OAuth2](https://developer.wordpress.com/docs/oauth2/) and the [WordPress.com REST endpoint](https://developer.wordpress.com/docs/api/).

## Contributing

Read our [Contributing Guide](CONTRIBUTING.md) to learn about reporting issues, contributing code, and more ways to contribute.

## Security

If you happen to find a security vulnerability, we would appreciate you letting us know at https://hackerone.com/automattic and allowing us to respond before disclosing the issue publicly.

## Getting in Touch ##

If you have questions about getting setup or just want to say hi, join the [WordPress Slack](https://chat.wordpress.org) and drop a message on the `#mobile` channel.

## Resources

- The [docs](docs/) contain information about our development practices.
- [WordPress Mobile Blog](http://make.wordpress.org/mobile)
- [WordPress Mobile Handbook](http://make.wordpress.org/mobile/handbook/)

## License

WordPress for iOS is an Open Source project covered by the [GNU General Public License version 2](LICENSE).
"
19,transmission/transmission,C,"## About

Transmission is a fast, easy, and free BitTorrent client. It comes in several flavors:
  * A native Mac OS X GUI application
  * GTK+ and Qt GUI applications for Linux, BSD, etc.
  * A headless daemon for servers and routers
  * A web UI for remote controlling any of the above

Visit https://transmissionbt.com/ for more information.

## Command line interface notes

Transmission is fully supported in transmission-remote, the preferred cli client.

Three standalone tools to examine, create, and edit .torrent files exist: transmission-show, transmission-create, and transmission-edit, respectively.

Prior to development of transmission-remote, the standalone client transmission-cli was created. Limited to a single torrent at a time, transmission-cli is deprecated and exists primarily to support older hardware dependent upon it. In almost all instances, transmission-remote should be used instead.

Different distributions may choose to package any or all of these tools in one or more separate packages.

## Building

Transmission has an Xcode project file (Transmission.xcodeproj) for building in Xcode.

For a more detailed description, and dependencies, visit: https://github.com/transmission/transmission/wiki

### Building a Transmission release from the command line

    $ tar xf transmission-2.92.tar.xz
    $ cd transmission-2.92
    $ mkdir build
    $ cd build
    $ cmake ..
    $ make
    $ sudo make install

### Building Transmission from the nightly builds

Download a tarball from https://build.transmissionbt.com/job/trunk-linux/ and follow the steps from the previous section.

If you're new to building programs from source code, this is typically easier than building from Git.

### Building Transmission from Git (first time)

    $ git clone https://github.com/transmission/transmission Transmission
    $ cd Transmission
    $ git submodule update --init
    $ mkdir build
    $ cd build
    $ cmake ..
    $ make
    $ sudo make install

### Building Transmission from Git (updating)

    $ cd Transmission/build
    $ make clean
    $ git pull --rebase --prune
    $ git submodule update
    $ cmake ..
    $ make
    $ sudo make install

## Contributing

### Code Style

You would want to setup your editor to make use of the uncrustify.cfg file located in the root of this repository and the eslint/prettier rules in web/package.json.

If for some reason you are unwilling or unable to do so, there is a shell script which you could run either directly or via docker-compose:

    $ ./code_style.sh
    or
    $ docker-compose build --pull
    $ docker-compose run --rm code_style
"
20,slorber/gcm-server-repository,Shell,"
# Maven repository for Google-Cloud-Messaging server library

This permits to manage the Google GCM library with Maven.
The gcm-server.jar provided with the Android SDK, used by servers to send push notification to Android devices, is not hosted in any official Maven repository.
This is why a created this personal repository.
The library has a dependency (simple-json) which is handled in the pom i've written.

## Usage

Add the repository to your maven configuration:

    <repository>
        <id>gcm-server-repository</id>
        <url>https://raw.githubusercontent.com/slorber/gcm-server-repository/master/releases/</url>
    </repository>

And then add the dependency

    <dependency>
        <groupId>com.google.android.gcm</groupId>
        <artifactId>gcm-server</artifactId>
        <version>1.0.2</version>
    </dependency>
    
If you build your client with Android you can also add the client dependency, thanks to cjbest:

    <dependency>
        <groupId>com.google.android.gcm</groupId>
        <artifactId>gcm-client</artifactId>
        <version>1.0.2</version>
    </dependency>

I've only deployed the 1.0.2 version, which is the version i've found in the ant script associated with the gcm-server sources. 


## Contribute

If you need another version, of gcm-server which is not already hosted on this server, don't hesitate to create a pull request.

You can see everything i've used here:
https://github.com/slorber/gcm-server-repository/deployer
- The source and jar files
- The pom
- A script to install the files to your local maven repository

The pom is required to handle the dependency of the project (simple-json 1.1 here).
The script deploys files to ~/.m2/repository/com/google/android/gcm/gcm-server/1.0.2/
You just need to copy this files in your gcm-server-repository/releases/com/google/android/gcm/gcm-server/1.0.2/ and create your pull request

How to build your own Github repository:
http://cemerick.com/2010/08/24/hosting-maven-repos-on-github/


## LICENSE

This repository respects the Apache License distribution rules.
No original files were modified

"
21,tegon/clone-org-repos,JavaScript,"Clone github organization repos
===

[![NPM](https://nodei.co/npm/clone-org-repos.png?downloads=true&downloadRank=true&stars=true)](https://nodei.co/npm/clone-org-repos/)
[![Build Status](https://travis-ci.org/tegon/clone-org-repos.svg?branch=master)](https://travis-ci.org/tegon/clone-org-repos)
[![Coverage Status](https://coveralls.io/repos/tegon/clone-org-repos/badge.svg?branch=master&service=github)](https://coveralls.io/github/tegon/clone-org-repos?branch=master)

This is a tool to clone all repositories from an github organization.
This could be helpful if you work at some company, or if you contribute to an open source project.

Why?
---
I went through this a few times, I need to clone all repositories from the company where I work, and, in the beginning, this line of Ruby code was sufficient:

```ruby
curl -s ""https://api.github.com/orgs/ORG_NAME/repos?per_page=100"" -u ""username"" | ruby -rubygems -e 'require ""json""; JSON.load(STDIN.read).each {|repo| %x[git clone #{repo[""ssh_url""]} ]}'
```

But things got a little complicated. Some repositories aren't used by me because they are from different teams. In this case this tool can be useful because it allows you to pass options to ignore some repositories.

Usage
---
```bash
  cloneorg [OPTIONS] [ORG]
```

Options:
---
```bash
  -p,  --perpage NUMBER number of repos per page (Default is 100)
  -t,  --type STRING    can be one of: all, public, private, forks, sources,
                         member  (Default is all)
  -e,  --exclude STRING   Exclude passed repos, comma separated
  -o,  --only STRING      Only clone passed repos, comma separated
  -r,  --regexp BOOLEAN   If true, exclude or only option will be evaluated as a
                         regexp
  -u,  --username STRING  Username for basic authentication. Required to
                         access github api
       --token STRING     Token authentication. Required to access github api
  -a, --gitaccess         Protocol to use in `git clone` command. Can be `ssh` (default), `https` or `git`
  -s, --gitsettings       Additional parameters to pass to git clone command. Defaults to empty.
       --debug            Show debug information
  -v,  --version          Display the current version
  -h,  --help             Display help and usage details
```

Examples:
---

clones all github/twitter repositories, with HTTP basic authentication. A password will be required

```bash
cloneorg twitter -u GITHUB_USERNAME
cloneorg twitter --username=GITHUB_USERNAME
```

clones all github/twitter repositories, with an access token provided by Github

```bash
cloneorg twitter --token GITHUB_TOKEN
```

If an environment variable `GITHUB_TOKEN` is set, it will be used.

```bash
export GITHUB_TOKEN='YOUR_GITHUB_API_TOKEN'
```

Add a -p or --perpage option to paginate response

```bash
cloneorg mozilla --token=GITHUB_TOKEN -p 10
```

Exclude and Only options
---

If you only need some repositories, you can pass -o or --only with their names

```bash
cloneorg angular --token=GITHUB_TOKEN -o angular
```

This can be an array to

```bash
cloneorg angular --token=GITHUB_TOKEN -o angular,material,bower-angular-i18n
```

This can also be an regular expression, with -r or --regexp option set to true.

```bash
cloneorg marionettejs --token=GITHUB_TOKEN -o ^backbone -r true
```

The same rules apply to exclude options

```bash
cloneorg jquery --token=GITHUB_TOKEN -e css-framework # simple
```

```bash
cloneorg emberjs --token=GITHUB_TOKEN -e website,examples # array
```

```bash
cloneorg gruntjs --token=GITHUB_TOKEN -e $-docs -r true # regexp
```

```bash
cloneorg gruntjs --token=GITHUB_TOKEN -e $-docs -r true --gitaccess=git # Clone using git protocol
```

```bash
# Clone using git protocol and pass --recurse to `git clone` to clone submodules also
cloneorg gruntjs --token=GITHUB_TOKEN -e $-docs -r true --gitaccess=git --gitsettings=""--recurse""
```

ToDo
---

- Progress bar while cloning repos
"
22,geometer/FBReaderJ,Java,
23,Sage-Bionetworks/Synapse-Repository-Services,Java,"Synapse-Repository-Services
===========================

The Synapse Repository Services project is the home of the JSON REST web services for Synapse.

General information for developers may be found here:
https://sagebionetworks.jira.com/wiki/display/PLFM/Home

Information on how github branches are used by Synapse may be found here:
https://sagebionetworks.jira.com/wiki/display/PLFM/Staging+Deployment%2C+Step+by+Step"
24,jenkinsci/repository-connector-plugin,Java,"## Repository Connector Plugin

[![Jenkins Plugin](https://img.shields.io/jenkins/plugin/v/repository-connector.svg)](https://plugins.jenkins.io/repository-connector)
[![GitHub release](https://img.shields.io/github/release/jenkinsci/repository-connector-plugin.svg?label=changelog)](https://github.com/jenkinsci/repository-connector-plugin/releases/latest)
[![Jenkins Plugin Installs](https://img.shields.io/jenkins/plugin/i/repository-connector.svg?color=blue)](https://plugins.jenkins.io/repository-connector)
[![Build Status](https://ci.jenkins.io/buildStatus/icon?job=Plugins/repository-connector-plugin/master)](https://ci.jenkins.io/job/Plugins/job/repository-connector-plugin/job/master/)

Jenkins [plugin](https://plugins.jenkins.io/) which allows for retrieval or deployment of artifacts to/from a repository such
as [Nexus](https://www.sonatype.com/nexus/repository-oss) or [Artifactory](https://jfrog.com/artifactory/)
and static servers implementing the [repository layout](https://cwiki.apache.org/confluence/display/MAVENOLD/Repository+Layout+-+Final).

### 2.0.0 Changes

With the release of version `2.0.0`  come the following changes, some of which may require jobs to be reconfigured, although none
should cease to work outright. :crossed_fingers:

* Jenkins >= 2.222.4 required
* Artifact deployment re-enabled
* Version resolution build parameters are required to have a name (possible breaking change)
* Separated repository and transfer logging into separate options
* Artifact resolution 'fail on error' can be specified per artifact

##### Credentials

Credentials are now stored in and provided by the [Credentials](https://plugins.jenkins.io/credentials/) plugin. Any previously stored
username/password combinations stored by the plugin configuration are migrated upon startup.

##### Aether Resolution / Deployment

The plugin has been updated to use the same underlying [aether library](https://github.com/apache/maven-resolver) as `maven` itself.
Artifact resolution policies are configured as part of the repository now instead of at the job level. It is also possible to specify
individual deployment endpoints for `snapshots` and `releases` if you are using a repository manager.

During artifact resolution, only the artifact itself will be resolved. Prior versions of the plugin would attempt to resolve all
transitive dependencies as well, but not install them into the local repository. This could lead to issues where resolution of the
artifact itself would succeed but still result in failure because a dependency could not be found.

### Configuration-as-Code

[JCasC](https://plugins.jenkins.io/configuration-as-code) is fully supported. Please visit the please visit the
[wiki](https://github.com/jenkinsci/repository-connector-plugin/wiki/Configuration) for additional details.

### Job DSL

The [jenkins-job-dsl](https://plugins.jenkins.io/job-dsl/) is fully supported using the [dynamic dsl](https://github.com/jenkinsci/job-dsl-plugin/wiki/Dynamic-DSL).
As of `2.0.0`, the built-in dsl provided by the `job-dsl` plugin will no longer work.

### Plugin Configuration

The plugin comes pre-configured with an entry for [maven central](https://repo1.maven.org/maven2) that allows for immediate use.
Select the `Maven Artifact Resolver` option from the `Build Steps` dropdown and and configure accordingly.

For more instructions, please visit the [wiki](https://github.com/jenkinsci/repository-connector-plugin/wiki).

### Other

#### Maven Central Snapshots

If you are looking to resolve `SNAPSHOT` versions from artifacts you would find on [maven central](https://repo1.maven.org/maven2), add
a repository configuration that uses the following endpoint, with the `release` policy disabled.

```
https://oss.sonatype.org/content/repositories/snapshots
```

#### I8N

Google translate was used for all German :de: translations, apologies for anything that is horrendously off. Fixes welcome!
"
25,zach-morris/repository.zachmorris,Python,
26,geotools/geotools,Java,"![GeoTools logo](/geotools-logo.png)

[GeoTools](http://geotools.org) is an open source Java library that provides
 tools for geospatial data. Our Users guide provides an [overview](http://docs.geotools.org/latest/userguide/geotools.html) of the core features, supported formats and standards support.

## License

GeoTools is licensed under the [LGPL](http://www.gnu.org/licenses/lgpl.html). The user guide [license](http://docs.geotools.org/latest/userguide/welcome/license.html) page describes the less restrictive license for documentation and source code examples.

## Contributing

The developers guide outlines ways to [contribute ](http://docs.geotools.org/latest/developer/procedures/contribute.html) to GeoTools using patches, pull requests and setting up new modules.

If you are already experienced with GitHub please check our [pull request](http://docs.geotools.org/latest/developer/procedures/pull_requests.html) page before you start!

## Building

GeoTools uses [Apache Maven](http://maven.apache.org/) for a build system. To 
build the library run maven from the root of the repository.

    % mvn clean install

See the [user guide](http://docs.geotools.org/latest/userguide/build/index.html) 
for more details.

## Bugs

GeoTools uses [JIRA](https://osgeo-org.atlassian.net/browse/GEOT), hosted by 
[Atlassian](https://www.atlassian.com/), for issue tracking.

## Mailing Lists

The [user list](mailto:geotools-gt2-users@lists.sourceforge.net) is for all questions 
related to GeoTools usage. 

The [dev list](mailto:geotools-devel@lists.sourceforge.net) is for questions related 
to hacking on the GeoTools library itself.

## More Information

Visit the [website](http://geotools.org/) or read the [docs](http://docs.geotools.org/). 

"
27,libjpeg-turbo/libjpeg-turbo,C,"libjpeg-turbo note:  This file has been modified by The libjpeg-turbo Project
to include only information relevant to libjpeg-turbo, to wordsmith certain
sections, and to remove impolitic language that existed in the libjpeg v8
README.  It is included only for reference.  Please see README.md for
information specific to libjpeg-turbo.


The Independent JPEG Group's JPEG software
==========================================

This distribution contains a release of the Independent JPEG Group's free JPEG
software.  You are welcome to redistribute this software and to use it for any
purpose, subject to the conditions under LEGAL ISSUES, below.

This software is the work of Tom Lane, Guido Vollbeding, Philip Gladstone,
Bill Allombert, Jim Boucher, Lee Crocker, Bob Friesenhahn, Ben Jackson,
Julian Minguillon, Luis Ortiz, George Phillips, Davide Rossi, Ge' Weijers,
and other members of the Independent JPEG Group.

IJG is not affiliated with the ISO/IEC JTC1/SC29/WG1 standards committee
(also known as JPEG, together with ITU-T SG16).


DOCUMENTATION ROADMAP
=====================

This file contains the following sections:

OVERVIEW            General description of JPEG and the IJG software.
LEGAL ISSUES        Copyright, lack of warranty, terms of distribution.
REFERENCES          Where to learn more about JPEG.
ARCHIVE LOCATIONS   Where to find newer versions of this software.
FILE FORMAT WARS    Software *not* to get.
TO DO               Plans for future IJG releases.

Other documentation files in the distribution are:

User documentation:
  usage.txt         Usage instructions for cjpeg, djpeg, jpegtran,
                    rdjpgcom, and wrjpgcom.
  *.1               Unix-style man pages for programs (same info as usage.txt).
  wizard.txt        Advanced usage instructions for JPEG wizards only.
  change.log        Version-to-version change highlights.
Programmer and internal documentation:
  libjpeg.txt       How to use the JPEG library in your own programs.
  example.txt       Sample code for calling the JPEG library.
  structure.txt     Overview of the JPEG library's internal structure.
  coderules.txt     Coding style rules --- please read if you contribute code.

Please read at least usage.txt.  Some information can also be found in the JPEG
FAQ (Frequently Asked Questions) article.  See ARCHIVE LOCATIONS below to find
out where to obtain the FAQ article.

If you want to understand how the JPEG code works, we suggest reading one or
more of the REFERENCES, then looking at the documentation files (in roughly
the order listed) before diving into the code.


OVERVIEW
========

This package contains C software to implement JPEG image encoding, decoding,
and transcoding.  JPEG (pronounced ""jay-peg"") is a standardized compression
method for full-color and grayscale images.  JPEG's strong suit is compressing
photographic images or other types of images that have smooth color and
brightness transitions between neighboring pixels.  Images with sharp lines or
other abrupt features may not compress well with JPEG, and a higher JPEG
quality may have to be used to avoid visible compression artifacts with such
images.

JPEG is lossy, meaning that the output pixels are not necessarily identical to
the input pixels.  However, on photographic content and other ""smooth"" images,
very good compression ratios can be obtained with no visible compression
artifacts, and extremely high compression ratios are possible if you are
willing to sacrifice image quality (by reducing the ""quality"" setting in the
compressor.)

This software implements JPEG baseline, extended-sequential, and progressive
compression processes.  Provision is made for supporting all variants of these
processes, although some uncommon parameter settings aren't implemented yet.
We have made no provision for supporting the hierarchical or lossless
processes defined in the standard.

We provide a set of library routines for reading and writing JPEG image files,
plus two sample applications ""cjpeg"" and ""djpeg"", which use the library to
perform conversion between JPEG and some other popular image file formats.
The library is intended to be reused in other applications.

In order to support file conversion and viewing software, we have included
considerable functionality beyond the bare JPEG coding/decoding capability;
for example, the color quantization modules are not strictly part of JPEG
decoding, but they are essential for output to colormapped file formats or
colormapped displays.  These extra functions can be compiled out of the
library if not required for a particular application.

We have also included ""jpegtran"", a utility for lossless transcoding between
different JPEG processes, and ""rdjpgcom"" and ""wrjpgcom"", two simple
applications for inserting and extracting textual comments in JFIF files.

The emphasis in designing this software has been on achieving portability and
flexibility, while also making it fast enough to be useful.  In particular,
the software is not intended to be read as a tutorial on JPEG.  (See the
REFERENCES section for introductory material.)  Rather, it is intended to
be reliable, portable, industrial-strength code.  We do not claim to have
achieved that goal in every aspect of the software, but we strive for it.

We welcome the use of this software as a component of commercial products.
No royalty is required, but we do ask for an acknowledgement in product
documentation, as described under LEGAL ISSUES.


LEGAL ISSUES
============

In plain English:

1. We don't promise that this software works.  (But if you find any bugs,
   please let us know!)
2. You can use this software for whatever you want.  You don't have to pay us.
3. You may not pretend that you wrote this software.  If you use it in a
   program, you must acknowledge somewhere in your documentation that
   you've used the IJG code.

In legalese:

The authors make NO WARRANTY or representation, either express or implied,
with respect to this software, its quality, accuracy, merchantability, or
fitness for a particular purpose.  This software is provided ""AS IS"", and you,
its user, assume the entire risk as to its quality and accuracy.

This software is copyright (C) 1991-2020, Thomas G. Lane, Guido Vollbeding.
All Rights Reserved except as specified below.

Permission is hereby granted to use, copy, modify, and distribute this
software (or portions thereof) for any purpose, without fee, subject to these
conditions:
(1) If any part of the source code for this software is distributed, then this
README file must be included, with this copyright and no-warranty notice
unaltered; and any additions, deletions, or changes to the original files
must be clearly indicated in accompanying documentation.
(2) If only executable code is distributed, then the accompanying
documentation must state that ""this software is based in part on the work of
the Independent JPEG Group"".
(3) Permission for use of this software is granted only if the user accepts
full responsibility for any undesirable consequences; the authors accept
NO LIABILITY for damages of any kind.

These conditions apply to any software derived from or based on the IJG code,
not just to the unmodified library.  If you use our work, you ought to
acknowledge us.

Permission is NOT granted for the use of any IJG author's name or company name
in advertising or publicity relating to this software or products derived from
it.  This software may be referred to only as ""the Independent JPEG Group's
software"".

We specifically permit and encourage the use of this software as the basis of
commercial products, provided that all warranty or liability claims are
assumed by the product vendor.


REFERENCES
==========

We recommend reading one or more of these references before trying to
understand the innards of the JPEG software.

The best short technical introduction to the JPEG compression algorithm is
        Wallace, Gregory K.  ""The JPEG Still Picture Compression Standard"",
        Communications of the ACM, April 1991 (vol. 34 no. 4), pp. 30-44.
(Adjacent articles in that issue discuss MPEG motion picture compression,
applications of JPEG, and related topics.)  If you don't have the CACM issue
handy, a PDF file containing a revised version of Wallace's article is
available at http://www.ijg.org/files/Wallace.JPEG.pdf.  The file (actually
a preprint for an article that appeared in IEEE Trans. Consumer Electronics)
omits the sample images that appeared in CACM, but it includes corrections
and some added material.  Note: the Wallace article is copyright ACM and IEEE,
and it may not be used for commercial purposes.

A somewhat less technical, more leisurely introduction to JPEG can be found in
""The Data Compression Book"" by Mark Nelson and Jean-loup Gailly, published by
M&T Books (New York), 2nd ed. 1996, ISBN 1-55851-434-1.  This book provides
good explanations and example C code for a multitude of compression methods
including JPEG.  It is an excellent source if you are comfortable reading C
code but don't know much about data compression in general.  The book's JPEG
sample code is far from industrial-strength, but when you are ready to look
at a full implementation, you've got one here...

The best currently available description of JPEG is the textbook ""JPEG Still
Image Data Compression Standard"" by William B. Pennebaker and Joan L.
Mitchell, published by Van Nostrand Reinhold, 1993, ISBN 0-442-01272-1.
Price US$59.95, 638 pp.  The book includes the complete text of the ISO JPEG
standards (DIS 10918-1 and draft DIS 10918-2).

The original JPEG standard is divided into two parts, Part 1 being the actual
specification, while Part 2 covers compliance testing methods.  Part 1 is
titled ""Digital Compression and Coding of Continuous-tone Still Images,
Part 1: Requirements and guidelines"" and has document numbers ISO/IEC IS
10918-1, ITU-T T.81.  Part 2 is titled ""Digital Compression and Coding of
Continuous-tone Still Images, Part 2: Compliance testing"" and has document
numbers ISO/IEC IS 10918-2, ITU-T T.83.

The JPEG standard does not specify all details of an interchangeable file
format.  For the omitted details, we follow the ""JFIF"" conventions, revision
1.02.  JFIF version 1 has been adopted as ISO/IEC 10918-5 (05/2013) and
Recommendation ITU-T T.871 (05/2011): Information technology - Digital
compression and coding of continuous-tone still images: JPEG File Interchange
Format (JFIF).  It is available as a free download in PDF file format from
https://www.iso.org/standard/54989.html and http://www.itu.int/rec/T-REC-T.871.
A PDF file of the older JFIF 1.02 specification is available at
http://www.w3.org/Graphics/JPEG/jfif3.pdf.

The TIFF 6.0 file format specification can be obtained from
http://mirrors.ctan.org/graphics/tiff/TIFF6.ps.gz.  The JPEG incorporation
scheme found in the TIFF 6.0 spec of 3-June-92 has a number of serious
problems.  IJG does not recommend use of the TIFF 6.0 design (TIFF Compression
tag 6).  Instead, we recommend the JPEG design proposed by TIFF Technical Note
#2 (Compression tag 7).  Copies of this Note can be obtained from
http://www.ijg.org/files/.  It is expected that the next revision
of the TIFF spec will replace the 6.0 JPEG design with the Note's design.
Although IJG's own code does not support TIFF/JPEG, the free libtiff library
uses our library to implement TIFF/JPEG per the Note.


ARCHIVE LOCATIONS
=================

The ""official"" archive site for this software is www.ijg.org.
The most recent released version can always be found there in
directory ""files"".

The JPEG FAQ (Frequently Asked Questions) article is a source of some
general information about JPEG.  It is available at
http://www.faqs.org/faqs/jpeg-faq.


FILE FORMAT COMPATIBILITY
=========================

This software implements ITU T.81 | ISO/IEC 10918 with some extensions from
ITU T.871 | ISO/IEC 10918-5 (JPEG File Interchange Format-- see REFERENCES).
Informally, the term ""JPEG image"" or ""JPEG file"" most often refers to JFIF or
a subset thereof, but there are other formats containing the name ""JPEG"" that
are incompatible with the DCT-based JPEG standard or with JFIF (for instance,
JPEG 2000 and JPEG XR).  This software therefore does not support these
formats.  Indeed, one of the original reasons for developing this free software
was to help force convergence on a common, interoperable format standard for
JPEG files.

JFIF is a minimal or ""low end"" representation.  TIFF/JPEG (TIFF revision 6.0 as
modified by TIFF Technical Note #2) can be used for ""high end"" applications
that need to record a lot of additional data about an image.


TO DO
=====

Please send bug reports, offers of help, etc. to jpeg-info@jpegclub.org.
"
28,HandBrake/HandBrake,C,"# HandBrake [![macOS Build](https://github.com/HandBrake/HandBrake/workflows/macOS%20build/badge.svg)](https://github.com/HandBrake/HandBrake/actions?query=workflow%3A%22macOS+build%22) [![Windows Build](https://github.com/HandBrake/HandBrake/workflows/Windows%20Build/badge.svg)](https://github.com/HandBrake/HandBrake/actions?query=workflow%3A%22Windows+Build%22) [![Linux Build](https://github.com/HandBrake/HandBrake/workflows/Linux%20Build/badge.svg)](https://github.com/HandBrake/HandBrake/actions?query=workflow%3A%22Linux+Build%22)


HandBrake is an open-source video transcoder available for Linux, Mac, and Windows, licensed under the [GNU General Public License (GPL) Version 2](LICENSE).

HandBrake takes videos you already have and makes new ones that work on your mobile phone, tablet, TV media player, game console, computer, or web browser—nearly anything that supports modern video formats.

HandBrake works with most common video files and formats, including ones created by consumer and professional video cameras, mobile devices such as phones and tablets, game and computer screen recordings, and DVD and Blu-ray discs. HandBrake leverages tools such as FFmpeg, x264, and x265 to create new MP4 or MKV video files from these *Sources*.

For information on downloading, building/installing, and using HandBrake, see the official [HandBrake Documentation](https://handbrake.fr/docs).


## Community Support

Visit the [HandBrake Community Forums](https://forum.handbrake.fr/).

Chat on [#handbrake on Freenode IRC](https://webchat.freenode.net/?channels=handbrake) (irc://irc.freenode.net/#handbrake).

For information on HandBrake's community support channels, please see [Community Support](https://handbrake.fr/docs/en/latest/help/community-support.html).

Our [community rules](https://forum.handbrake.fr/app.php/rules) and [code of conduct](https://github.com/HandBrake/HandBrake/blob/master/CODE_OF_CONDUCT.md) apply to both our site and GitHub.


## Contributing

We welcome most contributions. While it is our goal to allow everyone to contribute, contributions not meeting the project's goals or  standards may be rejected.

Please read our [guide to contributing](https://handbrake.fr/docs/en/latest/contributing/contribute.html). This will provide you with all the information you need to start contributing to the project. 

## Translations

We are now accepting translations via  [Transifex](https://www.transifex.com/HandBrakeProject/public) 

Please read our [Translations Guide](https://github.com/HandBrake/HandBrake/blob/master/CODE_OF_CONDUCT.md) and follow the instructions if you are interested in joining the translation effort.


## Additional Information

[Authors](AUTHORS.markdown)  
[License](LICENSE)  
[News](NEWS.markdown)  

## Special Thanks

<a href=""https://www.macstadium.com/""><img width=""200"" alt=""MacStadium"" src=""https://uploads-ssl.webflow.com/5ac3c046c82724970fc60918/5c019d917bba312af7553b49_MacStadium-developerlogo.png""></a>

and to many others who have contributed! [Thanks](THANKS.markdown)
"
29,Auxilus/termux-x-repository,CSS,"# Termux repository of X/GUI packages
This repository contains various X/GUI packages (such as **dosbox**) that can be installed to [Termux](https://github.com/termux/termux-app).

## How to enable this repository
Make sure that needed tools are installed:
```
pkg upgrade
pkg install apt-transport-https nano gnupg wget
```

Download gpg key for this repository:
```
wget https://xeffyr.github.io/termux-x-repository/pubkey.gpg
apt-key add pubkey.gpg
```

Edit your sources.list by adding a line with correct CPU architecture:
```
## For AArch64
deb [arch=all,aarch64] https://xeffyr.github.io/termux-x-repository/ termux x-gui

## For ARM
deb [arch=all,arm] https://xeffyr.github.io/termux-x-repository/ termux x-gui

## For i686
deb [arch=all,i686] https://xeffyr.github.io/termux-x-repository/ termux x-gui

## For x86_64
deb [arch=all,x86_64] https://xeffyr.github.io/termux-x-repository/ termux x-gui
```

Then update apt lists:
```
apt update
```

## Note
This repository uses Github Pages. Since Github Pages use caching, sometimes when repository is updated you may receive errors like
'Hash Sum mismatch' or '404 Not Found'. Usually, the problem should gone away in ~10 minutes after repository update.

## Things that have to be done
Basic (tigervnc, xclock and dependencies):
- [x] Add architecture 'aarch64'
- [x] Add architecture 'arm'
- [x] Add architecture 'i686'
- [x] Add architecture 'x86_64'

Additional useful packages:
- [ ] Openbox
- [x] SDL
- [ ] FLTK
- [x] DosBox
- [ ] XTerm
- [x] QEMU
"
30,toltec-dev/toltec,Shell,"## Toltec

![Status of the stable repository](https://github.com/toltec-dev/toltec/workflows/stable/badge.svg)
![Status of the testing repository](https://github.com/toltec-dev/toltec/workflows/testing/badge.svg)
[![rm1](https://img.shields.io/badge/rM1-supported-green)](https://remarkable.com/store/remarkable)
[![rm2](https://img.shields.io/badge/rM2-experimental-yellow)](https://remarkable.com/store/remarkable-2)
[![Discord](https://img.shields.io/discord/463752820026376202.svg?label=reMarkable&logo=discord&logoColor=ffffff&color=7389D8&labelColor=6A7EC2)](https://discord.gg/ATqQGfu)

Toltec is a community-maintained repository of free software for [the reMarkable tablet](https://remarkable.com/).

### Install it

Toltec works on top of the [Opkg](https://code.google.com/archive/p/opkg/) package manager and the [Entware](https://github.com/Entware/Entware) distribution, which are in widespread use in embedded devices.
To automatically install Opkg, Entware and Toltec, run the bootstrap script in a [SSH session](https://remarkablewiki.com/tech/ssh) on your reMarkable:

```sh
$ wget http://toltec-dev.org/bootstrap
$ echo ""46f556b06f5624b48e974ae040b6213828eff6aa2cc78618a4d8961a27cdc8b3  bootstrap"" | sha256sum -c && bash bootstrap
```

> **Warning:**
> Make sure to run the second line above, which verifies the integrity of the downloaded script before running it.
> Since the built-in wget binary does not implement TLS, _you will expose yourself to MITM attacks if you skip this step!_
> The bootstrap script takes care of replacing the built-in wget with a safer version.

> **What does this script do?**
> This script will create a `.entware` folder in your home directory, containing a complete Entware distribution (fetched from <https://bin.entware.net/armv7sf-k3.2/>), and permanently mount it to `/opt`.
> It will then configure Opkg for use with Toltec and configure your system to automatically find binaries from `/opt`.
> You are encouraged to [audit the script](scripts/bootstrap/bootstrap) yourself if you can.

> **Compatibility with [remarkable_entware](https://github.com/evidlo/remarkable_entware).**
> If you have already installed Entware through Evidlo’s remarkable\_entware, this script will detect the existing install and configure Toltec on top of it.

You now have access to all of the Toltec and Entware packages!

[Browse the list of available packages →](https://toltec-dev.org/stable)

### Use it

To install a package:

```sh
$ opkg install calculator
```

To remove a package:

```sh
$ opkg remove calculator
```

To update all packages:

```sh
$ opkg update
$ opkg upgrade
```

[See information about advanced Opkg commands →](https://openwrt.org/docs/guide-user/additional-software/opkg)\
[Choose between _stable_ and _testing_ →](docs/branches.md)

### Build it

This Git repository contains all the tools and recipes required to build the packages published on the package repository.
This repository is automatically built and published every time that a commit is pushed to Git, using [Github Actions](https://docs.github.com/en/actions).
Since all the packaged software in Toltec is free, you can also **build them from source yourself** instead of using the pre-built binaries.
The build process is fully [reproducible](https://reproducible-builds.org/), which means that you can verify that the published packages have not been tampered with during the automated build process.

[Learn how to build the repository from source →](docs/building.md)

### Improve it

Your contribution is welcome for adding new packages, updating existing ones or improving the build tooling.

[Learn how to contribute to Toltec →](docs/contributing.md)
"
31,HIGHWAY99/repository.thehighway,Python,"TheHighway's xbmc plugin repository!

-------------------------------


* Install:

You can manually install via System - Add-ons - Install from zip file... 

if you wish to manually download the zip from:

http://raw.github.com/HIGHWAY99/repository.thehighway/master/repo/repository.thehighway/repository.thehighway-0.0.6.zip



** Another means is via SuperRepo.org:

As shown @ https://superrepo.org/get-started/

Add Files source named ""SuperRepo"": http://srp.nu/

Then goto: System - Addons - Install from zip file - SuperRepo (the Files source you just made).

Select your type of xbmc/kodi installed such as ""Helix"".

Select ""all"".

Select ""repository.superrepo.org.helix.all-latest.zip"" to install SR's repository of user repositories among seemingly all other things.

Now back up to where you clicked ""Install from zip file"" and click ""Get Add-ons"".

Next click ""SuperRepo  All [helix]"" or simular.  Sometimes you'll have to right click this and hit force refresh a couple times or so until it populates a list.

Once it comes up with a list, click ""Add-on repository"", there should be a HUGE list of repositories here.

Scroll way down or up to get to the bottom and up a bit more until you find the T's, then select ""TheHighway's Addons"" and include Install on the next screen.

Once it's done downloading and installing, you man then go back to the ""Get Add-ons"" list and select TheHighway's Addons"" (sometimes having to force refresh), and browse a list of addons by TheHighway for download.



* Contact:
** Can normally find me on IRC Chat.  @  irc.snoonnet.org:6667 in #The_Projects
** If you don't have an IRC Chat client installed, you can always try a webchat(like: http://webchat.snoonet.org/#The_Projects ) applet.
** Can also catch check out /r/The_Projects/ ( http://www.reddit.com/r/The_Projects/ ) on reddit."
32,esendir/MongoRepository,C#,"[![Version](https://img.shields.io/nuget/v/Repository.Mongo.svg?style=flat-square)](https://www.nuget.org/packages/Repository.Mongo)
[![Downloads](https://img.shields.io/nuget/dt/Repository.Mongo.svg?style=flat-square)](https://www.nuget.org/packages/Repository.Mongo)

## MongoRepository
Repository pattern for MongoDB with extended features

### MongoDB Driver Version
2.11.4

### Definition

#### Model
You don't need to create a model, but if you are doing so you need to extend Entity
```csharp
// If you are able to define your model
public class User : Entity 
{
    public string Username { get; set; }
    public string Password { get; set; }
}	
```

#### Repository
There are multiple base constructors, read summaries of others
```csharp
public class UserRepository : Repository<User> 
{
    public UserRepository (string connectionString) : base (connectionString) { }

    // Custom method
    public User FindbyUsername (string username) 
    {
        return First (i => i.Username == username);
    }

    // Custom method2
    public void UpdatePassword (User item, string newPassword) 
    {
        repo.Update (item, i => i.Password, newPassword);
    }

    // Custom async method
    public async Task<User> FindbyUsernameAsync (string username) 
    {
        return await FirstAsync (i => i.Username == username);
    }
}
```

*If you want to create a repository for already defined non-entity model*
```csharp
public class UserRepository : Repository<Entity<User>> 
{
    public UserRepository (string connectionString) : base (connectionString) { }


    // Custom method
    public User FindbyUsername (string username) 
    {
        return First (i => i.Content.Username == username);
    }
}	
```

### Usage

Each method has multiple overloads, read method summary for additional parameters

```csharp
UserRepository repo = new UserRepository (""mongodb://localhost/sample"")

// Get
User user = repo.Get (""58a18d16bc1e253bb80a67c9"");

// Insert
User item = new User () 
{
    Username = ""username"",
    Password = ""password""
};
repo.Insert (item);

// Update
// Single property
repo.Update (item, i => i.Username, ""newUsername"");

// Multiple property
// Updater has many methods like Inc, Push, CurrentDate, etc.
var update1 = Updater.Set (i => i.Username, ""oldUsername"");
var update2 = Updater.Set (i => i.Password, ""newPassword"");
repo.Update (item, update1, update2);

// All entity
item.Username = ""someUsername"";
repo.Replace (item);

// Delete
repo.Delete (item);

// Queries - all queries has filter, order and paging features
var first = repo.First ();
var last = repo.Last ();
var search = repo.Find (i => i.Username == ""username"");
var allItems = repo.FindAll ();

// Utils
var any = repo.Any (i => i.Username.Contains (""user""));

// Count
// Get number of filtered documents
var count = repo.Count (p => p.Age > 20);

// EstimatedCount
// Get number of all documents
var count = repo.EstimatedCount ();
```

### List of Functions
```csharp
Delete(T entity)
Task<bool> DeleteAsync(T entity)

Delete(Expression<Func<T, bool>> filter)
Task<bool> DeleteAsync(Expression<Func<T, bool>> filter)

IEnumerable<T> Find(Expression<Func<T, bool>> filter, int pageIndex, int size)
IEnumerable<T> Find(Expression<Func<T, bool>> filter, Expression<Func<T, object>> order, int pageIndex, int size)

IEnumerable<T> FindAll(int pageIndex, int size)
IEnumerable<T> FindAll(Expression<Func<T, object>> order, int pageIndex, int size)

T First()
T First(FilterDefinition<T> filter)
T First(Expression<Func<T, bool>> filter)
T First(Expression<Func<T, bool>> filter, Expression<Func<T, object>> order)
T First(Expression<Func<T, bool>> filter, Expression<Func<T, object>> order, bool isDescending)

Task<T> FirstAsync(FilterDefinition<T> filter)
Task<T> FirstAsync(Expression<Func<T, bool>> filter)

T Last()
T Last(FilterDefinition<T> filter)
T Last(Expression<Func<T, bool>> filter)
T Last(Expression<Func<T, bool>> filter, Expression<Func<T, object>> order)
T Last(Expression<Func<T, bool>> filter, Expression<Func<T, object>> order, bool isDescending)

void Replace(IEnumerable<T> entities)

T FindOneAndUpdate(FilterDefinition<T> filter, UpdateDefinition<T> update, FindOneAndUpdateOptions<T> options = null)
T FindOneAndUpdate(Expression<Func<T, bool>> filter, UpdateDefinition<T> update, FindOneAndUpdateOptions<T> options = null)

Task<T> FindOneAndUpdateAsync(FilterDefinition<T> filter, UpdateDefinition<T> update, FindOneAndUpdateOptions<T> options = null)
Task<T> FindOneAndUpdateAsync(Expression<Func<T, bool>> filter, UpdateDefinition<T> update, FindOneAndUpdateOptions<T> options = null)

bool Update<TField>(T entity, Expression<Func<T, TField>> field, TField value)

Task<bool> UpdateAsync<TField>(T entity, Expression<Func<T, TField>> field, TField value)

bool Any(Expression<Func<T, bool>> filter)

bool Update<TField>(FilterDefinition<T> filter, Expression<Func<T, TField>> field, TField value)
bool Update(FilterDefinition<T> filter, params UpdateDefinition<T>[] updates)
bool Update(Expression<Func<T, bool>> filter, params UpdateDefinition<T>[] updates)

Task<bool> UpdateAsync(FilterDefinition<T> filter, params UpdateDefinition<T>[] updates)
Task<bool> UpdateAsync(Expression<Func<T, bool>> filter, params UpdateDefinition<T>[] updates)
Task<bool> UpdateAsync<TField>(FilterDefinition<T> filter, Expression<Func<T, TField>> field, TField value)

long EstimatedCount()
long Count(Expression<Func<T, bool>> filter)
long EstimatedCount(EstimatedDocumentCountOptions options)

Task<long> EstimatedCountAsync()
Task<long> CountAsync(Expression<Func<T, bool>> filter)
Task<long> EstimatedCountAsync(EstimatedDocumentCountOptions options)
```"
33,prof-membrane/repository.membrane,Python,"# Wilkommen zu meinem Kodi Krypton Repo

Dieses Repo stellt Add-ons zu verschiedenen Mediatheken zur Verfügung. Dies kann [hier](https://github.com/prof-membrane/repository.membrane/raw/master/repository.membrane.zip) heruntergeladen werden. Im kodi.tv Wiki gibt es eine [Installationsanleitung](http://kodi.wiki/view/HOW-TO:Install_add-ons_from_zip_files). Im [Wiki](https://github.com/prof-membrane/repository.membrane/wiki) finden sich Informationen zu den einzelnen Add-ons. 

Für Bugreports oder Veränderungswünsche geht bitte [wie im Wiki beschrieben](https://github.com/prof-membrane/repository.membrane/wiki/Wie-Bugs-und-Featurerequests-zu-%C3%BCbermitteln-sind) vor.

# Inhalt des Repos

## Mediatheken
- 3sat Mediathek
- ARD Mediathek
- BR Mediathek
- Das Erste Mediathek
- Funk Mediathek
- HR Mediathek
- KiKa Mediathek
- MDR Mediathek
- NDR Mediathek
- Phoenix Mediathek
- SR Mediathek
- SWR Mediathek
- WDR Mediathek
- ZDF Mediathek

## Sonstige Add-ons
- Sportschau
- Unithek
- WDR Rockpalast

## Noch nicht hinzugefügte Add-ons
- NHL Videocenter
- PVR RePlay Beta
- ServusTV Mediathek


# Jarvis und ältere Versionen
Es existiert ein Fork, in dem alte Versionen der Add-ons aufbewahrt werden. Diese werden jedoch nicht gepflegt werden und früher oder später Fehler entwickeln. Diese Add-ons werden NICHT unterstützt!

https://github.com/prof-membrane/repository.membrane/tree/jarvis"
34,DSpace/xoai,Java,"# XOAI

What is XOAI?

XOAI is the most powerful and flexible OAI-PMH Java Toolkit (initially developed by Lyncode). XOAI contains common Java classes allowing to easily 
implement OAI-PMH data and service providers.

- - - 

Maven
-----

XOAI could be integrated with maven support.

**Moving**: XOAI is now being moved to the DSpace umbrella
	
Changes
-------

**4.2.0**

Bug Fixes:
- Close stream in ListSetsHandler (see #52 and #53)
- Updated dependency Apache httpclient to 4.5.3 (see #63)

Enhancements:
- Add user agent to HttpOAIClient (see #61)
- Handle cases when unknown metadataPrefix is received (see #54)

**4.0.0 to 4.1.0**

- Several fixes on the Service provider
- Supporting granularity on parameters (service provider) 

**3.2.7 to 4.0.0**

- Centralized OAI-PMH model (for data and service provider)
- Data provider configuration simplified
- Service provider made lazy
- Service provider with pipeline processing (centralized xoai schema)


License
-------         

[DSpace BSD License](https://raw.github.com/DSpace/DSpace/master/LICENSE)
"
35,xiaoxiaoqingyi/mine-android-repository,Java,"# mine-android-repository
这个是我平时项目中一些自己封装的代码库，如自定义控件、dialog、小模块等。
  使用正则表达式来匹配 文本，然后高亮显示，点击跳到相应的页面。 如微博为的 @某人， #新闻热搜#  话题。
  
  平时的学习资源、经典案例
  https://github.com/xiaoxiaoqingyi/mine-study-resource
"
36,Azure-Samples/PartitionedRepository,C#,"---
page_type: sample
languages:
- csharp
- powershell
products:
- azure
description: ""This project implements a to-do service using ASP.NET Core. It demonstrates how to use the repository pattern with Azure Cosmos DB SQL API.""
urlFragment: repository-pattern-with-azure-cosmos-db-sql-api
---

# Repository Pattern with Azure Cosmos DB SQL API

This project implements a sample to-do service using ASP.NET core. It demonstrates how to use the repository pattern with Azure Cosmos DB SQL API. It also demonstrates other best practices for Azure hosted ASP.NET core web applications, such as logging and telemetry with Application Insights and key/secret management via Azure Key Vault.

### Repository Pattern
This repository is based on the [repository design pattern](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/repository-pattern?view=aspnetcore-2.1) which isolates data access behind interface abstractions. Connecting to the database and manipulating data storage objects is performed through methods provided by the interface's implementation. Consequently, there is no need to call code to deal with database concerns, such as connections, commands, and readers.

Azure Cosmos DB utilizes [partition keys](https://docs.microsoft.com/en-us/azure/cosmos-db/partition-data) to enable quick look ups in unlimited scaled databases. This repository implements the repository design pattern to support partition keys for Cosmos DB. For details on the execution, please see the code within `./TodoService.Infrastructure/Data`.

### Benefits of the Partitioned Repository Pattern Implementation

* Easy reuse of the database access code, because the database communications is centralized in a single place.
* The business domain can be unit tested independent off the database layer.
* Integrated partition key support for large scale Cosmos DB projects.

## Getting Started

### Deploy Azure Resources Using a PowerShell Script
A script for deploying the necessary Cosmos DB resources is located in the **AzureResources.DeploymentScript** folder. The script will check the existence of a resource group, a Cosmos DB account, a Cosmos DB database, and one or more Cosmos DB collections with partition keys. If the defined resources do not exist they will be created by the script.

The following Parameters are inputs to the script:

- **resourceGroup**: where the Cosmos DB instance be created
- **location**: where the ResourceGroup and Cosmos DB will be hosted
- **cosmosAccount**: Cosmos DB account name
- **cosmosDatabase**: Cosmos DB database name
- **cosmosCollections**: Cosmos DB collection names separated by comma delimiter i.e col1,col2,col3
- **cosmosCollectionsPartitionKeys**: Cosmos DB collection partition key names separated by comma delimiter i.e PK1,PK2,PK3

### Prerequisites
 - [ASP.NET Core SDK v2.1.300](https://www.microsoft.com/net/download/thank-you/dotnet-sdk-2.1.300-windows-x64-installer)
 - [ASP.NET Core Runtime 2.1.0](https://www.microsoft.com/net/download/thank-you/dotnet-runtime-2.1.0-windows-hosting-bundle-installer)
 - [Visual Studio 2017 15.7 or newer](https://docs.microsoft.com/en-us/visualstudio/install/update-visual-studio)

### Build and Test
The configuration for this solution depends on secrets, either stored locally or in Azure Key Vault. This is to prevent sensitive information from being stored in a public manner. Please see [SECRET_MANAGEMENT.md](./SECRET_MANAGEMENT.md) for more information on setting up the secret management system.

### Manually Deploying the Solution
The solution can be run locally for development purposes. If you are looking to deploy the solution to an Azure Web App, follow the steps in [MANUAL_DEPLOYMENT.md](./MANUAL_DEPLOYMENT.md)


## Resources

* Repository Design Pattern: https://docs.microsoft.com/en-us/aspnet/core/fundamentals/repository-pattern?view=aspnetcore-2.1
* Azure Free Account : https://azure.microsoft.com/en-us/free/
* Azure App Service : https://azure.microsoft.com/en-us/services/app-service/
* Azure Cosmos DB : https://azure.microsoft.com/en-us/services/cosmos-db/
* Swagger : https://swagger.io/
"
37,adlerpagliarini/RepositoryPattern-Dapper-EFCore,C#,"# <a href=""https://medium.com/@adlerpagliarini"" target=""_blank"">Medium - @adlerpagliarini</a>

# C#�.Net Core - Criando uma aplica��o utilizando Repository Pattern com dois ORMs diferentes Dapper e Entity Framework Core

# <a href=""https://medium.com/@adlerpagliarini/97e8aa6ca35"" target=""_blank"">Parte�1</a>
- Modelagem das classes de dom�nio.
- Cria��o do banco de dados com Entity Framework Core e migrations.

# <a href=""https://medium.com/@adlerpagliarini/a821d501e317"" target=""_blank"">Parte�2</a>
- Reposit�rios gen�ricos e espec�ficos com Entity Framework Core.
- Testes integrados com utiliza��o de transa��es

# <a href=""https://medium.com/@adlerpagliarini/bf5373206ac"" target=""_blank"">Parte�3</a>
- Reposit�rios gen�ricos e espec�ficos com Dapper.
- Testes integrados com utiliza��o de transa��es

# <a href=""https://medium.com/@adlerpagliarini/8a4157e88d61"" target=""_blank"">Parte�4</a>
- Camada de servi�os e inje��o de depend�ncia.

# <a href=""https://medium.com/@adlerpagliarini/44093e0083c7"" target=""_blank"">Parte�5</a>
- Asp.Net Core Web Application

#
<img src=""https://github.com/adlerpagliarini/RepositoryPattern-Dapper-EFCore/blob/master/screen.JPG"" />"
38,Yara-Rules/rules,YARA,"[![Build Status](https://travis-ci.org/Yara-Rules/rules.svg?branch=master)](https://travis-ci.org/Yara-Rules/rules) <img src=""http://img.shields.io/liberapay/patrons/yararules.svg?logo=liberapay"">


# Project

This project covers the need of a group of IT Security Researchers to have a single repository where different Yara signatures are compiled, classified and kept as up to date as possible, and began as an open source community for collecting Yara rules. Our Yara ruleset is under the GNU-GPLv2 license and open to any user or organization, as long as you use it under this license.

Yara is becoming increasingly used, but knowledge about the tool and its usage is dispersed across many different places. The Yara Rules project aims to be the meeting point for Yara users by gathering together a ruleset as complete as possible thusly providing users a quick way to get Yara ready for usage.

We hope this project is useful for the Security Community and all Yara Users, and are looking forward to your feedback. Join this community by subscribing to our mailing list.

# Contribute

If you’re interested in sharing your Yara rules with us and the Security Community, you can join our mailing list, send a message to our Twitter account or send a pull request here.

Twitter account: https://twitter.com/yararules

# Requirements

Yara **version 3.0** or higher is required for most of our rules to work. This is mainly due to the use of the ""pe"" module introduced in that version.

You can check your installed version with `yara -v`

Packages available in Ubuntu 14.04 LTS default repositories are too old.  You can alternatively install from source or use the packages available in the [Remnux repository](https://launchpad.net/~remnux/+archive/ubuntu/stable).

~~Also, you will need [Androguard Module](https://github.com/Koodous/androguard-yara) if you want to use the rules in the 'mobile_malware' category.~~

We have deprecated mobile_malware rules that depend on Androguard Module because it seems an abandoned project.

# Categories

## Anti-debug/Anti-VM

In this section you will find Yara Rules aimed toward the detection of anti-debug and anti-virtualization techniques used by malware to evade automated analysis.

## Capabilities

In this section you will find Yara rules to detect capabilities that do not fit into any of the other categories.  They are useful to know for analysis but may not be malicious indicators on their own.

## CVE Rules

In this section you will find Yara Rules specialised toward the identification of specific Common Vulnerabilities and Exposures (CVEs)

## Crypto

In this section you will find Yara rules aimed toward the detection and existence of cryptographic algorithms.

## Exploit Kits

In this section you will find Yara rules aimed toward the detection and existence of Exploit Kits.

## Malicious Documents

In this section you will find Yara Rules to be used with documents to find if they have been crafted to leverage malicious code.

## Malware

In this section you will find Yara rules specialised toward the identification of well-known malware.

## Packers

In this section you will find Yara Rules aimed to detect well-known software packers, that can be used by malware to hide itself.

## WebShells

In this section you will find Yara rules specialised toward the identification of well-known webshells.

## Email

In this section you will find Yara rules specialised toward the identification of malicious e-mails.

## Malware Mobile

In this section you will find Yara rules specialised toward the identification of well-known mobile malware.

## Deprecated

In this section you will find Yara rules deprecated.

# Contact

Webpage: https://yara-rules.github.io/blog/

Twitter account: https://twitter.com/yararules

"
39,racket/racket,Scheme,"[Racket](https://racket-lang.org/) is a general-purpose programming
language and an ecosystem for language-oriented programming.

This repository holds the source code for the core of Racket plus some
related packages. The rest of the Racket distribution source code is
in other repositories, mostly under [the Racket GitHub
organization](https://github.com/racket).

Quick Start
-----------

Pre-built versions of Racket for a variety of operating systems and
architectures, as well as convenient source distributions are
available at

  [https://download.racket-lang.org](https://download.racket-lang.org)

Racket comes with extensive documentation, including several tutorials.
You can read all of this documentation, as well as documentation for
third-party packages at

  [https://docs.racket-lang.org](https://docs.racket-lang.org)

Building from Source
--------------------

For information on building Racket from this repository, see the
[Build Guide](build.md).

Contributing
------------

Contribute to Racket by [submitting a pull request](https://github.com/racket/racket), joining the
[development mailing list](https://lists.racket-lang.org), or visiting
the [IRC](https://botbot.me/freenode/racket/) or [Slack](https://racket-slack.herokuapp.com/) channels.

By making a contribution, you are agreeing that your contribution is
licensed under the LGPLv3, Apache 2.0, and MIT licenses. Those
licenses are available in this repository in the files
racket/src/LICENSE-LGPL.txt, racket/src/LICENSE-APACHE.txt, and
racket/src/LICENSE-MIT.txt.

See the [Racket Build Guide](build.md) for more guidance on
contributing.

The [Friendly Environment Policy](https://racket-lang.org/friendly.html) contains guidelines on expected behavior within the Racket community.

License
-------

Racket is free software; see [LICENSE](LICENSE) for more details.
"
40,oamg/leapp-repository,Python,"**Before doing anything, please read
[Leapp framework documentation](https://leapp.readthedocs.io/).**

---

## Troubleshooting

### Where can I report an issue or RFE related to the framework or other actors?

- GitHub issues are preferred:
  - Leapp framework: [https://github.com/oamg/leapp/issues/new/choose](https://github.com/oamg/leapp/issues/new/choose)
  - Leapp actors: [https://github.com/oamg/leapp-repository/issues/new/choose](https://github.com/oamg/leapp-repository/issues/new/choose)

- When filing an issue, include:
  - Steps to reproduce the issue
  - *All files in /var/log/leapp*
  - */var/lib/leapp/leapp.db*
  - *journalctl*
  - If you want, you can optionally send anything else would you like to provide (e.g. storage info)

**For your convenience you can pack all logs with this command:**

`# tar -czf leapp-logs.tgz /var/log/leapp /var/lib/leapp/leapp.db`

Then you may attach only the `leapp-logs.tgz` file.

### Where can I seek help?
We’ll gladly answer your questions and lead you to through any troubles with the
actor development.

You can reach us at IRC: `#leapp` on freenode.
"
41,xmake-io/xmake-repo,Lua,"<div align=""center"">
  <a href=""https://xmake.io"">
    <img width=""160"" heigth=""160"" src=""https://tboox.org/static/img/xmake/logo256c.png"">
  </a>  

  <h1>xmake-repo</h1>

  <div>
    <a href=""https://github.com/xmake-io/xmake-repo/actions?query=workflow%3AWindows"">
      <img src=""https://img.shields.io/github/workflow/status/xmake-io/xmake-repo/Windows/dev.svg?style=flat-square&logo=windows"" alt=""github-ci"" />
    </a>
    <a href=""https://github.com/xmake-io/xmake-repo/actions?query=workflow%3ALinux"">
      <img src=""https://img.shields.io/github/workflow/status/xmake-io/xmake-repo/Linux/dev.svg?style=flat-square&logo=linux"" alt=""github-ci"" />
    </a>
    <a href=""https://github.com/xmake-io/xmake-repo/actions?query=workflow%3AmacOS"">
      <img src=""https://img.shields.io/github/workflow/status/xmake-io/xmake-repo/macOS/dev.svg?style=flat-square&logo=apple"" alt=""github-ci"" />
    </a>
    <a href=""https://github.com/xmake-io/xmake-repo/actions?query=workflow%3AAndroid"">
      <img src=""https://img.shields.io/github/workflow/status/xmake-io/xmake-repo/Android/dev.svg?style=flat-square&logo=android"" alt=""github-ci"" />
    </a>
  </div>
  <div>
    <a href=""https://www.reddit.com/r/tboox/"">
      <img src=""https://img.shields.io/badge/chat-on%20reddit-ff3f34.svg?style=flat-square"" alt=""Reddit"" />
    </a>
    <a href=""https://gitter.im/tboox/tboox?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge"">
      <img src=""https://img.shields.io/gitter/room/tboox/tboox.svg?style=flat-square&colorB=96c312"" alt=""Gitter"" />
    </a>
    <a href=""https://t.me/tbooxorg"">
      <img src=""https://img.shields.io/badge/chat-on%20telegram-blue.svg?style=flat-square"" alt=""Telegram"" />
    </a>
    <a href=""https://jq.qq.com/?_wv=1027&k=5hpwWFv"">
      <img src=""https://img.shields.io/badge/chat-on%20QQ-ff69b4.svg?style=flat-square"" alt=""QQ"" />
    </a>
    <a href=""https://discord.gg/XXRp26A4Gr"">
      <img src=""https://img.shields.io/badge/chat-on%20discord-7289da.svg?style=flat-square"" alt=""Discord"" />
    </a>
    <a href=""https://github.com/xmake-io/xmake-repo/blob/master/LICENSE.md"">
      <img src=""https://img.shields.io/github/license/xmake-io/xmake-repo.svg?colorB=f48041&style=flat-square"" alt=""license"" />
    </a>
    <a href=""http://xmake.io/pages/donation.html#donate"">
      <img src=""https://img.shields.io/badge/donate-us-orange.svg?style=flat-square"" alt=""Donate"" />
    </a>
  </div>

  <p>An official xmake package repository</p>
</div>

## Supporting the project

Support this project by becoming a sponsor. Your logo will show up here with a link to your website. 🙏 [[Become a sponsor](https://xmake.io/#/about/sponsor)]

<a href=""https://opencollective.com/xmake#backers"" target=""_blank""><img src=""https://opencollective.com/xmake/backers.svg?width=890""></a>

## Introduction ([中文](/README_zh.md))

xmake-repo is an official xmake package repository. 

## Package dependences

<img src=""https://xmake.io/assets/img/index/add_require.png"" width=""70%"" />

## Package management

<img src=""https://xmake.io/assets/img/index/package_manage.png"" width=""80%"" />

If you want to know more, please refer to the xmake documentation:

* [Documents](https://xmake.io/#/package/remote_package)
* [Github](https://github.com/xmake-io/xmake)
* [HomePage](https://xmake.io)

## Xrepo

xrepo is a cross-platform C/C++ package manager based on [Xmake](https://github.com/xmake-io/xmake).

It is based on the runtime provided by xmake, but it is a complete and independent package management program. Compared with package managers such as vcpkg/homebrew, xrepo can provide C/C++ packages for more platforms and architectures at the same time.

If you want to know more, please refer to the xrepo documentation: 

* [Documents](https://xrepo.xmake.io/#/getting_started) 
* [Github](https://github.com/xmake-io/xrepo) 
* [Gitee](https://gitee.com/tboox/xrepo)

![](https://xrepo.xmake.io/assets/img/xrepo.gif)

## Submit package to repository

Write a xmake.lua of new package in `packages/x/xxx/xmake.lua` and push a pull-request to the dev branch.

For example, [packages/z/zlib/xmake.lua](https://github.com/xmake-io/xmake-repo/blob/dev/packages/z/zlib/xmake.lua):

If you want to known more, please see: [Create and Submit packages to the official repository](https://xmake.io/#/package/remote_package?id=submit-packages-to-the-official-repository)


"
42,ErickWendel/generic-repository-nodejs-typescript-article,TypeScript,"## Patterns — Generic Repository with Typescript and Node.js - Article

That examples are from post writes on Medium, about Typescript and Generic Reposity Pattern. 

### Running
You need download and install the following requirements

```
Node.js 8+
Typescript 2.6.2+
MongoDB 3.6+
VSCode
```
- After install run
```
    npm i -g typescript 
    tsc 
    node lib/Index.js
```


### Links
    - "
43,cpina/github-action-push-to-another-repository,Shell,"# github-action-push-to-another-repository

When to use this GitHub Action? It is useful in case that you have a GitHub repository with a a directory that you want to push to another GitHub repository using GitHub Actions (automated on push, for example). It is also useful if using GitHub Actions you generate certain files that you want to push to another GitHub repository.

Flow:

The [example repository](https://github.com/cpina/push-to-another-repository-example) has a MarkDown file [main.md](https://github.com/cpina/push-to-another-repository-example/blob/main/main.md)), during the [GitHub Actions flow](https://github.com/cpina/push-to-another-repository-example/blob/main/.github/workflows/ci.yml#L19) it executes [build.sh](https://github.com/cpina/push-to-another-repository-example/blob/main/build.sh) and the output/ directory (configurable via [source-directory](https://github.com/cpina/push-to-another-repository-example/blob/main/.github/workflows/ci.yml#L27) appears in the [output repository](https://github.com/cpina/push-to-another-repository-output).

Please bear in mind: files in the target repository are deleted. This is to make sure that it contains only the generated files in the last run without previously generated files.

There are different variables to setup the behaviour:

## Inputs
### `source-directory` (argument)
From the repository that this Git Action is executed the directory that contains the files to be pushed into the repository.

### `destination-github-username` (argument)
For the repository `https://github.com/cpina/push-to-another-repository-output` is `cpina`.

### `destination-repository-name` (argument)
For the repository `https://github.com/cpina/push-to-another-repository-output` is `push-to-another-repository-output`

*Warning:* this Github Action currently deletes all the files and directories in the destination repository. The idea is to copy from an `output` directory into the `destination-repository-name` having a copy without any previous files there.

### `user-email` (argument)
The email that will be used for the commit in the destination-repository-name.

### `user-name` (argument) [optional]
The name that will be used for the commit in the destination-repository-name. If not specified, the `destination-github-username` will be used instead.

### `destination-repository-username` (argument) [optional]
The Username/Organization for the destination repository, if different from `destination-github-username`. For the repository `https://github.com/cpina/push-to-another-repository-output` is `cpina`.

### `target-branch` (argument) [optional]
The branch name for the destination repository. It defaults to `main` for historical reasons, feel free to change it to `main`.

### `commit-message` (argument) [optional]
The commit message to be used in the output repository. Optional and defaults to ""Update from $REPOSITORY_URL@commit"".

The string `ORIGIN_COMMIT` is replaced by `$REPOSITORY_URL@commit`.

### `API_TOKEN_GITHUB` (environment)
E.g.:
  `API_TOKEN_GITHUB: ${{ secrets.API_TOKEN_GITHUB }}`

Generate your personal token following the steps:
* Go to the Github Settings (on the right hand side on the profile picture)
* On the left hand side pane click on ""Developer Settings""
* Click on ""Personal Access Tokens"" (also available at https://github.com/settings/tokens)
* Generate a new token, choose ""Repo"". Copy the token.

Then make the token available to the Github Action following the steps:
* Go to the Github page for the repository that you push from, click on ""Settings""
* On the left hand side pane click on ""Secrets""
* Click on ""Add a new secret"" and name it ""API_TOKEN_GITHUB""

## Example usage
```yaml
      - name: Pushes to another repository
        uses: cpina/github-action-push-to-another-repository@main
        env:
          API_TOKEN_GITHUB: ${{ secrets.API_TOKEN_GITHUB }}
        with:
          source-directory: 'output'
          destination-github-username: 'cpina'
          destination-repository-name: 'pandoc-test-output'
          user-email: carles3@pina.cat
          target-branch: main
```

Working example:

https://github.com/cpina/push-to-another-repository-example/blob/main/.github/workflows/ci.yml

It generates files from:
https://github.com/cpina/push-to-another-repository-example

To:
https://github.com/cpina/push-to-another-repository-output
"
44,rinvex/laravel-repositories,PHP,"# Rinvex Repository

⚠️ **This package is abandoned and no longer maintained. No replacement package was suggested.** ⚠️

👉 [Contact me if you are interested in maintaining it!](https://twitter.com/Omranic)


![Rinvex Repository Diagram](https://rinvex.com/assets/frontend/layout/img/products/rinvex.repository.v2.diagram.png)

**Rinvex Repository** is a simple, intuitive, and smart implementation of Active Repository with extremely flexible & granular caching system for Laravel, used to abstract the data layer, making applications more flexible to maintain.

[![Packagist](https://img.shields.io/packagist/v/rinvex/laravel-repositories.svg?label=Packagist&style=flat-square)](https://packagist.org/packages/rinvex/laravel-repositories)
[![Scrutinizer Code Quality](https://img.shields.io/scrutinizer/g/rinvex/laravel-repositories.svg?label=Scrutinizer&style=flat-square)](https://scrutinizer-ci.com/g/rinvex/laravel-repositories/)
[![Travis](https://img.shields.io/travis/rinvex/laravel-repositories.svg?label=TravisCI&style=flat-square)](https://travis-ci.org/rinvex/laravel-repositories)
[![StyleCI](https://styleci.io/repos/61269204/shield)](https://styleci.io/repos/61269204)
[![License](https://img.shields.io/packagist/l/rinvex/laravel-repositories.svg?label=License&style=flat-square)](https://github.com/rinvex/laravel-repositories/blob/develop/LICENSE)


💡 If you are looking for **Laravel 5.5** support, use the `dev-develop` branch. It's stable but not tagged yet since test suites isn't complete. 💡


## Features

- Cache, Cache, Cache!
- Prevent code duplication.
- Reduce potential programming errors.
- Granularly cache queries with flexible control.
- Apply centrally managed, consistent access rules and logic.
- Implement and centralize a caching strategy for the domain model.
- Improve the code’s maintainability and readability by separating client objects from domain models.
- Maximize the amount of code that can be tested with automation and to isolate both the client object and the domain model to support unit testing.
- Associate a behavior with the related data. For example, calculate fields or enforce complex relationships or business rules between the data elements within an entity.


## Quick Example (TL;DR)

The `Rinvex\Repository\Repositories\BaseRepository` is an abstract class with bare minimum that concrete implementations must extend.

The `Rinvex\Repository\Repositories\EloquentRepository` is currently the only available repository implementation (more to come in the future and [you can develop your own](#add-custom-implementation)), it makes it easy to create new eloquent model instances and to manipulate them easily. To use `EloquentRepository` your repository MUST extend it first:

```php
namespace App\Repositories;

use Rinvex\Repository\Repositories\EloquentRepository;

class FooRepository extends EloquentRepository
{
    protected $repositoryId = 'rinvex.repository.uniqueid';

    protected $model = 'App\Models\User';
}
```
That's it, you're done! Yes, it's that simple.

But if you'd like more control over the container instance, or would like to pass model name dynamically you can alternatively do as follow:

```php
namespace App\Repositories;

use Illuminate\Contracts\Container\Container;
use Rinvex\Repository\Repositories\EloquentRepository;

class FooRepository extends EloquentRepository
{
    // Instantiate repository object with required data
    public function __construct(Container $container)
    {
        $this->setContainer($container)
             ->setModel(\App\Models\User::class)
             ->setRepositoryId('rinvex.repository.uniqueid');

    }
}
```

Now inside your controller, you can either instantiate the repository traditionally through `$repository = new \App\Repositories\FooRepository();` or to use Laravel's awesome dependency injection and let the IoC do the magic:

```php
namespace App\Http\Controllers;

use App\Repositories\FooRepository;

class BarController
{
    // Inject `FooRepository` from the IoC
    public function baz(FooRepository $repository)
    {
        // Find entity by primary key
        $repository->find(1);

        // Find all entities
        $repository->findAll();

        // Create a new entity
        $repository->create(['name' => 'Example']);
    }
}
```

**Rinvex Repository Workflow - Create Repository**
![Rinvex Repository Workflow - Create Repository](https://rinvex.com/assets/frontend/layout/img/products/rinvex.repository.v2.workflow-1.gif)

**Rinvex Repository Workflow - Use In Controller**
![Rinvex Repository Workflow - Use In Controller](https://rinvex.com/assets/frontend/layout/img/products/rinvex.repository.v2.workflow-2.gif)

[UML Diagram](https://rinvex.com/assets/frontend/layout/img/products/rinvex.repository.v2.uml-diagram.png)

---

**Mission accomplished! You're good to use this package right now! :white_check_mark:**

**Unless you need to dig deeper & know some advanced stuff, you can skip the following steps! :wink:**

---


## Table Of Contents

- [Installation](#installation)
    - [Compatibility](#compatibility)
    - [Require Package](#require-package)
    - [Install Dependencies](#install-dependencies)
- [Integration](#integration)
    - [Native Integration](#native-integration)
    - [Laravel Integration](#laravel-integration)
- [Configuration](#configuration)
- [Usage](#usage)
    - [Quick Example](#quick-example)
    - [Detailed Documentation](#detailed-documentation)
        - [`setContainer()`, `getContainer()`](#setcontainer-getcontainer)
        - [`setConnection()`, `getConnection()`](#setconnection-getconnection)
        - [`setModel()`, `getModel()`](#setmodel-getmodel)
        - [`setRepositoryId()`, `getRepositoryId()`](#setrepositoryid-getrepositoryid)
        - [`setCacheLifetime()`, `getCacheLifetime()`](#setcachelifetime-getcachelifetime)
        - [`setCacheDriver()`, `getCacheDriver()`](#setcachedriver-getcachedriver)
        - [`enableCacheClear()`, `isCacheClearEnabled()`](#enablecacheclear-iscacheclearenabled)
        - [`createModel()`](#createmodel)
        - [`forgetCache()`](#forgetcache)
        - [`with()`](#with)
        - [`where()`](#where)
        - [`whereIn()`](#wherein)
        - [`whereNotIn()`](#wherenotin)
        - [`whereHas()`](#wherehas)
        - [`offset()`](#offset)
        - [`limit()`](#limit)
        - [`orderBy()`](#orderby)
        - [`find()`](#find)
        - [`findBy()`](#findby)
        - [`findFirst()`](#findFirst)
        - [`findAll()`](#findall)
        - [`paginate()`](#paginate)
        - [`simplePaginate()`](#simplepaginate)
        - [`findWhere()`](#findwhere)
        - [`findWhereIn()`](#findwherein)
        - [`findWhereNotIn()`](#findwherenotin)
        - [`findWhereHas()`](#findwherehas)
        - [`create()`](#create)
        - [`update()`](#update)
        - [`store()`](#store)
        - [`delete()`](#delete)
        - [`beginTransaction()`](#begintransaction)
        - [`commit()`](#commit)
        - [`rollBack()`](#rollback)
    - [Code To An Interface](#code-to-an-interface)
    - [Add Custom Implementation](#add-custom-implementation)
    - [EloquentRepository Fired Events](#eloquentrepository-fired-events)
    - [Mandatory Repository Conventions](#mandatory-repository-conventions)
    - [Automatic Guessing](#automatic-guessing)
    - [Flexible & Granular Caching](#flexible--granular-caching)
        - [Whole Application Cache](#whole-application-cache)
        - [Individual Query Cache](#individual-query-cache)
        - [Temporary Skip Individual HTTP Request Cache](#temporary-skip-individual-http-request-cache)
- [Final Thoughts](#final-thoughts)
- [Changelog](#changelog)
- [Support](#support)
- [Contributing & Protocols](#contributing--protocols)
- [Security Vulnerabilities](#security-vulnerabilities)
- [About Rinvex](#about-rinvex)
- [License](#license)


## Installation

The best and easiest way to install this package is through [Composer](https://getcomposer.org/).

### Compatibility

This package fully compatible with **Laravel** `5.1.*`, `5.2.*`, and `5.3.*`.

While this package tends to be framework-agnostic, it embraces Laravel culture and best practices to some extent. It's tested mainly with Laravel but you still can use it with other frameworks or even without any framework if you want.

### Require Package

Open your application's `composer.json` file and add the following line to the `require` array:
```json
""rinvex/laravel-repositories"": ""3.0.*""
```

> **Note:** Make sure that after the required changes your `composer.json` file is valid by running `composer validate`.

### Install Dependencies

On your terminal run `composer install` or `composer update` command according to your application's status to install the new requirements.

> **Note:** Checkout Composer's [Basic Usage](https://getcomposer.org/doc/01-basic-usage.md) documentation for further details.


## Integration

**Rinvex Repository** package is framework-agnostic and as such can be integrated easily natively or with your favorite framework.

### Native Integration

Integrating the package outside of a framework is incredibly easy, just require the `vendor/autoload.php` file to autoload the package.

> **Note:** Checkout Composer's [Autoloading](https://getcomposer.org/doc/01-basic-usage.md#autoloading) documentation for further details.

Run the following command on your terminal to publish config files:
```shell
php artisan vendor:publish --tag=""rinvex-repository-config""
```

> **Note:** Checkout Laravel's [Configuration](https://laravel.com/docs/master/#configuration) documentation for further details.

You are good to go. Integration is done and you can now use all the available methods, proceed to the [Usage](#usage) section for an example.


## Configuration

If you followed the previous integration steps, then your published config file reside at `config/rinvex.repository.php`.

Config options are very expressive and self explanatory, as follows:

```php
return [

    /*
    |--------------------------------------------------------------------------
    | Models Directory
    |--------------------------------------------------------------------------
    |
    | Here you may specify the default models directory, just write
    | directory name, like 'Models' not the full path.
    |
    | Default: 'Models'
    |
    */

    'models' => 'Models',

    /*
    |--------------------------------------------------------------------------
    | Caching Strategy
    |--------------------------------------------------------------------------
    */

    'cache' => [

        /*
        |--------------------------------------------------------------------------
        | Cache Keys File
        |--------------------------------------------------------------------------
        |
        | Here you may specify the cache keys file that is used only with cache
        | drivers that does not support cache tags. It is mandatory to keep
        | track of cache keys for later usage on cache flush process.
        |
        | Default: storage_path('framework/cache/rinvex.repository.json')
        |
        */

        'keys_file' => storage_path('framework/cache/rinvex.repository.json'),

        /*
        |--------------------------------------------------------------------------
        | Cache Lifetime
        |--------------------------------------------------------------------------
        |
        | Here you may specify the number of seconds that you wish the cache
        | to be remembered before it expires. If you want the cache to be
        | remembered forever, set this option to -1. 0 means disabled.
        |
        | Default: -1
        |
        */

        'lifetime' => -1,

        /*
        |--------------------------------------------------------------------------
        | Cache Clear
        |--------------------------------------------------------------------------
        |
        | Specify which actions would you like to clear cache upon success.
        | All repository cached data will be cleared accordingly.
        |
        | Default: ['create', 'update', 'delete']
        |
        */

        'clear_on' => [
            'create',
            'update',
            'delete',
        ],

        /*
        |--------------------------------------------------------------------------
        | Cache Skipping URI
        |--------------------------------------------------------------------------
        |
        | For testing purposes, or maybe some certain situations, you may wish
        | to skip caching layer and get fresh data result set just for the
        | current request. This option allows you to specify custom
        | URL parameter for skipping caching layer easily.
        |
        | Default: 'skipCache'
        |
        */

        'skip_uri' => 'skipCache',

    ],

];
```


## Usage

### Detailed Documentation

#### `setContainer()`, `getContainer()`

The `setContainer` method sets the IoC container instance, while `getContainer` returns it:

```php
// Set the IoC container instance
$repository->setContainer(new \Illuminate\Container\Container());

// Get the IoC container instance
$container = $repository->getContainer();
```

#### `setConnection()`, `getConnection()`

The `setConnection` method sets the connection associated with the repository, while `getConnection` returns it:

```php
// Set the connection associated with the repository
$repository->setConnection('mysql');

// Get the current connection for the repository
$connection = $repository->getConnection();
```

> **Note:** The name passed to the `setConnection` method should correspond to one of the connections listed in your `config/database.php` configuration file.

#### `setModel()`, `getModel()`

The `setModel` method sets the repository model, while `getModel` returns it:

```php
// Set the repository model
$repository->setModel(\App\Models\User::class);

// Get the repository model
$repositoryModel = $repository->getModel();
```

#### `setRepositoryId()`, `getRepositoryId()`

The `setRepositoryId` method sets the repository identifier, while `getRepositoryId` returns it (it could be anything you want, but must be **unique per repository**):

```php
// Set the repository identifier
$repository->setRepositoryId('rinvex.repository.uniqueid');

// Get the repository identifier
$repositoryId = $repository->getRepositoryId();
```

#### `setCacheLifetime()`, `getCacheLifetime()`

The `setCacheLifetime` method sets the repository cache lifetime, while `getCacheLifetime` returns it:

```php
// Set the repository cache lifetime
$repository->setCacheLifetime(123);

// Get the repository cache lifetime
$cacheLifetime = $repository->getCacheLifetime();
```

#### `setCacheDriver()`, `getCacheDriver()`

The `setCacheDriver` method sets the repository cache driver, while `getCacheDriver` returns it:

```php
// Set the repository cache driver
$repository->setCacheDriver('redis');

// Get the repository cache driver
$cacheDriver = $repository->getCacheDriver();
```

#### `enableCacheClear()`, `isCacheClearEnabled()`

The `enableCacheClear` method enables repository cache clear, while `isCacheClearEnabled` determines it's state:

```php
// Enable repository cache clear
$repository->enableCacheClear(true);

// Disable repository cache clear
$repository->enableCacheClear(false);

// Determine if repository cache clear is enabled
$cacheClearStatus = $repository->isCacheClearEnabled();
```

#### `createModel()`

The `createModel()` method creates a new repository model instance:

```php
$repositoryModelInstance = $repository->createModel();
```

#### `forgetCache()`

The `forgetCache()` method forgets the repository cache:

```php
$repository->forgetCache();
```

#### `with()`

The `with` method sets the relationships that should be eager loaded:

```php
// Pass a string
$repository->with('relationship');

// Or an array
$repository->with(['relationship1', 'relationship2']);
```

#### `where()`

The `where` method adds a basic where clause to the query:

```php
$repository->where('slug', '=', 'example');
```

#### `whereIn()`

The `whereIn` method adds a ""where in"" clause to the query:

```php
$repository->whereIn('id', [1, 2, 5, 8]);
```

#### `whereNotIn()`

The `whereNotIn` method adds a ""where not in"" clause to the query:

```php
$repository->whereNotIn('id', [1, 2, 5, 8]);
```

#### `whereHas()`

The `whereHas` method adds a ""where has relationship"" clause to the query:

```php
use Illuminate\Database\Eloquent\Builder;

$repository->whereHas('attachments', function (Builder $builder) use ($attachment) {
    $builder->where('attachment_id', $attachment->id);
});
```

> **Note:** All of the `where*` methods are chainable & could be called multiple times in a single request. It will hold all where clauses in an array internally and apply them all before executing the query.

#### `offset()`

The `offset` method sets the ""offset"" value of the query:

```php
$repository->offset(5);
```

#### `limit()`

The `limit` method sets the ""limit"" value of the query:

```php
$repository->limit(9);
```

#### `orderBy()`

The `orderBy` method adds an ""order by"" clause to the query:

```php
$repository->orderBy('id', 'asc');
```

#### `find()`

The `find` method finds an entity by it's primary key:

```php
$entity = $repository->find(1);
```

#### `findOrFail()`

The `findOrFail()` method finds an entity by its primary key or throw an exception:

```php
$entity = $repository->findOrFail(1);
```

#### `findOrNew()`

The `findOrNew()` method finds an entity by its primary key or return fresh entity instance:

```php
$entity = $repository->findOrNew(1);
```

#### `findBy()`

The `findBy` method finds an entity by one of it's attributes:

```php
$entity = $repository->findBy('id', 1);
```

#### `findFirst()`

The `findFirst` method finds first entity:

```php
$firstEntity = $repository->findFirst();
```

#### `findAll()`

The `findAll` method finds all entities:

```php
$allEntities = $repository->findAll();
```

#### `paginate()`

The `paginate` method paginates all entities:

```php
$entitiesPagination = $repository->paginate(15, ['*'], 'page', 2);
```
As you can guess, this query the first 15 records, in the second page.

#### `simplePaginate()`

The `simplePaginate` method paginates all entities into a simple paginator:

```php
$entitiesSimplePagination = $repository->simplePaginate(15);
```

#### `findWhere()`

The `findWhere` method finds all entities matching where conditions:

```php
// Matching values with equal '=' operator
$repository->findWhere(['slug', '=', 'example']);
```

#### `findWhereIn()`

The `findWhereIn` method finds all entities matching whereIn conditions:

```php
$includedEntities = $repository->findwhereIn(['id', [1, 2, 5, 8]]);
```

#### `findWhereNotIn()`

The `findWhereNotIn` method finds all entities matching whereNotIn conditions:

```php
$excludedEntities = $repository->findWhereNotIn(['id', [1, 2, 5, 8]]);
```

#### `findWhereHas()`

The `findWhereHas` method finds all entities matching whereHas conditions:

```php
use Illuminate\Database\Eloquent\Builder;

$entities = $repository->findWhereHas(['attachments', function (Builder $builder) use ($attachment) {
    $builder->where('attachment_id', $attachment->id);
}]);
```

> **Notes:**
> - The `findWhereHas` method will return a collection of entities that match the condition inside the closure. If you need to embed the `attachments` relation, in this case, you'll need to call `with()` method before calling `findWhereHas()` like this: `$repository->with('attachments')->findWhereHas([...]);`
> - Signature of all of the `findWhere`, `findWhereIn`, and `findWhereNotIn` methods has been changed since **v2.0.0**.
> - All of the `findWhere`, `findWhereIn`, and `findWhereNotIn` methods utilize the `where`, `whereIn`, and `whereNotIn` methods respectively, and thus takes first argument as an array of same parameters required by the later ones.
> - All of the `find*` methods are could be filtered with preceding `where` clauses, which is chainable by the way. All `where` clauses been hold in an array internally and applied before executing the query. Check the following examples:

Example of filtered `findAll` method:
```php
$allFilteredEntities = $repository->where('slug', '=', 'example')->findAll();
```

Another example of filtered `findFirst` method with chained clauses:
```php
$allFilteredEntities = $repository->where('name', 'LIKE', '%TEST%')->where('slug', '=', 'example')->findFirst();
```

#### `create()`

The `create` method creates a new entity with the given attributes:
```php
$createdEntity = $repository->create(['name' => 'Example']);
```

#### `update()`

The `update` method updates an entity with the given attributes:
```php
$updatedEntity = $repository->update(1, ['name' => 'Example2']);
```

#### `store()`

The `store` method stores the entity with the given attributes:
```php
// Existing Entity
$storedEntity = $repository->store(1, ['name' => 'Example2']);

// New Entity
$storedEntity = $repository->store(null, ['name' => 'Example2']);
```

> **Note:** This method is just an alias for both `create` & `update` methods. It's useful in case where single form is used for both create & update processes.

#### `delete()`

The `delete` method deletes an entity with the given id:
```php
$deletedEntity = $repository->delete(1);
```

#### `beginTransaction()`

The `beginTransaction` method starts a database transaction:
```php
$repository->beginTransaction();
```

#### `commit()`

The `commit` method commits a database transaction:
```php
$repository->commit();
```

#### `rollBack()`

The `rollback` method rollbacks a database transaction:
```php
$repository->rollBack();
```

> **Notes:**
> - All `find*` methods take one more optional parameter for selected attributes.
> - All `set*` methods returns an instance of the current repository, and thus can be chained.
> - `create`, `update`, and `delete` methods always return an array with two values, the first is action status whether it's success or fail as a boolean value, and the other is an instance of the model just operated upon.
> - It's recommended to set IoC container instance, repository model, and repository identifier explicitly through your repository constructor like the above example, but this package is smart enough to guess any missing requirements. [Check Automatic Guessing Section](#automatic-guessing)

### Code To An Interface

As a best practice, it's recommended to code for an interface, specifically for scalable projects. The following example explains how to do so.

First, create an interface (abstract) for every entity you've:
```php
use Rinvex\Repository\Contracts\CacheableContract;
use Rinvex\Repository\Contracts\RepositoryContract;

interface UserRepositoryContract extends RepositoryContract, CacheableContract
{
    //
}
```

Second, create a repository (concrete implementation) for every entity you've:
```php
use Rinvex\Repository\Repositories\EloquentRepository;

class UserEloquentRepository extends EloquentRepository implements UserRepositoryContract
{
    //
}
```

Now in a Laravel Service Provider bind both to the IoC (inside the `register` method):
```php
$this->app->bind(UserRepositoryContract::class, UserEloquentRepository::class)
```
This way we don't have to instantiate the repository manually, and it's easy to switch between multiple implementations. The IoC Container will take care of the required dependencies.

> **Note:** Checkout Laravel's [Service Providers](https://laravel.com/docs/master/providers) and [Service Container](https://laravel.com/docs/master/container) documentation for further details.

### Add Custom Implementation

Since we're focusing on abstracting the data layer, and we're separating the abstract interface from the concrete implementation, it's easy to add your own implementation.

Say your domain model uses a web service, or a filesystem data store as it's data source, all you need to do is just extend the `BaseRepository` class, that's it. See:
```php
class FilesystemRepository extends BaseRepository
{
    // Implement here all `RepositoryContract` methods that query/persist data to & from filesystem or whatever datastore
}
```

### EloquentRepository Fired Events

Repositories fire events at every action, like `create`, `update`, `delete`. All fired events are prefixed with repository's identifier (you set before in your [repository's constructor](#eloquentrepository)) like the following example:

- rinvex.repository.uniqueid.entity.created
- rinvex.repository.uniqueid.entity.updated
- rinvex.repository.uniqueid.entity.deleted

For your convenience, the events suffixed with `.entity.created`, `.entity.updated`, or `.entity.deleted` have listeners that take actions accordingly. Usually we need to flush cache -if enabled & exists- upon every success action.

There's one more event `rinvex.repository.uniqueid.entity.cache.flushed` that's fired on cache flush. It has no listeners by default, but you may need to listen to it if you've model relations for further actions.

### Mandatory Repository Conventions

Here some conventions important to know while using this package. This package adheres to best practices trying to make development easier for web artisans, and thus it has some conventions for standardization and interoperability.

- All Fired Events has a unique suffix, like `.entity.created` for example. Note the `.entity.` which is mandatory for automatic event listeners to subscribe to.

- Default directory structure of any package uses **Rinvex Repository** is as follows:
```
├── config                  --> config files
|
├── database
|   ├── factories           --> database factory files
|   ├── migrations          --> database migration files
|   └── seeds               --> database seed files
|
├── resources
|   └── lang
|       └── en              --> English language files
|
├── routes                  --> Routes files
|   ├── api.php
|   ├── console.php
|   └── web.php
|
├── src                     --> self explanatory directories
|   ├── Console
|   |   └── Commands
|   |
|   ├── Http
|   |   ├── Controllers
|   |   ├── Middleware
|   |   └── Requests
|   |
|   ├── Events
|   ├── Exceptions
|   ├── Facades
|   ├── Jobs
|   ├── Listeners
|   ├── Models
|   ├── Overrides
|   ├── Policies
|   ├── Providers
|   ├── Repositories
|   ├── Scopes
|   ├── Support
|   └── Traits
|
└── composer.json           --> composer dependencies file
```

> **Note:** **Rinvex Repository** adheres to [PSR-4: Autoloader](http://www.php-fig.org/psr/psr-4/) and expects other packages that uses it to adhere to the same standard as well. It's required for [Automatic Guessing](#automatic-guessing), such as when repository model is missing, it will be guessed automatically and resolved accordingly, and while that full directory structure might not required, it's the standard for all **Rinvex** packages.

### Automatic Guessing

While it's **recommended** to explicitly set IoC container, repository identifier, and repository model; This package is smart enough to guess any of these required data whenever missing.

- **IoC Container** `app()` helper is used as a fallback if IoC container instance not provided explicitly.
- **Repository Identifier** It's recommended to set repository identifier as a doted name like `rinvex.repository.uniqueid`, but if it's missing fully qualified repository class name will be used (actually the value of `static::class`).
- **Repository Model** Conventionally repositories are namespaced like this `Rinvex\Demos\Repositories\ItemRepository`, so corresponding model supposed to be namespaced like this `Rinvex\Demos\Models\Item`. That's how this packages guess the model if it's missing according to the [Default Directory Structure](#mandatory-repository-conventions).

### Flexible & Granular Caching

**Rinvex Repository** has a powerful, yet simple and granular caching system, that handles almost every edge case. While you can enable/disable your application's cache as a whole, you have the flexibility to enable/disable cache granularly for every individual query! That gives you the ability to except certain queries from being cached even if the method is normally cached by default or otherwise.

Let's see what caching levels we can control:

#### Whole Application Cache

Checkout Laravel's [Cache](https://laravel.com/docs/master/cache) documentation for more details.

#### Individual Query Cache

Change cache per query or disable it:
```php
// Set cache lifetime for this individual query to 123 seconds
$repository->setCacheLifetime(123);

// Set cache lifetime for this individual query to forever
$repository->setCacheLifetime(-1);

// Disable cache for this individual query
$repository->setCacheLifetime(0);
```

Change cache driver per query:
```php
// Set cache driver for this individual query to redis
$repository->setCacheDriver('redis');
```

Both `setCacheLifetime` & `setCacheDriver` methods are chainable:
```php
// Change cache lifetime & driver on runtime
$repository->setCacheLifetime(123)->setCacheDriver('redis')->findAll();

// Use default cache lifetime & driver
$repository->findAll();
```

Unless disabled explicitly, cache is enabled for all repositories by default, and kept for as long as your `rinvex.repository.cache.lifetime` config value, using default application's cache driver `cache.default` (which could be changed per query as well).

Caching results is totally up to you, while all retrieval `find*` methods have cache enabled by default, you can enable/disable cache for individual queries or control how it's being cached, for how long, and using which driver as you wish.

#### Temporary Skip Individual HTTP Request Cache

Lastly, you can skip cache for an individual request by passing the following query string in your URL `skipCache=true`. You can modify this parameter to whatever name you may need through the `rinvex.repository.cache.skip_uri` config option.


## Final Thoughts

- Since this is an evolving implementation that may change accordingly depending on real-world use cases.
- Repositories intelligently pass missing called methods to the underlying model, so you actually can implement any kind of logic, or even complex queries by utilizing the repository model.
- For more insights about the Active Repository implementation, I've published an article on the topic titled [Active Repository is good & Awesomely Usable](https://blog.omranic.com/active-repository-is-good-awesomely-usable-6991cfd58774), read it if you're interested.
- Repositories utilizes cache tags in a very smart way, even if your chosen cache driver doesn't support it. Repositories will manage it virtually on it's own for precise cache management. Behind scenes it uses a json file to store cache keys. Checkout the `rinvex.repository.cache.keys_file` config option to change file path.
- **Rinvex Repository** follows the FIG PHP Standards Recommendations compliant with the [PSR-1: Basic Coding Standard](http://www.php-fig.org/psr/psr-1/), [PSR-2: Coding Style Guide](http://www.php-fig.org/psr/psr-2/) and [PSR-4: Autoloader](http://www.php-fig.org/psr/psr-4/) to ensure a high level of interoperability between shared PHP code.
- I don't see the benefit of adding a more complex layer by implementing the **Criteria Pattern** for filtration at the moment, rather I'd prefer to keep it as simple as it is now using traditional where clauses since we can achieve same results. (do you've different thoughts? explain please)


## Changelog

Refer to the [Changelog](CHANGELOG.md) for a full history of the project.


## Support

The following support channels are available at your fingertips:

- [Chat on Slack](https://bit.ly/rinvex-slack)
- [Help on Email](mailto:help@rinvex.com)
- [Follow on Twitter](https://twitter.com/rinvex)


## Contributing & Protocols

Thank you for considering contributing to this project! The contribution guide can be found in [CONTRIBUTING.md](CONTRIBUTING.md).

Bug reports, feature requests, and pull requests are very welcome.

- [Versioning](CONTRIBUTING.md#versioning)
- [Pull Requests](CONTRIBUTING.md#pull-requests)
- [Coding Standards](CONTRIBUTING.md#coding-standards)
- [Feature Requests](CONTRIBUTING.md#feature-requests)
- [Git Flow](CONTRIBUTING.md#git-flow)


## Security Vulnerabilities

If you discover a security vulnerability within this project, please send an e-mail to [help@rinvex.com](help@rinvex.com). All security vulnerabilities will be promptly addressed.


## About Rinvex

Rinvex is a software solutions startup, specialized in integrated enterprise solutions for SMEs established in Alexandria, Egypt since June 2016. We believe that our drive The Value, The Reach, and The Impact is what differentiates us and unleash the endless possibilities of our philosophy through the power of software. We like to call it Innovation At The Speed Of Life. That’s how we do our share of advancing humanity.


## License

This software is released under [The MIT License (MIT)](LICENSE).

(c) 2016-2020 Rinvex LLC, Some rights reserved.
"
45,openvinotoolkit/openvino,C++,"# OpenVINO™ Toolkit
[![Stable release](https://img.shields.io/badge/version-2021.3-green.svg)](https://github.com/openvinotoolkit/openvino/releases/tag/2021.3)
[![Apache License Version 2.0](https://img.shields.io/badge/license-Apache_2.0-green.svg)](LICENSE)
![GitHub branch checks state](https://img.shields.io/github/checks-status/openvinotoolkit/openvino/master?label=GitHub%20checks)
![Azure DevOps builds (branch)](https://img.shields.io/azure-devops/build/openvinoci/b2bab62f-ab2f-4871-a538-86ea1be7d20f/13?label=Public%20CI)

This toolkit allows developers to deploy pre-trained deep learning models
through a high-level C++ Inference Engine API integrated with application logic.

This open source version includes several components: namely [Model Optimizer], [nGraph] and
[Inference Engine], as well as CPU, GPU, MYRIAD, multi device and heterogeneous plugins to accelerate deep learning inferencing on Intel® CPUs and Intel® Processor Graphics.
It supports pre-trained models from the [Open Model Zoo], along with 100+ open
source and public models in popular formats such as Caffe\*, TensorFlow\*,
MXNet\* and ONNX\*.

## Repository components:
* [Inference Engine]
* [nGraph]
* [Model Optimizer]

## License
Deep Learning Deployment Toolkit is licensed under [Apache License Version 2.0](LICENSE).
By contributing to the project, you agree to the license and copyright terms therein
and release your contribution under these terms.

## Resources:
* Docs: https://docs.openvinotoolkit.org/
* Wiki: https://github.com/openvinotoolkit/openvino/wiki
* Issue tracking: https://github.com/openvinotoolkit/openvino/issues
* Storage: https://storage.openvinotoolkit.org/
* Additional OpenVINO™ modules: https://github.com/openvinotoolkit/openvino_contrib
* [Intel® Distribution of OpenVINO™ toolkit Product Page](https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html)
* [Intel® Distribution of OpenVINO™ toolkit Release Notes](https://software.intel.com/en-us/articles/OpenVINO-RelNotes)

## Support
Please report questions, issues and suggestions using:

* The [`openvino`](https://stackoverflow.com/questions/tagged/openvino) tag on StackOverflow\*
* [GitHub* Issues](https://github.com/openvinotoolkit/openvino/issues)
* [Forum](https://software.intel.com/en-us/forums/computer-vision)

---
\* Other names and brands may be claimed as the property of others.

[Open Model Zoo]:https://github.com/opencv/open_model_zoo
[Inference Engine]:https://software.intel.com/en-us/articles/OpenVINO-InferEngine
[Model Optimizer]:https://software.intel.com/en-us/articles/OpenVINO-ModelOptimizer
[nGraph]:https://docs.openvinotoolkit.org/latest/openvino_docs_nGraph_DG_DevGuide.html
[tag on StackOverflow]:https://stackoverflow.com/search?q=%23openvino

"
46,ga4gh/data-repository-service-schemas,Python,"<img src=""https://www.ga4gh.org/wp-content/themes/ga4gh-theme/gfx/GA-logo-horizontal-tag-RGB.svg"" alt=""GA4GH Logo"" style=""width: 400px;""/>

# Data Repository Service (DRS) API

<sup>`develop` branch status: </sup>[![Build Status](https://travis-ci.org/ga4gh/data-repository-service-schemas.svg?branch=develop)](https://travis-ci.org/ga4gh/data-repository-service-schemas?branch=develop)
<a href=""https://ga4gh.github.io/data-repository-service-schemas/preview/develop/swagger.yaml""><img src=""http://online.swagger.io/validator?url=https://ga4gh.github.io/data-repository-service-schemas/preview/develop/swagger.yaml"" alt=""Swagger Validator"" height=""20em"" width=""72em""></A> [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1405753.svg)](https://doi.org/10.5281/zenodo.1405753)

<!--
[![Read the Docs badge](https://readthedocs.org/projects/data-repository-service/badge/)](https://data-repository-service.readthedocs.io/en/latest)
![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ga4gh-drs-schemas.svg)-->


The [Global Alliance for Genomics and Health](http://genomicsandhealth.org/) (GA4GH) is an international coalition, formed to enable the sharing of genomic and clinical data.

# About the GA4GH Cloud Work Stream

The GA4GH [Cloud Work Stream](http://ga4gh.cloud) helps the genomics and health communities take full advantage of modern cloud environments.
Our initial focus is on “bringing the algorithms to the data”, by creating standards for defining, sharing, and executing portable workflows.

We work with platform development partners and industry leaders to develop standards that will facilitate interoperability.

# What is DRS?

The Data Repository Service (DRS) API provides a generic interface to data repositories so data consumers, including workflow systems, can access data in a single, standardized way regardless of where it’s stored or how it’s managed.
The primary functionality of DRS is to map a logical ID to a means for physically retrieving the data represented by the ID.

For more information see our HTML documentation links in the table below.

# API Definition

|  **Branch** | **Reference Documentation** | **[OpenAPI YAML description](openapi/data_repository_service.swagger.yaml)** |
| --- | --- | --- |
| **master**: The current release | [HTML](https://ga4gh.github.io/data-repository-service-schemas/docs/) | [Swagger](https://ga4gh.github.io/data-repository-service-schemas/swagger-ui/#/DataRepositoryService/) |
| **develop**: the stable development branch, into which feature branches are merged | [HTML](https://ga4gh.github.io/data-repository-service-schemas/preview/develop/docs/) | [Swagger](https://ga4gh.github.io/data-repository-service-schemas/preview/develop/swagger-ui/#/DataRepositoryService/) |
| **release 1.1.0**: The 1.1.0 release of DRS that includes *no* API changes only documentation changes. This introduces a new URI convention using compact identifiers along with clear directions on how to use identifiers.org/n2t.net to resolve them. | [HTML](https://ga4gh.github.io/data-repository-service-schemas/preview/release/drs-1.1.0/docs/) | [Swagger](https://ga4gh.github.io/data-repository-service-schemas/preview/release/drs-1.1.0/swagger-ui/#/DataRepositoryService/) |
| **release 1.0.0**: The 1.0.0 release of DRS that is now an approved GA4GH standard | [HTML](https://ga4gh.github.io/data-repository-service-schemas/preview/release/drs-1.0.0/docs/) | [Swagger](https://ga4gh.github.io/data-repository-service-schemas/preview/release/drs-1.0.0/swagger-ui/#/DataRepositoryService/) |
| **release 0.1**: Simplifying DRS to core functionality | [HTML](https://ga4gh.github.io/data-repository-service-schemas/preview/release/drs-0.1.0/docs/) | [Swagger](https://ga4gh.github.io/data-repository-service-schemas/preview/release/drs-0.1.0/swagger-ui/#/DataRepositoryService/) |
| **release 0.0.1**: The initial DRS after the rename from DOS | [HTML](https://ga4gh.github.io/data-repository-service-schemas/preview/release/0.0.1/docs/) | [Swagger](https://ga4gh.github.io/data-repository-service-schemas/preview/release/0.0.1/swagger-ui/#/DataRepositoryService/) |

To monitor development work on various branches, add 'preview/\<branch-name\>' to the master URLs above (e.g., 'https://ga4gh.github.io/data-repository-service-schemas/preview/\<branch-name\>/docs').

# How to Contribute Changes

See [CONTRIBUTING.md](CONTRIBUTING.md).

If a security issue is identified with the specification, please send an email to security-notification@ga4gh.org detailing your concerns.

# License

See the [LICENSE](LICENSE).

# More Information

* [Global Alliance for Genomics and Health](http://genomicsandhealth.org)
* [GA4GH Cloud Work Stream](http://ga4gh.cloud)
"
47,popomore/projj,JavaScript,"# Projj

Manage repository easily.

[![NPM version][npm-image]][npm-url]
[![build status][travis-image]][travis-url]
[![Test coverage][codecov-image]][codecov-url]
[![David deps][david-image]][david-url]
[![Known Vulnerabilities][snyk-image]][snyk-url]
[![npm download][download-image]][download-url]

[npm-image]: https://img.shields.io/npm/v/projj.svg?style=flat-square
[npm-url]: https://npmjs.org/package/projj
[travis-image]: https://img.shields.io/travis/popomore/projj.svg?style=flat-square
[travis-url]: https://travis-ci.org/popomore/projj
[codecov-image]: https://codecov.io/gh/popomore/projj/branch/master/graph/badge.svg
[codecov-url]: https://codecov.io/gh/popomore/projj
[david-image]: https://img.shields.io/david/popomore/projj.svg?style=flat-square
[david-url]: https://david-dm.org/popomore/projj
[snyk-image]: https://snyk.io/test/npm/projj/badge.svg?style=flat-square
[snyk-url]: https://snyk.io/test/npm/projj
[download-image]: https://img.shields.io/npm/dm/projj.svg?style=flat-square
[download-url]: https://npmjs.org/package/projj

## Why?

How do you manage git repository?

Maybe you create a directory and clone to it. However if you want to clone repository that has same name? Or Do something in every directory like `clean`?

`Projj` provide a structure making it easy.

```
$BASE
|- github.com
|  `- popomore
|     `- projj
`- gitlab.com
   `- popomore
      `- projj
```

And you can `DO` everything in repository by [Hook](#hook).

## Feature

- ✔︎ Add repository using `projj add`
- ✔︎ Command Hook
- ✘ Buildin Hook
- ✔︎ Custom Hook
- ✔︎ Run Hook in All Repositories
- ✔︎ Git Support

## Installation

Install `projj` globally.

```bash
$ npm i projj -g
```

## Usage

### Initialize

```bash
$ projj init
```

Set base directory which repositories will be cloned to, default is `~/projj`.

You can change base directory in `~/.projj/config.json`.

### Add Repository

```bash
$ projj add git@github.com:popomore/projj.git
```

it's just like `git clone`, but the repository will be cached by projj. You can find all repositories in `~/.projj/cache.json`

also support alias which could config at `alias` of `~/.projj/config.json`:

```bash
$ projj add github://popomore/projj
```

### Importing

If you have some repositories in `~/code`, projj can import by `projj import ~/code`.

Or projj can import repositories from `cache.json` when you change laptop by `projj import --cache`

### Find Repository

projj provide a easy way to find the location of your repositories.

```bash
$ projj find [repo]
```

You can set `change_directory` in `~/.projj/config.json` to change directory automatically.

### Sync

`projj sync` will check the repository in cache.json whether exists, the repository will be removed from cache if not exist.

## Hook

Hook is flexible when manage repositories.

### Command Hook

When run command like `projj add`, hook will be run. `preadd` that run before `projj add`, and `postadd` that run after `projj add`.

Config hook in `~/.projj/config.json`

```json
{
  ""hooks"": {
    ""postadd"": ""cat package.json""
  }
}
```

Then will show the content of the package of repository.

**Only support `add` now**

### Define Hook

You can define own hook.

```json
{
  ""hooks"": {
    ""hook_name"": ""command""
  }
}
```

For Example, define a hook to show package.

```json
{
  ""hooks"": {
    ""show_package"": ""cat package.json""
  }
}
```

Then you can use `projj run show_package` to run the hook in current directory.

`Command` can be used in `$PATH`, so you can use global node_modules like `npm`.

```json
{
  ""hooks"": {
    ""npm_install"": ""npm install""
  }
}
```

### Write Hook

Write a command

```js
// clean
#!/usr/bin/env node

'use strict';

const cp = require('child_process');
const cwd = process.cwd();
const config = JSON.parse(process.env.PROJJ_HOOK_CONFIG);
if (config.node_modules === true) {
  cp.spawn('rm', [ '-rf', 'node_modules' ]);
}
```

You can get `PROJJ_HOOK_CONFIG` from `projj` if you have defined in `~/.projj/config.json`.

```json
{
  ""hooks"": {
    ""clean"": ""clean""
  },
  ""clean"": {
    ""node_modules"": true
  }
}
```

### Run Hook

`projj run clean` in current directory.

`projj runall clean` in every repositories from `cache.json`

## License

[MIT](LICENSE)
"
48,ezsystems/repository-forms,PHP,"# Repository forms

[![Build Status](https://img.shields.io/travis/ezsystems/repository-forms.svg?style=flat-square)](https://travis-ci.org/ezsystems/repository-forms)
[![Downloads](https://img.shields.io/packagist/dt/ezsystems/repository-forms.svg?style=flat-square)](https://packagist.org/packages/ezsystems/repository-forms)
[![Latest release](https://img.shields.io/github/release/ezsystems/repository-forms.svg?style=flat-square)](https://github.com/ezsystems/repository-forms/releases)
[![License](https://img.shields.io/github/license/ezsystems/repository-forms.svg?style=flat-square)](LICENSE)

repository-forms bundle provides form-based integration for the Symfony Forms into Repository Value objects in Kernel.
- [Repository documentation](https://doc.ezplatform.com/en/latest/guide/repository/)
- [Value objects](https://doc.ezplatform.com/en/latest/api/public_php_api/#value-objects)
- [Symfony bundle](https://doc.ezplatform.com/en/latest/api/public_php_api_customization/#symfony-bundle)
"
49,AllenDowney/ThinkBayes,TeX,"ThinkBayes
==========

Code repository for Think Bayes: Bayesian Statistics Made Simple
by Allen B. Downey

Available from Green Tea Press at http://thinkbayes.com.

Published by O'Reilly Media, October 2013.

"
50,GoogleCloudPlatform/repository-gardener,Shell,"# Repository Gardener

[![Actions Status][gh-actions-badge]][gh-actions]

The repository gardener maintains code samples by running some automatable
tasks. For example, it can automatically update dependencies and then after
running tests to ensure they still work, send a Pull Request for the update.

## Example

The following commands will clone the dotnet-docs-samples repository and update
its dependencies to the latest versions.

```shell
source set-env.sh \
  && git clone https://github.com/GoogleCloudPlatform/dotnet-repo-tools.git
  && ./clone-and-checkout.sh -b dpebot-updatedeps GoogleCloudPlatform/dotnet-docs-samples \
  && ( \
    cd repo-to-update \
    && ../use-latest-deps-dotnet.sh -d GoogleCloudPlatform/dotnet-docs-samples
 )
```

The `-d` option in `use-latest-deps-dotnet.sh` is to do a dry run (don't commit
or push). Remove `-d` to actually push and send PR.

## Contributing changes

* See [CONTRIBUTING.md](CONTRIBUTING.md)

## Licensing

* See [LICENSE](LICENSE)

## Disclaimer

This is not an official Google product or sample.

## Adding the bot to your repository (Googlers only)

A hosted version of the bot is [available](https://goto.google.com/dpebot) for Googlers.

[gh-actions]: https://github.com/GoogleCloudPlatform/repository-gardener/actions
[gh-actions-badge]: https://github.com/GoogleCloudPlatform/repository-gardener/workflows/CI%20Tests/badge.svg
"
51,rlidwka/sinopia,JavaScript,"`sinopia` - a private/caching npm repository server

[![npm version badge](https://img.shields.io/npm/v/sinopia.svg)](https://www.npmjs.org/package/sinopia)
[![travis badge](http://img.shields.io/travis/rlidwka/sinopia.svg)](https://travis-ci.org/rlidwka/sinopia)
[![downloads badge](http://img.shields.io/npm/dm/sinopia.svg)](https://www.npmjs.org/package/sinopia)

It allows you to have a local npm registry with zero configuration. You don't have to install and replicate an entire CouchDB database. Sinopia keeps its own small database and, if a package doesn't exist there, it asks npmjs.org for it keeping only those packages you use.

<p align=""center""><img src=""https://f.cloud.github.com/assets/999113/1795553/680177b2-6a1d-11e3-82e1-02193aa4e32e.png""></p>

## Use cases

1. Use private packages.

   If you want to use all benefits of npm package system in your company without sending all code to the public, and use your private packages just as easy as public ones.

   See [using private packages](#using-private-packages) section for details.

2. Cache npmjs.org registry.

   If you have more than one server you want to install packages on, you might want to use this to decrease latency
   (presumably ""slow"" npmjs.org will be connected to only once per package/version) and provide limited failover (if npmjs.org is down, we might still find something useful in the cache).

   See [using public packages](#using-public-packages-from-npmjsorg) section for details.

3. Override public packages.

   If you want to use a modified version of some 3rd-party package (for example, you found a bug, but maintainer didn't accept pull request yet), you can publish your version locally under the same name.

   See [override public packages](#override-public-packages) section for details.

## Installation

```bash
# installation and starting (application will create default
# config in config.yaml you can edit later)
$ npm install -g sinopia
$ sinopia

# npm configuration
$ npm set registry http://localhost:4873/

# if you use HTTPS, add an appropriate CA information
# (""null"" means get CA list from OS)
$ npm set ca null
```

Now you can navigate to [http://localhost:4873/](http://localhost:4873/) where your local packages will be listed and can be searched.

### Docker

A Sinopia docker image [is available](https://registry.hub.docker.com/u/keyvanfatehi/sinopia/)

### Chef

A Sinopia Chef cookbook [is available at Opscode community](http://community.opscode.com/cookbooks/sinopia) source: https://github.com/BarthV/sinopia-cookbook

### Puppet

A Sinopia puppet module [is available at puppet forge](http://forge.puppetlabs.com/saheba/sinopia) source: https://github.com/saheba/puppet-sinopia

## Configuration

When you start a server, it auto-creates a config file.

## Adding a new user

```bash
npm adduser --registry http://localhost:4873/
```

This will prompt you for user credentials which will be saved on the Sinopia server.

## Using private packages

You can add users and manage which users can access which packages.

It is recommended that you define a prefix for your private packages, for example ""local"", so all your private things will look like this: `local-foo`. This way you can clearly separate public packages from private ones.

## Using public packages from npmjs.org

If some package doesn't exist in the storage, server will try to fetch it from npmjs.org. If npmjs.org is down, it serves packages from cache pretending that no other packages exist. Sinopia will download only what's needed (= requested by clients), and this information will be cached, so if client will ask the same thing second time, it can be served without asking npmjs.org for it.

Example: if you successfully request express@3.0.1 from this server once, you'll able to do that again (with all it's dependencies) anytime even if npmjs.org is down. But say express@3.0.0 will not be downloaded until it's actually needed by somebody. And if npmjs.org is offline, this server would say that only express@3.0.1 (= only what's in the cache) is published, but nothing else.

## Override public packages

If you want to use a modified version of some public package `foo`, you can just publish it to your local server, so when your type `npm install foo`, it'll consider installing your version.

There's two options here:

1. You want to create a separate fork and stop synchronizing with public version.

   If you want to do that, you should modify your configuration file so sinopia won't make requests regarding this package to npmjs anymore. Add a separate entry for this package to *config.yaml* and remove `npmjs` from `proxy_access` list and restart the server.

   When you publish your package locally, you should probably start with version string higher than existing one, so it won't conflict with existing package in the cache.

2. You want to temporarily use your version, but return to public one as soon as it's updated.

   In order to avoid version conflicts, you should use a custom pre-release suffix of the next patch version. For example, if a public package has version 0.1.2, you can upload 0.1.3-my-temp-fix. This way your package will be used until its original maintainer updates his public package to 0.1.3.

## Compatibility

Sinopia aims to support all features of a standard npm client that make sense to support in private repository. Unfortunately, it isn't always possible.

Basic features:

- Installing packages (npm install, npm upgrade, etc.) - supported
- Publishing packages (npm publish) - supported

Advanced package control:

- Unpublishing packages (npm unpublish) - supported
- Tagging (npm tag) - not yet supported, should be soon
- Deprecation (npm deprecate) - not supported

User management:

- Registering new users (npm adduser {newuser}) - supported
- Transferring ownership (npm owner add {user} {pkg}) - not supported, sinopia uses its own acl management system

Misc stuff:

- Searching (npm search) - supported in the browser client but not command line
- Starring (npm star, npm unstar) - not supported, doesn't make sense in private registry

## Storage

No CouchDB here. This application is supposed to work with zero configuration, so filesystem is used as a storage.

If you want to use a database instead, ask for it, we'll come up with some kind of a plugin system.

## Similar existing things

- npm + git (I mean, using git+ssh:// dependencies) - most people seem to use this, but it's a terrible idea... *npm update* doesn't work, can't use git subdirectories this way, etc.
- [reggie](https://github.com/mbrevoort/node-reggie) - this looks very interesting indeed... I might borrow some code there.
- [shadow-npm](https://github.com/dominictarr/shadow-npm), [public service](http://shadow-npm.net/) - it uses the same code as npmjs.org + service is dead
- [gemfury](http://www.gemfury.com/l/npm-registry) and others - those are closed-source cloud services, and I'm not in a mood to trust my private code to somebody (security through obscurity yeah!)
- npm-registry-proxy, npm-delegate, npm-proxy - those are just proxies...
- Is there something else?

"
52,visminer/repositoryminer,Java,"# RepositoryMiner

RepositoryMiner is a Java framework that helps developers and researchers to mining software repositories. With RepositoryMiner, you can easily extract informations from a software repository (e.g. Git), such as commits, developers, modifications, source code, and software metrics. All the informations extracted are stored in the MongoDB for futher analysis/mining over these data.

Take a look at our wiki and [talk to us](https://groups.google.com/forum/#!forum/visminer).

# What I need to run?

To run the RepositoryMiner you will ony need Java (7 or above) and MongoDB (3 or above) installed.

# Getting Started

Currently the project is not in the maven repositories. So you will need to download the java project. The project is build  using gradle, so you can choose your environment freely.

After downloaded and configured you can create your own jar to use in your project. Soon as possible we will provide the project in the maven repositories.

We recommend to always use the version in the master branch for newest updates and bug fixes.

The project is compound by various others projects. In this file we will show you how to execute each one of our projects. 

The API is very simple and straight foward. After added our project as dependency you only need write a little  of java code to extract the data from the software repository. 

Below we show how execute the modules, take a  look at our wiki too. There we provide some explanations and interesting link about the tool.

## RM-Core

Below follows an example how to execute the rm-core project. This code is just a sample, for more options of metrics check the package `org.repositoryminer.metric`, for more Code Smells checks `org.repositoryminer.codesmell` and for more parsers checks `org.repositoryminer.parser`.

```java
import org.repositoryminer.codesmell.GodClass;
import org.repositoryminer.codesmell.ICodeSmell;
import org.repositoryminer.codesmell.LongMethod;
import org.repositoryminer.domain.ReferenceType;
import org.repositoryminer.metric.CYCLO;
import org.repositoryminer.metric.IMetric;
import org.repositoryminer.metric.LOC;
import org.repositoryminer.mining.ReferenceEntry;
import org.repositoryminer.mining.RepositoryMiner;
import org.repositoryminer.parser.IParser;
import org.repositoryminer.parser.java.JavaParser;
import org.repositoryminer.persistence.Connection;
import org.repositoryminer.scm.GitSCM;

public class Main {
	public static void main(String[] args) throws IOException {
		// Connects with the database.
		Connection conn = Connection.getInstance();
		conn.connect(""mongodb://localhost"", ""test_database"");

		// This class is the main interface of core module.
		RepositoryMiner rm = new RepositoryMiner();

		// Here we set the basic repository informations.
		// Pay attention to rm.setRepositoryKey, in this method you will 
		// set an unique identifier for the repository.
		rm.setRepositoryKey(""junit4"");
		rm.setRepositoryName(""Junit4"");
		rm.setRepositoryDescription(""A programmer-oriented testing framework for Java."");
		rm.setRepositoryPath(""/home/felipe/git/junit4"");

		// Here we set the SCM.
		rm.setScm(new GitSCM());

		// The steps below are optional, if you no want to do any code analysis just
		// call rm.mine method.

		// Here we set the parser for the programming languages.
		List<IParser> parsers = new ArrayList<>();
		parsers.add(new JavaParser());
		rm.setParsers(parsers);

		// Here we set the software metrics.
		List<IMetric> metrics = Arrays.asList(new LOC(), new CYCLO());
		rm.setMetrics(metrics);

		// Here we set the code smells.
		List<ICodeSmell> codeSmells = Arrays.asList(new GodClass(), new LongMethod());
		rm.setCodeSmells(codeSmells);

		// Here we set the references(tag or branches) that we want to perform the code
		// analysis.
		Set<ReferenceEntry> refs = new HashSet<ReferenceEntry>();
		refs.add(new ReferenceEntry(""master"", ReferenceType.BRANCH));
		rm.setReferences(refs);

		// This method starts the mining.
		rm.mine();
	}
}
```

# How do I cite RepositoryMiner?
```
@INPROCEEDINGS{170925,
    AUTHOR=""Thiago Mendes and Renato Novais and Manoel Mendonca and Luis Carvalho and Felipe Gomes"",
    TITLE=""RepositoryMiner - uma ferramenta extensível de mineração de repositórios de software para identificação automática de Dívida Técnica"",
    BOOKTITLE=""CBSoft 2017 - Sessao de Ferramentas () "",
    ADDRESS="""",
    DAYS=""18-22"",
    MONTH=""sep"",
    YEAR=""2017"",
    ABSTRACT=""Software projects commonly incur in Technical Debt, since small amounts of debt can increase staff productivity in a short term. However, their presence brings risks to the project, making difficult the management as well as reducing staff productivity and project quality. It is possible to identify some of these technical debts through the calculation of metrics and detecting code smells. But, the existing tools only allow the identification of isolated types of metrics and analyze only one version of the software at a time. In this context, this work presents the RepositoryMiner (RM) tool. RM analyzes software repositories, collecting and combining various data related to software, allowing the calculation of software metrics and identification of different types of Technical Debt. It supports software engineers on the identification and monitoring of Technical Debt."",
    KEYWORDS=""Manutenção, Reengenharia e Refatoração de Software; Métricas e Medições em Engenharia de Software"", 
    URL=""http://XXXXX/170925.pdf""
} 
```

# Is there a discussion forum?

You can subscribe to our mailing list: https://groups.google.com/forum/#!forum/visminer.

# How I contribute?

Check the page [How to contribute](https://github.com/visminer/RepositoryMiner/wiki/How-to-contribute)

# License

This software is licensed under the Apache 2.0 License.
"
53,ecomfe/spec,,"This repository contains the specifications.


- [JavaScript编码规范](javascript-style-guide.md) <span class=""std-rec"">[1.3]</span>
- [JavaScript编码规范 - ESNext补充篇](es-next-style-guide.md) <span class=""std-rec"">[draft]</span>
- [HTML编码规范](html-style-guide.md) <span class=""std-rec"">[1.2]</span>
- [CSS编码规范](css-style-guide.md) <span class=""std-rec"">[1.2]</span>
- [Less编码规范](less-code-style.md) <span class=""std-rec"">[1.1]</span>
- [E-JSON数据传输标准](e-json.md) <span class=""std-rec"">[1.0]</span>
- [模块和加载器规范](module.md) <span class=""std-rec"">[1.1]</span>
- [包结构规范](package.md) <span class=""std-rec"">[1.1]</span>
- [项目目录结构规范](directory.md) <span class=""std-rec"">[1.1]</span>
- [图表库标准](chart.md) <span class=""std-rec"">[1.0]</span>
- [react编码规范](react-style-guide.md) <span class=""std-rec"">[draft]</span>


Lint and fix tool：[FECS](http://fecs.baidu.com/)
"
54,CastagnaIT/repository.castagnait,Python,"# CastagnaIT Repository for Kodi 18.x (LEIA) add-ons

[Repository installation file [repository.castagnait-1.0.1.zip]](https://github.com/castagnait/repository.castagnait/raw/master/repository.castagnait-1.0.1.zip)

Installation instructions are in the Wiki contained in the add-on repository.

---

## Content of the repository

- plugin.video.netflix

Updates published through this channel are only release versions, not daily builds.

For any other download links refer to the Readme in the add-on repository
"
55,xuezhijian/Spring-Demo-Repository,Java,"#Spring 例子仓库
"
56,yona-projects/yona,JavaScript,"<a name=""korean""></a>[[English]](#english)

[![Build Status](https://travis-ci.org/yona-projects/yona.svg?branch=master)](https://travis-ci.org/yona-projects/yona)
![Downloads Status](https://img.shields.io/github/downloads/yona-projects/yona/total.svg)


<img src='public/images/yona_logo.png' width='300px'>
21세기 협업 개발 플랫폼

DEMO: [http://repo.yona.io](http://repo.yona.io)

Official Site: [http://yona.io](http://yona.io)

Yona?
--
- Git 저장소 기능이 내장된 설치형 이슈트래커
- Naver, Naver Labs 를 비롯하여 게임회사, 통신회사 고객센터, 공공기관, 투자사, 학교, 기업등에서 수년 간 실제로 사용되어 왔고 개선되어 온(Real world battled) 애플리케이션입니다

주요기능
---
- 서비스 종료나 데이터 종속 걱정없는 설치형
- 프로젝트 기반의 유연한 이슈트래커와 게시판
   - 편리한 프로젝트간 이슈 이동
   - 서브 태스크 이슈
   - 본문 변경이력 보기
   - 이슈 템플릿 기능
- 자체 내장된 코드 저장소
   - Git/SVN 선택 가능
   - 온라인 수정 및 커밋 지원
   - 프로젝트 멤버만 코드에 접근 가능 기능 등
- 블럭기반 코드리뷰 
   - 코드 블럭 및 리뷰 스레드 지원
   - 리뷰 점수 지원
- 그룹 기능
   - 그룹 이슈 및 게시글 통합관리
   - 그룹 프로젝트, 그룹 멤버
- 한글 기반
   - 프로젝트 이름 및 그룹 이름에 한글을 사용가능
- LDAP 지원
   - LDAP 장애시에도 사용가능한 기능 제공
- 다른 제품이나 서비스로의 마이그레이션 기능 제공
   - Github/Github Enterprise, 또 다른 Yona 인스턴스, Redmine 등
- 로그인 관련 보안을 높일 수 있는 소셜로그인 지원

등을 비롯하여 일상적인 업무에서 SW 개발 전반에 필요한 다양한 기능을 포함하고 있습니다.

추가 읽을거리
---
- [왜 Yona를 써야 하나요? (Why Yona?)](https://repo.yona.io/yona-projects/yona/post/3)
- [기본 워크플로우](https://repo.yona.io/yona-projects/yona-help/post/2)


라이선스
--
Yona는 Apache 2.0 라이선스로 제공됩니다.

**이어지는 설치 및 실행, 백업 등등에 대한 자세한 설명은 [Wiki](https://github.com/yona-projects/yona/wiki)에 따로 세분화되어 정리되어 있습니다.**

Yona 설치 및 실행
===

Yona 배포판
---
현재 Yona는 버전별로 두 개의 배포판을 [릴리즈 메뉴](https://github.com/yona-projects/yona/releases)를 통해 제공하고 있습니다.

- MariaDB 버전
  - 기본 권장 버전
  - `yona-v1.11.0-bin.zip` 같은 형식으로 파일로 배포
  - DB 설치에 약간의 시간이 필요하지만 안정적으로 운영이 가능
- H2 DB 내장형
  - DB 설정없이 내려받아서 바로 실행해서 쓸 수 있는 버전
  - `yona-h2-v1.11.0-bin.zip` 같은 형식으로 파일로 배포
  - USB 등에 담아서 이동해가면서 사용하거나 작업후 통째로 zip으로 묶어서 들고 다니는 것이 가능함
  - 대규모 사이트에서 사용하기에는 적합하지 않음. 참고: [Yona가 MariaDB를 기본 DB로 사용하게 된 이유](https://repo.yona.io/yona-projects/yona/post/4)

Yona 설치
---
Yona는 크게 다음과 같은 2단계로 설치합니다.

- [MariaDB 설치](docs/ko/install-mariadb.md)
- [Yona 설치](docs/ko/install-yona-server.md)

#### Docker를 이용한 설치
[Docker](https://www.docker.com/)를 이용해 설치하실분은 [pokev25](https://github.com/pokev25) 님의 https://github.com/pokev25/docker-yona 를 이용해주세요. 

#### Amazon AWS 에 설치 
https://okdevtv.com/mib/yona 에서 가이드를 볼 수 있습니다. by [Kenu](https://www.facebook.com/kenu.heo)님

Yona 실행 및 업그레이드/백업 및 복구/문제 해결
---
- [실행 및 재시작 방법](docs/ko/yona-run-and-restart.md)
- 안정적인 운영을 위한 [실행 옵션들](docs/ko/yona-run-options.md)
- [업그레이드](docs/ko/yona-upgrade.md)
- [백업 및 복구](docs/ko/yona-backup-restore.md)
- [알림메일 발송 기능 설정](docs/ko/yona-mail-settings.md)
- [발생 가능한 문제상황들과 해결방법](docs/ko/trouble-shootings.md)


소스코드를 직접 내려 받아서 빌드하거나 자신만의 배포판을 만들기
---
자신의 입맛에 맛게 코드를 직접 수정해서 작업하거나 코드를 기여하고 싶을 경우에는 코드 저장소로부터 코드를 직접 내려받아서 빌드/실행하는 것도 가능합니다.
[소스코드를 직접 내려 받아서 실행하기](https://repo.yona.io/yona-projects/yona/post/5)를 참고해 주세요

서버 관련 설정들
---
- [application.conf 설명](docs/ko/application-conf-desc.md)
- [소셜 로그인 설정](docs/ko/yona-social-login-settings.md)

Google Analytics
---
- 기본적으로는 Google Analytics 가 활성화 되어 함께 배포됩니다. 
- 설치형으로 제공되는 Yona의 특성상 제품이 지속적으로 개발/유지되기 위해서는 사용자들이 현재 어느정도 내려받아서 사용하고 있는지에 대한 정보가 필요합니다.
- 만약 이부분에 대해 도움을 주기 곤란한 경우 application.conf 에서 아래 항목을 false로 수정합니다.
```
application.send.yona.usage = true
```

마이그레이션
---
- 기본적으로 Yona 에서 Github/Github Enterprise 로 이전하는 기능을 제공합니다.
    - [Yona에서 Github으로 이사가는 방법](https://repo.yona.io/yona-projects/yona-help/post/4)
    - [설정](https://github.com/yona-projects/yona/blob/master/conf/application.conf.default#L297)
- [Yona Export](https://github.com/yona-projects/yona-export)
    - 프로젝트 로컬 백업
    - Yona 에서 다른 Yona 인스턴스로 이전 지원
       - 일명 '출장용 Yona 기능'이라고도 할 수 있는 하는 기능입니다. 
          - DB내장형 경량 Yona인, [Yona H2 Embedded 버전]을 사용해서 출장/파견 나가서 작업하다가 작업 완료후에 Export 받아서 본점 Yona에 Import 하는 것이 v1.6.0부터 가능합니다.
    - Export 파일 포맷만 일치시킨다면 어떤 소스로부터도 마이그레이션이나 이동이 가능합니다


Contribution
---
- 코드 기여의 기준이 되는 브랜치는 `master`입니다.
- 저장소를 fork 한 다음 `master` 브랜치를 기준으로 작업하신다음 `master` 브랜치로 pull request를 보내주세요.
  - `next`브랜치는 내부 개발용입니다. 어떠한 기능들이 추가되고 있는지 현장을 보고 싶으시면 `next`브랜치를 참고해주세요.
- 코드리뷰 후 merge 되면 Yona Author로 파일에 기록되며 작은 기념품을 보내드립니다. 

<br/>

<a name=""english""></a>[[한국어]](#korean)


Yona
=======
Yona is a web-based project hosting software.

What you can do with Yona:
--
Yona is designed to increase the speed and efficiency of team work and team development.

- Issue tracker
   - Issues can be transferred to other projects
   - Issues' change histories can be viewed
- Bulletin board
- Embedded Git/SVN respository features 
- Pull requests & Block-based code review
- Online Commits
- LDAP support
- Social login
- Migration to/from other services or Yona instances
     - Github/Github Enterprise, Redmine, Yona

Requirements
---
- Java 8+
- System Memory 2Gb+ (Recommendation: 4Gb+)

Distribution
---
Currently, There are two distribution types.

#### MariaDB version
- Recommended version
- It takes a little effort to install DB, but it guarantees stable operation

#### Embedded H2 DB version
- Portable version that can be downloaded and run immediately. 
  - Setting a DB is not required.
  - Also, can run the software directly from a USB device
- Suitable for small teams (under 500 users).

How to install
---
Basically, Yona installation is in two steps:

- [MariaDB install](docs/install-mariadb.md)
- [Yona install](docs/install-yona-server.md)


If you want to use [Docker](https://www.docker.com/), See https://github.com/pokev25/docker-yona by [pokev25](https://github.com/pokev25)


Start/Upgrade/Backup/Trouble Shootings
---
- [Start and Restart](docs/yona-run-and-restart.md)
- [Start Options](docs/yona-run-options.md) for stable operation
- [Upgrade](docs/yona-upgrade.md)
- [Backup/Restore](docs/yona-backup-restore.md)
- [Mail settings for Notification](docs/yona-mail-settings.md)
- [Trouble Shootings](docs/trouble-shootings.md)

Server Settings
---
- [application.conf Settings](docs/application-conf-desc.md)
- [Social Login Settings](docs/yona-social-login-settings.md)

Migration
---
- [Yona Export](https://github.com/yona-projects/yona-export)
    - Local backup
    - Move projects to another Yona instance
    - If you can match the format, anything can be imported into Yona
- Github/Github Enterprise migration
    - [See here](https://github.com/yona-projects/yona/blob/master/conf/application.conf.default#L297)
    
Google Analytics
---
- Distributed Yona includes Google Analytics
- This data is used for making us to improve Yona
- If you want to disable this for any reason, set the following option to false in conf/application.conf file.
```
application.send.yona.usage = true
```

Contribution
---
- The branch for contributions is `master`.
- At first, fork the repository, then work on the `master` branch. And send a pull request to the`master` branch.
   - The `next` branch is for internal development. If you want to see what features are being added, please refer to the `next` branch.


License
--
Copyright Yona Authors, NAVER Corp. and NAVER LABS under the Apache License, Version 2.0
"
57,packagemgmt/repositorytools,Python,".. image:: https://travis-ci.org/packagemgmt/repositorytools.svg?branch=master
    :target: https://travis-ci.org/packagemgmt/repositorytools
    :alt: CI Build

.. image:: https://img.shields.io/gratipay/stardust85.svg
    :target: https://gratipay.com/~stardust85/
    :alt: Fundraising

Python API and command-line interface for working with Sonatype Nexus
=====================================================================

How to install
--------------

::

    pip install repositorytools

Some command line examples
--------------------------

Preparing env. variables
~~~~~~~~~~~~~~~~~~~~~~~~
::

    export REPOSITORY_URL=https://repo.example.com
    export REPOSITORY_USER=admin
    export REPOSITORY_PASSWORD=mysecretpassword

Uploading an artifact
~~~~~~~~~~~~~~~~~~~~~
::

    artifact upload foo-1.2.3.ext releases com.fooware

Resolving artifact's URL
~~~~~~~~~~~~~~~~~~~~~~~~
::

    artifact resolve com.fooware:foo:latest

Deleting artifacts
~~~~~~~~~~~~~~~~~~
::

    # by url
    artifact delete https://repo.example.com/content/repositories/releases/com/fooware/foo/1.2.3/foo-1.2.3.ext

    # by coordinates
    artifact resolve com.fooware:foo:latest | xargs artifact delete

Working with staging repositories
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Nexus Professional only

::

    repo create -h
    repo close -h
    repo release -h
    repo drop -h
    repo list -h

Working with custom maven metadata
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Nexus Professional only

::

    artifact get-metadata -h
    artifact set-metadata -h



Some library examples
---------------------
For most of methods the same env. variables as above have to be exported or specified in call of repository_client_factory()

Uploading artifacts
~~~~~~~~~~~~~~~~~~~
::

    import repositorytools

    artifact = repositorytools.LocalArtifact(local_path='~/foo-1.2.3.jar', group='com.fooware')
    client = repositorytools.repository_client_factory(user='admin', password='myS3cr3tPasswOrd')
    remote_artifacts = client.upload_artifacts(local_artifacts=[artifact], repo_id='releases')
    print(remote_artifacts)

Resolving artifacts
~~~~~~~~~~~~~~~~~~~
Works even without authentication.
::

    import repositorytools

    artifact = repositorytools.RemoteArtifact.from_repo_id_and_coordinates('test', 'com.fooware:foo:1.2.3')
    client = repositorytools.repository_client_factory()
    client.resolve_artifact(artifact)
    print(artifact.url)

Deleting artifacts
~~~~~~~~~~~~~~~~~~

::

    import repositorytools

    artifact = repositorytools.RemoteArtifact.from_repo_id_and_coordinates('test', 'com.fooware:foo:1.2.3')
    client = repositorytools.repository_client_factory(user='admin', password='myS3cr3tPasswOrd')
    client.resolve_artifact(artifact)
    client.delete_artifact(artifact.url)


Documentation
-------------

is on http://repositorytools.readthedocs.org/en/latest/

Support
-------
You can support my effort many ways:
 * By creating issues
 * By fixing issues (=sending pull requests)
 * donating at https://liberapay.com/stardust85/, you can send even 1 cent per week ;)
"
58,Antergos/antergos-packages,Shell,"## Description ##
PKGBUILD's for antergos packages
"
59,udacity/Sunshine-Version-2,Java,"Sunshine
========

Sunshine is the companion Android app for the Udacity course [Developing Android Apps: Android Fundamentals](https://www.udacity.com/course/ud853).

Take the course to find out how to build this app a step at a time, and eventually create your own Android App!

This is the second version of the Sunshine code. The repository has been updated on:

* **October 18th, 2015** - Updated to support use of the openweathermap.org API key.
* **February 13th, 2015** - Major update
* February 25, 2015 - Minor bug fixes
* March 4th, 2015 - Minor bug fixes

### Open Weather Map API Key is required.

In order for the Sunshine app to function properly as of October 18th, 2015 an API key for openweathermap.org must be included with the build.

We recommend that each student obtain a key via the following [instructions](http://openweathermap.org/appid#use), and include the unique key for the build by adding the following line to [USER_HOME]/.gradle/gradle.properties

`MyOpenWeatherMapApiKey=""<UNIQUE_API_KEY"">`

For help migrating an existing repo (fork or clone prior to 10/18/15), please check out this [guide.](https://docs.google.com/document/d/1e8LXahedBlCW1_dp_FyvQ3ugUAwUBJDuJCoKf3tgNVs/pub?embedded=true) 

========
For the original version, please go [here](https://github.com/udacity/Sunshine).

A changelog for the course can be found [here](https://docs.google.com/a/knowlabs.com/document/d/193xJb_OpcNCqgquMhxPrMh05IEYFXQqt0S6-6YK8gBw/pub).
"
60,opencv/opencv_contrib,C++,"## Repository for OpenCV's extra modules

This repository is intended for the development of so-called ""extra"" modules,
contributed functionality. New modules quite often do not have stable API,
and they are not well-tested. Thus, they shouldn't be released as a part of
official OpenCV distribution, since the library maintains binary compatibility,
and tries to provide decent performance and stability.

So, all the new modules should be developed separately, and published in the
`opencv_contrib` repository at first. Later, when the module matures and gains
popularity, it is moved to the central OpenCV repository, and the development team
provides production-quality support for this module.

### How to build OpenCV with extra modules

You can build OpenCV, so it will include the modules from this repository. Contrib modules are under constant development and it is recommended to use them alongside the master branch or latest releases of OpenCV.

Here is the CMake command for you:

```
$ cd <opencv_build_directory>
$ cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules <opencv_source_directory>
$ make -j5
```

As the result, OpenCV will be built in the `<opencv_build_directory>` with all
modules from `opencv_contrib` repository. If you don't want all of the modules,
use CMake's `BUILD_opencv_*` options. Like in this example:

```
$ cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules -DBUILD_opencv_legacy=OFF <opencv_source_directory>
```

If you also want to build the samples from the ""samples"" folder of each module, also include the ""-DBUILD_EXAMPLES=ON"" option.

If you prefer using the gui version of cmake (cmake-gui), then, you can add `opencv_contrib` modules within `opencv` core by doing the following:

1. Start cmake-gui.

2. Select the opencv source code folder and the folder where binaries will be built (the 2 upper forms of the interface).

3. Press the `configure` button. You will see all the opencv build parameters in the central interface.

4. Browse the parameters and look for the form called `OPENCV_EXTRA_MODULES_PATH` (use the search form to focus rapidly on it).

5. Complete this `OPENCV_EXTRA_MODULES_PATH` by the proper pathname to the `<opencv_contrib>/modules` value using its browse button.

6. Press the `configure` button followed by the `generate` button (the first time, you will be asked which makefile style to use).

7. Build the `opencv` core with the method you chose (make and make install if you chose Unix makefile at step 6).

8. To run, linker flags to contrib modules will need to be added to use them in your code/IDE. For example to use the aruco module, ""-lopencv_aruco"" flag will be added.

### Update the repository documentation

In order to keep a clean overview containing all contributed modules, the following files need to be created/adapted:

1. Update the README.md file under the modules folder. Here, you add your model with a single line description.

2. Add a README.md inside your own module folder. This README explains which functionality (separate functions) is available, links to the corresponding samples and explains in somewhat more detail what the module is expected to do. If any extra requirements are needed to build the module without problems, add them here also.
"
61,offensive-security/exploitdb,C,"# The Exploit Database Git Repository

This is an official repository of [The Exploit Database](https://www.exploit-db.com/), a [project](https://www.offensive-security.com/community-projects/) sponsored by [Offensive Security](https://www.offensive-security.com/).
Our repositories are:

  - Exploits & Shellcodes: [https://github.com/offensive-security/exploitdb](https://github.com/offensive-security/exploitdb)
  - Binary Exploits: [https://github.com/offensive-security/exploitdb-bin-sploits](https://github.com/offensive-security/exploitdb-bin-sploits)
  - Papers: [https://github.com/offensive-security/exploitdb-papers](https://github.com/offensive-security/exploitdb-papers)

The Exploit Database is an archive of public exploits and corresponding vulnerable software, developed for use by penetration testers and vulnerability researchers. Its aim is to serve as the most comprehensive collection of [exploits](https://www.exploit-db.com/), [shellcode](https://www.exploit-db.com/shellcodes) and [papers](https://www.exploit-db.com/papers) gathered through direct submissions, mailing lists, and other public sources, and present them in a freely-available and easy-to-navigate database. The Exploit Database is a repository for exploits and Proof-of-Concepts rather than advisories, making it a valuable resource for those who need actionable data right away.
You can learn more about the project [here (Top Right -> About Exploit-DB)](https://www.exploit-db.com/) and [here (History)](https://www.exploit-db.com/history).

This repository is updated daily with the most recently added submissions. Any additional resources can be found in our [binary exploits repository](https://github.com/offensive-security/exploitdb-bin-sploits).

Exploits are located in the [`/exploits/`](https://github.com/offensive-security/exploitdb/tree/master/exploits) directory, shellcodes can be found in the [`/shellcodes/`](https://github.com/offensive-security/exploitdb/tree/master/shellcodes) directory.

- - -

## License

This project (and SearchSploit) is released under ""[GNU General Public License v2.0](https://github.com/offensive-security/exploitdb/blob/master/LICENSE.md)"".

- - -

# SearchSploit

Included with this repository is the **SearchSploit** utility, which will allow you to search through exploits, shellcodes and papers _(if installed)_ using one or more terms.
For more information, please see the **[SearchSploit manual](https://www.exploit-db.com/searchsploit)**.

## Usage/Example

```
kali@kali:~$ searchsploit -h
  Usage: searchsploit [options] term1 [term2] ... [termN]

==========
 Examples
==========
  searchsploit afd windows local
  searchsploit -t oracle windows
  searchsploit -p 39446
  searchsploit linux kernel 3.2 --exclude=""(PoC)|/dos/""
  searchsploit -s Apache Struts 2.0.0
  searchsploit linux reverse password
  searchsploit -j 55555 | json_pp

  For more examples, see the manual: https://www.exploit-db.com/searchsploit

=========
 Options
=========
## Search Terms
   -c, --case     [Term]      Perform a case-sensitive search (Default is inSEnsITiVe)
   -e, --exact    [Term]      Perform an EXACT & order match on exploit title (Default is an AND match on each term) [Implies ""-t""]
                                e.g. ""WordPress 4.1"" would not be detect ""WordPress Core 4.1"")
   -s, --strict               Perform a strict search, so input values must exist, disabling fuzzy search for version range
                                e.g. ""1.1"" would not be detected in ""1.0 < 1.3"")
   -t, --title    [Term]      Search JUST the exploit title (Default is title AND the file's path)
       --exclude=""term""       Remove values from results. By using ""|"" to separate, you can chain multiple values
                                e.g. --exclude=""term1|term2|term3""

## Output
   -j, --json     [Term]      Show result in JSON format
   -o, --overflow [Term]      Exploit titles are allowed to overflow their columns
   -p, --path     [EDB-ID]    Show the full path to an exploit (and also copies the path to the clipboard if possible)
   -v, --verbose              Display more information in output
   -w, --www      [Term]      Show URLs to Exploit-DB.com rather than the local path
       --id                   Display the EDB-ID value rather than local path
       --colour               Disable colour highlighting in search results

## Non-Searching
   -m, --mirror   [EDB-ID]    Mirror (aka copies) an exploit to the current working directory
   -x, --examine  [EDB-ID]    Examine (aka opens) the exploit using $PAGER

## Non-Searching
   -h, --help                 Show this help screen
   -u, --update               Check for and install any exploitdb package updates (brew, deb & git)

## Automation
       --nmap     [file.xml]  Checks all results in Nmap's XML output with service version
                                e.g.: nmap [host] -sV -oX file.xml

=======
 Notes
=======
 * You can use any number of search terms
 * By default, search terms are not case-sensitive, ordering is irrelevant, and will search between version ranges
   * Use '-c' if you wish to reduce results by case-sensitive searching
   * And/Or '-e' if you wish to filter results by using an exact match
   * And/Or '-s' if you wish to look for an exact version match
 * Use '-t' to exclude the file's path to filter the search results
   * Remove false positives (especially when searching using numbers - i.e. versions)
 * When using '--nmap', adding '-v' (verbose), it will search for even more combinations
 * When updating or displaying help, search terms will be ignored

kali@kali:~$
kali@kali:~$ searchsploit afd windows local
---------------------------------------------------------------------------------------- -----------------------------------
 Exploit Title                                                                          |  Path
---------------------------------------------------------------------------------------- -----------------------------------
Microsoft Windows (x86) - 'afd.sys' Local Privilege Escalation (MS11-046)               | windows_x86/local/40564.c
Microsoft Windows - 'afd.sys' Local Kernel (PoC) (MS11-046)                             | windows/dos/18755.c
Microsoft Windows - 'AfdJoinLeaf' Local Privilege Escalation (MS11-080) (Metasploit)    | windows/local/21844.rb
Microsoft Windows 7 (x64) - 'afd.sys' Dangling Pointer Privilege Escalation (MS14-040)  | windows_x86-64/local/39525.py
Microsoft Windows 7 (x86) - 'afd.sys' Dangling Pointer Privilege Escalation (MS14-040)  | windows_x86/local/39446.py
Microsoft Windows XP - 'afd.sys' Local Kernel Denial of Service                         | windows/dos/17133.c
Microsoft Windows XP/2003 - 'afd.sys' Local Privilege Escalation (K-plugin) (MS08-066)  | windows/local/6757.txt
Microsoft Windows XP/2003 - 'afd.sys' Local Privilege Escalation (MS11-080)             | windows/local/18176.py
---------------------------------------------------------------------------------------- -----------------------------------
Shellcodes: No Result
kali@kali:~$
kali@kali:~$ searchsploit -p 39446
  Exploit: Microsoft Windows 7 (x86) - 'afd.sys' Dangling Pointer Privilege Escalation (MS14-040)
      URL: https://www.exploit-db.com/exploits/39446
     Path: /usr/share/exploitdb/exploits/windows_x86/local/39446.py
File Type: Python script, ASCII text executable, with CRLF line terminators

Copied EDB-ID #39446's path to the clipboard.
kali@kali:~$
```

- - -

## Install

SearchSploit requires either ""CoreUtils"" or ""utilities"" (e.g. `bash`, `sed`, `grep`, `awk`, etc.) for the core features to work.
The self updating function will require `git`, and for the Nmap XML option to work, will require `xmllint` (found in the `libxml2-utils` package in Debian-based systems).

You can find a **more in-depth guide in the [SearchSploit manual](https://www.exploit-db.com/searchsploit)**.

**Kali Linux**

Exploit-DB/SearchSploit is already packaged inside of Kali-Linux. A method of installation is:

```
kali@kali:~$ sudo apt -y install exploitdb
```

_NOTE: Optional is to install the additional packages:_

```
kali@kali:~$ sudo apt -y install exploitdb-bin-sploits exploitdb-papers
```

**Git**

In short: clone the repository, add the binary into `$PATH`, and edit the config file to reflect the git path:

```
$ sudo git clone https://github.com/offensive-security/exploitdb.git /opt/exploitdb
$ sudo ln -sf /opt/exploitdb/searchsploit /usr/local/bin/searchsploit
```

**Homebrew**

If you have [homebrew](http://brew.sh/) ([package](https://github.com/Homebrew/homebrew-core/blob/master/Formula/exploitdb.rb), [formula](https://formulae.brew.sh/formula/exploitdb)) installed, running the following will get you set up:

```
user@MacBook:~$ brew update && brew install exploitdb
```

- - -

## Credit

The following people made this possible:

- [Offensive Security](https://www.offensive-security.com/)
- [Unix-Ninja](https://github.com/unix-ninja)
- [g0tmi1k](https://blog.g0tmi1k.com/)
"
62,XvBMC/repository.xvbmc,Python,"# REPOsitory.XvBMC
 
**NOTE:** XvBMC Nederland (xbmc nl) wij zijn géén helpdesk van/voor boxverkopers
 
  
   
### XvBMC_Nederland: 
* https://bit.ly/XvBMC-NL (shutdown)
* https://www.fb.com/groups/XvBMCnederland/ (shutdown)
 
### OFFLINE / AFGESLOTEN / SHUTDOWN / DOWN / QUIT / THE END / GESCHLOSSEN / FERMÉ
 
----------
 
*With kind regards,*
 
*Team X(v)BMC Nederland*
 
----------
 
(c) [XvBMC Nederland](https://bit.ly/AddonBlacklist) (r)"
63,kisslinux/repo,C,"|/
|\ISS                                                           https://k1ss.org
________________________________________________________________________________


Official Repositories
________________________________________________________________________________

The official repositories for KISS Linux maintained by Dylan Araps.

More information:

    - https://k1ss.org
    - https://k1ss.org/package-system
    - https://repology.org/repository/kiss_main


Signature Verification
________________________________________________________________________________

Enabling signature verification: https://k1ss.org/install#3.0


Dylan Araps' public key:
________________________________________________________________________________


-----BEGIN PGP PUBLIC KEY BLOCK-----
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=7sQn
-----END PGP PUBLIC KEY BLOCK-----
"
64,XiumingLee/MingRepository,Java,"# MingRepository
个人案例整理合集

> - `qiniu-spring-boot-starter`:以七牛云为例写的Spring Boot Starter
> - `sharding-sphere-demo`:ShardingSphere分库分表 及读写分离测试
> - `websocket`:Spring Websocket实时统计在线用户数
> - `JT809-Server`:JT809 协议的简单实现
> - `MyWeb-Static`:个人静态网页的实现(大二时的H5作业)
> - `BiliVedioDownload`:抓取Bilibili哔哩哔哩网站视频(Java和Python双版本实现)
"
65,Kitware/CMake,C,"CMake
*****

Introduction
============

CMake is a cross-platform, open-source build system generator.
For full documentation visit the `CMake Home Page`_ and the
`CMake Documentation Page`_. The `CMake Community Wiki`_ also
references useful guides and recipes.

.. _`CMake Home Page`: https://cmake.org
.. _`CMake Documentation Page`: https://cmake.org/documentation
.. _`CMake Community Wiki`: https://gitlab.kitware.com/cmake/community/-/wikis/home

CMake is maintained and supported by `Kitware`_ and developed in
collaboration with a productive community of contributors.

.. _`Kitware`: http://www.kitware.com/cmake

License
=======

CMake is distributed under the OSI-approved BSD 3-clause License.
See `Copyright.txt`_ for details.

.. _`Copyright.txt`: Copyright.txt

Building CMake
==============

Supported Platforms
-------------------

* Microsoft Windows
* Apple macOS
* Linux
* FreeBSD
* OpenBSD
* Solaris
* AIX

Other UNIX-like operating systems may work too out of the box, if not
it should not be a major problem to port CMake to this platform.
Please post to the `CMake Discourse Forum`_ to ask if others have
had experience with the platform.

.. _`CMake Discourse Forum`: https://discourse.cmake.org

Building CMake from Scratch
---------------------------

UNIX/Mac OSX/MinGW/MSYS/Cygwin
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You need to have a C++ compiler (supporting C++11) and a ``make`` installed.
Run the ``bootstrap`` script you find in the source directory of CMake.
You can use the ``--help`` option to see the supported options.
You may use the ``--prefix=<install_prefix>`` option to specify a custom
installation directory for CMake.  Once this has finished successfully,
run ``make`` and ``make install``.

For example, if you simply want to build and install CMake from source,
you can build directly in the source tree::

  $ ./bootstrap && make && sudo make install

Or, if you plan to develop CMake or otherwise run the test suite, create
a separate build tree::

  $ mkdir cmake-build && cd cmake-build
  $ ../cmake-source/bootstrap && make

Windows
^^^^^^^

There are two ways for building CMake under Windows:

1. Compile with MSVC from VS 2015 or later.
   You need to download and install a binary release of CMake.  You can get
   these releases from the `CMake Download Page`_.  Then proceed with the
   instructions below for `Building CMake with CMake`_.

2. Bootstrap with MinGW under MSYS2.
   Download and install `MSYS2`_.  Then install the required build tools::

     $ pacman -S --needed git base-devel mingw-w64-x86_64-gcc

   and bootstrap as above.

.. _`CMake Download Page`: https://cmake.org/download
.. _`MSYS2`: https://www.msys2.org/

Building CMake with CMake
-------------------------

You can build CMake as any other project with a CMake-based build system:
run the installed CMake on the sources of this CMake with your preferred
options and generators. Then build it and install it.
For instructions how to do this, see documentation on `Running CMake`_.

.. _`Running CMake`: https://cmake.org/runningcmake

To build the documentation, install `Sphinx`_ and configure CMake with
``-DSPHINX_HTML=ON`` and/or ``-DSPHINX_MAN=ON`` to enable the ""html"" or
""man"" builder.  Add ``-DSPHINX_EXECUTABLE=/path/to/sphinx-build`` if the
tool is not found automatically.

.. _`Sphinx`: http://sphinx-doc.org

Reporting Bugs
==============

If you have found a bug:

1. If you have a patch, please read the `CONTRIBUTING.rst`_ document.

2. Otherwise, please post to the `CMake Discourse Forum`_ and ask about
   the expected and observed behaviors to determine if it is really
   a bug.

3. Finally, if the issue is not resolved by the above steps, open
   an entry in the `CMake Issue Tracker`_.

.. _`CMake Issue Tracker`: https://gitlab.kitware.com/cmake/cmake/-/issues

Contributing
============

See `CONTRIBUTING.rst`_ for instructions to contribute.

.. _`CONTRIBUTING.rst`: CONTRIBUTING.rst
"
66,helm/chart-releaser,Go,"# Chart Releaser

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
![CI](https://github.com/helm/chart-releaser/workflows/CI/badge.svg?branch=main&event=push)

**Helps Turn GitHub Repositories into Helm Chart Repositories**

`cr` is a tool designed to help GitHub repos self-host their own chart repos by adding Helm chart artifacts to GitHub Releases named for the chart version and then creating an `index.yaml` file for those releases that can be hosted on GitHub Pages (or elsewhere!).

## Installation

### Binaries (recommended)

Download your preferred asset from the [releases page](https://github.com/helm/chart-releaser/releases) and install manually.

### Homebrew

```console
$ brew tap helm/tap
$ brew install chart-releaser
```

### Go get (for contributing)

```console
$ # clone repo to some directory outside GOPATH
$ git clone https://github.com/helm/chart-releaser
$ cd chart-releaser
$ go mod download
$ go install ./...
```

### Docker (for Continuous Integration)

Docker images are pushed to the [helmpack/chart-releaser](https://quay.io/repository/helmpack/chart-releaser?tab=tags) Quay container registry. The Docker image is built on top of Alpine and its default entry-point is `cr`. See the [Dockerfile](./Dockerfile) for more details.

## Usage

Currently, `cr` can create GitHub Releases from a set of charts packaged up into a directory and create an `index.yaml` file for the chart repository from GitHub Releases.

```console
$ cr --help
Create Helm chart repositories on GitHub Pages by uploading Chart packages
and Chart metadata to GitHub Releases and creating a suitable index file

Usage:
  cr [command]

Available Commands:
  help        Help about any command
  index       Update Helm repo index.yaml for the given GitHub repo
  upload      Upload Helm chart packages to GitHub Releases
  package     Package Helm charts
  version     Print version information

Flags:
      --config string   Config file (default is $HOME/.cr.yaml)
  -h, --help            help for cr

Use ""cr [command] --help"" for more information about a command.
```

### Create GitHub Releases from Helm Chart Packages

Scans a path for Helm chart packages and creates releases in the specified GitHub repo uploading the packages.

```console
$ cr upload --help
Upload Helm chart packages to GitHub Releases

Usage:
  cr upload [flags]

Flags:
  -c, --commit string                  Target commit for release
  -b, --git-base-url string            GitHub Base URL (only needed for private GitHub) (default ""https://api.github.com/"")
  -r, --git-repo string                GitHub repository
  -u, --git-upload-url string          GitHub Upload URL (only needed for private GitHub) (default ""https://uploads.github.com/"")
  -h, --help                           help for upload
  -o, --owner string                   GitHub username or organization
  -p, --package-path string            Path to directory with chart packages (default "".cr-release-packages"")
      --release-name-template string   Go template for computing release names, using chart metadata (default ""{{ .Name }}-{{ .Version }}"")
  -t, --token string                   GitHub Auth Token

Global Flags:
      --config string   Config file (default is $HOME/.cr.yaml)
```

### Create the Repository Index from GitHub Releases

Once uploaded you can create an `index.yaml` file that can be hosted on GitHub Pages (or elsewhere).

```console
$ cr index --help
Update a Helm chart repository index.yaml file based on a the
given GitHub repository's releases.

Usage:
  cr index [flags]

Flags:
  -c, --charts-repo string             The URL to the charts repository
  -b, --git-base-url string            GitHub Base URL (only needed for private GitHub) (default ""https://api.github.com/"")
  -r, --git-repo string                GitHub repository
  -u, --git-upload-url string          GitHub Upload URL (only needed for private GitHub) (default ""https://uploads.github.com/"")
  -h, --help                           help for index
  -i, --index-path string              Path to index file (default "".cr-index/index.yaml"")
  -o, --owner string                   GitHub username or organization
  -p, --package-path string            Path to directory with chart packages (default "".cr-release-packages"")
      --release-name-template string   Go template for computing release names, using chart metadata (default ""{{ .Name }}-{{ .Version }}"")
  -t, --token string                   GitHub Auth Token (only needed for private repos)

Global Flags:
      --config string   Config file (default is $HOME/.cr.yaml)
```

## Configuration

`cr` is a command-line application.
All command-line flags can also be set via environment variables or config file.
Environment variables must be prefixed with `CR_`.
Underscores must be used instead of hyphens.

CLI flags, environment variables, and a config file can be mixed.
The following order of precedence applies:

1. CLI flags
1. Environment variables
1. Config file

### Examples

The following example show various ways of configuring the same thing:

#### CLI

    cr upload --owner myaccount --git-repo helm-charts --package-path .deploy --token 123456789

#### Environment Variables

    export CR_OWNER=myaccount
    export CR_GIT_REPO=helm-charts
    export CR_PACKAGE_PATH=.deploy
    export CR_TOKEN=""123456789""
    export CR_GIT_BASE_URL=""https://api.github.com/""
    export CR_GIT_UPLOAD_URL=""https://uploads.github.com/""

    cr upload

#### Config File

`config.yaml`:

```yaml
owner: myaccount
git-repo: helm-charts
package-path: .deploy
token: 123456789
git-base-url: https://api.github.com/
git-upload-url: https://uploads.github.com/
```

#### Config Usage

    cr upload --config config.yaml


`cr` supports any format [Viper](https://github.com/spf13/viper) can read, i. e. JSON, TOML, YAML, HCL, and Java properties files.

Notice that if no config file is specified, `cr.yaml` (or any of the supported formats) is loaded from the current directory, `$HOME/.cr`, or `/etc/cr`, in that order, if found.

#### Notes for Github Enterprise Users

For Github Enterprise, `chart-releaser` users need to set `git-base-url` and `git-upload-url` correctly, but the correct values are not always obvious to endusers.

By default they are often along these lines:

```
https://ghe.example.com/api/v3/
https://ghe.example.com/api/uploads/
```

If you are trying to figure out what your `upload_url` is try to use a curl command like this:
`curl -u username:token https://example.com/api/v3/repos/org/repo/releases`
and then look for `upload_url`. You need the part of the URL that appears before `repos/` in the path.

##### Known Bug

Currently, if you set the upload URL incorrectly, let's say to something like `https://example.com/uploads/`, then `cr upload` will appear to work, but the release will not be complete. When everything is working there should be 3 assets in each release, but instead there will only be the 2 source code assets. The third asset, which is what helm actually uses, is missing. This issue will become apparent when you run `cr index` and it always claims that nothing has changed, because it can't find the asset it expects for the release.

It appears like the [go-github Do call](https://github.com/google/go-github/blob/master/github/github.go#L520) does not catch the fact that the upload URL is incorrect and pass back the excpected error. If the asset upload fails, it would be better if the release was rolled back (deleted) and an appropriate log message is be displayed to the user.

The `cr index` command should also generate a warning when a release has no assets attached to it, to help people detect and troubleshoot this type of problem.
"
67,pixta-dev/repository-mirroring-action,Shell,"# README

A GitHub Action for mirroring a repository to another repository on GitHub, GitLab, BitBucket, AWS CodeCommit, etc.

This will copy all commits, branches and tags.

>⚠️ Note that the other settings will not be copied. Please check which branch is 'default branch' after the first mirroring.

## Usage

Customize following example workflow (namely replace `<username>/<target_repository_name>` with the right information) and save as `.github/workflows/main.yml` on your source repository.

To find out how to create and add the `GITLAB_SSH_PRIVATE_KEY`, follow the steps below:
1. [How to generate an SSH key pair](https://docs.gitlab.com/ee/ssh/#generate-an-ssh-key-pair). Recommended encryption would be _at least_ `2048-bit RSA`.
2. Add the _public_ key to [your gitlab account](https://gitlab.com/-/profile/keys)
3. Add the _private_ key as a secret to your workflow. More information on [creating and using secrets](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/creating-and-using-encrypted-secrets).


```yaml
name: Mirroring

on: [push, delete]

jobs:
  to_gitlab:
    runs-on: ubuntu-18.04
    steps:                                              # <-- must use actions/checkout@v1 before mirroring!
    - uses: actions/checkout@v1
    - uses: pixta-dev/repository-mirroring-action@v1
      with:
        target_repo_url:
          git@gitlab.com:<username>/<target_repository_name>.git
        ssh_private_key:                                # <-- use 'secrets' to pass credential information.
          ${{ secrets.GITLAB_SSH_PRIVATE_KEY }}

  to_codecommit:                                        # <-- different jobs are executed in parallel.
    runs-on: ubuntu-18.04
    steps:
    - uses: actions/checkout@v1
    - uses: pixta-dev/repository-mirroring-action@v1
      with:
        target_repo_url:
          ssh://git-codecommit.<somewhere>.amazonaws.com/v1/repos/<target_repository_name>
        ssh_private_key:
          ${{ secrets.CODECOMMIT_SSH_PRIVATE_KEY }}
        ssh_username:                                   # <-- (for codecommit) you need to specify ssh-key-id as ssh username.
          ${{ secrets.CODECOMMIT_SSH_PRIVATE_KEY_ID }}
```
"
68,doxygen/doxygen,C++,"Doxygen
===============
[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=9HHLRBCC8B2B8)

Doxygen is the de facto standard tool for generating documentation from
annotated C++ sources, but it also supports other popular programming
languages such as C, Objective-C, C#, PHP, Java, Python, IDL
(Corba, Microsoft, and UNO/OpenOffice flavors), Fortran, VHDL,
and to some extent D.

Doxygen can help you in three ways:

1. It can generate an on-line documentation browser (in HTML) and/or an
   off-line reference manual (in LaTeX) from a set of documented source files.
   There is also support for generating output in RTF (MS-Word), PostScript,
   hyperlinked PDF, compressed HTML, DocBook and Unix man pages.
   The documentation is extracted directly from the sources, which makes
   it much easier to keep the documentation consistent with the source code.
2. You can configure doxygen to extract the code structure from undocumented
   source files. This is very useful to quickly find your way in large
   source distributions. Doxygen can also visualize the relations between
   the various elements by means of include dependency graphs, inheritance
   diagrams, and collaboration diagrams, which are all generated automatically.
3. You can also use doxygen for creating normal documentation (as I did for
   the doxygen user manual and doxygen web-site).

Download
---------
The latest binaries and source of Doxygen can be downloaded from:
* https://www.doxygen.nl/

Developers
---------
* Linux & Windows and MacOS Build Status: <a href=""https://github.com/doxygen/doxygen/actions""><img alt=""Github Actions Build Status"" src=""https://github.com/doxygen/doxygen/workflows/CMake%20Build%20for%20Doxygen/badge.svg""></a>

* Coverity Scan Build Status: <a href=""https://scan.coverity.com/projects/2860""> <img alt=""Coverity Scan Build Status"" src=""https://scan.coverity.com/projects/2860/badge.svg""/> </a>

* Doxygen's Doxygen Documentation: <a href=""https://codedocs.xyz/doxygen/doxygen/""><img src=""https://codedocs.xyz/doxygen/doxygen.svg""/></a>

* Install: Please read the installation section of the manual (https://www.doxygen.nl/manual/install.html)

* Project stats: https://www.openhub.net/p/doxygen

Issues, bugs, requests, ideas
----------------------------------
Use the [issue](https://github.com/doxygen/doxygen/issues) tracker to report bugs.

Comms
----------------------------------
### Mailing Lists ###

There are three mailing lists:

* doxygen-announce@lists.sourceforge.net     - Announcement of new releases only
* doxygen-users@lists.sourceforge.net        - for doxygen users
* doxygen-develop@lists.sourceforge.net      - for doxygen developers
* To subscribe follow the link to
    * https://sourceforge.net/projects/doxygen/

Source Code
----------------------------------
In May 2013, Doxygen moved from
subversion to git hosted at GitHub
* https://github.com/doxygen/doxygen

Enjoy,

Dimitri van Heesch (doxygen at gmail.com)
"
69,openscenegraph/OpenSceneGraph,C++,"[![Build Status](https://travis-ci.org/openscenegraph/OpenSceneGraph.svg?branch=master)](https://travis-ci.org/openscenegraph/OpenSceneGraph)
[![Coverity Status](https://scan.coverity.com/projects/9159/badge.svg)](https://scan.coverity.com/projects/openscenegraph-openscenegraph)
[![Documentation](https://codedocs.xyz/openscenegraph/OpenSceneGraph.svg)](https://codedocs.xyz/openscenegraph/OpenSceneGraph/)
[ABI Tracker](https://abi-laboratory.pro/tracker/timeline/openscenegraph/ ""ABI Tracker"")

# Introduction

Welcome to the OpenSceneGraph (OSG).

For up-to-date information on the project, in-depth details on how to compile and run libraries and examples, see the documentation on the OpenSceneGraph website:

    http://www.openscenegraph.org/index.php/documentation

For support subscribe to our public mailing list or forum, details at:

    http://www.openscenegraph.org/index.php/support

For the impatient, we've included quick build instructions below, these are are broken down is three parts:

  1) General notes on building the OpenSceneGraph
  2) macOS release notes
  3) iOS release notes

If details below are not sufficient then head over to the openscenegraph.org to the Documentation/GettingStarted and Documentation/PlatformSpecifics sections for more indepth instructions.

Robert Osfield.
Project Lead.
26th April 2018.

---

## Section 1. How to build OpenSceneGraph

If you are using the [vcpkg](https://github.com/Microsoft/vcpkg/) dependency manager you can download and install OpenSceneGraph from source with CMake integration using a single command:
```
vcpkg install osg
```

The OpenSceneGraph uses the CMake build system to generate a platform-specific build environment.  CMake reads the `CMakeLists.txt` files that you'll find throughout the OpenSceneGraph directories, checks for installed dependencies and then generates files for the selected build system.

If you don't already have CMake installed on your system you can grab it from http://www.cmake.org, use version 2.8.0 or later.  Details on the OpenSceneGraph's CMake build can be found at:

    http://www.openscenegraph.org/projects/osg/wiki/Build/CMake

Under Unix-like systems (i.e. Linux, IRIX, Solaris, Free-BSD, HP-UX, AIX, macOS) use the `cmake` or `ccmake` command-line utils. Note that `cmake .` defaults to building Release to ensure that you get the best performance from your final libraries/applications.

    cd OpenSceneGraph
    cmake .
    make
    sudo make install

Alternatively, you can create an out-of-source build directory and run cmake or ccmake from there. The advantage to this approach is that the temporary files created by CMake won't clutter the OpenSceneGraph source directory, and also makes it possible to have multiple independent build targets by creating multiple build directories. In a directory alongside the OpenSceneGraph use:

    mkdir build
    cd build
    cmake ../OpenSceneGraph
    make
    sudo make install

Under Windows use the GUI tool CMakeSetup to build your VisualStudio files. The following page on our wiki dedicated to the CMake build system should help guide you through the process:

    http://www.openscenegraph.org/index.php/documentation/platform-specifics/windows

Under macOS you can either use the CMake build system above, or use the Xcode projects that you will find in the OpenSceneGraph/Xcode directory. See release notes on macOS CMake build below.

For further details on compilation, installation and platform-specific information read ""Getting Started"" guide:

    http://www.openscenegraph.org/index.php/documentation/10-getting-started


## Section 2. Release notes on macOS build, by Eric Sokolowski et al.

There are two ways to compile OpenSceneGraph under macOS.  The recommended way is to use CMake to generate Xcode project files and then use Xcode to build the library. The default project will be able to build Debug or Release libraries, examples, and sample applications.

The alternative is to build OpenSceneGraph from the command line using `make` or `ninja` using the instructions for Unix-like systems above.

Here are some key settings to consider when using CMake:

- BUILD_OSG_EXAMPLES - By default this is turned off. Turn this setting on to compile many great example programs.
- CMAKE_OSX_ARCHITECTURES - Xcode can create applications, executables, libraries, and frameworks that can be run on more than one architecture. Use this setting to indicate the architectures on which to build OSG. x86_64 is the only supported value for OS versions > 10.7.
- OSG_BUILD_APPLICATION_BUNDLES - Normally only executable binaries are created for the examples and sample applications. Turn this option on if you want to create real macOS .app bundles. There are caveats to creating `.app` bundles, see below.
- OSG_DEFAULT_IMAGE_PLUGIN_FOR_OSX - By default macOS uses the `imageio` plugin instead of the plugins for the individual file types (e.g. `jpg`, `gif`, etc.) to load image file types. The `imageio` plugin can handle all popular file formats through the ImageIO framework.
- OSG_WINDOWING_SYSTEM - You have the choice to use Cocoa, Carbon, or X11 when building applications on macOS. Cocoa is the default for OS versions >= 10.5. Carbon and X11 are no longer actively supported, either by Apple or the OSG community.


### APPLICATION BUNDLES (.app bundles)

The example programs when built as application bundles only contain the executable file. They do not contain the dependent libraries as would a normal bundle, so they are not generally portable to other machines.
They also do not know where to find plugins. An environmental variable OSG_LIBRARY_PATH may be set to point to the location where the plugin .so files are located. OSG_FILE_PATH may be set to point to the location where data files are located. Setting OSG_FILE_PATH to the OpenSceneGraph-Data directory is very useful when testing OSG by running the example programs.

Many of the example programs use command-line arguments. When double-clicking on an application (or using the equivalent ""open"" command on the command line) only those examples and applications that do not require command-line arguments will successfully run. The executable file within the .app bundle can be run from the command-line if command-line arguments are needed.


## Section 3. Release notes on iOS build, by Thomas Hogarth

With CMake 3.11, XCode 9.4 and the iOS sdk 11.4 installed you can generate an iOS XCode project using the following command line:

    export THIRDPARTY_PATH=/path/to/3rdParty
    cmake ./ -G Xcode -DOSG_BUILD_PLATFORM_IPHONE:BOOL=ON \
    -DIPHONE_SDKVER=""11.4"" \
    -DIPHONE_VERSION_MIN=""10.0"" \
    -DOPENGL_PROFILE:STRING=GLES3 \
    -DOSG_CPP_EXCEPTIONS_AVAILABLE:BOOL=ON \
    -DBUILD_OSG_APPLICATIONS:BOOL=OFF \
    -DBUILD_OSG_EXAMPLES:BOOL=ON \
    -DOSG_WINDOWING_SYSTEM:STRING=IOS \
    -DOSG_DEFAULT_IMAGE_PLUGIN_FOR_OSX=""imageio"" \
    -DDYNAMIC_OPENSCENEGRAPH:BOOL=OFF \
    -DDYNAMIC_OPENTHREADS:BOOL=OFF \
    -DCURL_INCLUDE_DIR:PATH=""$THIRDPARTY_PATH/curl-ios-device/include"" \
    -DCURL_LIBRARY:PATH=""$THIRDPARTY_PATH/curl-ios-device/lib/libcurl.a"" \
    -DFREETYPE_INCLUDE_DIR_freetype2:PATH=""$THIRDPARTY_PATH/freetype-ios-universal/include/freetype"" \
    -DFREETYPE_INCLUDE_DIR_ft2build:PATH=""$THIRDPARTY_PATH/freetype-ios-universal/include"" \
    -DFREETYPE_LIBRARY:PATH=""$THIRDPARTY_PATH/freetype-ios-universal/lib/libFreetype2.a"" \
    -DTIFF_INCLUDE_DIR:PATH=""$THIRDPARTY_PATH/tiff-ios-device/include"" \
    -DTIFF_LIBRARY:PATH=""$THIRDPARTY_PATH/tiff-ios-device/lib/libtiff.a"" \
    -DGDAL_INCLUDE_DIR:PATH=""$THIRDPARTY_PATH/gdal-ios-device/include"" \
    -DGDAL_LIBRARY:PATH=""$THIRDPARTY_PATH/gdal-ios-device/lib/libgdal.a""


Be sure to set the THIRDPARTY_PATH to the path containing your thirdparty dependencies. Set IPHONE_SDKVER to the version of the iOS sdk you have installed, in this instance 11.4. IPHONE_VERSION_MIN controls the deployment sdk used by xcode, and lastly set OPENGL_PROFILE to the version of GLES you want to use.

Once this completes an XCode project will have been generated in the osg root folder. Open the generated Xcode project, select the example_osgViewerIPhone target. In 'General' tab set a development team.

Once this is done you should be able to build and deploy the `example_osgViewerIPhone` target on your device."
70,RedHatTraining/DO180-apps,JavaScript,"# DO180-apps
DO180 Repository for Sample Applications
"
71,Rarst/release-belt,PHP,"# Release Belt — Composer repo for ZIPs
[![Scrutinizer Code Quality](https://scrutinizer-ci.com/g/Rarst/release-belt/badges/quality-score.png?b=master)](https://scrutinizer-ci.com/g/Rarst/release-belt/?branch=master)
[![Build Status](https://scrutinizer-ci.com/g/Rarst/release-belt/badges/build.png?b=master)](https://scrutinizer-ci.com/g/Rarst/release-belt/build-status/master)
[![Code Coverage](https://scrutinizer-ci.com/g/Rarst/release-belt/badges/coverage.png?b=master)](https://scrutinizer-ci.com/g/Rarst/release-belt/?branch=master)
[![Latest Stable Version](https://poser.pugx.org/rarst/release-belt/version)](https://packagist.org/packages/rarst/release-belt)
[![PHP from Packagist](https://img.shields.io/packagist/php-v/rarst/release-belt.svg)](https://packagist.org/packages/rarst/laps)
[![PDS Skeleton](https://img.shields.io/badge/pds-skeleton-blue.svg?style=flat-square)](https://github.com/php-pds/skeleton)

Release Belt is a Composer repository, which serves to quickly integrate third–party non–Composer releases into Composer workflow. Once Release Belt is installed and you upload your zip files with their respective version number, Release Belt does the rest.

Given the following folder tree:

```
releases/wordpress-plugin/rarst/plugin.1.0.zip
```

It will serve the following Composer repository at `/packages.json` automagically:

```json
{
    ""packages"": {
        ""rarst/plugin"": {
            ""1.0"": {
                ""name"": ""rarst/plugin"",
                ""version"": ""1.0"",
                ""dist"": {
                    ""url"": ""http://example.com/rarst/plugin.1.0.zip"",
                    ""type"": ""zip""
                },
                ""type"": ""wordpress-plugin"",
                ""require"": {
                    ""composer/installers"": ""^1.5""
                }
            }
        }
    }
}
```

## Installation

### 1. Install the project

Release Belt is a `project` type Composer package. It is recommended to use Git checkout to keep up with updates more easily.

There is a helper Composer script provided that tries to fetch latest stable version and performs Composer install. 

#### Install

```bash
git clone https://github.com/Rarst/release-belt
cd release-belt
composer belt-update
```

#### Update

```bash
composer belt-update
```

### 2. Place release ZIPs into `releases/` directory

The directory structure should be: `releases/[type]/[vendor name]/[release zip file]`.

A `[type]` could be:
- a [native Composer type](https://getcomposer.org/doc/04-schema.md#type) (e.g. `library` for the default);
- any type [`composer/installers` supports](https://github.com/composer/installers) (e.g. `wordpress-plugin`);
- or completely arbitrary.

### 3. Configure a web server

The `public/` directory should be used as web root and `index.php` in it as the file to handle requests.

Please refer to [web server configuration](https://www.slimframework.com/docs/v3/start/web-servers.html) in Slim documentation and/or your web hosting’s resources for setup specifics.

Visit home page and `/packages.json` in a web browser to check if it is working.

## Use

Once Release Belt is installed you can add the repository to the `composer.json` of your projects.

Release Belt home page will automatically generate some `composer.json` boilerplate for you to use.

### Configuration

You can configure Release Belt by creating a `config/config.php` file, which returns an array of options to override.

See [`config/configExample.php`](config/configExample.php) for the annotated example.

#### Authentication & permissions

Release Belt implements HTTP authentication to password protect your repository and control access to specific packages. You can configure it via `users` configuration option.

There is a `bin/encodePassword.php` command line helper included for hashing passwords:

```bash
>php bin/encodePassword.php foo
$2y$10$3i9/lVd8UOFIJ6PAMFt8gu3/r5g0qeCJvoSlLCsvMTythye19F77a
```

If authentication is enabled, Release Belt home page will automatically generate `auth.json` boilerplate for you to use.

## F.A.Q.

### Why not Packagist/Satis?

Composer infrastructure is awesome, but it expects vendors that are willing to play nice with it.

Release Belt is a solution for unwilling vendors and it was faster and easier to build a dedicated solution from scratch. 

### Why not artifacts?

Composer artifacts require `composer.json` in them. This is for releases that don't even have that.

### But is it web scale?

No.

# License

MIT
"
72,IonicaBizau/repository-downloader,JavaScript,"<!-- Please do not edit this file. Edit the `blah` field in the `package.json` instead. If in doubt, open an issue. -->


















# Repository Downloader

 [![Support me on Patreon][badge_patreon]][patreon] [![Buy me a book][badge_amazon]][amazon] [![PayPal][badge_paypal_donate]][paypal-donations] [![Ask me anything](https://img.shields.io/badge/ask%20me-anything-1abc9c.svg)](https://github.com/IonicaBizau/ama) [![Version](https://img.shields.io/npm/v/repository-downloader.svg)](https://www.npmjs.com/package/repository-downloader) [![Downloads](https://img.shields.io/npm/dt/repository-downloader.svg)](https://www.npmjs.com/package/repository-downloader) [![Get help on Codementor](https://cdn.codementor.io/badges/get_help_github.svg)](https://www.codementor.io/johnnyb?utm_source=github&utm_medium=button&utm_term=johnnyb&utm_campaign=github)

<a href=""https://www.buymeacoffee.com/H96WwChMy"" target=""_blank""><img src=""https://www.buymeacoffee.com/assets/img/custom_images/yellow_img.png"" alt=""Buy Me A Coffee""></a>







> Download all the repositories from BitBucket and GitHub, including your account, teams and where you created pull requests.





















## Installation

```sh
$ git clone https://github.com/IonicaBizau/repository-downloader.git repo-downloader
$ cd repo-downloader
$ npm i
$ npm i -g git-stats-importer
```

## Usage


 1. Copy `config.tmpl.json` into `config.json` and edit it with your GitHub and BitBucket usernames and passwords. If you're using [two factor authentication](https://help.github.com/articles/about-two-factor-authentication/) on github, please [create an access token](https://developer.github.com/v3/oauth_authorizations/#create-a-new-authorization) and fill it on password field.
 2. Run `./start` and wait! :smile:


:bulb: You can pass arguments which are understood by `git-stats-importer` (e.g. `./start -e 'alice@example.com,bob@example.com'`).

















## :question: Get Help

There are few ways to get help:



 1. Please [post questions on Stack Overflow](https://stackoverflow.com/questions/ask). You can open issues with questions, as long you add a link to your Stack Overflow question.
 2. For bug reports and feature requests, open issues. :bug:
 3. For direct and quick help, you can [use Codementor](https://www.codementor.io/johnnyb). :rocket:
















## :yum: How to contribute
Have an idea? Found a bug? See [how to contribute][contributing].


## :sparkling_heart: Support my projects
I open-source almost everything I can, and I try to reply to everyone needing help using these projects. Obviously,
this takes time. You can integrate and use these projects in your applications *for free*! You can even change the source code and redistribute (even resell it).

However, if you get some profit from this or just want to encourage me to continue creating stuff, there are few ways you can do it:


 - Starring and sharing the projects you like :rocket:
 - [![Buy me a book][badge_amazon]][amazon]—I love books! I will remember you after years if you buy me one. :grin: :book:
 - [![PayPal][badge_paypal]][paypal-donations]—You can make one-time donations via PayPal. I'll probably buy a ~~coffee~~ tea. :tea:
 - [![Support me on Patreon][badge_patreon]][patreon]—Set up a recurring monthly donation and you will get interesting news about what I'm doing (things that I don't share with everyone).
 - **Bitcoin**—You can send me bitcoins at this address (or scanning the code below): `1P9BRsmazNQcuyTxEqveUsnf5CERdq35V6`

    ![](https://i.imgur.com/z6OQI95.png)


Thanks! :heart:
























## :scroll: License

[MIT][license] © [Ionică Bizău][website]






[license]: /LICENSE
[website]: https://ionicabizau.net
[contributing]: /CONTRIBUTING.md
[docs]: /DOCUMENTATION.md
[badge_patreon]: https://ionicabizau.github.io/badges/patreon.svg
[badge_amazon]: https://ionicabizau.github.io/badges/amazon.svg
[badge_paypal]: https://ionicabizau.github.io/badges/paypal.svg
[badge_paypal_donate]: https://ionicabizau.github.io/badges/paypal_donate.svg
[patreon]: https://www.patreon.com/ionicabizau
[amazon]: http://amzn.eu/hRo9sIZ
[paypal-donations]: https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=RVXDDLKKLQRJW
"
73,layerfMRI/repository,MATLAB,"# repository
this does everything: preprocessing, BOLD correction of VASO, motion correction, layering etc. Mostly C++ with a bit of Matlab here and there

90% of it is in C++, 
very little bit in matlab/SPM

Most of the C++ evaluations uses a nii O/I from ODIN. Since it is a bit involved to install I use it in a virtual box. 
My virtual box can be downloaded here: https://nimhactivecho.nimh.nih.gov/t/libijxy7

Example fMRI data of layer activity can be downloaded here: https://activecho.cit.nih.gov/t/i5d1hoj6

For example pipelines see www.layerfMRI.com

E.g.:
 
-> example on how to obtain layers in EPI-space see: https://layerfmri.com/2017/11/26/getting-layers-in-epi-space/

-> example on a VASO evaluation scheme including BOLD correction and VASO-specific motion correction see: https://layerfmri.com/2017/11/25/motion-and-bold-correction/

-> A documentation on how to use the scand-alone C++ nifti-i/o can be found here: https://layerfmri.com/2017/11/30/using-a-standalone-nii-i-o-in-c/




"
74,EgeBalci/Mass-Hacker-Arsenal,Shell,"# Mass-Hacker-Arsenal
Massive arsenal of hacker tools...
"
75,rmccue/test-repository,PHP,This is just a test repository to work with Git commands.
76,renfeng/android-repository,Shell,"# android/repository

It mirrors Android SDKs, which Android Studio built-in and
standalone SDK Manager both can download from. Ideal for a crowded place,
e.g. classroom, [Study Jams](http://developerstudyjams.com/),
and [Code labs](https://codelabs.developers.google.com/?cat=Android).

Tested on 

* Linux Mint Rebecca (17.1, Ubuntu 14.04),
* Windows 7 64bit
* macOS Sierra (10.12.6)

## Prerequisites

* 127 GB storage on your disk, as of 2020-12-30
* wget
  * macOS, `brew install wget` (http://brew.sh/)
  * Windows, install wget with [Cygwin](https://cygwin.com/install.html)

## Server setup

Notes
* Files will be downloaded to your working directory, which can be different than
* The directory the project is cloned into, referred as `${ANDROID_REPOSITORY_HOME}`.
* The mirror can be hosted with an HTTP server. Sample configurations of Apache HTTPd and nginx will be printed at the end.

```bash
${ANDROID_REPOSITORY_HOME}/download2.sh
```

By default, it downloads from https://dl.google.com. To download from some mirror,
set environment variable, `DL_HOST`. e.g.

```bash
export DL_HOST=https://some-mirror
```

See also [Known Mirrors](https://github.com/renfeng/android-repository/wiki/Known-Mirrors).

## Client setup

Set environment variable, `SDK_TEST_BASE_URL`, and launch Android Studio e.g. on macOS

```bash
export SDK_TEST_BASE_URL=http://your-mirror/${DL_PATH}/
open -a 'Android Studio'
```
"
77,aircrack-ng/aircrack-ng-archive,C,"Documentation, tutorials, ... can be found on https://www.aircrack-ng.org
See also manpages and the forum.

Installing
==========

This version now requires more libraries than 0.X versions to be compiled. 
See INSTALLING file for more information

OpenWrt Devices
===============

You can use airodump-ng on OpenWrt devices. You'll have to use specify
prism0 as interface. Airodump-ng will automatically create it.
Rq: Aireplay DOESN'T work on OpenWrt (2.4 kernel) with broadcom chipset since the driver doesn't support injection. It *may* work with 2.6 kernels >= 2.6.24 (kamikaze 8.09+ custom-built).


Known bugs:
===========

Drivers
-------

	Madwifi-ng
	----------
	
	The cause of most of these problems (1, 2 and 3) is that Madwifi-ng cannot easily change the rate in monitor mode.
	Technically, when changing rate while in monitor mode, the raw socket gets invalidated and we have to get it again.
	Madwifi-ng is getting replaced by several drivers: ath5k, ath9k and ar9170.
	
	
	Problem 1: No client can associate to an airbase soft AP.
	Solution: Use a more recent driver. Madwifi-ng has been deprecated for years.
	
	
	Problem 2: When changing rate while you are capturing packet makes airodump-ng stall
	Solution 2: Restart airodump-ng or change rate before starting it.
	
	Problem 3: After some time it stops capturing packets and you're really sure no network manager are running at all.
	Solution 3: That's a known bug in the driver, it may happen at any time (the time before it fails can vary a lot: 
	            from 5 minutes to 50 or even more). Try (as root) unloading completely the driver with 'madwifi-unload'
	            and then run 'modprobe ath_pci autocreate=monitor'.
		  
	
	Problem 4: When creating a new VAP airodump-ng takes up to 10-15 seconds to see the first packet
	Solution 4: It's the behavior of madwifi-ng, don't worry (... be happy ;)).


	Orinoco
	-------

	Problem: BSSID is not reported correctly or is 00:00:00:00:00:00 or signal is not reported correctly.
	Solution: None. Consider replacing your card, orinoco is really really old.


Aircrack-ng
-----------

	Aireplay-ng
	-----------

	Problem: Fakeauth on a WRT54G with WEP (shared authentication) doesn't work.
	Solution: None at this time (we'll try to fix it in an upcoming release).


	Airolib-ng
	----------

	Problem: On windows only, opening/creating a database doesn't work when airolib-ng is in directories containing
	         special characters like '�', '�', '�', '�', ... (directories containing spaces are not affected).
	Reason: It's a SQLite issue.
	Solution: Rename the directory or move the database into another directory.


	Airodump-ng
	-----------

	Problem: Airodump-ng stop working after some time.
	Solution 1: You may have a network manager running that puts back the card in managed mode. 
	            You'll have to disable it (the fastest solution is killing the process) then restart airodump-ng.
	Solution 2: See Problem 3 of Madwifi-ng.
	
	Problem: On windows, it doesn't display a list of adapters like the old 0.X
	Solution: It requires you to develop your own DLL.

	Problem: Handshake is not captured/detected
	Reason: You might be too far and your signal is bad (or too close with a signal too strong).
	        Another possibility is that Airodump-ng didn't detect the handshake properly due to
	        being far apart in the capture.
	Solution 1: Check out our tutorial 'WPA Packet Capture Explained' in the wiki.
	Solution 2: Try running Aircrack-ng on your capture, it might detect the capture.
	Solution 3: Check out our wpaclean tool.
	Note: It will be fixed in an upcoming release.

	Cygwin
	------
	
	Problem: Aircrack-ng doesn't build on Cygwin64
	Solution: None at this time. Build it using 32 bit cygwin.

	Problem: /usr/include/sys/reent.h:14:20: fatal error: stddef.h: No such file or directory
	Solution: It happens because the gcc and g++ version are different. Make sure they are the same.
	
Sample files
============

wep.open.system.authentication.cap:
    It show a connexion (authentication then association) to a WEP network (open authentication).

wep.shared.key.authentication.cap:
    It shows a connexion (authentication then association to a WEP network (shared authentication).
    The difference with open authentication is that the client has to encrypt a challenge text
    and send it back (encrypted) to the AP to prove it has the right key.

wpa.cap:
    This is a sample file with a WPA handshake. It is located in the test/ directory of the install files. 
    The passphrase is 'biscotte'. Use the password file (password.lst) which is in the same directory.

wpa2.eapol.cap: 
    This is a sample file with a WPA2 handshake. 
    It is located in the test/ directory of the install files. 
    The passphrase is '12345678'. Use the password file (password.lst) which is in the same directory.

test.ivs (http://download.aircrack-ng.org/wiki-files/other/test.ivs): 
    This is a 128 bit WEP key file.
    The key is AE:5B:7F:3A:03:D0:AF:9B:F6:8D:A5:E2:C7.

wep_64_ptw.cap (http://dl.aircrack-ng.org/ptw.cap): 
    This is a 64 bit WEP key file suitable for the PTW method.
    The key is '1F:1F:1F:1F:1F'.

wpa-psk-linksys.cap:
    This is a sample file with a WPA1 handshake along with some encrypted packets.
    Useful for testing with airdecap-ng. The password is 'dictionary'.

wpa2-psk-linksys.cap:
    This is a sample file with a WPA2 handshake along with some encrypted packets.
    Useful for testing with airdecap-ng. The password is 'dictionary'.

wps2.0.pcap:
    This is a test file with WPS 2.0 beacon.

password.lst
    This is a sample wordlist for WPA key cracking. More wordlists can be found at
    https://www.aircrack-ng.org/doku.php?id=faq#where_can_i_find_good_wordlists

password.db
    This is a sample airolib-ng database for WPA key cracking.

pingreply.c
    Replies to all ping requests. Useful for testing sniffing/injecting packets with airtun-ng.

Chinese-SSID-Name.pcap
    Contains a beacon with an SSID displayed in Chinese.

verify_inject.py
    Testing DNS requests using airtun-ng.


More sample pcap are available from Wireshark: 
    https://wiki.wireshark.org/SampleCaptures#Wifi_.2F_Wireless_LAN_captures_.2F_802.11
"
78,pfsense/pfsense,PHP,"# pfSense

## Overview

The pfSense project is a free network firewall distribution, based on the FreeBSD operating system with a custom kernel and including third party free software packages for additional functionality. pfSense software, with the help of the package system, is able to provide the same functionality or more of common commercial firewalls, without any of the artificial limitations. It has successfully replaced every big name commercial firewall you can imagine in numerous installations around the world, including Check Point, Cisco PIX, Cisco ASA, Juniper, Sonicwall, Netgear, Watchguard, Astaro, and more.

pfSense software includes a web interface for the configuration of all included components. There is no need for any UNIX knowledge, no need to use the command line for anything, and no need to ever manually edit any rule sets. Users familiar with commercial firewalls catch on to the web interface quickly, though there can be a learning curve for users not familiar with commercial-grade firewalls.

pfSense started in 2004 as a fork of the [m0n0wall](http://m0n0.ch/wall/index.php ""m0n0wall project homepage"") Project (which ended 2015/02/15), though has diverged significantly since.

pfSense is Copyright 2004-2021 [Rubicon Communications, LLC (Netgate)](https://pfsense.org/license ""License Information"") and published under an open source license.
Read more at [https://pfsense.org/](https://pfsense.org/ ""The pfSense homepage"") and support the team by buying bundled hardware appliances or commercial support.

## Contribute

For information on how to contribute to the pfSense project, see [CONTRIBUTING](.github/CONTRIBUTING.md).
"
79,microsoft/WPF-Samples,Rich Text Format,"# WPF-Samples
This repo contains the samples that demonstrate the API usage patterns and popular features for the Windows Presentation Foundation in the .NET Core for Desktop. These samples were initially hosted on [MSDN](https://msdn.microsoft.com/en-us/library/vstudio/ms771633.aspx), and we are gradually 
moving all the interesting WPF samples over to GitHub. All the samples have been retargeted to  [.NET Core 3.0](https://github.com/dotnet/core-sdk).

You can also find an archive of samples targeting .NET 4.7.2 in the [netframework](https://github.com/microsoft/WPF-Samples/tree/netframework) branch.

The samples in this repo are generally about illustrating specific concepts and may go against accessibility best practices. However, the team has spent some time illustrating accessibility best practices in a subset of these samples.

* [ExpenseItIntro](https://github.com/microsoft/WPF-Samples/tree/master/Getting%20Started/WalkthroughFirstWPFApp)
* [ExpenseItDemo](https://github.com/microsoft/WPF-Samples/tree/master/Sample%20Applications/ExpenseIt/ExpenseItDemo)
* [DataBindingDemo](https://github.com/microsoft/WPF-Samples/tree/master/Sample%20Applications/DataBindingDemo)
* [CustomComboBox](https://github.com/microsoft/WPF-Samples/tree/master/Sample%20Applications/CustomComboBox)
* [EditingExaminerDemo](https://github.com/microsoft/WPF-Samples/tree/master/Sample%20Applications/EditingExaminerDemo)

For additional WPF samples, see [WPF Samples](https://msdn.microsoft.com/en-us/library/vstudio/ms771633.aspx).

Please note that the documentation on the repo is still being updated, so all links might not point to the right location.

## License
Unless otherwise mentioned, the samples are released under the [MIT license](https://github.com/Microsoft/WPF-Samples/blob/master/LICENSE)

## Help us improve our samples
Help us improve out samples by sending us a pull-request or opening a [GitHub Issue](https://github.com/Microsoft/WPF-Samples/issues)

Questions: mail wpfteam@microsoft.com

## WPF development

These samples require Visual Studio 2019 to build, test, and deploy, and also require the .NET Core 3 SDK.

   [Get a free copy of Visual Studio 2019 Community Edition with support for building WPF apps](https://www.visualstudio.com/wpf-vs)

   [.NET Core SDK](https://github.com/dotnet/core-sdk)

WPF on .NET Core 3.0 has been open-sourced, and is now available on [Github](https://github.com/dotnet/wpf)

**Note:** If you are using the latest version of Visual Studio 2019 (16.9.0 and above), the .NET Core 3 SDK must be downloaded and installed manually to successfully build these samples. You can find installers for the .NET Core 3 SDK at the [github](https://github.com/dotnet/core-sdk) or at <https://dot.net/core>
   
## Using the samples

To use the samples with Git, clone the WPF-Samples repository with 'git clone https://github.com/microsoft/WPF-Samples'

After cloning the WPF-Samples respository, there will be two solution files in the root directory: WPF-Samples.sln and WPF-Samples.msbuild.sln 

* To build the samples, open one of the solution files in Visual Studio 2019 and build the solution.
* Alternatively, navigate to the directory of a sample and build with 'dotnet build' or 'msbuild' specifying the target project file. 
* WPF-Samples.msbuild.sln contains projects that can be built only with `msbuild` or Visual Studio, and will not compile with `dotnet build`. These projects contain C++ code, for which there is no support in `dotnet build`

The easiest way to use these samples without using Git is to download the zip file containing the current version (using the link below or by clicking the ""Download ZIP"" button on the repo page). You can then unzip the entire archive and use the samples in Visual Studio 2019.

   [Download the samples ZIP](../../archive/master.zip)

   **Notes:** 
   * Before you unzip the archive, right-click it, select Properties, and then select Unblock.
   * Most samples should work independently
   * By default, all the samples target .NET core 3.0. (Installers for the .NET core 3 SDK can be found at <https://dot.net/core>)

For more info about the programming models, platforms, languages, and APIs demonstrated in these samples, please refer to the guidance  available in  [MSDN](https://msdn.microsoft.com/en-us/library/ms754130.aspx). These samples are provided as-is in order to indicate or demonstrate the functionality of the programming models and feature APIs for WPF.
"
80,twosigma/git-meta,JavaScript,"<!--
    Copyright (c) 2016, Two Sigma Open Source
    All rights reserved.

    Redistribution and use in source and binary forms, with or without
    modification, are permitted provided that the following conditions are met:

    * Redistributions of source code must retain the above copyright notice,
      this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright notice,
      this list of conditions and the following disclaimer in the documentation
      and/or other materials provided with the distribution.

    * Neither the name of git-meta nor the names of its
      contributors may be used to endorse or promote products derived from
      this software without specific prior written permission.

    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
    AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
    IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
    ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
    LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
    CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
    SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
    INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
    CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
    ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
    POSSIBILITY OF SUCH DAMAGE.
-->

<p align=""center"">
<img src=""/doc/git-meta-logo.png"" width=""600"">
</p>

[![Build Status](https://travis-ci.org/twosigma/git-meta.svg?branch=master)](https://travis-ci.org/twosigma/git-meta)

# What is git-meta?

Git-meta allows developers to work with extremely large codebases --
performance only degrades very slowly when physical size, number of
files, number of contributors increases, or the depth of history grows.
You can use granular ACLs with git-meta to help refine the scope of work.
Users only need to clone the subsets of the code that they need, yet they
can still make atomic commits across the entire codebase.  Development and
collaboration are done mostly using normal Git commands; we provide a Git
plug-in for ease-of-use.

## A little more detail

Git-meta both describes an architecture and provides a set of tools to
facilitate the implementation of a *mono-repo* and attendant workflows.  Aside
from the ability to install the tools provided in this repository, git-meta
requires only Git.  Git-meta is not tied to any specific Git hosting solution,
and does not provide operations that are hosting-solution-specific, such as the
ability to create new (server-side) repositories.

A detailed description of the architecture of Git-meta is provided in
[doc/architecture.md](doc/architecture.md).

# Getting Started

## Installation

To install the git-meta plugin:

```bash
$ git clone https://github.com/twosigma/git-meta.git
$ cd git-meta/node
$ npm install -g
```

## Quick Start / Basic Usage

### Clone

Clone your organization's meta-repository as you normally would with Git:

```bash
$ git clone http://example.com/my-meta-repo.git meta
$ cd meta
````

At this point, your working directory is likely full of empty directories where
sub-repos are mounted.  Open the one(s) you're interested in working on and
create out a feature branch to work on:

```bash
$ git meta open my-repo
$ git meta checkout -b my-feature
```

Now, change a file:

```bash
$ cd my-repo
$ echo ""new work"" >> some-file
```

Make a commit:

```bash
$ git meta commit -a -m ""I made a change.""
```

And push your change back upstream:

```bash
$ git meta push origin my-feature
```

# Documentation

## User Guide

Run `git meta --help` to see information about git-meta commands, or see the
user guide at [doc/user-guide.md](doc/user-guide.md) for more information.

## Administration

To learn how to set up and maintain a mono-repo using git-meta, please see:
[doc/administration.md](doc/administration.md).

## Architecture

A detailed description of the architecture of Git meta is provided in
[doc/architecture.md](doc/architecture.md).

"
81,winneon/tutorial-repository,Shell,"# Tutorial Repository
Welcome to this tutorial repository! (I really need a different name for this thing.) This README should give you everything you need to setup your own repository on GitHub or any other web-hoster you hold! For the sake of time and money, I'll be using GitHub for this tutorial, because you know, well, it's free.

## Prerequisites
* Linux. Sorry boys, but Cydia runs off of Debian packages (.deb) and the quickest and most efficient way to make these is with a Debian-based Linux installation, such as [Ubuntu](http://ubuntu.com) or [Debian](http://debian.org) itself. But, don't worry, Linux is free! #linuxmasterrace
* Git version 1.7 or higher. (This is only required if you are pushing your repository to a git-based system, such as GitHub, which will be used in this tutorial.) To install this with your average Debian-based Linux distribution, type the command `sudo apt-get install git` in a terminal window.

## Installation
1. Download the zip file [here](https://github.com/WinneonSword/tutorial-repository/archive/master.zip) and extract it to the location of your choice or open up your favourite terminal with Git installed and type in the command `git clone https://github.com/WinneonSword/tutorial-repository.git repo`! This will create the the folder at the location you specified in your terminal with all of this repository's contents inside.
2. ????
3. Hold on, no profit for you yet.

## How It Works
Okay, so now you've downloaded the thing and it's a folder with a bunch of random files in it that make absolutely no sense whatsoever. Now what?! Here's what. You see you unroll it, then you stick it i&mdash; whoops.

Never mind that, the first thing you need to understand is how these debian things work. To start, Cydia, as you probably already know now, is the main program created by /u/saurik for jailbroken iOS devices for easy and quick access to the tweaks and themes that you want. You probably have never thought of how Cydia does this, unless you're into that sort of stuff. Cydia basically to put it into laymon terms, uses Debian packages to download and install these tweaks. Debian packages, more commonly known as the .deb files that Debian uses, are basically advanced .zip files, like Java JARs, also known as .jar files. Debian reads these special .zip files and installs them accordingly with information found in the package's control file. This control file is put into a special location inside the .deb file.

As an example, I have one .deb file in this repository that is already fully packaged and ready to be downloaded in Cydia. If you crack this .deb file with your favourite uncompresser (7-Zip for the win), you will see two folders. A folder marked `DEBIAN`, and a folder marked `var`.

The folder marked `DEBIAN` is the folder that contains the control file, which is called `control`. Bet you didn't see that coming. (I'm so good at jokes! Hehehe... heh....) The control file contains many fields that have specific information that Debian reads when installing the package. We'll get into that in a bit.

The folder marked `var` is where you want your files for your tweak or theme to go. I have no prior experience for tweaking, seeing as I don't own a Mac, but for themers such as myself, the place to put your average theme would be at `/Library/Themes/`. This would mean to make a folder called `Library`, and then a folder called `Themes` inside of that, in your package folder. The reason why the folder in this case is called `var`, is because I'm installing a test file to `/var/mobile/Documents/`. But I'm pretty sure you get the idea.

## The control file
The control file is made up of several fields. I'm going to cover the main ones you will be using. Inside the package folder, which in this case, is called `Package` (I'm no longer inside the example .deb file anymore if you didn't know.), is a `DEBIAN` folder, and inside that is the control file. Crack this puppy open, and you will see several fields. The fields you see should look similar to the ones below.

```markdown
Package: net.winneonsword.package
Name: Test Package
Version: 1.0
Architecture: iphoneos-arm
Description: This is a test package from /u/WinneonSword's tutorial repository!
Depiction: http://repo.winneonsword.net/packages/package.html
Homepage: http://repo.winneonsword.net
Maintainer: Jesse Bryan <winneonsword@gmail.com>
Author: Jesse Bryan <winneonsword@gmail.com>
Section: Themes
```

Let's go down the line and explain what each one means.

* **Package:** This is the name of your package. If you're not familiar with programming, the package name is usually where most of the files the program code relates to goes. The way to name this is like so: `[prefix of your website].[name of your website].[name of package]`. For example, mine says `net.winneonsword.packge`. The `net` is the prefix of my website, the `winneonsword` is the name of my website, which both together puts winneonsword.net, which is my website, and the last part, the `package` is the name of the package. For example's sake, it's just called package in this tutorial.
* **Name:** This is what will show as the name when you are looking at your package in Cydia. You know, the big bold text that is the main attraction.
* **Version:** This is the version of your package. Versioning is something to get into the habit of. The way I do it though, is if it is a initial release, then it obviously is version 1.0. If it is a minor change, then I add a number to the 3rd digit, which in this case would turn into 1.0.1. If it was a major change, then I would add a number to the 2rd digit and remove the 3rd, which would turn into 1.1. Get it?
* **Architecture:** This is the type of the package. **Don't change this.**
* **Description:** This is the description of your package. This text section will appear under entries in Cydia in a list view, or if there is no depiction for the file (below), when viewing the package itself in the dotted-line division area.
* **Depiction:** This is the webpage that pops up when viewing the package itself in Cydia. This part is optional, and should only be used if you know what you are doing around mobile web development. Keep in mind, that if you have a depication specified, the description of the package will *not* show up when viewing the package directly in Cydia.
* **Homepage:** This part is pretty obvious; it's the package's homepage, or mainly your website's homepage.
* **Maintainer:** This is the person who is currently maintaining the package you are making. To put the person's email, enclose it in tag symbols, like so: `<winneonsword@gmail.com>`. This is shown in the above example.
* **Author:** This is the person who originally created the package you are maintaining. This often is the same as the Maintainer.
* **Section:** Last but not least, this is the Section that the package will go under when you view the section view in Cydia. Pretty straightforward, eh?

## Making Your Packages and Sending Them Off
This is reaching the end of this tutorial, so now to conclude the repository making process. Below are step by step instructions on how to make your packages into .deb files, and then send it off to a Git based system, in this case, will be GitHub.

1. Update your packages.sh file. To do so, crack open the packages.sh file, and you should see `dpkg-deb -b Package`. Repeat this pattern for every package that your have in your repository, so if you were to have 3 packages, the packages.sh file should look like so. Keep in mind that the name of the package is the same as the folder name in the repository.

    ```markdown
    dpkg-deb -b Package
    dpkg-deb -b Package2
    dpkg-deb -b Package3
    ```

2. Run the update.sh file in your terminal by typing `./update.sh`. If you get an error mid-way, run this command: `chmod +x packages.sh && chmod +x remove.sh && chmod +x update.sh`. Then, run the file again.
3. Now that you have all of your .debs properly built, you need to send the files along with the Packages.bz2 to the Git repository of your choice. If you are not using GitHub, you are done with these steps. Otherwise, create a GitHub repository on the [GitHub website](https://github.com/) with *this name:* `[your github username].github.com`. Replace the [your github username] part with your actual GitHub username. **This is very important.**
4. Now, in your favourite terminal full of rainbows and sparkles and glitter and ponies and all that neat stuff, type the following set of commands in the repository directory:

    ```markdown
    git init
    git remote add origin https://github.com/[your github username]/[your github username].github.com.git
    git add --all
    git commit -m ""[Put a creative name for this commit here.]""
    git push origin master
    ```

5. ????????????
6. Congratulations! You may have your PROFIT you've been waiting for. Your repository should now be up and running at `http://[your github username].github.com/`! Add that web address to your Cydia sources and behold your packages should appear!

## Conclusion
Thank you for listening for my ongoing blabber about Debian and how things work. This method is how you would get an actual repository up and running without using myrepospace or any similar service. This method also allows full control over all of your packages to your hearts content, and it allows a neat and clean way to redirect the web address to your custom domain of your choice, using GitHub's CNAME system. (I will not be going into that in this tutorial. Sorry!) I hope you enjoyed this tutorial, and farewell!

&mdash; /u/WinneonSword"
82,kodibae/repository.kodibae,Python,"# Installing the Exodus Addon for Kodi

<b>Get Better Streaming and Support Exodus with a VPN from <a href=""https://www.exodusvpn.com/"">EXODUSVPN.COM</a></b>

1. Launch Kodi
2. Settings (top Left)
3. File manager
4. Add source
5. <None> (Enter the paths or browse for media locations)
6. https://kodibae.github.io
7. OK
8. Enter a name for this media source
9. kodibae
10. OK
11. OK
12. Press back twice
13. Add-ons
14. Box icon (top left)
15. Install from zip file
16. Settings (if unknown sources warning is displayed, otherwise skip to step 21)
17. Toggle Unknown sources
18. Yes
19. Press back
20. Install from zip file
21. kodibae
22. repository.kodibae-3.0.0.zip
23. Install from repository
24. Kodi Bae Repository
25. Video add-ons
26. Exodus
27. Install
28. OK
29. Press back four times
30. Launch Exodus

Kodi Bae Repository - Kodi is a registered trademark of the XBMC Foundation. We are not connected to or in any other way affiliated with Kodi - DMCA: kodibae@openmail.cc
"
83,besnik/generic-repository,C#,"# Generic Repository
GenericRepository project is generic implementation of Repository pattern in .NET.
For detailed discussion please see project's wiki pages and especially [Introduction](https://github.com/besnik/generic-repository/wiki/Introduction).


# Lightweight
It is lightweight thin layer between domain model and data mappers (e.g. ORMs like NHibernate, Linq2Sql or Entity Framework). The goal is to avoid recreating same repositories over and over again in all projects where repository pattern is used. Designed with respect to DDD (domain driven design). Implements Filter pattern and best used with factory and/or Service locator patter (DI/IoC). I used the name specification, but it turned out to be confusing of other design pattern, so from now on I call it filters.

Example of usage:
``` java
var customer = new Customer { Name = ""Peter Bondra"", Age = 37 };
var specificationLocator = this.IoC.Resolve<ISpecificationLocator>();

using ( var unitOfWork = this.IoC.Resolve<IUnitOfWorkFactory>().BeginUnitOfWork() )
{
  ICustomerRepository cr = this.IoC.Resolve<ICustomerRepository>(unitOfWork, specificationLocator);
  
  using ( var transaction = unitOfWork.BeginTransaction() )
  {
	cr.Insert(customer);
	transaction.Commit();
  }
}
```

# Fluent filter pattern
The generic repository natively supports filter pattern that decouples the filter logic (queries) from the repository. It contains extension point for your custom filters and the filter interface (called specification) shall be implemented using fluent interface pattern.

Example of fluent filter pattern usage (I call it specification in the code):
``` java
ICustomerRepository customerRepository = 
  this.IoC.Resolve<ICustomerRepository>(unitOfWork, specificationLocator);

IList<Customer> = customerRepository.Specify<ICustomerSpecification>()
  .NameStartsWith(""Peter"")
  .OlderThan(18)
  .ToResult()
  .Take(3)
  .ToList();
```

# CRUD and DRY
Don't repeat yourself. GenericRepository provides implementation of repository patter, so you can focus on your domain model and business rules, instead of fighting with specific data mapper technology (some knowledge is still required for doing mapping configuration). You can start loading and saving data to the data storage extremly fast. If the Generic repository does not support requested functionalit natively, you can still very easily use underlying data mapper.


# Abstraction of data mappers
Nice sideeffect is that it is very easy to switch between several data mappers, for example from Entity Framework to NHibernate and so on.

# Learning resource
Last but not least: great learning resource. By implementing generic repository with various data mappers, it is very easy to see the design and architecture differences between the libraries. For example, GetById method differs in LinqToSql and NHibernate, because NHibernate accepts type Object as ID, in comparing to LinqToSql that is strongly typed. This small difference makes it more complex to implement base class for LinqToSql repositories as you can see in the sources. Note that IGenericRepository and it's methods are all strongly typed, the difference is just under the hood. So that was one example. Other can be found in the source codes.


# Unit tested
Ships with unit tests.
Note: when running tests, make sure to adapt connection strings in config directory (or data mapper configuration). There is a plan to make default data provider some file based database like sql ce, so the unit tests would work on of the box.

# Well documented
Source codes fully documented.
External documentation and pattern explanation can be found at my [blog](http://besnikgeek.blogspot.com/search/label/generic%20repository).

# Feedback
I would really like to get feedback what features are missing for your project. The layer is very thin and it is easy to extend and add new features. Contributors are welcome.

# Dependencies
The solution file is for Visual Studio 2010 and projects are targeting CLR 4.0 (.NET 4.0);
If there is a need for older version of CLR, it is no problem to build it for CLR 2.0 (.NET 2.0, 3.0, 3.5). Some of the linked libraries may still be compiled for CLR 2.0, the plan is to migrate everything to CLR 4.0.
Unit tests are using NUnit and Mock frameworks.


# Licence
Open to use in commerce and non-commerce projects.

# Supported data mappers
  * NHibernate
  * Linq2Sql
  * Entity Framework

Plan is to implement suppport for
  * NoSql data mappers like Mongo, RavenDb, CouchDB, etc.
  * Anything the community requests

You can always implement IGenericRepository interface for your favourite data mapper and push the implementation."
84,ethereum/EIPs,Solidity,"# Ethereum Improvement Proposals (EIPs)

[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/ethereum/EIPs?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)

Ethereum Improvement Proposals (EIPs) describe standards for the Ethereum platform, including core protocol specifications, client APIs, and contract standards.

**Before you initiate a pull request**, please read the [EIP-1](https://eips.ethereum.org/EIPS/eip-1) process document. Ideas should be thoroughly discussed prior to opening a pull request, such as on the [Ethereum Magicians forums](https://ethereum-magicians.org) or in a GitHub issue in this repository.

This repository tracks the ongoing status of EIPs. It contains:

- [Draft](https://eips.ethereum.org/all#draft) proposals which intend to complete the EIP review process.
- [Last Call](https://eips.ethereum.org/all#last-call) for proposals that may become final (see also [RSS feed](https://eips.ethereum.org/last-call.xml)).
- [Accepted](https://eips.ethereum.org/all#accepted) proposals which are awaiting implementation or deployment by Ethereum client developers.
- [Final](https://eips.ethereum.org/all#final) and [Active](https://eips.ethereum.org/all#active) proposals that are recorded.
- The [EIP process](./EIPS/eip-1.md#eip-work-flow) that governs the EIP repository.

Achieving ""Final"" status in this repository only represents that a proposal has been reviewed for technical accuracy. It is solely the responsibility of the reader to decide whether a proposal will be useful to them.

Browse all current and draft EIPs on [the official EIP site](https://eips.ethereum.org/).

Once your first PR is merged, we have a bot that helps out by automatically merging PRs to draft EIPs. For this to work, it has to be able to tell that you own the draft being edited. Make sure that the 'author' line of your EIP contains either your GitHub username or your email address inside \<triangular brackets>. If you use your email address, that address must be the one publicly shown on [your GitHub profile](https://github.com/settings/profile).

## Project Goal

The Ethereum Improvement Proposals repository exists as a place to share concrete proposals with potential users of the proposal and the Ethereum community at large.

## Preferred Citation Format

The canonical URL for a EIP that has achieved draft status at any point is at https://eips.ethereum.org/. For example, the canonical URL for EIP-1 is https://eips.ethereum.org/EIPS/eip-1.

Please consider anything which is not published on https://eips.ethereum.org/ as a working paper.

And please consider anything published at https://eips.ethereum.org/ with a status of ""draft"" as an incomplete draft.

# Validation

EIPs must pass some validation tests.  The EIP repository ensures this by running tests using [html-proofer](https://rubygems.org/gems/html-proofer) and [eip_validator](https://rubygems.org/gems/eip_validator).

It is possible to run the EIP validator locally:
```sh
gem install eip_validator
eip_validator <INPUT_FILES>
```

# Automerger

The EIP repository contains an ""auto merge"" feature to ease the workload for EIP editors.  If a change is made via a PR to a draft EIP, then the authors of the EIP can GitHub approve the change to have it auto-merged by the [eip-automerger](https://github.com/eip-automerger/automerger) bot.

# Local development

## Prerequisites

1. Open Terminal.

2. Check whether you have Ruby 2.1.0 or higher installed:

```sh
$ ruby --version
```

3. If you don't have Ruby installed, install Ruby 2.1.0 or higher.

4. Install Bundler:

```sh
$ gem install bundler
```

5. Install dependencies:

```sh
$ bundle install
```

## Build your local Jekyll site

1. Bundle assets and start the server:

```sh
$ bundle exec jekyll serve
```

2. Preview your local Jekyll site in your web browser at `http://localhost:4000`.

More information on Jekyll and GitHub pages [here](https://help.github.com/en/enterprise/2.14/user/articles/setting-up-your-github-pages-site-locally-with-jekyll).
"
85,gperftools/gperftools,C++,"gperftools
----------
(originally Google Performance Tools)

The fastest malloc we’ve seen; works particularly well with threads
and STL. Also: thread-friendly heap-checker, heap-profiler, and
cpu-profiler.


OVERVIEW
---------

gperftools is a collection of a high-performance multi-threaded
malloc() implementation, plus some pretty nifty performance analysis
tools.

gperftools is distributed under the terms of the BSD License. Join our
mailing list at gperftools@googlegroups.com for updates:
https://groups.google.com/forum/#!forum/gperftools

gperftools was original home for pprof program. But do note that
original pprof (which is still included with gperftools) is now
deprecated in favor of golang version at https://github.com/google/pprof


TCMALLOC
--------
Just link in -ltcmalloc or -ltcmalloc_minimal to get the advantages of
tcmalloc -- a replacement for malloc and new.  See below for some
environment variables you can use with tcmalloc, as well.

tcmalloc functionality is available on all systems we've tested; see
INSTALL for more details.  See README_windows.txt for instructions on
using tcmalloc on Windows.

when compiling.  gcc makes some optimizations assuming it is using its
own, built-in malloc; that assumption obviously isn't true with
tcmalloc.  In practice, we haven't seen any problems with this, but
the expected risk is highest for users who register their own malloc
hooks with tcmalloc (using gperftools/malloc_hook.h).  The risk is
lowest for folks who use tcmalloc_minimal (or, of course, who pass in
the above flags :-) ).


HEAP PROFILER
-------------
See docs/heapprofile.html for information about how to use tcmalloc's
heap profiler and analyze its output.

As a quick-start, do the following after installing this package:

1) Link your executable with -ltcmalloc
2) Run your executable with the HEAPPROFILE environment var set:
     $ HEAPPROFILE=/tmp/heapprof <path/to/binary> [binary args]
3) Run pprof to analyze the heap usage
     $ pprof <path/to/binary> /tmp/heapprof.0045.heap  # run 'ls' to see options
     $ pprof --gv <path/to/binary> /tmp/heapprof.0045.heap

You can also use LD_PRELOAD to heap-profile an executable that you
didn't compile.

There are other environment variables, besides HEAPPROFILE, you can
set to adjust the heap-profiler behavior; c.f. ""ENVIRONMENT VARIABLES""
below.

The heap profiler is available on all unix-based systems we've tested;
see INSTALL for more details.  It is not currently available on Windows.


HEAP CHECKER
------------
See docs/heap_checker.html for information about how to use tcmalloc's
heap checker.

In order to catch all heap leaks, tcmalloc must be linked *last* into
your executable.  The heap checker may mischaracterize some memory
accesses in libraries listed after it on the link line.  For instance,
it may report these libraries as leaking memory when they're not.
(See the source code for more details.)

Here's a quick-start for how to use:

As a quick-start, do the following after installing this package:

1) Link your executable with -ltcmalloc
2) Run your executable with the HEAPCHECK environment var set:
     $ HEAPCHECK=1 <path/to/binary> [binary args]

Other values for HEAPCHECK: normal (equivalent to ""1""), strict, draconian

You can also use LD_PRELOAD to heap-check an executable that you
didn't compile.

The heap checker is only available on Linux at this time; see INSTALL
for more details.


CPU PROFILER
------------
See docs/cpuprofile.html for information about how to use the CPU
profiler and analyze its output.

As a quick-start, do the following after installing this package:

1) Link your executable with -lprofiler
2) Run your executable with the CPUPROFILE environment var set:
     $ CPUPROFILE=/tmp/prof.out <path/to/binary> [binary args]
3) Run pprof to analyze the CPU usage
     $ pprof <path/to/binary> /tmp/prof.out      # -pg-like text output
     $ pprof --gv <path/to/binary> /tmp/prof.out # really cool graphical output

There are other environment variables, besides CPUPROFILE, you can set
to adjust the cpu-profiler behavior; cf ""ENVIRONMENT VARIABLES"" below.

The CPU profiler is available on all unix-based systems we've tested;
see INSTALL for more details.  It is not currently available on Windows.

NOTE: CPU profiling doesn't work after fork (unless you immediately
      do an exec()-like call afterwards).  Furthermore, if you do
      fork, and the child calls exit(), it may corrupt the profile
      data.  You can use _exit() to work around this.  We hope to have
      a fix for both problems in the next release of perftools
      (hopefully perftools 1.2).


EVERYTHING IN ONE
-----------------
If you want the CPU profiler, heap profiler, and heap leak-checker to
all be available for your application, you can do:
   gcc -o myapp ... -lprofiler -ltcmalloc

However, if you have a reason to use the static versions of the
library, this two-library linking won't work:
   gcc -o myapp ... /usr/lib/libprofiler.a /usr/lib/libtcmalloc.a  # errors!

Instead, use the special libtcmalloc_and_profiler library, which we
make for just this purpose:
   gcc -o myapp ... /usr/lib/libtcmalloc_and_profiler.a


CONFIGURATION OPTIONS
---------------------
For advanced users, there are several flags you can pass to
'./configure' that tweak tcmalloc performance.  (These are in addition
to the environment variables you can set at runtime to affect
tcmalloc, described below.)  See the INSTALL file for details.


ENVIRONMENT VARIABLES
---------------------
The cpu profiler, heap checker, and heap profiler will lie dormant,
using no memory or CPU, until you turn them on.  (Thus, there's no
harm in linking -lprofiler into every application, and also -ltcmalloc
assuming you're ok using the non-libc malloc library.)

The easiest way to turn them on is by setting the appropriate
environment variables.  We have several variables that let you
enable/disable features as well as tweak parameters.

Here are some of the most important variables:

HEAPPROFILE=<pre> -- turns on heap profiling and dumps data using this prefix
HEAPCHECK=<type>  -- turns on heap checking with strictness 'type'
CPUPROFILE=<file> -- turns on cpu profiling and dumps data to this file.
PROFILESELECTED=1 -- if set, cpu-profiler will only profile regions of code
                     surrounded with ProfilerEnable()/ProfilerDisable().
CPUPROFILE_FREQUENCY=x-- how many interrupts/second the cpu-profiler samples.

PERFTOOLS_VERBOSE=<level> -- the higher level, the more messages malloc emits
MALLOCSTATS=<level>    -- prints memory-use stats at program-exit

For a full list of variables, see the documentation pages:
   docs/cpuprofile.html
   docs/heapprofile.html
   docs/heap_checker.html


COMPILING ON NON-LINUX SYSTEMS
------------------------------

Perftools was developed and tested on x86 Linux systems, and it works
in its full generality only on those systems.  However, we've
successfully ported much of the tcmalloc library to FreeBSD, Solaris
x86, and Darwin (Mac OS X) x86 and ppc; and we've ported the basic
functionality in tcmalloc_minimal to Windows.  See INSTALL for details.
See README_windows.txt for details on the Windows port.


PERFORMANCE
-----------

If you're interested in some third-party comparisons of tcmalloc to
other malloc libraries, here are a few web pages that have been
brought to our attention.  The first discusses the effect of using
various malloc libraries on OpenLDAP.  The second compares tcmalloc to
win32's malloc.
  http://www.highlandsun.com/hyc/malloc/
  http://gaiacrtn.free.fr/articles/win32perftools.html

It's possible to build tcmalloc in a way that trades off faster
performance (particularly for deletes) at the cost of more memory
fragmentation (that is, more unusable memory on your system).  See the
INSTALL file for details.


OLD SYSTEM ISSUES
-----------------

When compiling perftools on some old systems, like RedHat 8, you may
get an error like this:
    ___tls_get_addr: symbol not found

This means that you have a system where some parts are updated enough
to support Thread Local Storage, but others are not.  The perftools
configure script can't always detect this kind of case, leading to
that error.  To fix it, just comment out (or delete) the line
   #define HAVE_TLS 1
in your config.h file before building.


64-BIT ISSUES
-------------

There are two issues that can cause program hangs or crashes on x86_64
64-bit systems, which use the libunwind library to get stack-traces.
Neither issue should affect the core tcmalloc library; they both
affect the perftools tools such as cpu-profiler, heap-checker, and
heap-profiler.

1) Some libc's -- at least glibc 2.4 on x86_64 -- have a bug where the
libc function dl_iterate_phdr() acquires its locks in the wrong
order.  This bug should not affect tcmalloc, but may cause occasional
deadlock with the cpu-profiler, heap-profiler, and heap-checker.
Its likeliness increases the more dlopen() commands an executable has.
Most executables don't have any, though several library routines like
getgrgid() call dlopen() behind the scenes.

2) On x86-64 64-bit systems, while tcmalloc itself works fine, the
cpu-profiler tool is unreliable: it will sometimes work, but sometimes
cause a segfault.  I'll explain the problem first, and then some
workarounds.

Note that this only affects the cpu-profiler, which is a
gperftools feature you must turn on manually by setting the
CPUPROFILE environment variable.  If you do not turn on cpu-profiling,
you shouldn't see any crashes due to perftools.

The gory details: The underlying problem is in the backtrace()
function, which is a built-in function in libc.
Backtracing is fairly straightforward in the normal case, but can run
into problems when having to backtrace across a signal frame.
Unfortunately, the cpu-profiler uses signals in order to register a
profiling event, so every backtrace that the profiler does crosses a
signal frame.

In our experience, the only time there is trouble is when the signal
fires in the middle of pthread_mutex_lock.  pthread_mutex_lock is
called quite a bit from system libraries, particularly at program
startup and when creating a new thread.

The solution: The dwarf debugging format has support for 'cfi
annotations', which make it easy to recognize a signal frame.  Some OS
distributions, such as Fedora and gentoo 2007.0, already have added
cfi annotations to their libc.  A future version of libunwind should
recognize these annotations; these systems should not see any
crashes.

Workarounds: If you see problems with crashes when running the
cpu-profiler, consider inserting ProfilerStart()/ProfilerStop() into
your code, rather than setting CPUPROFILE.  This will profile only
those sections of the codebase.  Though we haven't done much testing,
in theory this should reduce the chance of crashes by limiting the
signal generation to only a small part of the codebase.  Ideally, you
would not use ProfilerStart()/ProfilerStop() around code that spawns
new threads, or is otherwise likely to cause a call to
pthread_mutex_lock!

---
17 May 2011
"
86,samvera/active_fedora,Ruby,"# ActiveFedora

Code: [![Version](https://badge.fury.io/rb/active-fedora.png)](http://badge.fury.io/rb/active-fedora)
[![Build Status](https://circleci.com/gh/samvera/active_fedora.svg?style=svg)](https://circleci.com/gh/samvera/active_fedora)
[![Coverage Status](https://coveralls.io/repos/github/samvera/active_fedora/badge.svg?branch=master)](https://coveralls.io/github/samvera/active_fedora?branch=master)

Docs: [![Contribution Guidelines](http://img.shields.io/badge/CONTRIBUTING-Guidelines-blue.svg)](./CONTRIBUTING.md)
[![Apache 2.0 License](http://img.shields.io/badge/APACHE2-license-blue.svg)](./LICENSE)

Jump in: [![Slack Status](http://slack.samvera.org/badge.svg)](http://slack.samvera.org/)

# What is ActiveFedora?

ActiveFedora is a Ruby gem for creating and
managing objects in the Fedora Repository Architecture
([http://fedora-commons.org](http://fedora-commons.org)). ActiveFedora
is loosely based on “ActiveRecord” in Rails. Version 9.0+ works with Fedora 4 and prior versions work on Fedora 3. Version 9.2+ works with Solr 4.10. Version 10.0+ works with Fedora >= 4.5.1.

## Product Owner & Maintenance
ActiveFedora is a Core Component of the Samvera community. The documentation for
what this means can be found
[here](http://samvera.github.io/core_components.html#requirements-for-a-core-component).

### Product Owner

[no-reply](https://github.com/no-reply)

# Help

The Samvera community is here to help. Please see our [support guide](./SUPPORT.md).

# Getting Started

The [Dive into Hydra](https://github.com/samvera/hydra/wiki/Dive-into-Hydra)
gives you a brief tour through ActiveFedora’s features on the command line.

## Prerequisites

- A Fedora Commons Repository installation (configured by URL in fedora.yml)
- A Solr index (configured by URL in solr.yml)
- A JDK8+ installation (if running the test suite)

## Installation

The gem is hosted on rubygems.

```bash
gem install active-fedora
```

## Generators

You can generate a model inheriting from ActiveFedora::Base.

```bash
rails generate active_fedora:model Book
```

## Testing (this Gem)

In order to run the RSpec tests, you need to have a copy of the
ActiveFedora source code, and then run bundle install in the source
directory. You can download the source code by doing the following:

```bash
git clone https://github.com/samvera/active_fedora.git
cd active_fedora
bundle install
```

### Using the continuous integration server

You can test ActiveFedora using the same process as our continuous
integration server. This will automatically pull down a copy of Solr and Fedora Content Repository.

The `ci` rake task will download solr and fedora, start them,
and run the tests for you.

```bash
rake active_fedora:ci
```

### Testing Manually

If you want to run the tests manually, follow these instructions:

```bash
solr_wrapper
```

To start FCRepo, open another shell and run:

```bash
fcrepo_wrapper -p 8986
```

Now you’re ready to run the tests. In the directory where active\_fedora
is installed, run:

```bash
rake spec
```

# Release Process

The [release process](https://github.com/samvera/active_fedora/wiki/Release-management-process) is documented on the wiki.

# Acknowledgments

This software has been developed by and is brought to you by the Samvera community.  Learn more at the
[Samvera website](http://samvera.org/).

![Samvera Logo](https://wiki.duraspace.org/download/thumbnails/87459292/samvera-fall-font2-200w.png?version=1&modificationDate=1498550535816&api=v2)
"
87,geoserver/geoserver,Java,"<img src=""/doc/en/themes/geoserver/static/GeoServer_500.png"" width=""353"">

[![Gitter](https://badges.gitter.im/geoserver/geoserver.svg)](https://gitter.im/geoserver/geoserver?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

[GeoServer](http://geoserver.org) is an open source software server written in Java that 
allows users to share and edit geospatial data. Designed for interoperability, it publishes data from 
any major spatial data source using open standards.

Being a community-driven project, GeoServer is developed, tested, and supported by a diverse group of 
individuals and organizations from around the world.

GeoServer is the reference implementation of the Open Geospatial Consortium (OGC) 
Web Feature Service (WFS) and Web Coverage Service (WCS) standards, as well as a high performance 
certified compliant Web Map Service (WMS), compliant Catalog Service for the Web (CSW)
and implementing Web Processing Service (WPS). 
GeoServer forms a core component of the Geospatial Web.

## License

GeoServer licensed under the [GPL](http://docs.geoserver.org/latest/en/user/introduction/license.html).

## Using

Please refer to the [user guide](http://docs.geoserver.org/latest/en/user/) for information
on how to install and use GeoServer.

## Building

GeoServer uses [Apache Maven](http://maven.apache.org/) for a build system. To 
build the application run maven from the ```src``` directory.

    mvn clean install

See the [developer guide](http://docs.geoserver.org/latest/en/developer/) 
for more details.

## Bugs

GeoServer uses [JIRA](https://osgeo-org.atlassian.net/projects/GEOS), hosted by 
[Atlassian](https://www.atlassian.com/), for issue tracking.

## Mailing Lists

The [mailing list page](http://geoserver.org/comm/) on the GeoServer web site provides
access to the various mailing list, as well as some indication of the [code of conduct](http://geoserver.org/comm/userlist-guidelines.html) when posting to the lists

## Contributing

Please read [the contribution guidelines](https://github.com/geoserver/geoserver/blob/main/CONTRIBUTING.md) before contributing pull requests to the GeoServer project.

## More Information

Visit the [website](http://geoserver.org/) or read the [docs](http://docs.geoserver.org/). 

"
88,zhaohehe/z-repository,PHP,"# z-repository

>z-repository是一个为laravel5提供的数据库抽象层，目的是为了将应用的数据库操作和核心的业务逻辑分离开，保证controller的精致。

## 简介
>z-repository提供了criteria和transformer来接管数据库的查询和查询结果的展示，使得各部分分离开来，解开耦合。同时repository接管model层，使得model层专注于数据模型本身的定义，比如relationship，fillable等

## 安装

在终端中输入以下命令，通过composer来安装


 ```bash
 composer require ""zhaohehe/zrepository""

 php artisan package:discover
 ```



## 使用

### repository

首先，创建你的repository类，你可以在命令行中使用如下命令自动生成该类
```bash
php artisan make:repository Poem --model Poem
```
其中，--model Poem 是可选的，用来指定repository中model的名称，默认情况下，会根据repository的名称自动产生model名，你可以在repository.php配置文件中设置该类的命名空间等，后面的criteria和transforme的自动生成也是这样，生成的文件如下：

```php
<?php namespace App\Repositories;

use Zhaohehe\Repositories\Eloquent\Repository;

class PoemRepository extends Repository 
{

    public function model() 
    {
        return 'App\Models\Poem';
    }
}
```
当然，你也可以手动创建repository类，该类务必继承自```Zhaohehe\Repositories\Eloquent\Repository``` ，且实现model()方法，该方法用来指定该repository对应的数据模型。
现在，来创建你的```App\Poem``` 数据模型：


```php
<?php namespace App;

use Illuminate\Database\Eloquent\Model;

class Poem extends Model 
{

}
```

最后，在你的controller中使用repository

```php
<?php namespace App\Http\Controllers;

use App\Repositories\PoemRepository as Poem;

class PoemController extends Controller 
{

    protected $poem;

    public function __construct(Poem $poem) 
    {

        $this->poem = $poem;
    }

    public function index() 
    {
        return $this->poem->all();
    }
}
```

#### 暂时可用的方法

下面这些方法是暂时可以使用的，显然不够，后面会陆续添加

##### Zhaohehe\Repositories\Contracts\RepositoryInterface

```php
public function all($columns = ['*']);    //获取所有记录

public function paginate($perPage = 15, $columns = ['*']);    //分页，默认每页15条

public function create(array $data);    //创建一条记录

public function save(array $data);    //保存

public function delete($id);    //删除一条记录

public function update(array $data, $id);    //更新记录

public function find($id, $columns = ['*']);    //按id查找

public function findBy($field, $value, $columns = ['*']);    //按指定字段查找

public function findWhere($where, $columns = ['*']);    //按多个条件查找
```


#### 例子


创建一条记录:

```php
$this->Poem->create(Input::all());
```

更新记录:

```php
$this->Poem->update(Input::all(), $id);
```

删除记录:

```php
$this->poem->delete($id);
```

按id查找;

```php
$this->poem->find($id);
```

你可以指定要查询的字段:

```php
$this->poem->find($id, ['title', 'description', 'author']);
```

根据指定字段的值来查找.

```php
$this->poem->findBy('title', $title);
```

多个条件查找

```php
$this->poem->findWhere([
    'author' => $author_id,
    ['year','>',$year]
]);
```

### Criteria

>Criteria是一个让你可以根据具体的或者一系列复杂的条件来向你的repository发起查询的方式，你可以将一些可能会在多个接口或者情况下用到的查询条件放到这里，到达复用的目的，而且可以将复杂的查询条件从你的controller中抽离出来，精简代码的同时，也使得各部分之间的耦合更加松散，你的criteria类必须继承自```Zhaohehe\Repositories\Criteria\Criteria``` 抽象类。

一个简单的例子，比如你要查询所有唐代的诗词，（实际情况下，查询的条件可能要复杂的多，否则就没必要将它抽离出来了）:

```php
<?php namespace App\Repositories\Criteria\Poem;

use Zhaohehe\Repositories\Criteria\Criteria;
use Zhaohehe\Repositories\Contracts\RepositoryInterface as Repository;

class CreatedInTangDynasty extends Criteria 
{

    public function apply($model, Repository $repository)
    {
        $model = $model->where('dynasty', '=', '唐朝');
        return $model;
    }
}
```

现在，在你的controller里面，你可以调用repository的```pushCriteria```方法:

```php
<?php namespace App\Http\Controllers;

use App\Repositories\Criteria\CreatedInTangDynasty;
use App\Repositories\PoemRepository as Poem;

class PoemController extends Controller 
{
    private $poem;

    public function __construct(Poem $poem) 
    {
        $this->poem = $poem;
    }

    public function index() 
    {
        $this->poem->pushCriteria(new CreatedInTangDynasty());
        return $this->poem->all();
    }
}
```

### Transformer
>Transformers 的作用是按照接口的需要来包装你从数据库查询出来的结果，你可以在这里方便的设置你需要哪些字段，每一个字段的数据类型，或者你要联查多个表来组成接口所需要的数据时，你可以在这里利用eloquent的relationship方便的完成，每一个Transformer都需要继承自```League\Fractal\TransformerAbstract```抽象类，这是一个第三方的包，需要你用composer引入

 ```composer require league/fractal```

###### 用以下命令创建一个Transformer

```bash
php artisan make:transformer Poem
```
这会创建下面这样的一个Transformer类，比如你需要从接口返回一首诗的id，title，和author，而你的Poem表里面只存有author_id，你可以用Model的relationship方便的做到，这需要你在Poem Model中去定义和Author Model之间的关系，这部分详情请看官方文档的介绍：

```php
use App\Models\Poem;
use League\Fractal\TransformerAbstract;

class PoemTransformer extends TransformerAbstract
{
    public function transform(Poem $model)
    {
        return [
            'id'      => (int) $model->id,
            'title'   => $model->title,
            'author'  => $model->author->name
        ];
    }
}
```
###### 使用repository

在repository中使用
```php
namespace App\Repositories;
use Zhaohehe\Repositories\Eloquent\Repository;

class PoemRepository extends Repository
{
    public function model()
    {
        return 'App\Poem';
    }
    
    public function transfomer()
    {
        return ""App\\Transformers\\PoemTransformer"";
    }
}
```
你也可以在controller中调用```setTransformer```方法来使用
```php
$this->poem->setTransformer(""App\\Transformers\\PoemTransformer"");
```

###### 在Model后使用
当你在对repository使用了transformer之后，也许在某些场景下你不希望查询结果自动的被transform掉，你可以调用repository的```skipTransformer()```方法来跳过转换。这个时候你可以让你的Model去实现```Zhaohehe\Repositories\Contracts\Transformable```接口，这样你就可以在你想要的时候直接调用Model的```transform```方法来灵活地呈现你的查询结果的样式，当你的Model实现了```Zhaohehe\Repositories\Contracts\Transformable```接口之后，你必须要使用```Zhaohehe\Repositories\Traits\TransformableTraits```来赋予Model相应的功能，代码如下：

```php
<?php

namespace App;

use Illuminate\Database\Eloquent\Model;
use Zhaohehe\Repositories\Contracts\Transformable;
use Zhaohehe\Repositories\Traits\TransformableTraits;

class Poem extends Model implements Transformable
{
    use TransformableTraits;
}

```
现在，你可以随心所欲地transform查询结果了：
```php
$repository = app('App\Repositories\PoemRepository');
$repository->setTransformer(""App\\Transformers\\PoemTransformerr"");

$poem = $repository->find(1);    //获取被转换过的查询结果

dd( $poem );    //这会返回一个按照Poemtransformer定义过的数组

...

$poem = $repository->skipTransformer()->find(1);    //跳过transformer，返回原始的数据模型查询结果

dd( $poem );    //这会返回一个普通model的查询结果
dd( $posem->transform() );    //调用$poem的```transform```方法，返回转换过的结果
```
### EventObserver

>事件观察者允许你在repository中方便的针对数据库操作流程中的某一个具体的节点绑定你想要执行的事件

#### 在Model中实现接口
如果你想开启该功能，你需要在你的Model类中实现```Zhaohehe\Repositories\Contracts\ModelEventInterface```接口，并使用```Zhaohehe\Repositories\Traits\ModelEventTraits```来赋予你的数据模型该功能，你的Model类大概会长成这样：
```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Model;
use Zhaohehe\Repositories\Traits\ModelEventTraits;
use Zhaohehe\Repositories\Contracts\ModelEventInterface;

class Poem extends Model implements ModelEventInterface
{
    use ModelEventTraits;

    protected $fillable = [
   
    ];

}
```
#### 在repository中注册observer

比如你想在每一次新建一条Poem记录之后，将当前的某些信息记录到操作日志中，你可以在PoemRepository中重载```onCreated```方法，代码如下:

```php
<?php

namespace App\Repositories;

use Zhaohehe\Repositories\Eloquent\Repository;

class PoemRepository extends Repository
{
    public function model()
    {
        return 'App\Models\Poem';
    }

    public function onCreated()
    {
        //do something
    }
}
```
这样，当你在调用repository的```create```方法创建新的记录的时候，```onCreated```会在创建的动作结束后被调用。

#### 可用的方法

```php
function onCreating();
function onCreated();

function onUpdating();
function onUpdated();

function onSaving();
function onSaved();

function onDeleting();
function onDeleted();
```
以上方法会在相应的节点被激活。

## 最后

以上的思路和代码是在阅读了很多大神的代码之后产生的，尤其是[这个](https://github.com/prettus/l5-repository)，很荣幸可以站在巨人们的肩膀上。
"
89,arch4edu/arch4edu,Shell,"Archlinux Repository for Education
========
Making your Archlinux and ArchlinuxARM better for researching and developing.

### Introduction

Arch4edu is a community repository for Archlinux and ArchlinuxARM that strives to provide the latest versions of most software used by college students.

Please visit the [wiki](../../wiki) site for more information.

### Usage

* [Add arch4edu to your Archlinux](https://github.com/arch4edu/arch4edu/wiki/Add-arch4edu-to-your-Archlinux)

### Links

* Sina Weibo: [@arch4edu](https://weibo.com/arch4edu)
* Telegram channel: [@arch4edu](https://t.me/arch4edu)
* Twitter: [@arch4edu](https://twitter.com/arch4edu)
"
90,simpligility/maven-repository-tools,Java,"simpligility technologies inc. presents 

= Maven Repository Tools

== Introduction

A collection of tools to work with Maven repositories.

Maven Repository Provisioner::  a command line tool and library to
provision a component and all its transitive dependencies from a source Maven repository to a target 
Maven repository. See more in the https://github.com/simpligility/maven-repository-tools/tree/master/maven-repository-provisioner[readme file of the module].

Repository Provisioner Maven Plugin:: a maven plugin for the same
task - to be done.

== Requirememts

All tools require Java 8 or higher.

== Download

Everything is available from the Central Repository in the
simpligility space:

* https://repo1.maven.org/maven2/com/simpligility/maven/


== Roadmap, Issues, Changes

Please check out the
https://github.com/simpligility/maven-repository-tools/issues[issues
list] for upcoming changes, existing problems and so on.

For past releases and already implemented changes, see the https://github.com/simpligility/maven-repository-tools/blob/master/changelog.asciidoc[changelog] as
well as the https://github.com/simpligility/maven-repository-tools/commits/master[commit history].

== License

Eclipse Public License - v 1.0

For full text see the `LICENSE` file or https://www.eclipse.org/legal/epl-v10.html
 
== Building 

Run 

----
mvn clean install
----

Verify full build for release with

----
mvn clean deploy -P release
----

Release with the usual 

----
mvn release:prepare release:perform
----

== Contributions

are very welcome. Send a pull request or report issues on GitHub. Even just a 
spelling fix in the readme or anything else really is a welcome help. 

== Contributors

- Manfred Moser http://www.simpligility.com - project management and
  all coding
- Jason van Zyl https://github.com/jvanzyl - Maven and Aether help
- Igor Fedorenko https://github.com/ifedorenko - Maven and Aether help
- Ian Williams - initial discussion around concept
- More details on github - https://github.com/simpligility/maven-repository-tools/network/members[members] and https://github.com/simpligility/maven-repository-tools/graphs/contributors[contributors]

"
91,psalterio/repository,TeX,"Repositorio de músicas no Psaltério
================================
Este é o repositório oficial das músicas do Psaltério (www.psalterio.net).
Todas as músicas estão no formato latex que permite criar uma impressão de qualidade tipográfica, exemplo: [Separata IMPACTO 2014](https://github.com/psalterio/repository/blob/master/songbooks/2014/2014-impacto/separata_impacto_2014_chords.pdf) e também gera a base de dados automaticamente para site do psaltério e as Apps ([iOS](https://itunes.apple.com/pt/app/psalterio/id858825872) e [Android](https://play.google.com/store/apps/details?id=net.psalterio.psalterioandroid)). É um verdadeiro 3 em 1 :)


Como ajudar?
------------
Existem várias formas de ajudar. Normalmente estas são as maiores tarefas:
- [corrigir músicas](https://github.com/psalterio/repository/wiki/Corrigir-Música)
- [adicionar músicas](https://github.com/psalterio/repository/wiki/Adicionar-Música)
- [criar separata](https://github.com/psalterio/repository/wiki/criar-separata)

Mas também existem outras tarefas:
- [lista de tarefas (milestones)](https://github.com/psalterio/repository/milestones)
- [outras formas de ajudar](https://github.com/psalterio/repository/wiki/Outras-formas-de-ajudar)

Acordes no Psaltério
--------------------

Resumo do standard usado:
- C é um acorde de dó maior (dó, mi, sol)
- Cm é um acorde de dó menor (dó, mi-b, sol)
- C7+ é o acorde de sétima maior (dó, mi, sol, si). C7+ é o mesmo que Cmaj7 ou CM7 noutras notações.
- C4 é um acorde suspenso de quarta (dó, fá, sol). É o mesmo que Csus4 ou Csus noutras notações.
- C4(7) é um acorde de 4a suspensa com sétima menor (dó, fá, sol, si-b). Noutras notações aparece normalmente como C7sus4.

O standard dos acordes no psaltério está nestes mapas de acordes:
- [mapa 1](https://github.com/psalterio/repository/blob/master/songbooks/psalterio/scan_psalterio_original/0-3_mapa_acordes.jpg)
- [mapa 2](https://github.com/psalterio/repository/blob/master/songbooks/psalterio/scan_psalterio_original/0-4_mapa_acordes.jpg)
- [mapa 3](https://github.com/psalterio/repository/blob/master/songbooks/psalterio/scan_psalterio_original/0-5_mapa_acordes.jpg)
- [mapa 4](https://github.com/psalterio/repository/blob/master/songbooks/psalterio/scan_psalterio_original/0-6_mapa_acordes.jpg)

Links do Projecto
-----------------

- Website     : http://www.psalterio.net
- Facebook    : https://www.facebook.com/psalterio.net
- Musescore   : https://musescore.com/user/26352
- Youtube     : https://www.youtube.com/user/psalterio7
- Twitter     : https://twitter.com/psalterio
- [Android App](https://play.google.com/store/apps/details?id=net.psalterio.psalterioandroid)
- [iOS App](https://itunes.apple.com/pt/app/psalterio/id858825872)
- [Wikipedia](http://pt.wikipedia.org/wiki/Psaltério)

Links Relevantes
-----------------

- Músicas ACNAC Tições: http://acnac-ticoes.blogspot.pt
- Juventude Adventista: http://www.juventudeadventista.pt
- Juventude Adventista Facebook: https://www.facebook.com/juventudeadventista/
- Serviço de Música e Liturgia: http://musica.adventistas.org.pt
- Serviço de Música e Liturgia Facebook: https://www.facebook.com/Serviço-de-Musica-Liturgia-255635654562511/
"
92,cheeaun/repokemon,CSS,"Repokémon
===

Showcase of GitHub repos with Pokémon names.

**👉👉👉 Read Story: [Building Repokémon](https://cheeaun.com/blog/2016/08/building-repokemon/) 👈👈👈**

[![Screenshot](screenshot.png)](https://cheeaun.github.io/repokemon/)

How?
---

1. Scrape a list of all pokémons.
2. Use GitHub API and search for every pokémon name.
3. Get a matching repository name with highest number of stars.
4. 💥💥💥

Criteria
---

The showcase only list repositories with these criteria:

- **Repository name** matches exactly the same as Pokémon name.
  - ✅ `name/pikachu`
  - ✅ `name/mr-mime`
  - ✅ `name/mr_mime`
  - ⛔️ `name/pikachu-awesome`
  - ⛔️ `name/pikachuuuu`
- **Most stars**. If there are few repositories with the same name, the one with most stars will be listed.
- **Contains description**, because... it has to be descriptive.
- **Contains `lang`**, the repository language determined by GitHub.

Dev
---

- `npm i` - install dependencies
- `npm run pokemon` - grab the pokémons
- `npm run repokemon` - grab the Github repos
  - Copy `example.env` to `.env`, configure it with Client ID and Client Secret from GitHub
  - Takes about 30 minutes to prevent API rate limit issues
- `npm run gemoji` - replace emoji names like `:smile:` to `😄`
- `npm run min-repokemon` - generate a *minified* version of `data/repokemon.json`, containing only the data that is rendered
- `npm run stats` - show stats of the collected data
- `npm run images` - download all pokémon images to `data/images` (not included in this repo)
- `npm run sprite` - generate a sprite image `data/pokemon-*.jpg` (compressed with [TinyJPG](https://tinyjpg.com/) API)
- `npm run css-min` - compress `pokemon.css` to smaller `pokemon.min.css`
- `npm start` - start a local server at `localhost:1337`

Credits
---

Pokémon data and images are extracted from the [official Pokédex web site](http://www.pokemon.com/us/pokedex/). All Pokémon content is © Nintendo, Game Freak, and The Pokémon Company.

Repokémon is not affiliated with GitHub, Nintendo, Game Freak, or The Pokémon Company in any way.

The logo uses the [Pokémon font](https://www.dafont.com/pokemon.font).

The (Poké Ball) logo is created by [@limhenry](https://github.com/limhenry). Available on [Codepen](http://codepen.io/limhenry/full/rLYkWY/).
"
93,I-Hope-Peace/ChangeDetectionRepository,Python,"# Change Detection Repository
In this repository, we provide python implementation of some traditional change detection methods, such as SFA, MAD, some deep learning-based change detection methods, such as SiamCRNN, DSFA, and FCN-based methods, or their original websites. Some [multi-temporal datasets](https://github.com/I-Hope-Peace/ChangeDetectionRepository/tree/master/Dataset) are also contained in this repository. We would be very glad if this repository can provide some help to your research in change detection or remote sensing image interpretation.


## Traditional Methods
### Change Vector Analysis (CVA)
Change vector analysis (CVA) [1] is a most commonly used method, which can provide change intensity and change direction. 

### Slow Feature Analysis (SFA)
<div align=center><img src=""./Figure/SFA.png"" width=""60%"" height=""60%""></div>
Wu et al. [2] proposed a novel CD method based on slow feature analysis (SFA), which aims to find the most invariant component in multitemporal images to highlight changed regions. In addition to change detection, SFA was also used in radiometric correction [3] and scene change detection [4]. This reporisty contains the Python implementation of SFA and iterative SFA. The MATLAB implementation can be founded in http://sigma.whu.edu.cn/resource.php. 

### Multivariate Alteration Detection (MAD)
MAD is a change detection algorithm based on canonical correlation analysis (CCA) that aims to maximize the variance of projection feature difference. For the detailed introduction about MAD, please refer to [5] and [6]. This reporisty contains the python implementation of MAD. The MATLAB implementation can be founded in http://www.imm.dtu.dk/~alan/software.html. 

### PCA-Kmeans
<div align=center><img src=""./Figure/PCA_Kmeans.png"" width=""50%"" height=""50%""></div>
PCA-Kmeans [12] partitones the difference image into nonoverlapping blocks. Orthonormal eigenvectors are extracted through PCA of nonoverlapping block set to create an eigenvector space. Each pixel in the difference image is represented with an S-dimensional feature vector which is the projection difference image data onto the generated eigenvector space. The change detection is achieved by partitioning the feature vector space into two clusters using k-means. 

## Deep Learning Methods
### Deep Slow Feature Analysis (DSFA)
<div align=center><img src=""./Figure/DSFA.png"" width=""60%"" height=""60%""></div>
DSFA is an unsupervised change detection model that utilizes a dual-stream deep neural network to learn non-linear features and highlights changes via linear SFA. For the detailed introduction about DSFA, please refer to [7]. The Tensorflow implementation of DSFA can be founded in https://github.com/rulixiang/DSFANet or http://sigma.whu.edu.cn/resource.php. 

### Deep Siamese Convolutional Multiple-Layers Recurrent Neural Network (SiamCRNN)
<div align=center><img src=""./Figure/SiamCRNN.png"" width=""70%"" height=""70%""></div>
SiamCRNN is an end-to-end general multi-source change detection architecture that consists of three subnetworks: deep siamese convolutional neural network (DSCNN), multiple-layers RNN (MRNN), and fully connected (FC) layers. The DSCNN has a flexible structure for multisource image and is able to extract spatial–spectral features from homogeneous or heterogeneous VHR image patches. The MRNN stacked by long-short term memory (LSTM) units is responsible for mapping the spatial–spectral features extracted by DSCNN into a new latent feature space and mining the change information between them. In addition, FC, the last part of SiamCRNN, is adopted to predict change probability. For the detailed introduction about DSFA, please refer to [8]. The Tensorflow implementation of SiamCRNN can be founded in https://github.com/I-Hope-Peace/SiamCRNN.

### Deep Kernel PCA Convolutional Mapping Network (KPCA-MNet)
<div align=center><img src=""./Figure/KPCAMNet.png"" width=""70%"" height=""70%""></div>
KPCA-MNet is designed for unsupervised binary and multi-class change detection in very-high-resolution images. In the KPCA-MNet, the high-level spatial-spectral feature maps are extracted by a deep siamese network consisting of weight-shared KPCA convolutional layers. Then, the change information in the feature difference map is mapped into a 2-D polar domain. Finally, the change detection results are generated by threshold segmentation and clustering algorithms. For the detailed introduction about DSFA, please refer to [9]. The Python implementation can be founded in https://github.com/I-Hope-Peace/KPCAMNet. 

### Deep Siamese Multi-scale Convolutional Neural Network
In iterature [14] and [15], a multi-scale feature convolution unit (MFCU) is adopted for change detection in multi-temporal VHR images. MFCU can extract multi-scale spatial-spectral features in the same layer. Based on the unit two novel deep siamese convolutional neural networks, called as deep siamese multi-scale convolutional network (DSMS-CN) and deep siamese multi-scale fully convolutional network (DSMS-FCN), are designed for unsupervised and supervised change detection, respectively. Tensorflow implementation of this work can be founded in https://github.com/I-Hope-Peace/DSMSCN.

### SARPCANet
<div align=center><img src=""./Figure/SAR_PCANet_1.png"" width=""60%"" height=""60%""></div>
SARPCANet utilizes Gabor wavelets and FCM as the pre-classification method to select training samples [10], and then trains a PCANet [11] model with the selected image patches. The original MATLAB implementation could be founded in https://github.com/summitgao/SAR_Change_Detection_GarborPCANet. 

### FDCNN
<div align=center><img src=""./Figure/FDCNN.png"" width=""60%"" height=""60%""></div>
FDCNN [13] uses scene-level samples of remote sensing scene classification for learning deep features from different remote sensing scenes at different scales. Then, a new CNN structure and training strategies are proposed for remote sensing image change detection, which is supervised but requires very few pixel-level training samples.The original Caffe implementation could be founded in https://github.com/MinZHANG-WHU/FDCNN. 

### DCVA
DCVA [16] processes pre-change and post-change images through a pre-trained network and extracts bi-temporal deep features for subsequent processing in CD framework. The original Caffe implementation could be founded in https://github.com/sudipansaha/dcvaVHROptical. 

### CorrFusionNet
<div align=center><img src=""./Figure/CorrFusionNet.jpg"" width=""60%"" height=""60%""></div>
CorrFusionNet [17] is a unified network called CorrFusionNet for scene change detection. The CorrFusionNet firstly extracts the features of the bi-temporal inputs with deep convolutional networks. Then the extracted features will be projected into a lower dimension space to computed the instance level canonical correlation. The cross-temporal fusion will be performed based on the computed correlation in the CorrFusion module. In the objective function, the authors introduced a new formulation for calculating the temporal correlation. The original Tensorflow implementation could be founded in https://github.com/rulixiang/CorrFusionNet. 

### SNUNet
<div align=center><img src=""./Figure/SNUNet.png"" width=""60%"" height=""60%""></div>
SNUNet-CD [18] is a densely connected siamese network for change detection, namely SNUNet-CD (the combination of Siamese network and NestedUNet). SNUNet-CD alleviates the loss of localization information in the deep layers of neural network through compact information transmission between encoder and decoder, and between decoder and decoder. In addition, Ensemble Channel Attention Module (ECAM) is proposed for deep supervision. The original pytorch implementation could be founded in https://github.com/likyoo/Siam-NestedUNet. 


## Other Change Detection Repository
There also exist some other change detection repositories, you can visit them through below links:  
[1] https://github.com/Bobholamovic/ChangeDetectionToolbox  
[2] https://github.com/MinZHANG-WHU/Change-Detection-Review  
[3] https://github.com/wenhwu/awesome-remote-sensing-change-detection



## Reference
[1] F. Bovolo and L. Bruzzone, “A Theoretical Framework for Unsupervised Change Detection Based on Change Vector Analysis in the Polar Domain,” IEEE Trans. Geosci. Remote Sens., vol. 45, no. 1, pp. 218–236, 2007.  
[2] C. Wu, B. Du, and L. Zhang, “Slow feature analysis for change detection in multispectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52, no. 5, pp. 2858–2874, 2014.  
[3] L. Zhang, C. Wu, and B. Du, “Automatic radiometric normalization for multitemporal remote sensing imagery with iterative slow feature analysis,” IEEE Trans. Geosci. Remote Sens., vol. 52, no. 10, pp. 6141–6155, 2014.  
[4] C. Wu, L. Zhang, and B. Du, “Kernel Slow Feature Analysis for Scene Change Detection,” IEEE Trans. Geosci. Remote Sens., vol. 55, no. 4, pp. 2367–2384, 2017.  
[5] A. A. Nielsen, K. Conradsen, and J. J. Simpson, “Multivariate alteration detection (MAD) and MAF Postprocessing in multispectral, bitemporal image data: New approaches to change detection studies,” Remote Sens. Environ., vol. 64, pp. 1–19, 1998.  
[6] A. A. Nielsen, “The regularized iteratively reweighted MAD method for change detection in multi- and hyperspectral data,” IEEE Trans. Image Process., vol. 16, no. 2, pp. 463–478, 2007.  
[7] B. Du, L. Ru, C. Wu, and L. Zhang, “Unsupervised Deep Slow Feature Analysis for Change Detection in Multi-Temporal Remote Sensing Images,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 12, pp. 9976–9992, 2019.  
[8] H. Chen, C. Wu, B. Du, L. Zhang, and L. Wang, “Change Detection in Multisource VHR Images via Deep Siamese Convolutional Multiple-Layers Recurrent Neural Network,” IEEE    Trans. Geosci. Remote Sens., vol. 58, no. 4, pp. 2848–2864, 2020.  
[9] C. Wu,  H. Chen, B. Do, and L. Zhang, “Unsupervised Change Detection in Multi-temporal VHR Images Based on Deep Kernel PCA Convolutional Mapping Network,” arXiv preprint arXiv:1912.08628, 2019. https://arxiv.org/abs/1912.08628v1.  
[10] F. Gao, J. Dong, B. Li, and Q. Xu, “Automatic Change Detection in Synthetic Aperture Radar Images Based on PCANet,” IEEE Geosci. Remote Sens. Lett., vol. 13, no. 12, pp. 1792–1796, 2016.  
[11] T. H. Chan, K. Jia, S. Gao, J. Lu, Z. Zeng, and Y. Ma, “PCANet: A Simple Deep Learning Baseline for Image Classification?,” IEEE Trans. Image Process., vol. 24, no. 12, pp. 5017–5032, 2015.  
[12] T. Celik, “Unsupervised change detection in satellite images using principal component analysis and K-means clustering,” IEEE Geosci. Remote Sens. Lett., vol. 6, no. 4, pp. 772–776, 2009.  
[13] M. Zhang and W. Shi, “A Feature Difference Convolutional Neural Network-Based Change Detection Method,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 10, pp. 7232–7246, 2020.
[14] H. Chen, C. Wu, B. Du and L. Zhang, ""Deep Siamese Multi-scale Convolutional Network for Change Detection in Multi-temporal VHR Images,"" 2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp), Shanghai, China, 2019, pp. 1-4.  
[15] H. Chen, C. Wu, B. Du and L. Zhang, ""Change Detection in Multi-temporal VHR Images Based on Deep Siamese Multi-scale Convolutional Neural Network,"" arXiv preprint arXiv:1912.08628, 2020. https://arxiv.org/abs/1906.11479.  
[16] S. Saha, F. Bovolo, and L. Bruzzone, “Unsupervised deep change vector analysis for multiple-change detection in VHR Images,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 6, pp. 3677–3693, 2019.  
[17] L. Ru, B. Du and C. Wu, ""Multi-Temporal Scene Classification and Scene Change Detection with Correlation based Fusion,"" in IEEE Transactions on Image Processing, doi: 10.1109/TIP.2020.3039328.  
[18] S. Fang, K. Li, J. Shao and Z. Li, ""SNUNet-CD: A Densely Connected Siamese Network for Change Detection of VHR Images,"" in IEEE Geoscience and Remote Sensing Letters, doi: 10.1109/LGRS.2021.3056416.  
## Q & A
**For any questions, please [contact us.](mailto:Qschrx@gmail.com)**
"
94,covenantkodi/repository.colossus,,"# repository.colossus
Colossus Repository for Kodi Addons - Kodi is a registered trademark of the XBMC Foundation. We are not connected to or in any other way affiliated with Kodi - DMCA: marty.mcgibbins@vfemail.net
"
95,DefinitelyTyped/DefinitelyTyped,TypeScript,"# Definitely Typed

> La repo per le definizioni di tipi Typescript di *alta qualità*.

*Puoi leggere questo README anche in [Spagnolo](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/README.es.md), [Coreano](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/README.ko.md), [Russo](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/README.ru.md), [Cinese](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/README.cn.md), [Portoghese](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/README.pt.md) and [Giapponese](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/README.ja.md)!*

*Link per il [manuale dell'amministratore](./docs/admin.md)*

## Contenuti

* [Stato attuale](#stato-attuale)
* [Cosa cono i file di dichiarazione e come li ottengo?](#what-are-declaration-files-and-how-do-i-get-them)
* [Come posso contribuire?](#how-can-i-contribute)
  - [Test](#testing)
    - [Fare una pull request](#make-a-pull-request)<details><summary></summary>
      - [Clonare parzialmente](#partial-clone)
        - [Editare un package che già esite](#edit-an-existing-package)
        - [Creare un nuovo package](#create-a-new-package)
        - [Rimuovere un package](#removing-a-package)
        - [Eseguire test](#running-tests)
        - [Nomenclatura](#naming)
        - [`<mio-package>-test.ts`](#my-package-teststs)
        - [Linter: `tslint.json`](#linter-tslintjson)
        - [`tsconfig.json`](#tsconfigjson)
        - [`package.json`](#packagejson)
        - [`OTHER_FILES.txt`](#other_filestxt)
        - [Errori comuni](#common-mistakes)
        </details>
      - [Definizione dei proprietari](#definition-owners)
  * [Domande frequenti](#faq)
* [Licenza](#license)


## Stato attuale

Questa sezione tiene traccia della salute della repo e del processo di pubblicazione.
Può tornare utile per i contributori che stanno avendo problemi con le loro PR e package.

* Ultima build [type-checked/linted](https://github.com/Microsoft/dtslint) pulita: [![Build Status](https://dev.azure.com/definitelytyped/DefinitelyTyped/_apis/build/status/DefinitelyTyped.DefinitelyTyped?branchName=master)](https://dev.azure.com/definitelytyped/DefinitelyTyped/_build/latest?definitionId=1&branchName=master)
* Tutti i package sono sottoposti a controllo dei tipi e linting con typescript@next: [![Build status](https://dev.azure.com/definitelytyped/DefinitelyTyped/_apis/build/status/Nightly%20dtslint)](https://dev.azure.com/definitelytyped/DefinitelyTyped/_build/latest?definitionId=8)
* Tutti i package vengono [pubblicati su npm](https://github.com/microsoft/DefinitelyTyped-tools/tree/master/packages/publisher) in under an hour: [![Publish Status](https://dev.azure.com/definitelytyped/DefinitelyTyped/_apis/build/status/DefinitelyTyped.types-publisher-watchdog?branchName=master)](https://dev.azure.com/definitelytyped/DefinitelyTyped/_build/latest?definitionId=5&branchName=master)
* Il [bot di Typescript](https://github.com/typescript-bot) è stato attivo su Definitely Typed [![Activity Status](https://dev.azure.com/definitelytyped/DefinitelyTyped/_apis/build/status/DefinitelyTyped.typescript-bot-watchdog?branchName=master)](https://dev.azure.com/definitelytyped/DefinitelyTyped/_build/latest?definitionId=6&branchName=master)
* [Aggiornamenti dello stato dell'infrastruttura](https://github.com/DefinitelyTyped/DefinitelyTyped/issues/44317) attuale

Se qualcosa sembra sbagliato o una qualunque delle cose qui in alto fallisce, fatecelo sapere [sul canale di Definitely Typed nel server discord della comunità di Typescript](https://discord.gg/typescript):

## Cosa sono i file di dichiarazione e come li ottengo

Leggi il [manuale di TypeScript](http://www.typescriptlang.org/docs/handbook/declaration-files/introduction.html).

### npm

Il metodo preferito per installarli è usando npm. 

Ad esempio:

```sh
npm install --save-dev @types/node
```

I tipi dovrebbero poi essere inclusi automaticamente dal complilatore.
Potresti dover aggiungere un riferimento `types` se non stai usando i moduli:

```ts
/// <reference types=""node"" />
```

Approfondisci leggendo il [manuale](http://www.typescriptlang.org/docs/handbook/declaration-files/consumption.html).

Per un package ""pippo"", i tipi associati saranno disponibili sul package ""@types/pippo"".
Se non riesci a trovare i tipi di un package, cercalo su [TypeSearch](https://microsoft.github.io/TypeSearch/).

Se ancora non riesci a trovarlo, vuol dire che sono [inclusi](http://www.typescriptlang.org/docs/handbook/declaration-files/publishing.html) nel package stesso.
Di solito vengono specificati nel campo `""types""` o `""typings""` nel `package.json`;
se vuoi puoi semplicemente cercare per dei file "".d.ts"" nel package ed includerli manualmente con `/// <reference path="""" />`.

#### Vecchie vesioni di TypeScript (3.3 e precedenti)

Definitely Typed testa packages solo su versioni di TypeScript che hanno meno di due anni.
Attualmente vengono testate le versioni 3.4 e successive.
Se stai usando una versiond di TypeScript tra 2.0 e la 3.3, puoi ancora provare ad installare i package `@types`, in quanto la maggior parte di questi non usano le funzionalità più all'avanguardia di Typescript.
Non c'è comunque nessuna garanzia che funzioneranno.
Ecco le informazioni riguardanti le versioni supportate:

| Versione | Data rilascio | Data fine supporto  |
| ------- | -------------- | --------------      |
| 2.8     | Marzo 2018     | Marzo 2020          |
| 2.9     | Maggio 2018    | Maggio 2020         |
| 3.0     | Luglio 2018    | Agosto 2020         |
| 3.1     | Settembre 2018 | Settembre 2020      |
| 3.2     | Novembre 2018  | Novembre 2020       |
| 3.3     | Gennaio 2019   | Gennaio 2021        |
| 3.4     | Marzo 2019     | Marzo 2021          |
| 3.5     | Maggio 2019    | Maggio 2021         |
| 3.6     | Agosto 2019    | Agosto 2021         |
| 3.7     | Novembre 2019  | Novembre 2021       |
| 3.8     | Febbraio 2020  | Febbraio 2022       |
| 3.9     | Maggio 2020    | Maggio 2022         |
| 4.0     | Agosto 2020    | Agosto 2022         |
| 4.1     | Novembre 2020  | Novembre 2022       |
| 4.2     | Febbraio 2021  | Febbraio 2023       |

I package `@types` hanno dei tag per la versione di TypeScript che supportano, quindi di solito puoi installare vecchi package che sono più vecchi di due anni.
Ad esempio se esegui `npm dist-tags @types/react`, vedrai che TypeScript 2.5 potrà usare i tipi per react@16.0, mentre TypeScript 2.6 e 2.7 potranno usare i tipi per react@16.4:

| Tag    | Versione |
| ------ | -------  |
| ultima | 16.9.23  |
| ts2.0  | 15.0.1   |
| ...    | ...      |
| ts2.5  | 16.0.36  |
| ts2.6  | 16.4.7   |
| ts2.7  | 16.4.7   |
| ...    | ...      |

#### TypeScript 1.*

* Scarica manualmente la branch `master` di questa repo e collocala nel tuo progetto.
* ~~[Typings](https://github.com/typings/typings)~~ (usa altre alternative migliori, in quanto typings è deprecata)
* ~~[NuGet](http://nuget.org/packages?q=DefinitelyTyped)~~ (usa altre alternative, in quanto la pubblicazione di tipi su nuget DT non è più possibile)

Potresti dover aggiungere manualmente i [riferimenti](http://www.typescriptlang.org/docs/handbook/triple-slash-directives.html).

## Come posso contribuire?

Definitely Typed ha successo e può funzionare solamente grazie a contributori come te!

### Test

Prima di condividere il tuo contributo col mondo, provalo tu stesso.

#### Test per la modifica di un package DT (@types) preesistente

Per testare localmente le tue modifiche, puoi usare [module augmentation](http://www.typescriptlang.org/docs/handbook/declaration-merging.html#module-augmentation) per estendere i tipi esistenti del modulo DT su cui vuoi lavorare.
Altrimenti, puoi modificare i tipi direttamente nel `node_modules/@types/foo/index.d.ts` per validare le tue modifiche, per poi riportarle su questa repo seguendo i passi che troverai scritti più in basso.

#### Test per un nuovo package DT

Aggiungi al tuo `tsconfig.json`:

```json
""baseUrl"": ""types"",
""typeRoots"": [""types""],
```

Crea un `types/foo/index.d.ts` contenente le dichiarazioni per il tuo modulo ""pippo"".
Ora dovresti essere in grado di importare `""pippo""` nel tuo codice, con i tipi riferiti alle dichiarazioni che hai appena creato.
Poi fai una build **ed** esegui il codice per essere sicuro che le definizioni dei tipi corrispondano effettivamente a ciò che capita a runtime.

Una volta che hai testato le definizioni su del codice reale, fai una [pull request](#make-a-pull-request), 
poi segui le istruzioni per [modificare un package preesistente](#modificare-un-package-preesistente] o [creare un nuovo package](#creare-un-nuovo-package).

### Fai una pull request

Una volta che hai testato il tuo package, puoi condividerlo su Definitely Typed.

Inanzitutto, [sdoppia](https://guides.github.com/activities/forking/) questa repo, [clonala](#partial-clone), installa [node](https://nodejs.org/) ed esegui `npm install`. Se stai usando `npm` v7 devi aggiungere `--legacy-peer-deps` al comando.

Utilizziamo un bot per far sì che un gran numero di pull request su Definitely Typed possano essere gestite interamente in modo autonomo. Puoi scoprire di più a riguardo di come e perchè [qui](https://devblogs.microsoft.com/typescript/changes-to-how-we-manage-definitelytyped/). 

Ecco qui un'immagine che mostra il ciclo vitale di una pull request su Definitely Typed.

<img src=""https://github.com/DefinitelyTyped/dt-mergebot/blob/master/docs/dt-mergebot-lifecycle.svg"">

#### Clone parziale

Puoi clonare l'intera repo [come di consuetudine](https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository) che però è molto grande ed include numerose cartelle di package DT. Ci vorrà quindi del tempo per clonarla e può essere un procedimento inutilmente lungo.
Per clonare la repo in modo più agibile, clonando _solo_ i package DT per te rilevanti, pui usare le funzionalità [`sparse-checkout`](https://git-scm.com/docs/git-sparse-checkout), [`--filter`](https://git-scm.com/docs/git-rev-list#Documentation/git-rev-list.txt---filterltfilter-specgt) ed [`--depth`](https://git-scm.com/docs/git-clone#Documentation/git-clone.txt---depthltdepthgt) di git.

>:warning: Richiede almeno la [versione di git 2.27.0](https://git-scm.com/downloads), che solitamente è più recente di quelle installate di default. Per le versioni più vecchie sono disponibili procedure più complesse, che non trattiamo qui.

1. `git clone --sparse --filter=blob:none --depth=1 <forkedUrl>`
    - `--sparse` initializes the sparse-checkout file so the working directory starts with only the files in the root of the repository.
        - `--filter=blob:none` will exclude files, fetching them only as needed.
        - `--depth=1` will further improve clone speed by truncating commit history, but it may cause issues as summarized [here](https://github.blog/2020-12-21-get-up-to-speed-with-partial-clone-and-shallow-clone/).
    2. `git sparse-checkout add types/<type> types/<dependency-type> ...`

#### Modificare un package preesistente

* `cd types/<package to edit>`
* Fai le tue modifiche. Ricorda di [testle](#my-package-teststs).
  Se fai modifiche essenziali, non ti dimenticare di [aggiornare il major della versione](#if-a-library-is-updated-to-a-new-major-version-with-breaking-changes-how-should-i-update-its-type-declaration-package).
  * [Esegui `npm test <package da testare>`](#running-tests).

Quando crei una pull request ad un package che esiste già, `dt-bot` dovrebbe @menzionare gli autori precedenti.
Se non lo fa, puoi farlo direttamente tu nel commento associato alla pull request.

#### Creare un nuovo package

Se sei l'autore della libreria ed il tuo package è scritto in TypeScript, [includi i file di dichiarazione generati automaticamente] (http://www.typescriptlang.org/docs/handbook/declaration-files/publishing.html) nel tuo package invece di pubblicarli su Definitely Typed.

Se stai aggiungendo i tipo per un package npm, crea una cartella con lo stesso nome.
Se il package a cui stai aggiungendo i tipi non è su npm, assicurati che il nome che scegli non entri in conflitto con quello di un package npm.
(Puoi usare `npm info <nome-package>` per verificare l'esistenza del package `<nome-package`).

Il tuo package dovrebbe avere questa struttura:

| File          | Scopo |
| ------------- | ------- |
| `index.d.ts`  | Contiene le dichiarazioni dei tipi del package. |
| [`<nome-package>-tests.ts`](#my-package-teststs)  | Contiene codice di esempio con test delle dichiarazioni dei tipi. Se il codice *non* funziona anche se viene traspilato da tsc senza errori.
| [`tsconfig.json`](#tsconfigjson) | Ti permette di eseguire `tsc` all'interno del package. |
| [`tslint.json`](#linter-tslintjson)   | Abilita il linting. |

Generali eseguento `npx dts-gen --dt --name <mio-package> --template module` se hai npm ≥ 5.2.0, altrimenti `npm install -g dts-gen` and `dts-gen --dt --name <my-package> --template module`.
Leggi tutte le opzioni su [dts-gen](https://github.com/Microsoft/dts-gen).

Se hai file `.d.ts` oltre all'`index.d.ts`, assicurati che siano referenziati o nell'`index.d.ts` o nei test.

I membri di Definitely Typed controllano continuamente le nuove pull request, perciò sii al corrente che un alto numero di pull request potrebbe rallentarci il lavoro.

Per l'esempio di un buon package, guarda [base64-js](https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/base64-js).

#### Rimuovere un package

Quando un package [include](http://www.typescriptlang.org/docs/handbook/declaration-files/publishing.html) i suoi dichiarazioni, esse dovrebbero venire rimosse da Definitely Typed per evitare di far confusione.

Puoi rimuoverli eseguendo `npm run not-needed -- <nome-package> <versione> [<nome-libreria>]`.
* `<nome-package>`: È il nome della cartella da rimuovere.
* `<versione>`: Verrà pubblicato uno stab su `@types/<nome-package>` con questa versione. Dev'essere più alto della versione attualmente pubblicata e deve essere una versione di `<ome-libreria>` su npm.
* `<nome-libreria>`: Nome del package npm che sostituisce il package DT. Solitamente è identioc a `<nome-package>` e puoi ometterlo.

Qualunque altro package di Definitely Typed che si riferisce ad un altro package DT eliminato dovrebbe venir aggiornato facendolo riferire ai tipi inclusi nel package stesso.
Puoi farlo controllando gli errori che escono eseguendo `npm run test-all`.
Per correggere gli errori, [aggiungi un `package.json`](#packagejson) con `""dependencies"": { ""<nome-libreria>"": ""x.y.z"" }`.
Ad esempio:

```json
{
      ""private"": true,
    ""dependencies"": {
          ""<nome-libreria>"": ""^2.6.0""
      }
  }
```

Quando aggiungi un `package.json` a dei package che dipendono da `<nome-libreria>`, devi aprire anche una pull request per aggiungere `<nome-libreria>` [ad allowedPackageJsonDependencies.txt su DefinitelyTyped-tools](https://github.com/microsoft/DefinitelyTyped-tools/blob/master/packages/definitions-parser/allowedPackageJsonDependencies.txt).

Se un package non è mai stato su Definitely Typed, non c'è bisogno che venga aggiunto a `notNeededPackages.json`.

#### Eseguire test

Testa le tue modifiche eseguendo `npm test <package da testare>` dove `<package da testare>` è il nome del tuo package.

Lo script usa [dtslint](https://github.com/microsoft/dtslint) per eseguire il compilatore TypeScript sui tuoi file dts.

#### Nomenclatura

Se stai aggiungendo i tipi ad un package npm, crea una cartella con lo stesso nome.
Se invece non li stai aggiungendo ad un package npm, assicurati prima che non esista un package npm con lo stesso nome, per evitare futuri conflitti.
(Puoi usare `npm info <nome-package>` per controllare l'esistenza di un package `<nome-package>`).

Se un package che non è di npm è in conflitto con un package npm, prova ad aggiungere -browser alla fine del nome, ottenendo quindi `<mio-package>-browser`

#### `<mio-package>-tests.ts`

Ci dev'essere un file `<my-package>-tests.ts`, che viene considerato il tuo file di test, assieme ad ogni file `*.ts` che importa.
Se non vedi un file di test nella cartella del modulo, crea tu un file `<nome-package>-test.ts`.
Questi file sono usati per validare l'API esportata dai file `*.d.ts` che vengono pubblicati come `@types/<nome-package>`.

Ad esempio, questo codice cambia una funzione nel file `.d.ts`, aggiungendoci un nuovo parametro:

`index.d.ts`:

```diff
- export function twoslash(body: string): string
+ export function twoslash(body: string, config?: { version: string }): string
```

`<mio-package>-tests.ts`:

```diff
import {twoslash} from ""./""

// $ExpectType string
const result = twoslash(""//"")

+ // Handle options param
+ const resultWithOptions = twoslash(""//"", { version: ""3.7"" })
+ // When the param is incorrect
+ // $ExpectError
+ const resultWithOptions = twoslash(""//"", {  })
```

Se ti stai chiedendo da dove cominciare per fare i test, gli esempi nel README del modulo sono un buon punto da dove partire.

Puoi [validare le tue modifiche](#running-tests) con `npm test <package da testare>` nella root di questa repo, che prende in considerazione i file cambiati.
Usa `$ExpectType` per asserire che un'espressione è del tipo dato e `$ExpectError` per asserire un errore di compilazione. Ad esempio:

```js
// $ExpectType void
f(1);

// $ExpectError
f(""one"");
```

Per maggiori dettagli, leggi il readme di [dtslint](https://github.com/Microsoft/dtslint#write-tests).

#### Linter: `tslint.json`

Il file di configurazione del linter, `tslint.json`, dovrebbe contenere `{ ""extends"": ""dtslint/dt.json"" }` e nessun'altra regola.

Se per qualche ragione qualche regola necessita di essere disabilitata, [disabilitala solo per la riga di codice in cui dovrebbe esserlo](https://palantir.github.io/tslint/usage/rule-flags/#comment-flags-in-source-code:~:text=%2F%2F%20tslint%3Adisable%2Dnext%2Dline%3Arule1%20rule2%20rule3...%20%2D%20Disables%20the%20listed%20rules%20for%20the%20next%20line) usando `// tslint:disable-next-line:[ruleName]` e non disabilitandola per tutto il package. 

#### `tsconfig.json`

`tsconfig.json` dovrebbe avere `noImplicitAny`, `noImplicitThis`, `strictNullChecks` e `strictFunctionTypes` settati a `true`.

Potresti dover editare `tsconfig.json` per aggiungere nuovi file di test, per aggiungere le opzioni di traspilazione `""target"": ""es6""` (necessario per funzioni asincrone), `""lib""` o `""jsx""`.

#### `package.json`

Solitamente non ne avrai bisogno.
Un `package.json` potrebbe essere incluso per specificare le dipendenze che non sono altri `@types` package.
[Pikaday ne è un buon esempio](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/types/pikaday/package.json).
Anche se stai scrivendo un tuo `package.json`, upoi solo specificare dipendenze; altri campi come `""description""` non sono consentiti.
Puotresti anche aver bisogno di aggiungere una dipendenza alla [lista dei package consentiti](https://github.com/microsoft/DefinitelyTyped-tools/blob/master/packages/definitions-parser/allowedPackageJsonDependencies.txt).
Questa lista è aggiornata da persone, che ci danno l'opportunità di assicurarci che i package `@types` non dipendono da package rischiosi.
Nel raro caso in cui un package `@types` viene eliminato a favore dei tipi inclusi nel package npm analogo E avessi bisogno di dipendere dal vecchio package `@types`, puoi aggiungere una dipendenza ad un package `@types`.
Assicurati di spiegare tutto ciò quando lo aggiungi alla lista dei package consentiti cosicchè gli altri possano capire cosa stai facendo.

#### `OTHER_FILES.txt`

Se un file non è nè testato nè riferito nell'`index.d.ts`, aggiungilo in un file chiamato `OTHER_FILES.txt`. Questo file è una lista di altri file che serve che siano inclusi nel package DT. Elencali uno per riga.

#### Errori comuni

* Inanzitutto segui i consigli nel [manuale](http://www.typescriptlang.org/docs/handbook/declaration-files/do-s-and-don-ts.html).
* Formattazione: Usa 4 spazi. Prettier è abilitato su questa repo, quindi puoi eseguire `npm run prettier -- --write path/to/package/**/*.ts`. [Quando usi le assertion](https://github.com/SamVerschueren/tsd#assertions), aggiungi `// prettier-ignore` per marcare le linee di codice da escludere quando si fa la formattazione:
  ```tsx
    // prettier-ignore
    const incompleteThemeColorModes: Theme = { colors: { modes: { papaya: { // $ExpectError
    ```
* `function sum(nums: number[]): number`: Usa `ReadonlyArray` se una funzione non modifica i suoi parametri.
* `interface Foo { new(): Foo; }`:
  Definisce un tipo di oggetto su cui si può fare new. Probabilmente ciò che vuoi è `declare class Foo { constructor(); }`.
* `const Class: { new(): IClass; }`:
  Preferire sempre la dichiarazione di una classe `class Class { constructor(); }` al posto di una costante new.
* `getMeAT<T>(): T`:
  Se il parametro di un tipo non appare nei tipi di un parametro, ciò che vuoi non è una vera funzione generica.
    È consigliato usare una vera asserzione di tipo, ad esempio `getMeAT() as number`.
    Esempio dove un parametro è accettabile: `function id<T>(value: T): T;`.
    Esempio in cui non lo è: `function parseJson<T>(json: string): T;`.
    Eccezione: `new Map<string, number>()` va bene.
* Usare i tipi `Function` e `Object` non è quasi mai una buona idea. Nel 99% si può specificare un tipo più preciso. Alcuni esempi sono `(x: number) => number` per le [funzioni](http://www.typescriptlang.org/docs/handbook/functions.html#function-types) e `{ x: number, y: number }` per gli oggetti. Se non si è sicuri del tipo, [`any`](http://www.typescriptlang.org/docs/handbook/basic-types.html#any) è la scelta migliore, non `Object`. Se l'unica cosa di cui si sa è che il tipo è un oggetto di qualche tipo, usa il tipo [`object`](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-2-2.html#object-type), non `Object` o `{ [key: string]: any }`.
* `var foo: string | any`:
  Quando `any` è usato in un tipo disgiuntipo, il tipo risultanto rimane `any`. Quindi mentre la porzione `string` di questo tipo potrebbe _sembrare_ utile, in realtà non offre nessuna precisazione rispetto ad un banalissimo `any`.
    In funzione delle tue intenzioni, delle alternative accettabili sono `any`, `string` o `string | object`.
  
### Proprietari delle definizioni

DT ha il concetto di ""Proprietari delle definizioni"", che sono coloro i quali vogliono mantenere la qualità delle definizioni dei tipi di un certo modulo.

* Aggiungerti da solo farà sì che tu venga notificato (tramite il tuo nome utente GitHub) ogni volta che qualcuno fa una pull request o un issue su quel package.
* Le tue PR review avranno precedenza maggiore di quelle [dei bot](https://github.com/DefinitelyTyped/dt-mergebot) che mantengono questa repo.
* I mantenitori di DT stanno ponendo la loro fiducia sui proprietari delle definizioni per mantenere un ecosistema stabile, quindi non aggiungerti senza sapere quello che fai.

Per aggiungerti come pprietario delle definizioni:

* Aggiungi il tuo nome alla fine della riga, come su `// Definitions by: Alice <https://github.com/alice>, Bob <https://github.com/bob>`.
* Se ci sono più perone, puoi separarlo in più righe:
  ```typescript
    // Definitions by: Alice <https://github.com/alice>
    //                 Bob <https://github.com/bob>
    //                 Steve <https://github.com/steve>
    //                 John <https://github.com/john>
    ```
  
Una volta alla settimana i proprietari delle definizioni saranno sincronizzati nel file [.github/CODEOWNERS](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/.github/CODEOWNERS) che è la nostra fonte di verità.

## Domande frequenti

#### Come si relaziona questa repo con i package `@types` su npm?

Il branch `master` viene pubblicato automaticamente nello scope `@types` di npm grazie a [DefinitelyTyped-tools](https://github.com/microsoft/DefinitelyTyped-tools/tree/master/packages/publisher).

#### Ho inviato una pull request. Quanto ci vuole prima del merge?

Dipende, ma alla maggior parte delle pull request viene fatto un merge in meno di una settimana.
Ad altre pull request può essere fatto il merge dai proprietari di un modulo e ciò può avvenire molto più velocemente.
It depends, but most pull requests will be merged within a week.

All'incirca:

> Le pull request che cambiano solo i tipi di un modulo e che hanno i test corrispondenti vengono vengono accettati più velocemente

Alle pull request che sono state approvate da un'autore presente nella lista dei proprietari vengono di solito fatti i merge più rapidamente; pull request per nuove definizioni ci mettono più tempo in quando hanno bisogno di più controlli da parte dei mantenitori. Ogni pull request viene controllata da un membro di TypeScript o Definitely Type prima di ottenere un merge, quindi abbi pazienza in quanto fattori umani possono causare ritardi. Leggi la [New Pull Request Status Board](https://github.com/DefinitelyTyped/DefinitelyTyped/projects/5) tper vedere a che punto sono i mantenitori nel controllare le pull request.

#### La mia pull request ha ottenuto un merge; quand'è che il package `@types` verra aggiornato su npm?

I package dovrebbero venire aggiornati us npm in pochi minuti. Se ci mette più di un'ora, riferisciti al numero della pull request sul [serve discord di DefinitelyTyped nella community di TypeScript](https://discord.gg/typescript) ed il mantenitore assegnerà a chi di dovere per investigare.

#### Sto scrivendo un file dts che dipende da altri dts. Cosa dovrei usare, `<reference types="""" />` o `import`?

Se il modulo da cui dipendi è un modulo esterno (usa `export`), utilizza `import`.
Se il modulo da cui dipendi è un modulo d'ambiente (usa `declare module` o semplicemente dichiara le cose globali), usa `<reference types="""" />`.

#### Alcuni package non hanno un `tslint.json` ed ad altri mancano il `""noImplicitAny"": true`, il `""noImplicitThis"": true`, o  il`""strictNullChecks"": true` nel `tsconfig.json`.

Questo significa che non vanno bene e noi non ce ne siamo ancora accorti. Puoi contribuire inviando una pull request che li sistema.

####  Posso chiedere che venga implementata una definizione per un modulo che non le ha ancora?

Se un modulo che utilizzi non ha ancora delle definizioni, puoi chiedere che vengano implementate aprendo un Issue. Qui trovi le [richieste di implementazione](https://github.com/DefinitelyTyped/DefinitelyTyped/labels/Definition%3ARequest) attuali.

#### E per le definizioni dei tipi del DOM?

Se i tipi fanno parte di uno standard web, si può contribuire al [TSJS-lib-generator](https://github.com/Microsoft/TSJS-lib-generator), in modo tale che diventino parte del `lib.dom.d.ts` ufficiale.

####  È consigliato aggiunge un namespace vuoto ai package che non esportano un modulo, così da poter usare gli import ES6?

Alcuni package, come [chai-http](https://github.com/chaijs/chai-http), esportano una funzione.

Importare questo modulo tramite un import ES6 nella forma `import * as pippo from ""pippo"";` dà errore.

> error TS2497: Module 'foo' resolves to a non-module entity and cannot be imported using this construct

Questo errore può essere risolto fondendo la dichiarazione della funzione con un namespace vuoto dello stesso nome, ma di norma va evitato.
Questa [risposta su Stack Overflow](https://stackoverflow.com/questions/39415661/what-does-resolves-to-a-non-module-entity-and-cannot-be-imported-using-this) è tipicamente citata quando si parla di problemi di questo tipo.

Risulta preferibile importare un modulo usando la sintassi `import foo = require(""foo"");`.
In ogni caso, se vuoi usare un import default come `import foo from ""foo"";`, hai due possibilità:
- puoi usare l'opzione del traspilatore TypeScript [`--allowSyntheticDefaultImports`](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-1-8.html#support-for-default-import-interop-with-systemjs) se il tuo modulo supporta a ntime 

Importare il modulo usando la sintassi `import foo = require(""foo"");` è più appropriato. 
Comunque, se vuoi usare un default import come `import foo from ""foo"";` hai due possibilità:
- puoi usare [l'opzione di compilazione `--allowSyntheticDefaultImports`](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-1-8.html#support-for-default-import-interop-with-systemjs) se il tuo ambiente di runtime supporta uno schema interop per moduli non ECMAScript, come nel caso in cui gli import di default sono supportati (Webpack, SystemJS, esm, ...)
- puoi usare [l'opzione di compilazione `--esModuleInterop`](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-2-7.html#support-for-import-d-from-cjs-form-commonjs-modules-with---esmoduleinterop) se sia Typescript ad occuparsi dell'interop di moduli non ECMAScript (da TypeScript 2.7 in su).

#### Un package usa `export =`, ma preferirei usare un import di default. Posso cambiare `export =` in `export default`?

Come già scritto nella risposta precedente, fai riferimento alle opzioni di compilazione [`--allowSyntheticDefaultImports`](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-1-8.html#support-for-default-import-interop-with-systemjs)
e [`--esModuleInterop`](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-2-7.html#support-for-import-d-from-cjs-form-commonjs-modules-with---esmoduleinterop).

Non cambiare la definizione di tipo se è apposita.
Per un package npm, `export =` è apposita se `node -p 'require(""foo"")'` funziona per importare un modulo, mentre `export default` è apposita se funziona `node -p 'require(""foo"").default'`.

#### Voglio usare le funzionalità delle versioni più nuove di Typescript

Allora devi aggiungere un commento all'ultima riga del tuo header di definizione (dopo `// Definitions: https://github.com/DefinitelyTyped/DefinitelyTyped`): `// Minimum TypeScript Version: X.Y`. Ciò specifichera la versione minima supportata.

In ogni caso, se il tuo progetto avesse bisogno di mantenere tipi compatibili con, ad esempio, versioni 3.7 e successive ed **allo stesso tempo** vesioni 3.6 o precedenti, dovrai usare la funzionalità `typesVersions`.
Puoi trovare una spiegazione nel dettaglio di questa funzionalità nella [documentazione ufficiale di Typescript](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-1.html#version-selection-with-typesversions).

Ecco qui un breve esempio per iniziare:

1. Aggiungi un file `package.json` alle definizioni dei tuoi package, con il contenuto seguente.

   ```json
      {
        ""private"": true,
        ""types"": ""index"",
        ""typesVersions"": {
          ""<=3.6"": { ""*"": [""ts3.6/*""] }
        }
      }
   ```
   
2. Crea la sottocartella menzionata dal campo `typesVersions` dentro la tua cartella dei tipi (che nel nostro esempio è `ts3.6/`).
   `ts3.6/` suppporterà le versioni uguali od inferiori alla 3.6, quindi copia i tipi esistenti con i test lì.

   Dovrai eliminare l'header delle definizioni da `ts3.6/index.d.ts` dal momento che solo la root `index.d.ts` può averlo.
   
3. Cambia le opzioni `baseUrl` e `typeRoots` in `ts3.6/tsconfig.json` per correggere i path, dovrebbe essere simile a:
   ```json
    {
        ""compilerOptions"": {
            ""baseUrl"": ""../../"",
            ""typeRoots"": [""../../""]
        }
    }
    ```
   
4. Nel root del package, aggiungi le funzionalità di Typescript 3.7 che vuoi usare.
   Quando il package viene installato, Typescript 3.6 o inferiore partirà da `ts3.6/index.d.ts`, mentre Typescript 3.7 o superiore partirà da `index.d.ts`.

   Dai un'occhiata a [styled-components](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/types/styled-components) per averne un esempio.
   
#### Voglio aggiungere una API DOM che non è presente di default su Typescript.

Questo potrebbe riguardare [TSJS-Lib-Generator](https://github.com/Microsoft/TSJS-lib-generator#readme). Dai un'occhiata a quelle guide.
Se lo standard è ancora una bozza, allora riguarda questa repo.
Usa un nome che comincia per 'dom-' ed includi un collegamento allo standard come il collegamento ""Project"" nell'header.
Quando smetterà di essere una bozza, potremo rimuoverlo da Definitely Typed e deprecare il package `@types` associato.

#### Come fanno le versioni dei package Definitely Typed a coincidere con le versioni della libreria corrispondente?

*NOTA: Questa sezione assume familiarità con [Versioni semantiche](https://semver.org/)*

Ogni pacchetto Definitly Typed è versionato quando viene pubblicato su npm.
Il [DefinitelyTyped-tools](https://github.com/microsoft/DefinitelyTyped-tools/tree/master/packages/publisher), che è lo strumento che pubblica i pacchetti `@types` su npm) metterà la versione al pacchetto '@types` usando la versione `major.minor` scritta nella prima riga del suo file `index.d.ts`.
Ad esempio, queste sono le prime righe delle [dichiarazioni di tipi di Node](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/1253faabf5e0d2c5470db6ea87795d7f96fef7e2/types/node/index.d.ts) for version `10.12.x` at the time of writing:

```js
// Type definitions for Node.js 10.12
// Project: http://nodejs.org/
// Definitions by: Microsoft TypeScript <https://github.com/Microsoft>
//                 Definitely Typed <https://github.com/DefinitelyTyped>
//                 Alberto Schiabel <https://github.com/jkomyno>
```

Siccome c'è `10.12` alla fine della prima riga commentata, la versione del pacchetto npm `@types/node` sarà `10.12.x`.
Nota bene che la prima riga commentata del `index.d.ts` deve contenere solo le prime due parti (`major.minor`) della versione (come `10.12`), senza contenere la terza parte ""patch"" della versione (come `10.12.4`).
Questo è naturale, in quanto solo aggiornamenti delle parti `major.minor` della versione possono comportare variazioni delle definizioni e sono quindi allineate con la versione del pacchetto `@types`.
La parte patch del pacchetto `@types` (come `.0` in `10.12.0`) viene inizializzato a zero da Definitely Typed e viene incrementato di uno ogni volta che un nuovo pacchetto `@type`, con la stessa versione `major.minor`, viene pubblicato su npm per la stessa libreria.

A volte le versioni delle dichiarazioni dei tipi e quelle dei pacchetti corrispondenti può desincronizzarsi.
Qui sotto ci sono alcune delle cause più comuni, ordinate in base a quanti inconvenienti causano a chi utilizza la libreria.
Solitamente, solo l'ultima causa da problemi.

* Come messo precedentemente in evidenza, la parte patch della versione del pacchetto `@types` non è allineata con quella della libreria analoga.
  Questo permette a Definitely Typed di aggiornare in modo automatico e sicuro le dichiarazioni dei tipi di una libreria che hanno le stesse major/minor.
* Se aggiorni un pacchetto perchè introduci una nuova funzionalità, ricordati di aggiornare la versione anche sul pacchetto `@types`.
  Se gli autori dei pacchetti fanno corrispondere le versioni di JavaScript con quelle dei pacchetti `@types` analoghi, `npm update` dovrebbe funzionare alla perfezione.
* Molto spesso capita che le versioni delle dichiaraizoni dei tipi rimangano indietro rispetto a quelle del pacchetto JavaScript, questo perchè molte volte non sono gli stessi autori dei pacchetti JavaScript
  a farne anche le dichiarazioni dei tipi ma utenti terzi. Per questo motivo potrebbero esserci dei ritardi di giorni, settimane o perfino mesi prima che arrivi una pull request da qualche benefattore che riallinea il pacchetto `@type` con l'ultima versione JavaScript.
  Se ti ritrovi in questa situazione, puoi essere tu stesso a fare la differenza ed a risolvere il problema diventando un membro a tutti gli effetti della Community.
  
:exclamation: Se stai aggiornando le dichiarazioni dei tipi per una libreria, ricordati di cambiare sempre la `major.minor` nella prima riga del `index.d.ts` in modo che con quella della libreria dei cui tipi stai dichiarando! :exclamation:

#### Se l'aggiornamento di una libreria comprende modifiche sostanziali (aggiornamento major), come faccio ad aggiornare il suo pacchetto '@types`?

[Le versioni semantiche](https://semver.org/) richiedono che agli aggiornamenti con modifiche sostanziali venga incrementata la parte major della versione.
Ad esempio, se ad una libreria viene rimossa una funzione che esporta e la sua ultima versione è la `3.5.8`, la sua versione deve essere aggiornata alla `4.0.0`.
Inoltre, quando questa nuova versione `4.0.0` viene pubblicata, la versione del suo pacchetto di dichiarazione dei tipi dev'essere anch'esso aggiornato alla `4.0.0`, modificandone ovviamente le dichiarazioni in modo che riflettano le modifiche sostanziali della nuova versione.

Molte librerie hanno un vasto numero di utenti che le utilizzano (compresi mantenitori di altri pacchetti che usano la libreria come dipendenza) e che probabilimente non passeranno subito alla nuova versione major. Questo perchè potrebbero volerci mesi prima che il mantenitore trovi il tempo per adattare il codice alle nuove modifiche della libreria.
Nel frattempo, utenti che usano la vecchia versione della libreria potrebbero richiedere aggiornamenti per la vecchia versione delle dichiarazioni dei tipi, ad esempio nel caso in cui un utente che usa la vecchia versione della libreria trova un erroore nelle definizioni dei tipi.

Se hai intenzione di continuare a mantenere ed aggiornare le dichiarazioni dei tipi per le vecchie versioni di una libreria, puoi creare una sottocartella (es. `/v2/`) chiamata come la versione che presto diventerà vecchia e copiarci dentro tutte le dichiarazioni attuali.

Siccome la cartella principale deve sempre contenere le dichiarazioni dei tipi per la versione più nuova, avrai anche bisogno di fare alcune modifiche alle dichiarazioni della vecchia versione che ora sta nella sottocartella, per essere sicuro che gli eventuali indirizzi relativi siano riferiti alla sottocartella  e non più alla cartella principale.

1. Aggiorna gli indirizzi relativi nel `tsconfig.json` ed in un eventuale `tslint.json`.
2. Aggiungi delle regole per mappare gli indirizzi cosicchè i test vengano eseguiti per la versione voluta.

Ad esempio, la libreria [`history`](https://github.com/ReactTraining/history/) ha avuto delle modifiche sostanziali passando dalla versione `2.x` alla `3.x`.
Siccome molti utenti sono rimasti alla `2.x`, un mantenitore che voleva fare ulteriori aggiornamenti alle dichiarazioni dei tipi di questa vecchia versione ha aggiunto una sottocartella `v2` nella repo delle dichiarazioni dei tipi di questa libreria.
Nel momento in cui questo README è stato scritto, il [`tsconfig.json` della history v2](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/1253faabf5e0d2c5470db6ea87795d7f96fef7e2/types/history/v2/tsconfig.json) è così:

```json
{
    ""compilerOptions"": {
    ""baseUrl"": ""../../"",
    ""typeRoots"": [""../../""],
    ""paths"": {
        ""history"": [ ""history/v2"" ]
    }
    },
    ""files"": [
        ""index.d.ts"",
        ""history-tests.ts""
    ]
  }
```

Se ci sono pacchetti su Definitely Typed che sono incompatibili con la vesione più nuova di una libreria, dovrai mappare gli indirizzi alla vecchia versione, continuando ricorsivamente per gli altri pacchetti che dipendono da essa.

Per esempio, `react-router` dipende da `history@2`, quindi il [`tsconfig.json` di react-router](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/types/react-router/v2/tsconfig.json) ha mappato gli indirizzi alla versione vecchia (`""history"": [ ""history/v2"" ]`) fino a che non è passato alla nuova versione di `react-router`.
A sua volta, la libreria `react-router-bootstrap` (che dipende da `react-router`) ha dovuto aggiungere lo stesso mapping nel suo `tsconfig.json`, fino a che la sua dipendenza `react-router` non è stata aggiornata all'ultima versione.

Nota che `/// <reference types="".."" />` non funziona con il mapping degli indirizzi, quindi le dipendenze devono usare `import`.

#### Come si scrivono le definizioni dei tipi per le librerie che possono essere usata sia globalmente che come modulo?

La guida di Typescript spiega benissimo [come scrivere le definizioni dei tipi](https://www.typescriptlang.org/docs/handbook/declaration-files/introduction.html) ed ha anche [un file di esempio](https://www.typescriptlang.org/docs/handbook/declaration-files/templates/global-modifying-module-d-ts.html) che è esattamente una libreria che può essere usata sia come modulo su nodejs che come libreria globale in una pagina web.

Per accertarti che le tue definizioni possono essere usate sia globalmente che che modulo importato, crea una cartella `test` e creaci dentro due file di test. 
Chiamane uno `NomeLibreria-global.test.ts` e l'altro `YourLibraryName-module.test.ts`. 
Il file di test *global* dovrebbe controllare che le definizioni funzionano bene quando la libreria è usata globalmente in una pagina web (in questo caso non bisogna specificare un `import`). 
Il file di test *module*, invece, controlla se le definzioni funzionano quando la libreria viene importata come un modulo.
Se aggiungi la proprietà `files` nel tuo `tsconfig.json`, assicurati di includere entrambi questi file di test. Un [esempio pratico](https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/big.js/test) è disponibile nelle dichiarazioni dei tipi di `big.js`.


#### E per quanto riguarda i pacchetti npm con scope (`@nome_utente`)?

I tipi per un pacchetto `@foo/bar` devono andare su `types/foo__bar`. Nota il doppio underscore.

Quando `dts-gen` viene usato per uno pacchetto con scope, nel `tsconfig.json`, la proprietà `paths` va cambiata manualmente per referenziarlo correttamente.

```json
{
    ""paths"": {
        ""@foo/*"": [""foo__*""]
    }
}
```

#### La cronologia dei file su Github sembra incompleta.

Github non [supporta](http://stackoverflow.com/questions/5646174/how-to-make-github-follow-directory-history-after-renames) la cronologia per file rinominati. Usa invece [`git log --follow`](https://www.git-scm.com/docs/git-log) instead.

## Licenza

Questo progetto è sotto la licenza MIT.

I copyright nei file di definizione valgono per ogni contributore elencato all'inizio di ogni file di definizione.
"
96,eddelbuettel/drat,R,"## drat: Drat R Archive Template

[![Build Status](https://travis-ci.org/eddelbuettel/drat.svg)](https://travis-ci.org/eddelbuettel/drat)
[![CI](https://github.com/eddelbuettel/drat/workflows/ci/badge.svg)](https://github.com/eddelbuettel/drat/actions?query=workflow%3Aci)
[![License](https://img.shields.io/badge/license-GPL%20%28%3E=%202%29-brightgreen.svg?style=flat)](https://www.gnu.org/licenses/gpl-2.0.html)
[![CRAN](https://www.r-pkg.org/badges/version/drat)](https://cran.r-project.org/package=drat)
[![Dependencies](https://tinyverse.netlify.com/badge/drat)](https://cran.r-project.org/package=drat)
[![Downloads](https://cranlogs.r-pkg.org/badges/drat?color=brightgreen)](https://www.r-pkg.org/pkg/drat)
[![Last Commit](https://img.shields.io/github/last-commit/eddelbuettel/drat)](https://github.com/eddelbuettel/drat)
[![Documentation](https://img.shields.io/badge/documentation-is_here-blue)](https://eddelbuettel.github.io/drat/)
[![Fork](https://img.shields.io/badge/fork-this_instead-orange)](https://github.com/drat-base/drat)

> **drat**
>
> What cute people say when they are pissed off
>
> _""Oh Drat, i lost my wallet""_
>
> [Urban Dictionary](https://www.urbandictionary.com/define.php?term=drat)

### Nota Bene

Do **not** fork _this_ repo as a quick start towards creating your `drat`, fork [this
repo](https://github.com/drat-base/drat) instead. See below for more.

### Background

The R package ecosystem is one of the cornerstones of the success seen by R.
As of July 2020, over 16000 packages are on [CRAN](https://cran.r-project.org),
with about one thousand more at [BioConductor](https://www.bioconductor.org).

Support for multiple repositories is built deeply into R; mostly via the
(default) package `utils`. The
[`update.packages`](https://www.rdocumentation.org/packages/utils/functions/update.packages)
function (along with several others from the `utils` package) can be used with
ease for these three default repositories as well as many others. But it
seemed that support for _simple creation and use_ of local repositories was
missing.

[Drat](https://dirk.eddelbuettel.com/code/drat.html) tries to help here and supports two principal modes:

- *GitHub* by leveraging `gh-pages`
- *Other repos* by using other storage where you can write and provide html access

Please see the next section about how to get started, the package documentation, the
[drat package page](https://dirk.eddelbuettel.com/code/drat.html) or the
[blog section on drat](http://dirk.eddelbuettel.com/blog/code/drat/) for more.

### Getting Started

Do **not** fork _this_ repo as a quick start towards creating your `drat`, fork [this
repo](https://github.com/drat-base/drat) instead. See below for more.

See the vignettes

- [Drat FAQ](https://eddelbuettel.github.io/drat/vignettes/dratfaq/),
- [Drat for Package Authors](https://eddelbuettel.github.io/drat/vignettes/dratforauthors/),
- [Drat for Package Users](https://eddelbuettel.github.io/drat/vignettes/dratforusers/)
- [Drat Step by Step](https://eddelbuettel.github.io/drat/vignettes/dratstepbystep/)
- [Why Drat?](https://eddelbuettel.github.io/drat/vignettes/whydrat/)
- [Combining Drat and Travis](https://eddelbuettel.github.io/drat/vignettes/combiningdratandtravis/)
- [Drat Lightning Talk at useR!2015](https://dirk.eddelbuettel.com/papers/useR2015_drat.pdf)

for the FAQ, two principal uses cases, basic motivation and an overview / introduction.
The package documentation provides more details.  The
[drat package page](https://dirk.eddelbuettel.com/code/drat.html) has a longer
tutorial, and the
[blog section on drat](http://dirk.eddelbuettel.com/blog/code/drat/) has even
more.

Package documentation, help pages, vignettes, and more is also available
[here](https://eddelbuettel.github.io/drat/).



### Installation

The preferred approach is to install the released package from [CRAN](https://cran.r-project.org) via the standard

```{.r}
install.packages(""drat"")
```

command.


### Quick Start Towards Your Own Drat

`drat` comes with its own repo creation helper function
[`initRepo`](https://eddelbuettel.github.io/drat/man/initRepo/), but an even shorter path may be to
fork an existing repo.  This has been done numerous times with this original repo. However, over
time this repo accumulated code refinements along with more documentation making it a little
unwieldy. So we set up [dedicated repo](https://github.com/drat-base/drat) just to facilitate `drat`
forking.  Clone it, and you will have a ready-to-use `drat` repository.

### Status

The package has been available from [CRAN](https://cran.r-project.org) since
the Spring of 2015 and starting to get some use. Possible improvements,
additions and next steps are listed in the
[TODO.md](https://github.com/eddelbuettel/drat/blob/master/inst/TODO.md)
file.

A few drat repositories are starting to appear (besides this one). An incomplete list (looking at the direct forks as well as GitHub search):

 - [yutannihilation](https://github.com/yutannihilation/drat)
 - [gschofl](https://github.com/gschofl/drat/)
 - [csgillespie](https://github.com/csgillespie/drat)
 - [shabbychef](https://github.com/shabbychef/drat)
 - [RcppCore](https://github.com/RcppCore/drat)
 - [arilamstein](https://github.com/arilamstein/drat)
 - [piccolbo](https://github.com/piccolbo/drat)
 - [ghrr](https://github.com/ghrr/drat)
 - [cloudyr](https://cloudyr.github.io/drat/)
 - [dmlc](https://github.com/dmlc/drat)
 - [thospfuller](https://github.com/thospfuller/drat)

The [rOpenSci](https://ropensci.org) project uses
[drat](https://dirk.eddelbuettel.com/code/drat.html) to distribute their code
and has written a
[nice blog post](https://ropensci.org/blog/2015/08/04/a-drat-repository-for-ropensci/)
about it.

### Travis Integration

Colin Gillespie has started to integrate [Travis CI](https://www.travis-ci.org) with drat, see his
[dratTravis](https://github.com/csgillespie/dratTravis) repository for more details, and the
contributed vignette [Combining Drat and Travis](https://eddelbuettel.github.io/drat/vignettes/combiningdratandtravis/)

### Author

Dirk Eddelbuettel, with contributions by Carl Boettiger, Sebastian Gibb, Colin Gillespie, Matt
Jones, Thomas Leeper, Steven Pav, Jan Schulz, Christoph Stepper, Felix G.M. Ernst, and Patrick
Schratz.

### License

GPL (>= 2)
"
97,repology/repology-rules,Python,"# Repology ruleset

![CI](https://github.com/repology/repology-rules/workflows/CI/badge.svg)

There can be a huge discrepancy in how packages for single project
are named and versioned in different repositories, so Repology
needs a flexible ruleset in order to overcome the differences,
match packages and make versions comparable.

## TL;DR

You are welcome to submit pull requests with rules you need. Here's
a quick pointer of how to add specific rules:

### You want to **merge** differently named packages into a single entry?

- Choose target name (prefer least ambiguous and/or most widely used name)
- Open corresponding yaml file under `800.renames-and-merges/` (if there's no existing yaml file relevant to your package, use the file named with the first letter of your target name, like `a.yaml`)
- Add rule like `- { setname: <target name>, name: <original name> }`

### You want to mark incorrect version of specific package?

- Open corresponding yaml file under `900.version-fixes/`
- Add rule like: `- { name: <package name>, ver: <bad version>, ignore: true }`
- Consider using a `verpat` with regular expression to match similar
  bad versions which may appear in the future. Examples:
  - `verpat: ""20[0-9]{6}""` to match dates (`20110323`)
  - `verpat: ""20[0-9]{2}\\.[0-9]{2}\\.[0-9]{2}""` same, but for delimited date, (`2010.03.23`)
  - `verpat: "".*20[0-9]{6}.*""` to match dates anywhere in the version (`1.0.20110323`)
  - `verpat: ""[0-9a-f]{7}""` match something resembling a git commit (`a7b823f`)
  - `verpat: ""[0-9]{4,}""` match something resembling a build or revision number (`12345`)

### You want to split different projects with the same name

- Open corresponding yaml file under `850.split-ambiguities/`
- Add a set of rules which distinct packages, such as:
  - `- { name: <ambiguous name>, wwwpart: <part of the homepage url>, setname: <specific name> }`
  - `- { name: <ambiguous name>, category: <category>, setname: <specific name> }`
  - `- { name: <ambiguous name>, verpat: <version pattern>, setname: <specific name> }`
  - `- { name: <ambiguous name>, ruleset: <families>, setname: <specific name> }` as a least resort

## Contributing

Things to know if you're submitting pull request or have push access
to this repository.

- Repology is currently set up to automatically pull latest ruleset
from `master` branch in this repo on each update, so everything
committed here will be automatically applied to repology in several
hours.
- Repology runs `make check` after updating the repo, and if it
fails, rolls back to the latest good commit, so it's somewhat
protected from broken ruleset.
- In the worst case, broken ruleset will prevent repology from
updating until the problem is resolved.
- Still, please run `make check` before committing, and/or install
a git hook (`scripts/pre-push`) which runs it for you (you can copy
it into `.git/hooks` or just run `make install-hook`).
- The checker script requires python modules `voluptous` and `PyYAML`.
`pip install PyYAML voluptuous` should install them for you.
- In general, stay close to the style already used in the ruleset,
use existing rules as examples, keep it simple and have fun!
- If in doubt, you can always just submit a report on the website
and avoid all the work!

## Rule basics

Rules are stored in a set of files in [YAML](http://yaml.org/) format,
a flexible human friendly markup format for structured data. Each
rule is a single item of big array, and may be written in single or
multiple lines (depending on what's more convenient for the particular
case), for example this rule renames `etracer` into `extreme-tuxracer`.

```yaml
- { name: etracer, setname: extreme-tuxracer }
```

which is the same as:

```yaml
- name: etracer
  setname: extreme-tuxracer
```

Each rule has a set of keywords which specify how a package is matched
(by name, version, repository, category etc.) and how it is modified
(package is renamed, version scheme is changed, flags are applied etc.).

Rule order matters, as multiple rules may match a single package, and
they are applied in order. Further more, changes applied by earlier
rules are affecting further matches: for instance, if a package is
renamed, new name will be matched for the following rules.

While rules are basically arbitrary, it's practical though to attribute
each rule to specific class of action, most distinctive of which are:

- Rename or merge rules. Match name, and set another name. Main purpose
  is to merge differently named packages into the same project. Such as
  `etracer`, `extremetuxracer`, `extreme-tuxracer` → `extreme-tuxracer`.
- Split rules. Match name and some additional property (version, homepage
  or repository) and set another name. Used to split similarly named
  packages of different projects. Such as `clementine` → `clementine-wm`,
  `clementine-player`.
- Version fixes. Match name but do not change it, instead change versions
  or set some version-related flags. Used to fix incorrect versioning scheme
  (`v1.0` → `1.0`), mark some versions as devel (such as beta versions),
  or ignore some versions (e.g. snapshots like `20130523` while there's
  official version like `1.0`).

## Ruleset structure

Ruleset is split into several distinctive parts, mostly based on functional
class of rules described above. They are arranged in such a way that when
adding a rule into a specific part you don't need to be aware of the rest of
the ruleset.

- **100.prefix-suffix** - normalization of repository specific prefixes and
  suffixes which are not part of the meaningful package name. Such as removal
  of `lib32-` prefixes.
- **2xx.handpicked** - a block where access to unmodified package names is
  needed, such as manual whitelists or blacklists.
- **[45]xx.wildcard** - wildcard rules which affect a lot of packages. These
  mostly handle modules for specific languages such as Perl (which may be
  named like `p5-Foo-Bar` or `libfoo-bar-perl` in different repos) by adding
  distinctive prefix (`perl:` in this case) to them, so they do not conflict
  with modules for other languages and other software.

  There are three subsets here:
  - **pure** rules which are known to not have any false positives
  (e.g. packages from `CPAN` are always perl modules).
  - **exceptions** for the wildcard rules
  - **wildcard** rules themselves

- **750.exceptions** - the small set of remaining exceptions. If a package
  needs rule here, it's most positively incorrectly named.
- **800.renames-and-merges** - pure merge rules
- **850.split-ambiguities** - pure split rules
- **900.version-fixes** - pure version fixes
- **950.split-branches** - additional split section for project which
  have multiple development branches which are incompatible and may
  present in a single repository at the same time for compatibility
  purposes. Such as `gtk2` and `gtk3`.

- There are also some **fixme** subsets which are remainings of previous
  generation of the ruleset. These files will eventually be refactored
  and removed.

This may seem complex, but in practice the mostly used rulesets are
**800**, **850** and **900**, which cleanly correspond to three functional
classes of rules described in the previous section.

Other parts of the ruleset may need attention when new repositories are
introduced.

## Rule syntax

As already mentioned, keywords which rules consist are related to either
matching packages or modifying them. Here's detailed description for all
of them.

### Conditions

#### ruleset

Each repository Repology supports has a set of *rulesets* associated with
it. For instance, all Debian-based distros have ruleset `debuntu`. It may
be used to only match packages in specific repositories, but without need
to chase specific repository version. You may look up repositories and
their retails in [repos.d](https://github.com/repology/repology/tree/master/repos.d)
directory of main Repology repository.

You may specify a list of rulesets to match either of them.

```yaml
- { ruleset: freebsd, ... }

- { ruleset: [ arch, openbsd ], ... }
```

#### noruleset

Disable rule matching for specified ruleset(s).

```yaml
# applies to all Debian derivatives, but not Deepin
- { ruleset: debuntu, noruleset: deepin, ... }
```

#### family

Deprecated. Same as **ruleset** and may be just changed into it.

#### category

Matches package category(ies). Note that category information is not
available for all repositories and each repository may have its
own set of categories.

```yaml
- { category: games, ... }

- { category: [ mail-client, mail-filter, mail-mta ], ... }
```

#### maintainer

Matches package maintainer(s). Match is case insensitive.

```yaml
- { maintainer: ""nobody@nowhere.com"" }
```

#### name

Match exact package name(s).

```yaml
- { name: firefox, ... }

- { name: [postgresql-client, postgresql-server, postgresql-contrib], ... }
```

#### namepat

Matches package name against a regular expression. Whole name is
matched. May contain captures.

```yaml
- { name: ""swig[0-9]+"", ... }
```

#### ver

Matches exact package version(s).

```yaml
- { name: firefox, ver: ""50.0.1"", ... }
```

#### notver

The opposite of **ver**: matches if package version is none of specified
version(s).

```yaml
- { name: firefox, notver: [""50.0.1"", ""50.0.2""] }
```

#### verpat

Matches package version name against a regular expression. Whole
version is matched. Note that you need to escape periods which
mean ""any symbol"" in regular expressions. Matching is case insensitive.

```yaml
- { name: firefox, verpat: ""50\\.[0-9]+"", ... }

- { name: firefox, verpat: ""50\\..*"", ... }
```

#### vercomps

Matches versions components count.

```yaml
- { name: gimp, vercomps: 3, ...} # matches 1.2.3, but not 1.2 or 1.2.3.4
```

#### verlonger

Matches versions longer than a given number of dot-separated parts.

Mostly useful to match broken version schemes with extra versions
components added.

```yaml
- { name: gimp, verlonger: 3, ...} # 2.9.8.12345 is something unofficial
```

#### vergt, verge, verlt, verle, vereq, verne

Compares version to a given one and matches if it's:

- **vergt**: greater (>)
- **verge**: greater or equal (≥)
- **verlt**: lesser (<)
- **verle**: lesser or equal (≤)
- **vereq**: equal
- **verne**: not equal

```yaml
# match git >= 2.16
- { name: git, verge: ""2.16"", ...}
```

Be careful when using this with regard to pre-release versions:
`1.0beta1` is lesser than `1.0`, so it won't match `verge: 1.0`.
You may use **verpat** instead.

#### relgt, relge, rellt, relle, releq, relne

Similar to **verXX** family, but checks how a package version relates
to a specified release. A release includes all pre-release and
post-release with a given prefix, e.g. `releq: ""1.0""` would match
for `1.0alpha1`, `1.0`, `1.0patch`, `1.0.1`, but not `0.99` and `1.1`.

#### wwwpat

Matches package homepage against a regular expression. Note that
unlike namepat and verpat, partial match is allowed here. Also
note that it's preferred to escape dots with double slash, as `.`
means ""any character"" in regular expressions.

```yaml
- { name: firefox, wwwpat: ""mozilla\\.org"", ... }
```

#### wwwpart

Matches when a package homepage contains given substring. This
is usually more practical than **wwwpat** as in most cases you
just need to match an URL part and don't need complex patterns,
also you don't want to bother with escaping here. Matching is
case insensitive.

```yaml
- { name: firefox, wwwpart: ""mozilla.org"", ... }
```

#### summpart

Matches when a package summary contains given substring. Useful
as as an alternative to **wwwpart** for cases where package
homepage is not available. Matching is case insensitive.

```yaml
- { name: firefox, summpart: ""browser"", ... }
```

#### is_p_is_patch

Matches when a package has `p_is_patch` flag set (see `p_is_patch`
action below).

### Actions

#### setname

Effectively rename the package. You may use `$0` placeholder to
substitute original name or `$1`, `$2` etc. to subsided contents
of corresponding captures of regular expression used in **namepat**.
Note that you don't need to use neither **name** nor **namepat** for
`$0` to work, but you must have **namepat** with corresponding
captures to use `$1` and so on.

```yaml
# etracer→extreme-tuxracer
- { name: etracer, setname: extreme-tuxracer }

# aspell-dict-en→aspell-ru, aspell-dict-ru→aspell-ru etc.
- { namepat: ""aspell-dict-(.*)"", setname: ""aspell-$1"" }

# all packages in dev-perl Gentoo category are prepended `perl:`
# Locale-Msgfmt→perl:Locale-Msgfmt
- { ruleset: gentoo, category: dev-perl, setname: ""perl:$0"" }
```

#### setver

Changes the version of the package. As with **setname**, you may
use `$0`, `$1` placeholders.

```yaml
# remove bogus leading version component
- { verpat: ""0\\.(.*)"", setver: $1 }
```

#### remove

Set to `true` to completely remove package. It will not appear
anywhere in repology. Set to `false` to undo.

```yaml
# a metapackage which does not refer to any real project, we don't need it
- { name: ""x11-fonts"", remove: true }
```

#### devel

Set to `true` to mark version of matched package as development or
unstable version, so it does not make latest stable version outdated.
Set to `false` to undo.

```yaml
# mark versions with odd second component as devel
- { name: gnome-terminal, verpat: ""[0-9]+\\.[0-9]*[13579]\\..*"", devel: true }
```

#### altver

A project may use two parallel versioning schemes one of which contains
additional version components, such as build number:

`0.17`, `0.17.13509`, `0.17.13541`, `0.18`, `0.18.16131`

Normally, `0.18.16131` would outdate `0.18`, but if these refer to the same
version, this is not desired behavior. In such case, version scheme containing
extra components (e.g. one which compares greater) may be marked as **altver**,
which would allow both `0.18` and `0.18.16131` to be considered latest, and both
outdated by either `0.19` or `0.19.x`

```yaml
- { name: freecad, verlonger: 3, altver: true }
```

#### altscheme

Similar to **altver**, but for the case where versioning schemes do not have common
prefix and are totally incomatible:

`3.2.1`, `3207`, `3.2.2`, `3211`

Marking either of the schemes with this flag results in completely independent processing,
which would allow both `3.2.2` and `3211` to be treated as newest.

```yaml
- { name: sublime-text, verpat: ""[0-9]+"", altscheme: true }
```

#### ignore, incorrect, untrusted, noscheme, snapshot, successor, debianism, rolling

Set to `true` to ignore specific package versions. This is meant for the
cases where comparison is not possible - ignore version are excluded from
comparison and do not affect status of other versions. There are multiple
ignore flavors:

- `rolling` - package is fetched from always latest snapshot or VCS
  master/trunk. Its version has no meaning (like Gentoo's `9999`),
  it may contain repository specific format of commit hash, revision or
  date.
- `noscheme` - there's no official versioning scheme. Repositories may
  use random versions or dates, there's no point comparing them.
- `incorrect` - known incorrect version (e.g. version which was not
  released yet)
- `untrusted` - used for repositories which are known for providing
  incorrect versions, to ignore them proactively. It's common pattern
  to create a pair of `incorrect` rule matching specific version and
  `untrusted` rule for the following versions in a given repository.
- `ignored` - general ignore
- `successor` - currently alias for `devel` used to convey additional
  meaning: this is a fork of unmaintained original project
- `debianism` - currently alias for `devel` used to convey additional
  meaning: this package uses distribution maintained at debian (probably
  with version addendum)
- `snapshot` - currently alias for `ignored`

```yaml
# Fedora was known to use ""6.0.0"" version before it was actually released
# mark as incorrect and prevent future problems
- { name: llvm, ver: ""6.0.0"", ruleset: fedora, incorrect: true }
- { name: llvm, ruleset: fedora, untrusted: true }
```

#### p_is_patch

Set to `true` to indicate that this project uses `p` letter in version
to indicate post- or patch releases. This fixes version comparison, as
by default `p` is treated as pre-release.

```yaml
# sudo 1.8.21p2 > 1.8.21
- { name: sudo, p_is_patch: true }
```

#### any_is_patch

Set to `true` to indicate that this project uses any letter in version
to indicate post- releases.

```yaml
# rb here denotes a patchset, treat is as such
- { name: webalizer, verpat: "".*rb.*"", any_is_patch: true }
```

#### outdated

Set to `true` to force the package to be outdated, even if its version
is compared as greatest. `false` to undo.

```yaml
# when 0.20 follows 0.193:
- { version: ""0.193"", outdated: true }
```

#### legacy

Set to `true` to force the package to be legacy instead of outdated.
`false` to undo. Useful when a specific repository purposely contains
an outdated version of specific project for compatibility purposes.

```yaml
- { name: ruby-slack-notifier-1, ruleset: aur, legacy: true }
```

#### nolegacy

Set to `true` to prevent the package to ever have legacy status.
This is useful for marking packages which declare to be of development
version, but are never the less outdated.

```yaml
- { name: ffmpeg-git, nolegacy: true }
```

#### warning

Output a given warning when matched. Useful to catch places which

```yaml
# will catch unexpected versions
- { name: gtk, verpat: ""1\\..*"", setname: gtk1 }
- { name: gtk, verpat: ""2\\..*"", setname: gtk2 }
- { name: gtk, verpat: ""3\\..*"", setname: gtk3 }
- { name: gtk, verpat: ""4\\..*"", setname: gtk4 }
- { name: gtk, warning: ""Neither of gtk1,2,3,4 - need a new rule or some weirdness is going on"" }

# will trigger a warning if new project called ""tesseract"" appears
# ...or website changes, or just a package without website defined appears,
# so it'll require another condition
- { name: tesseract, setname: tesseract-game, wwwpart: tesseract.gg }
- { name: tesseract, setname: tesseract-ocr, wwwpart: tesseract-ocr }
- { name: tesseract, warning: ""Please add rule for tesseract"" }
```

#### addflavor

Flavors are used to distinct set of packages denoting a multiple
version of a project and a set of packages denoting a multiple parts
or variants of a project. Consider an example:

- `foo1 1.0` and `foo2 2.0` merged into `foo`. In this case they denote
  a multiple versions of the same project, flavors are not needed here
  and `foo1` will have `legacy` status.
- `foo-client 1.0` and `foo-server 1.1` merged into `foo`. In this case
  they denote a parts of the same project, which are expected to be of
  the same version. Flavors should be used in this case, so `foo-client`
  will have `outdated` status.

Flavors a plain strings and may be arbitrary, for example `client`
and `server` in the last example. You may specify flavor explicitly
or use `true` value to make flavor taken from the package name.

```yaml
- { name: postgresql-client, setname: postgresql, addflavor: client }
- { name: postgresql-server, setname: postgresql, addflavor: server }

# This works too
- { name: [postgresql-client, postgresql-server], setname: postgresql, addflavor: true }
```

#### resetflavors

Set to `true` to remove all previously added flavors.

#### last

Set to `true` to stop ruleset processing right after the current rule.

Consider this a legacy, it should not be needed

#### replaceinname

Takes pattern and replacement strings and applies them to the package
name. Used for low-level normalization.

```yaml
# slashes in package names are not allowed
- { replaceinname: { ""/"": ""-"" } }

# also useful for some repositories
- { replaceinname: { "" "": ""-"" } }
```

#### tolowername

Converts a package name to lowercase. This is called once in the
very beginning of the ruleset. The purpose of having this as a rule
action is to be able to have exceptions, e.g. packages which should
be distinguished solely by the case of their names.

```yaml
- { tolowername: true }
```

### Conditional rules

For additional flexibility, a mechanism exists to toggle some rules
based on the previous rules.

#### addflag

Sets a virtual flag (arbitrary string) which only exists for the duration
of rule processing and may be checked in the following rules.

```yaml
- { name: python, addflag: not_python_module }
```

#### flag, noflag

Only matches if the specified flag is (or is not) set.

```yaml
- { name: python, addflag: not_python_module }
...
# will add ""python:"" prefix to all packages in category ""python"",
# but not for ""python"" package
- { category: python, noflag: not_python_module, setname: ""python:$0"" }
```

### Annotations

These annotations do not affect package processing, but are related
to ruleset maintenance.

#### maintenance

Indicates that a rule needs manual maintenance. For example, when
development version cannot be determined from the version schema,
one would need to revisit and update the version occasionally.

```yaml
- { name: tor, verge: ""0.3.4"", devel: true, maintenance: true }
```

#### precious

Indicates that a rule should not be removed even if it doesn't
match any packages. That is, a rule is likely to be useful sometime
in the future.

#### disposable

Indicates that a rule may be removed if it doesn't match any packages.

## Author

* [Dmitry Marakasov](https://github.com/AMDmi3) <amdmi3@amdmi3.ru>

## License

GPLv3 or later, see [COPYING](COPYING).
"
98,rdpeng/ProgrammingAssignment2,R,"### Introduction

This second programming assignment will require you to write an R
function that is able to cache potentially time-consuming computations.
For example, taking the mean of a numeric vector is typically a fast
operation. However, for a very long vector, it may take too long to
compute the mean, especially if it has to be computed repeatedly (e.g.
in a loop). If the contents of a vector are not changing, it may make
sense to cache the value of the mean so that when we need it again, it
can be looked up in the cache rather than recomputed. In this
Programming Assignment you will take advantage of the scoping rules of
the R language and how they can be manipulated to preserve state inside
of an R object.

### Example: Caching the Mean of a Vector

In this example we introduce the `<<-` operator which can be used to
assign a value to an object in an environment that is different from the
current environment. Below are two functions that are used to create a
special object that stores a numeric vector and caches its mean.

The first function, `makeVector` creates a special ""vector"", which is
really a list containing a function to

1.  set the value of the vector
2.  get the value of the vector
3.  set the value of the mean
4.  get the value of the mean

<!-- -->

    makeVector <- function(x = numeric()) {
            m <- NULL
            set <- function(y) {
                    x <<- y
                    m <<- NULL
            }
            get <- function() x
            setmean <- function(mean) m <<- mean
            getmean <- function() m
            list(set = set, get = get,
                 setmean = setmean,
                 getmean = getmean)
    }

The following function calculates the mean of the special ""vector""
created with the above function. However, it first checks to see if the
mean has already been calculated. If so, it `get`s the mean from the
cache and skips the computation. Otherwise, it calculates the mean of
the data and sets the value of the mean in the cache via the `setmean`
function.

    cachemean <- function(x, ...) {
            m <- x$getmean()
            if(!is.null(m)) {
                    message(""getting cached data"")
                    return(m)
            }
            data <- x$get()
            m <- mean(data, ...)
            x$setmean(m)
            m
    }

### Assignment: Caching the Inverse of a Matrix

Matrix inversion is usually a costly computation and there may be some
benefit to caching the inverse of a matrix rather than computing it
repeatedly (there are also alternatives to matrix inversion that we will
not discuss here). Your assignment is to write a pair of functions that
cache the inverse of a matrix.

Write the following functions:

1.  `makeCacheMatrix`: This function creates a special ""matrix"" object
    that can cache its inverse.
2.  `cacheSolve`: This function computes the inverse of the special
    ""matrix"" returned by `makeCacheMatrix` above. If the inverse has
    already been calculated (and the matrix has not changed), then
    `cacheSolve` should retrieve the inverse from the cache.

Computing the inverse of a square matrix can be done with the `solve`
function in R. For example, if `X` is a square invertible matrix, then
`solve(X)` returns its inverse.

For this assignment, assume that the matrix supplied is always
invertible.

In order to complete this assignment, you must do the following:

1.  Fork the GitHub repository containing the stub R files at
    [https://github.com/rdpeng/ProgrammingAssignment2](https://github.com/rdpeng/ProgrammingAssignment2)
    to create a copy under your own account.
2.  Clone your forked GitHub repository to your computer so that you can
    edit the files locally on your own machine.
3.  Edit the R file contained in the git repository and place your
    solution in that file (please do not rename the file).
4.  Commit your completed R file into YOUR git repository and push your
    git branch to the GitHub repository under your account.
5.  Submit to Coursera the URL to your GitHub repository that contains
    the completed R code for the assignment.

### Grading

This assignment will be graded via peer assessment.
"
99,glarizza/puppet_repository,Ruby,
100,mateodelnorte/sourced-repo-mongo,JavaScript,"sourced-repo-mongo
==================

mongo data store and repository for sourced-style event sourcing models
"
101,tesseract-ocr/tesseract,C++,"# Tesseract OCR

[![Build Status](https://travis-ci.org/tesseract-ocr/tesseract.svg?branch=master)](https://travis-ci.org/tesseract-ocr/tesseract)
[![Build status](https://ci.appveyor.com/api/projects/status/miah0ikfsf0j3819/branch/master?svg=true)](https://ci.appveyor.com/project/zdenop/tesseract/)
![Build status](https://github.com/tesseract-ocr/tesseract/workflows/sw/badge.svg)<br>
[![Coverity Scan Build Status](https://scan.coverity.com/projects/tesseract-ocr/badge.svg)](https://scan.coverity.com/projects/tesseract-ocr)
[![Code Quality: Cpp](https://img.shields.io/lgtm/grade/cpp/g/tesseract-ocr/tesseract.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/tesseract-ocr/tesseract/context:cpp)
[![Total Alerts](https://img.shields.io/lgtm/alerts/g/tesseract-ocr/tesseract.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/tesseract-ocr/tesseract/alerts)
[![OSS-Fuzz](https://img.shields.io/badge/oss--fuzz-fuzzing-brightgreen)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=2&q=proj:tesseract-ocr)
<br/>
[![GitHub license](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](https://raw.githubusercontent.com/tesseract-ocr/tesseract/master/LICENSE)
[![Downloads](https://img.shields.io/badge/download-all%20releases-brightgreen.svg)](https://github.com/tesseract-ocr/tesseract/releases/)

## About

This package contains an **OCR engine** - `libtesseract` and a **command line program** - `tesseract`.
Tesseract 4 adds a new neural net (LSTM) based OCR engine which is focused
on line recognition, but also still supports the legacy Tesseract OCR engine of
Tesseract 3 which works by recognizing character patterns. Compatibility with
Tesseract 3 is enabled by using the Legacy OCR Engine mode (--oem 0).
It also needs [traineddata](https://tesseract-ocr.github.io/tessdoc/Data-Files.html) files which support the legacy engine, for example
those from the tessdata repository.

The lead developer is Ray Smith. The maintainer is Zdenko Podobny.
For a list of contributors see [AUTHORS](https://github.com/tesseract-ocr/tesseract/blob/master/AUTHORS)
and GitHub's log of [contributors](https://github.com/tesseract-ocr/tesseract/graphs/contributors).

Tesseract has **unicode (UTF-8) support**, and can **recognize more than 100 languages** ""out of the box"".

Tesseract supports **various output formats**: plain text, hOCR (HTML), PDF, invisible-text-only PDF, TSV. The master branch also has experimental support for ALTO (XML) output.

You should note that in many cases, in order to get better OCR results,
you'll need to **[improve the quality](https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html) of the image** you are giving Tesseract.

This project **does not include a GUI application**.
If you need one, please see the [3rdParty](https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty.html) documentation.

Tesseract **can be trained to recognize other languages**.
See [Tesseract Training](https://tesseract-ocr.github.io/tessdoc/Training-Tesseract.html) for more information.

## Brief history

Tesseract was originally developed at Hewlett-Packard Laboratories Bristol and
at Hewlett-Packard Co, Greeley Colorado between 1985 and 1994, with some
more changes made in 1996 to port to Windows, and some C++izing in 1998.
In 2005 Tesseract was open sourced by HP. Since 2006 it is developed by Google.

The latest (LSTM based) stable version is **[4.1.1](https://github.com/tesseract-ocr/tesseract/releases/tag/4.1.1)**, released on December 26, 2019.
Latest source code is available from [master branch on GitHub](https://github.com/tesseract-ocr/tesseract/tree/master).
Open issues can be found in [issue tracker](https://github.com/tesseract-ocr/tesseract/issues),
and [planning documentation](https://tesseract-ocr.github.io/tessdoc/Planning.html).

The latest 3.0x version is **[3.05.02](https://github.com/tesseract-ocr/tesseract/releases/tag/3.05.02)**, released on June 19, 2018. Latest source code for 3.05 is available from [3.05 branch on GitHub](https://github.com/tesseract-ocr/tesseract/tree/3.05).
There is no development for this version, but it can be used for special cases (e.g. see [Regression of features from 3.0x](https://tesseract-ocr.github.io/tessdoc/Planning.html#regression-of-features-from-30x)).

See **[Release Notes](https://tesseract-ocr.github.io/tessdoc/ReleaseNotes.html)**
and **[Change Log](https://github.com/tesseract-ocr/tesseract/blob/master/ChangeLog)** for more details of the releases.

## Installing Tesseract

You can either [Install Tesseract via pre-built binary package](https://tesseract-ocr.github.io/tessdoc/Home.html)
or [build it from source](https://tesseract-ocr.github.io/tessdoc/Compiling.html).

C++17 support is required for building.

## Running Tesseract

Basic **[command line usage](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html)**:

    tesseract imagename outputbase [-l lang] [--oem ocrenginemode] [--psm pagesegmode] [configfiles...]

For more information about the various command line options use `tesseract --help` or `man tesseract`.

Examples can be found in the [documentation](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html#simplest-invocation-to-ocr-an-image).

## For developers

Developers can use `libtesseract` [C](https://github.com/tesseract-ocr/tesseract/blob/master/include/tesseract/capi.h) or
[C++](https://github.com/tesseract-ocr/tesseract/blob/master/include/tesseract/baseapi.h) API to build their own application.
If you need bindings to `libtesseract` for other programming languages, please see the
[wrapper](https://tesseract-ocr.github.io/tessdoc/AddOns.html#tesseract-wrappers) section in the AddOns documentation.

Documentation of Tesseract generated from source code by doxygen can be found on [tesseract-ocr.github.io](https://tesseract-ocr.github.io/).

## Support

Before you submit an issue, please review **[the guidelines for this repository](https://github.com/tesseract-ocr/tesseract/blob/master/CONTRIBUTING.md)**.

For support, first read the [documentation](https://tesseract-ocr.github.io/tessdoc/),
particularly the [FAQ](https://tesseract-ocr.github.io/tessdoc/FAQ.html) to see if your problem is addressed there.
If not, search the [Tesseract user forum](https://groups.google.com/g/tesseract-ocr), the [Tesseract developer forum](https://groups.google.com/g/tesseract-dev) and [past issues](https://github.com/tesseract-ocr/tesseract/issues), and if you still can't find what you need, ask for support in the mailing-lists.

Mailing-lists:
* [tesseract-ocr](https://groups.google.com/g/tesseract-ocr) - For tesseract users.
* [tesseract-dev](https://groups.google.com/g/tesseract-dev) - For tesseract developers.

Please report an issue only for a **bug**, not for asking questions.

## License

    The code in this repository is licensed under the Apache License, Version 2.0 (the ""License"");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an ""AS IS"" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

**NOTE**: This software depends on other packages that may be licensed under different open source licenses.

Tesseract uses [Leptonica library](http://leptonica.com/) which essentially
uses a [BSD 2-clause license](http://leptonica.com/about-the-license.html).

## Dependencies

Tesseract uses [Leptonica library](https://github.com/DanBloomberg/leptonica)
for opening input images (e.g. not documents like pdf).
It is suggested to use leptonica with built-in support for [zlib](https://zlib.net),
[png](https://sourceforge.net/projects/libpng) and
[tiff](http://www.simplesystems.org/libtiff) (for multipage tiff).

## Latest Version of README

For the latest online version of the README.md see:

https://github.com/tesseract-ocr/tesseract/blob/master/README.md
"
102,botallen/repository.botallen,Python,"<h2 align=""center"">
  <br>
  <a href=""https://github.com/botallen/repository.botallen""><img src=""repository.botallen/icon2.png"" height=""60"" width=""60""></a>
  <br>
  BotAllen
  <br>
</h2>

<h4 align=""center"">BotAllen Kodi Repository</h4>

<p align=""center"">

  <!-- Release -->
  <a href=""https://github.com/botallen/repository.botallen/releases/latest"">
    <img src=""https://img.shields.io/github/v/release/botallen/repository.botallen?style=for-the-badge"">
  </a>
  
  <!-- Discord -->
  <a href=""https://botallen.com/discord"">
    <img src=""https://img.shields.io/discord/701304820530937954?color=%237289da&label=discord&style=for-the-badge&logo=discord"">
  </a>
  
  <!-- Downloads -->
  <a href=""https://github.com/botallen/repository.botallen/releases/latest"">
    <img src=""https://img.shields.io/github/downloads/botallen/repository.botallen/total?style=for-the-badge&logo=kodi&color=17B2E7"">
  </a>
  
 </p>
 <p align=""center"">
  
  <!-- License -->
  <a href=""https://github.com/botallen/repository.botallen/blob/master/LICENSE"">
    <img src=""https://img.shields.io/github/license/botallen/repository.botallen?style=flat-square"">
  </a>
  
  <!-- Open Issues -->
  <a href=""https://github.com/botallen/repository.botallen/issues"">
    <img src=""https://img.shields.io/github/issues/botallen/repository.botallen?style=flat-square"">
  </a>
  
  <!-- Last Commit -->
  <a href=""https://github.com/botallen/repository.botallen/commit/master"">
    <img src=""https://img.shields.io/github/last-commit/botallen/repository.botallen?style=flat-square"">
  </a>
  
 </p>

<br>

<h2 align=""center"">Add-ons</h2>

<p align=""center"">
  
<span style=""display: inline-block;"">
  <a href=""https://github.com/botallen/plugin.video.botallen.hotstar"">
    <img src=""https://raw.githubusercontent.com/botallen/plugin.video.botallen.hotstar/main/resources/icon.jpg"" width=""100"" height=""100"">
  </a>
</span>

<span style=""display: inline-block;"">
  <a href=""https://github.com/botallen/plugin.video.jiotv"">
    <img src=""https://raw.githubusercontent.com/botallen/plugin.video.jiotv/main/resources/icon.png"" width=""100"" height=""100"">
  </a>
</span>
</p>

## Install

<img align=""right"" src=""media/install.gif"" height=250>

- Add this file source in file manager : https://kodi.botallen.com
- Click on Install From zip file
- Select repository.botallen-2.0.0.zip
- Done

<br/>
<br/>
<br/>
<br/>

## Download

[**Download**](https://github.com/botallen/repository.botallen/releases/download/v2.0.0/repository.botallen-2.0.0.zip) the `.zip` file.
<br/>
<br/>

## Support

<a href=""https://botallen.com/#donate"" target=""_blank"" >https://botallen.com/#donate</a>
"
103,sonatype-nexus-community/nexus-repository-composer,Java,"<!--

    Sonatype Nexus (TM) Open Source Version
    Copyright (c) 2018-present Sonatype, Inc.
    All rights reserved. Includes the third-party code listed at http://links.sonatype.com/products/nexus/oss/attributions.

    This program and the accompanying materials are made available under the terms of the Eclipse Public License Version 1.0,
    which accompanies this distribution and is available at http://www.eclipse.org/legal/epl-v10.html.

    Sonatype Nexus (TM) Professional Version is available from Sonatype, Inc. ""Sonatype"" and ""Sonatype Nexus"" are trademarks
    of Sonatype, Inc. Apache Maven is a trademark of the Apache Software Foundation. M2eclipse is a trademark of the
    Eclipse Foundation. All other trademarks are the property of their respective owners.

-->
# Nexus Repository Composer Format

[![Maven Central](https://img.shields.io/maven-central/v/org.sonatype.nexus.plugins/nexus-repository-composer.svg?label=Maven%20Central)](https://search.maven.org/search?q=g:%22org.sonatype.nexus.plugins%22%20AND%20a:%22nexus-repository-composer%22) 
[![CircleCI](https://circleci.com/gh/sonatype-nexus-community/nexus-repository-composer.svg?style=shield)](https://circleci.com/gh/sonatype-nexus-community/nexus-repository-composer)
[![Join the chat at https://gitter.im/sonatype/nexus-developers](https://badges.gitter.im/sonatype/nexus-developers.svg)](https://gitter.im/sonatype/nexus-developers?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![DepShield Badge](https://depshield.sonatype.org/badges/sonatype-nexus-community/nexus-repository-composer/depshield.svg)](https://depshield.github.io)

# Table Of Contents
* [Developing](#developing)
   * [Requirements](#requirements)
   * [Building](#building)
* [Using Composer with Nexus Repository Manger 3](#using-composer-with-nexus-repository-manager-3)
* [Installing the plugin](#installing-the-plugin)
   * [Easiest Install](#easiest-install)
   * [Temporary Install](#temporary-install)
   * [(more) Permanent Install](#more-permanent-install)
   * [(most) Permament Install](#most-permanent-install)
* [The Fine Print](#the-fine-print)
* [Getting Help](#getting-help)
* [Composer Plugin](#composer-plugin)

## Developing

### Requirements

* [Apache Maven 3.3.3+](https://maven.apache.org/install.html)
* [Java 8](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)
* Network access to https://repository.sonatype.org/content/groups/sonatype-public-grid

Also, there is a good amount of information available at [Bundle Development](https://help.sonatype.com/display/NXRM3/Bundle+Development)

### Building

To build the project and generate the bundle use Maven

    mvn clean package

If everything checks out, the bundle for Composer should be available in the `target` folder

#### Build with Docker

    docker build -t nexus-repository-composer .

#### Run as a Docker container

    docker run -d -p 8081:8081 --name nexus-repository-composer nexus-repository-composer 

For further information like how to persist volumes check out [the GitHub repo for our official image](https://github.com/sonatype/docker-nexus3).

The application will now be available from your browser at http://localhost:8081

* As of Nexus Repository Manager Version 3.17, the default admin password is randomly generated.
  If running in a Docker container, you will need to view the generated password file 
  (/nexus-data/admin.password) in order to login to Nexus. The command below will open a bash shell 
  in the container named `nexus-repository-composer`:

      docker exec -it nexus-repository-composer /bin/bash
      $ cat /nexus-data/admin.password 
      
  Once logged into the application UI as `admin` using the generated password, you should also 
  turn on ""Enable anonymous access"" when prompted by the setup wizard.     

## Using Composer With Nexus Repository Manager 3

[We have detailed instructions on how to get started here!](docs/COMPOSER_USER_DOCUMENTATION.md)

## Installing the plugin

There are a range of options for installing the Composer plugin. You'll need to build it first, and
then install the plugin with the options shown below:

### Easiest Install

Thanks to some upstream work in Nexus Repository (versions newer than 3.15), it's become a LOT easier to install a plugin. To install this format plugin, you can either build locally or download from The Central Repository:

#### Option 1: Build a *.kar file locally from the GitHub Repo
* Clone this repo and `cd` to the cloned directory location
* Build the plugin with `mvn clean package -PbuildKar`
* There should now be a `nexus-repository-composer-<version>-bundle.kar` file in your `<cloned_repo>/target` directory 

#### Option 2: Download a *.kar file from The Central Repository 
* Download `nexus-repository-composer-<version>-bundle.kar` from [The Central Repository](https://search.maven.org/artifact/org.sonatype.nexus.plugins/nexus-repository-composer)

Once you've completed Option 1 or 2, copy the `nexus-repository-composer-<version>-bundle.kar` file into the `<nexus_dir>/deploy` folder for your Nexus Repository installation.

Restart Nexus Repo, or go ahead and start it if it wasn't running to begin with.

You should see the new repository types (e.g. `composer (hosted, proxy, group)`) in the available Repository Recipes to use, if all has gone according to plan :)

### Temporary Install

Installations done via the Karaf console will be wiped out with every restart of Nexus Repository. This is a
good installation path if you are just testing or doing development on the plugin.

* Enable the NXRM console: edit `<nexus_dir>/bin/nexus.vmoptions` and change `karaf.startLocalConsole`  to `true`.

  More details here: [Bundle Development](https://help.sonatype.com/display/NXRM3/Bundle+Development+Overview)

* Run NXRM's console:
  ```
  # sudo su - nexus
  $ cd <nexus_dir>/bin
  $ ./nexus run
  > bundle:install file:///tmp/nexus-repository-composer-0.0.8.jar
  > bundle:list
  ```
  (look for org.sonatype.nexus.plugins:nexus-repository-composer ID, should be the last one)
  ```
  > bundle:start <org.sonatype.nexus.plugins:nexus-repository-composer ID>
  ```

### (more) Permanent Install

For more permanent installs of the nexus-repository-composer plugin, follow these instructions:

* Copy the bundle (nexus-repository-composer-0.0.8.jar) into <nexus_dir>/deploy

This will cause the plugin to be loaded with each restart of Nexus Repository. As well, this folder is monitored
by Nexus Repository and the plugin should load within 60 seconds of being copied there if Nexus Repository
is running. You will still need to start the bundle using the karaf commands mentioned in the temporary install.

### (most) Permanent Install

If you are trying to use the Composer plugin permanently, it likely makes more sense to do the following:

* Copy the bundle into `<nexus_dir>/system/org/sonatype/nexus/plugins/nexus-repository-composer/0.0.8/nexus-repository-composer-0.0.8.jar`
* Make the following additions marked with + to `<nexus_dir>/system/org/sonatype/nexus/assemblies/nexus-core-feature/3.x.y/nexus-core-feature-3.x.y-features.xml`

   ```
         <feature prerequisite=""false"" dependency=""false"">wrap</feature>
   +     <feature prerequisite=""false"" dependency=""false"">nexus-repository-composer</feature>
   ```
   to the `<feature name=""nexus-core-feature"" description=""org.sonatype.nexus.assemblies:nexus-core-feature"" version=""3.x.y.xy"">` section below the last <feature prerequisite...> (above is an example, the exact last one may vary).

   And    
   ```
   + <feature name=""nexus-repository-composer"" description=""org.sonatype.nexus.plugins:nexus-repository-composer"" version=""0.0.8"">
   +     <details>org.sonatype.nexus.plugins:nexus-repository-composer</details>
   +     <bundle>mvn:org.sonatype.nexus.plugins/nexus-repository-composer/0.0.8</bundle>
   + </feature>
    </features>
   ```
   as the last feature.

This will cause the plugin to be loaded and started with each startup of Nexus Repository.

## The Fine Print

It is worth noting that this is **NOT SUPPORTED** by Sonatype, and is a contribution of ours
to the open source community (read: you!)

Remember:

* Use this contribution at the risk tolerance that you have
* Do NOT file Sonatype support tickets related to Composer support in regard to this plugin
* DO file issues here on GitHub, so that the community can pitch in

Phew, that was easier than I thought. Last but not least of all:

Have fun creating and using this plugin and the Nexus platform, we are glad to have you here!

## Getting help

Looking to contribute to our code but need some help? There's a few ways to get information:

* Chat with us on [Gitter](https://gitter.im/sonatype/nexus-developers)
* Check out the [Nexus3](http://stackoverflow.com/questions/tagged/nexus3) tag on Stack Overflow
* Check out the [Nexus Repository User List](https://groups.google.com/a/glists.sonatype.com/forum/?hl=en#!forum/nexus-users)

## Composer Plugin
The composer plugin `elendev/nexus-composer-push` (https://github.com/Elendev/nexus-composer-push) provide a composer command to push to a Nexus Repository using this plugin.
"
104,recca0120/laravel-repository,PHP,"# Repository Design Pattern for Laravel

[![StyleCI](https://styleci.io/repos/60332194/shield?style=flat)](https://styleci.io/repos/60332194)
[![Build Status](https://travis-ci.org/recca0120/laravel-repository.svg)](https://travis-ci.org/recca0120/laravel-repository)
[![Total Downloads](https://poser.pugx.org/recca0120/repository/d/total.svg)](https://packagist.org/packages/recca0120/repository)
[![Latest Stable Version](https://poser.pugx.org/recca0120/repository/v/stable.svg)](https://packagist.org/packages/recca0120/repository)
[![Latest Unstable Version](https://poser.pugx.org/recca0120/repository/v/unstable.svg)](https://packagist.org/packages/recca0120/repository)
[![License](https://poser.pugx.org/recca0120/repository/license.svg)](https://packagist.org/packages/recca0120/repository)
[![Monthly Downloads](https://poser.pugx.org/recca0120/repository/d/monthly)](https://packagist.org/packages/recca0120/repository)
[![Daily Downloads](https://poser.pugx.org/recca0120/repository/d/daily)](https://packagist.org/packages/recca0120/repository)
[![Scrutinizer Code Quality](https://scrutinizer-ci.com/g/recca0120/laravel-repository/badges/quality-score.png?b=master)](https://scrutinizer-ci.com/g/recca0120/laravel-repository/?branch=master)
[![Code Coverage](https://scrutinizer-ci.com/g/recca0120/laravel-repository/badges/coverage.png?b=master)](https://scrutinizer-ci.com/g/recca0120/laravel-repository/?branch=master)

## Install

To get the latest version of Laravel Exceptions, simply require the project using [Composer](https://getcomposer.org):

```bash
composer require recca0120/repository
```

Instead, you may of course manually update your require block and run `composer update` if you so choose:

```json
{
    ""require"": {
        ""recca0120/repository"": ""~2.0.0""
    }
}
```

## Methods

### Recca0120\Repository\EloquentRepository

- find($id, $columns = ['*']);
- findMany($ids, $columns = ['*']);
- findOrFail($id, $columns = ['*']);
- findOrNew($id, $columns = ['*']);
- firstOrNew(array $attributes, array $values = []);
- firstOrCreate(array $attributes, array $values = []);
- updateOrCreate(array $attributes, array $values = []);
- firstOrFail($criteria = [], $columns = ['*']);
- create($attributes);
- forceCreate($attributes);
- update($id, $attributes);
- forceUpdate($id, $attributes);
- delete($id);
- forceDelete($id);
- newInstance($attributes = [], $exists = false);
- get($criteria = [], $columns = ['*']);
- chunk($criteria, $count, callable $callback);
- each($criteria, callable $callback, $count = 1000);
- first($criteria = [], $columns = ['*']);
- paginate($criteria = [], $perPage = null, $columns = ['*'], $pageName = 'page', $page = null);
- simplePaginate($criteria = [], $perPage = null, $columns = ['*'], $pageName = 'page', $page = null);
- count($criteria = [], $columns = '*');
- min($criteria, $column);
- max($criteria, $column);
- sum($criteria, $column);
- avg($criteria, $column);
- average($criteria, $column);
- matching($criteria);
- getQuery($criteria = []);
- getModel();
- newQuery();

### Recca0120\Repository\Criteria

- static create()
- static expr($value)
- static raw($value)
- select($columns = ['*'])
- selectRaw($expression, array $bindings = [])
- selectSub($query, $as)
- addSelect($column)
- distinct()
- from($table)
- join($table, $first, $operator = null, $second = null, $type = 'inner', $where = false)
- joinWhere($table, $first, $operator, $second, $type = 'inner')
- leftJoin($table, $first, $operator = null, $second = null)
- leftJoinWhere($table, $first, $operator, $second)
- rightJoin($table, $first, $operator = null, $second = null)
- rightJoinWhere($table, $first, $operator, $second)
- crossJoin($table, $first = null, $operator = null, $second = null)
- mergeWheres($wheres, $bindings)
- tap($callback)
- where($column, $operator = null, $value = null, $boolean = 'and')
- orWhere($column, $operator = null, $value = null)
- whereColumn($first, $operator = null, $second = null, $boolean = 'and')
- orWhereColumn($first, $operator = null, $second = null)
- whereRaw($sql, $bindings = [], $boolean = 'and')
- orWhereRaw($sql, array $bindings = [])
- whereIn($column, $values, $boolean = 'and', $not = false)
- orWhereIn($column, $values)
- whereNotIn($column, $values, $boolean = 'and')
- orWhereNotIn($column, $values)
- whereNull($column, $boolean = 'and', $not = false)
- orWhereNull($column)
- whereNotNull($column, $boolean = 'and')
- whereBetween($column, array $values, $boolean = 'and', $not = false)
- orWhereBetween($column, array $values)
- whereNotBetween($column, array $values, $boolean = 'and')
- orWhereNotBetween($column, array $values)
- orWhereNotNull($column)
- whereDate($column, $operator, $value = null, $boolean = 'and')
- orWhereDate($column, $operator, $value)
- whereTime($column, $operator, $value, $boolean = 'and')
- orWhereTime($column, $operator, $value)
- whereDay($column, $operator, $value = null, $boolean = 'and')
- whereMonth($column, $operator, $value = null, $boolean = 'and')
- whereYear($column, $operator, $value = null, $boolean = 'and')
- whereNested(Closure $callback, $boolean = 'and')
- addNestedWhereQuery($query, $boolean = 'and')
- whereExists(Closure $callback, $boolean = 'and', $not = false)
- orWhereExists(Closure $callback, $not = false)
- whereNotExists(Closure $callback, $boolean = 'and')
- orWhereNotExists(Closure $callback)
- addWhereExistsQuery(Builder $query, $boolean = 'and', $not = false)
- dynamicWhere($method, $parameters)
- groupBy()
- having($column, $operator = null, $value = null, $boolean = 'and')
- orHaving($column, $operator = null, $value = null)
- havingRaw($sql, array $bindings = [], $boolean = 'and')
- orHavingRaw($sql, array $bindings = [])
- orderBy($column, $direction = 'asc')
- orderByDesc($column)
- latest($column = 'created_at')
- oldest($column = 'created_at')
- inRandomOrder($seed = '')
- orderByRaw($sql, $bindings = [])
- skip($value)
- offset($value)
- take($value)
- limit($value)
- forPage($page, $perPage = 15)
- forPageAfterId($perPage = 15, $lastId = 0, $column = 'id')
- union($query, $all = false)
- unionAll($query)
- lock($value = true)
- lockForUpdate()
- sharedLock()
- when($value, $callback, $default = null)
- unless($value, $callback, $default = null)
- whereKey($id)
- whereKeyNot($id)
- with($relations)
- without($relations)
- setQuery($query)
- setModel(Model $model)
- has($relation, $operator = '>=', $count = 1, $boolean = 'and', Closure $callback = null)
- orHas($relation, $operator = '>=', $count = 1)
- doesntHave($relation, $boolean = 'and', Closure $callback = null)
- whereHas($relation, Closure $callback = null, $operator = '>=', $count = 1)
- orWhereHas($relation, Closure $callback = null, $operator = '>=', $count = 1)
- whereDoesntHave($relation, Closure $callback = null)
- withCount($relations)
- mergeConstraintsFrom(Builder $from)
- withTrashed()
- withoutTrashed()
- onlyTrashed()

## Usage

### Eloquent

#### Create a Model

Create your model normally, but it is important to define the attributes that can be filled from the input form data.

```php

namespace App;

use Illuminate\Database\Eloquent\Model;

class Post extends Model
{
    protected $fillable = [
        'title',
        'author',
     ];
}
```

### Create a Contract

```php

namespace App\Repositories\Contracts;

interface PostRepository
{

}
```

#### Create a Repository

```php

namespace App\Repositories;

use App\Repositories\Contracts\PostRepository as PostRepositoryContract;
use App\Post;
use Recca0120\Repository\EloquentRepository;

class PostRepository extends EloquentRepository implements PostRepositoryContract
{
    public function __construct(Post $model)
    {
        $this->model = $model;
    }
}
```

### Bind

```php

namespace App\Providers;

use Illuminate\Support\ServiceProvider;
use App\Repositories\Contracts\PostRepository as PostRepositoryContract;

class AppServiceProvider extends ServiceProvider
{
    public function register()
    {
        $this->app->singleton(PostRepositoryContract::class, PostRepository::class);
    }
}

```

#### Controller

```php

namespace App\Http\Controllers;

use App\Repositories\Contracts\PostRepository;

class PostsController extends Controller
{
    protected $repository;

    public function __construct(PostRepository $repository)
    {
        $this->repository = $repository;
    }
}
```

## Methods

Find all results in Repository

```php
$posts = $this->repository->get();
```

Find all results in Repository with pagination

```php
$posts = $this->repository->paginate();
```

Count results in Repository

```php
$posts = $this->repository->count();
```

Create new entry in Repository

```php
$post = $this->repository->create(request()->all());
```

Update entry in Repository

```php
$post = $this->repository->update($id, request()->all());
```

Delete entry in Repository

```php
$this->repository->delete($id);
```

New instance

```php
$post = $this->repository->newInstance([
    'author' => 'author'
]);
```

Return Model With Conditions

```
$model = $this->repository->matching(Criteria::create()->where('title', '=', 'title'));
```

Find result by id

```php
$post = $this->repository->find($id);
```

### Find by conditions

#### Using the Criteria

Criteria is support all of Eloquent functions

##### Single Criteria

```php

use Recca0120\Repository\Criteria;

$criteria = Criteria::create()
    ->select('*')
    ->where('author', '=', 'author')
    ->orWhere('title', '=', 'title')
    ->orderBy('author', 'asc');

$this->repository->get($criteria);
$this->repository->paginate($criteria);
```

#### Multiple Criteria

```php

use Recca0120\Repository\Criteria;

$criteria = [];

$criteria[] = Criteria::create()
    ->orderBy('author', 'asc');

$criteria[] = Criteria::create()
    ->where('author', '=', 'author')
    ->orWhere('title', '=', 'title');

$this->repository->get($criteria);
// $this->repository->paginate($criteria);
```

##### With

```php

use Recca0120\Repository\Criteria;

$criteria = Criteria::create()
    ->with('author', function($criteria) {
        $criteria->where('author', 'author');
    });

$this->repository->get($criteria);
// $this->repository->paginate($criteria);
```

#### Join

```php

use Recca0120\Repository\Criteria;

$criteria = Criteria::create()
    ->join('author', function ($criteria) {
        $criteria->on('posts.author_id', '=', 'author.id');
    });

$this->repository->get($criteria);
// $this->repository->paginate($criteria);
```

#### Expression

```php

use Recca0120\Repository\Criteria;

$criteria = Criteria::create()
    ->where('created_at', '<=', Criteria::expr('NOW()'));

$this->repository->get($criteria);
// $this->repository->paginate($criteria);
```

#### Custom Criteria

```php

use Recca0120\Repository\Criteria;

class CustomCriteria extends Criteria
{
    public function __construct($id)
    {
        $this->where('id', '=', $id);
    }
}

$this->repository->get((new CustomCriteria(1))->where('autor', 'autor'));
```

## ToDo
- Cache
"
105,develar/settings-repository,Kotlin,"# Installation

Available for all IntelliJ Platform based products (build number greater than 139.69/140.2). Settings | Plugins | Browse repositories -> type in Settings Repository.

Don't try to install plugin from disk — otherwise you have to be aware of compatibility.

# Configuration

Use File -> Settings Repository… to configure.

Specify URL of upstream Git repository. File URL is supported, you will be prompted to init repository if specified path is not exists or repository is not created.
[GitHub](https://www.github.com) could be used to store settings.

Synchronization is performed automatically:
* after successful completion of ""Update Project"" or ""Push"" actions,
* on application exit or project close.

Also you can do sync using ""VCS -> Sync Settings"". The idea is do not disturb you. If you invoke such actions, so, you are ready to solve possible problems.

## Authentication
On first sync you will be prompted to specify username/password. In case of GitHub strongly recommended to use a [personal access token](https://help.github.com/articles/creating-an-access-token-for-command-line-use) (leave password empty if you use token instead of username). Remember when generating the token to  check `repo` scope permission. Bitbucket — [app password](https://bitbucket.org/account/admin/app-passwords).

If you still want to use username/password instead of access token or your Git hosting provider doesn't support it, recommended to configure [git credentials helper](https://help.github.com/articles/caching-your-github-password-in-git).

OS X Keychain is supported. It means that your credentials could be shared between all IntelliJ Platform based products (you will be prompted to grant access if origin application differs from requestor application (credentials were saved in IntelliJ IDEA, but requested from WebStorm).

## How to report issues
Use [JetBrains YouTrack](https://youtrack.jetbrains.com/issues?q=%23%7BSettings+Repository%7D) — project IntelliJ IDEA, subsystem Settings Repository ([issue template](https://youtrack.jetbrains.com/newIssue?project=IDEA&clearDraft=true&c=Subsystem+Settings+Repository)).

## Sources
**Plugin is part of IntelliJ IDEA Community Edition and bundled with IDEA 15, WebStorm 11 and PhpStorm 9.5. Please see https://github.com/JetBrains/intellij-community/tree/master/plugins/settings-repository This repository is archived.**
"
106,caffeinated/repository,PHP,"# This package has been abandoned and is no longer maintained.

# Caffeinated Repository
[![Source](http://img.shields.io/badge/source-caffeinated/repository-blue.svg?style=flat-square)](https://github.com/caffeinated/repository)
[![License](http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat-square)](https://tldrlegal.com/license/mit-license)

## Getting Started

### Introduction
The Caffeinated Repository package allows the means to implement a standard boilerplate repository interface. This covers the standard Eloquent methods in a non-static, non-facade driven way right out of the box. Fear not though Batman! The Caffeinated Repository package does not limit you in any way when it comes to customizing (e.g overriding) the provided interface or adding your own methods.

## Installing Caffeinated Repository
It is recommended that you install the package using Composer.

```
composer require caffeinated/repository
```

This package is compliant with [PSR-1](https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-1-basic-coding-standard.md), [PSR-2](https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-2-coding-style-guide.md), and [PSR-4](https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-4-autoloader.md). If you find any compliance oversights, please send a patch via pull request.

# Using Repositories

### Create a Model
Create your model like you normally would. We'll be wrapping our repository around our model to access and query the database for the information we need.

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Model

class Book extends Model
{
    //
}
```

### Create a Repository
Create a new Repository class - usually these classes are simply stored within a `Repositories` directory. There are a few requirements for each repository instance:

- Repository classes must extend the Caffeinated EloquentRepository class.
- Repository classes must specify a public property pointing to the model.
- Repository classes must specify an array of cache tags. These tags are used by the package to handle automatic cache busting when relevent values change within the database.

```php
<?php

namespace App\Repositories;

use App\Models\Book;
use Caffeinated\Repository\Repositories\EloquentRepository;

class BookRepository extends EloquentRepository
{
    /**
     * @var Model
     */
    public $model = Book::class;

    /**
     * @var array
     */
    public $tag = ['book'];
}
```

### Injecting a Repository
Once you've built and configured your repository instance, you may inject the class within your classes where needed:

```php
<?php

namespace App\Http\Controllers;

use App\Repositories\BookRepository;

class BookController extends Controller
{
    /**
     * @var BookRepository
     */
    protected $book;

    /**
     * Create a new BookController instance.
     *
     * @param  BookRepository  $book
     */
    public function __construct(BookRepository $book)
    {
        $this->book = $book;
    }

    /**
     * Display a listing of all books.
     *
     * @return Response
     */
    public function index()
    {
        $books = $this->book->findAll();

        return view('books.index', compact('books'));
    }
}
```
"
107,archlinuxcn/mirrorlist-repo,Python,"### Arch Linux CN Community repo mirrors list

Here is a list of public mirrors of our [community repository](https://github.com/archlinuxcn/repo).

If you interested in making a mirror of our repository, please contact us at repo@archlinuxcn.org.

```ini
## 重庆大学 (重庆) (ipv4, ipv6, https)
[archlinuxcn]
Server = https://mirrors.cqu.edu.cn/archlinuxcn/$arch
```

```ini
## 中国科学技术大学 (安徽合肥) (ipv4, ipv6, http, https)
[archlinuxcn]
Server = https://mirrors.ustc.edu.cn/archlinuxcn/$arch
```

```ini
## 清华大学 (北京) (ipv4, ipv6, http, https)
[archlinuxcn]
Server = https://mirrors.tuna.tsinghua.edu.cn/archlinuxcn/$arch
```

```ini
## Our main server (Amsterdam, the Netherlands) (ipv4, ipv6, http, https)
[archlinuxcn]
Server = https://repo.archlinuxcn.org/$arch
```

```ini
## xTom (Hong Kong server) (Hong Kong) (ipv4, ipv6, http, https)
## Added: 2017-09-18
## xTom Hong Kong Mirror
[archlinuxcn]
Server = https://mirror.xtom.com.hk/archlinuxcn/$arch
```

```ini
## xTom (US server) (Fremont, CA, United States) (ipv4, ipv6, http, https)
## Added: 2019-02-19
## xTom US Mirror
[archlinuxcn]
Server = https://mirror.xtom.com/archlinuxcn/$arch
```

```ini
## xTom (Netherlands server) (Amsterdam, the Netherlands) (ipv4, ipv6, http, https)
## Added: 2019-09-07
## xTom Netherlands Mirror
[archlinuxcn]
Server = https://mirror.xtom.nl/archlinuxcn/$arch
```

```ini
## xTom (Germany server) (Dueseeldorf, Germany) (ipv4, ipv6, http, https)
## Added: 2021-05-07
## xTom Germany Mirror
[archlinuxcn]
Server = https://mirror.xtom.de/archlinuxcn/$arch
```

```ini
## Open Computing Facility, UC Berkeley (Berkeley, CA, United States) (ipv4, ipv6, http, https)
## Added: 2019-02-19
[archlinuxcn]
Server = https://mirrors.ocf.berkeley.edu/archlinuxcn/$arch
```

```ini
## 北京外国语大学 (北京) (ipv4, ipv6, http, https)
## Added: 2020-05-18
[archlinuxcn]
Server = https://mirrors.bfsu.edu.cn/archlinuxcn/$arch
```

```ini
## 哈尔滨工业大学 (黑龙江哈尔滨) (ipv4, ipv6, http, https)
## Added: 2021-01-09
[archlinuxcn]
Server = https://mirrors.hit.edu.cn/archlinuxcn/$arch
```

```ini
## 浙江大学 (浙江杭州) (ipv4, http, https)
## Added: 2017-06-05
[archlinuxcn]
Server = https://mirrors.zju.edu.cn/archlinuxcn/$arch
```

```ini
## 上海科技大学 (上海) (ipv4, https)
## Added: 2016-04-07
[archlinuxcn]
Server = https://mirrors-wan.geekpie.club/archlinuxcn/$arch
```

```ini
## 网易 (浙江杭州) (ipv4, http, https)
[archlinuxcn]
Server = https://mirrors.163.com/archlinux-cn/$arch
```

```ini
## SJTUG 软件源镜像服务 (上海) (ipv4, https)
## Added: 2018-05-21
[archlinuxcn]
Server = https://mirrors.sjtug.sjtu.edu.cn/archlinux-cn/$arch
```

```ini
## 莞工 GNU/Linux 协会 开源软件镜像站 (广东东莞) (ipv4, https)
## Added: 2018-11-03
[archlinuxcn]
Server = https://mirrors.dgut.edu.cn/archlinuxcn/$arch
```

```ini
## 腾讯云 (上海) (ipv4, https)
## Added: 2018-11-23
[archlinuxcn]
Server = https://mirrors.cloud.tencent.com/archlinuxcn/$arch
```

```ini
## 阿里云 (Global CDN) (ipv4, http, https)
## Added: 2020-07-03
[archlinuxcn]
Server = https://mirrors.aliyun.com/archlinuxcn/$arch
```

```ini
## OpenTUNA (China CDN) (ipv4, https)
[archlinuxcn]
Server = https://opentuna.cn/archlinuxcn/$arch
```

```ini
## 华为云 (China CDN) (ipv4, http, https)
## Added: 2020-10-31
[archlinuxcn]
Server = https://repo.huaweicloud.com/archlinuxcn/$arch
```

"
108,sonatype-nexus-community/nexus-repository-apt,Java,"# Nexus repository APT plugin

[![Build Status](https://travis-ci.org/sonatype-nexus-community/nexus-repository-apt.svg?branch=master)](https://travis-ci.org/sonatype-nexus-community/nexus-repository-apt) [![DepShield Badge](https://depshield.sonatype.org/badges/sonatype-nexus-community/nexus-repository-apt/depshield.svg)](https://depshield.github.io)

> **Huzzah!** APT is now part of Nexus Repository Manager. Version 3.17.0 includes the APT plugin by default. The plugin source code is now in [nexus-public](https://github.com/sonatype/nexus-public) in [nexus-repository-apt](https://github.com/sonatype/nexus-public/tree/master/plugins/nexus-repository-apt)

> **Filing issues:** If using 3.17.0 or later please file any issues at https://issues.sonatype.org/.

> **Upgrading to 3.17.0:** If you are an existing APT user upgrading to 3.17 you will not be able to install the
 community plugin. No other changes are required and your existing data will remain intact.

>There are some differences from the community version of the plugin. First of all, we have added component records for 
the .deb files which adds support for features such as *Search*, *Cleanup Policies*, *Tagging (PRO only)* and *Moving 
between repositories (PRO only)*. We've also added support for *Restore Metadata Task*, *API for Repository creation 
via Groovy* and we've written *help documentation*. The ""retain N versions"" feature has been removed as this conflicts 
with Cleanup Policies and future work we are doing in that space.

Compatibility Matrix:

| Plugin Version                    | Nexus Repository Version |
|-----------------------------------|--------------------------|
| v1.0.2                            | <3.9.0                   |
| v1.0.5                            | 3.9.0                    |
| v1.0.7                            | 3.11.0                   |
| v1.0.8                            | 3.13.0                   |
| v1.0.9                            | 3.14.0                   |
| v1.0.10                           | 3.15.2                   |
| In product                        | 3.17.0                   |

### Build
* Clone the project:

  `git clone https://github.com/sonatype-nexus-community/nexus-repository-apt`
* Build the plugin:

  ```
  cd nexus-repository-apt
  mvn
  ```
### Build with docker and create an image based on nexus repository 3

``` docker build -t nexus-repository-apt:3.15.2 .```

### Run a docker container from that image

``` docker run -d -p 8081:8081 --name nexus-repo nexus-repository-apt:3.15.2 ```

For further information like how to persist volumes check out [the GitHub Repo for the official Nexus Repository 3 Docker image](https://github.com/sonatype/docker-nexus3).

The application will now be available from your browser at http://localhost:8081

### Install
* Stop Nexus:

  `service nexus stop`

  or

  ```
  cd <nexus_dir>/bin
  ./nexus stop
  ```

* Copy the bundle into `<nexus_dir>/system/net/staticsnow/nexus-repository-apt/1.0.10/nexus-repository-apt-1.0.10.jar`
* Make the following additions marked with + to `<nexus_dir>/system/org/sonatype/nexus/assemblies/nexus-core-feature/3.x.y/nexus-core-feature-3.x.y-features.xml`
   ```
         <feature version=""x.y.z"" prerequisite=""false"" dependency=""false"">nexus-repository-maven</feature>
   +     <feature version=""1.0.10"" prerequisite=""false"" dependency=""false"">nexus-repository-apt</feature>
     </feature>
   ```
   And
   ```
   + <feature name=""nexus-repository-apt"" description=""net.staticsnow:nexus-repository-apt"" version=""1.0.10"">
   +     <details>net.staticsnow:nexus-repository-apt</details>
   +     <bundle>mvn:net.staticsnow/nexus-repository-apt/1.0.10</bundle>
   +     <bundle>mvn:org.apache.commons/commons-compress/1.18</bundle>
   +     <bundle>mvn:org.tukaani/xz/1.8</bundle>
   + </feature>
    </features>
   ```
This will cause the plugin to be loaded and started with each startup of Nexus Repository.

### Manually upload a package to a new created repo:
`curl -u user:pass -X POST -H ""Content-Type: multipart/form-data"" --data-binary ""@package.deb""  http://nexus_url:8081/repository/repo_name/`

### Create a snapshot of the current package lists for the repo that can be pulled from:
`curl -u user:pass -X MKCOL http://nexus_url:8081/repository/repo_name/snapshots/$SNAPSHOT_ID`

### Create gpg key required for signing apt-packages
See https://help.github.com/articles/generating-a-new-gpg-key/

To sign packages use; `gpg --export-secret-key --armor <Name or ID>` and passphrase in the hosted repository configuration.

The private armored key should look like this:
```
-----BEGIN PGP PRIVATE KEY BLOCK-----

...Base64 key...
-----END PGP PRIVATE KEY BLOCK-----
```

## The Fine Print

It is worth noting that this is **NOT SUPPORTED** by Sonatype, and is a contribution of Mike Poindexter's
plus us to the open source community (read: you!)

Remember:

* Use this contribution at the risk tolerance that you have
* Do **NOT** file Sonatype support tickets related to APT support
* **DO** file issues here on GitHub, so that the community can pitch in

Phew, that was easier than I thought. Last but not least of all:

Have fun building and using this plugin on the Nexus platform, we are glad to have you here!

## Getting help

Looking to contribute to our code but need some help? There's a few ways to get information:

* Chat with us on [Gitter](https://gitter.im/sonatype/nexus-developers)
* Check out the [Nexus3](http://stackoverflow.com/questions/tagged/nexus3) tag on Stack Overflow
* Check out the [Nexus Repository User List](https://groups.google.com/a/glists.sonatype.com/forum/?hl=en#!forum/nexus-users)
"
109,dotnet/docs,PowerShell,"# .NET Docs

![Markdownlint](https://github.com/dotnet/docs/workflows/Markdownlint/badge.svg) [![target supported version](https://github.com/dotnet/docs/actions/workflows/version-sweep.yml/badge.svg)](https://github.com/dotnet/docs/actions/workflows/version-sweep.yml)

This repository contains the conceptual documentation for .NET. The [.NET documentation site](https://docs.microsoft.com/dotnet) is built from multiple repositories in addition to this one:

- [API reference](https://github.com/dotnet/dotnet-api-docs)
- [.NET Compiler Platform SDK reference](https://github.com/dotnet/roslyn-api-docs)

Issues and tasks for all but the API reference repository are tracked here. We have a large community using these resources. We make our best effort to respond to issues in a timely fashion. You can read more about our procedures for classifying and resolving issues in our [Issues policy](issues-policy.md) topic.

We welcome contributions to help us improve and complete the .NET docs. This is a very large repo, covering a large area. If this is your first visit, see our [labels and projects roadmap](styleguide/labels-projects.md) for help navigating the issues and projects in this repository.

To contribute, see the [Projects for .NET Community Contributors](https://github.com/dotnet/docs/projects/35) for ideas. The [Contributing Guide](CONTRIBUTING.md) has instructions on procedures we use.

If you're interested in helping migrate existing code that targets the .NET Framework from the [retired Code Gallery](https://docs.microsoft.com/teamblog/msdn-code-gallery-retired) site to .NET Core applications stored in our [samples repository](https://github.com/dotnet/samples) and downloadable from the [Samples Browser](https://docs.microsoft.com/samples/browse), see the [Code Gallery migration](https://github.com/dotnet/docs/projects/88) project. The code gallery samples were moved to the [Microsoft Archive](https://github.com/microsoftarchive?q=msdn-code-gallery) organization.

This project has adopted the code of conduct defined by the Contributor Covenant
to clarify expected behavior in our community.
For more information, see the [.NET Foundation Code of Conduct](https://dotnetfoundation.org/code-of-conduct).
"
110,dcmi/repository,HTML,"# archive
DCMI archive

MoinMoin wikis (2003-2010).  From 2003 to 2010, DCMI used MoinMoin wikis --
""wikis"", because they were not set up as one big wiki but as individual
instances of MoinMoin, thirteen to be exact. During this time, DCMI was careful
to say that the wikis had an uncertain future and that working groups should
only use them as temporary scratch pads.  The idea was that anything of
longer-term importance should eventually be converted into HTML and saved on
the main dublincore.org website; taking the initiative on this was to be the
responsibility of working groups.  Of course, some content potentially of
longer-term importance was never moved over, and there was no efficient way to
judge the future importance of materials, so at some point Tom archived all
contents of these wikis as static Web pages [2].

MediaWiki (2010-2017).

"
111,dvorka/mindforger-repository,,"# MindForger Repository

![MindForger](memory/mindforger/mindforger.png)

This is initial repository image of
[MindForger](https://github.com/dvorka/mindforger) - thinking notebook
and Markdown editor:

* [documentation index](memory/mindforger/index.md)

The repository is kept separate from MindForger source code to
distinquish changes to the application source code and repository
documents. Such separation enables independent releases and changes to
the application and it's default data.

* `memory/`
    * Notebooks with MindForger **documentation**
* `stencils/`
    * basic set of Notebook and Note **stencils**
* `limbo/`
    * place with no space and time where **forgotten knowledge** resides

To open initial repository with MindForger run the following command in 
this directory:

```sh
mindforger .
```
"
112,sonatype-nexus-community/nexus-repository-helm,Java,"<!--

    Sonatype Nexus (TM) Open Source Version
    Copyright (c) 2018-present Sonatype, Inc.
    All rights reserved. Includes the third-party code listed at http://links.sonatype.com/products/nexus/oss/attributions.

    This program and the accompanying materials are made available under the terms of the Eclipse Public License Version 1.0,
    which accompanies this distribution and is available at http://www.eclipse.org/legal/epl-v10.html.

    Sonatype Nexus (TM) Professional Version is available from Sonatype, Inc. ""Sonatype"" and ""Sonatype Nexus"" are trademarks
    of Sonatype, Inc. Apache Maven is a trademark of the Apache Software Foundation. M2eclipse is a trademark of the
    Eclipse Foundation. All other trademarks are the property of their respective owners.

-->
# Nexus Repository Helm Format

[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.sonatype.nexus.plugins/nexus-repository-helm/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.sonatype.nexus.plugins/nexus-repository-helm)

[![CircleCI](https://circleci.com/gh/sonatype-nexus-community/nexus-repository-helm.svg?style=shield)](https://circleci.com/gh/sonatype-nexus-community/nexus-repository-helm)

[![Join the chat at https://gitter.im/sonatype/nexus-developers](https://badges.gitter.im/sonatype/nexus-developers.svg)](https://gitter.im/sonatype/nexus-developers?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

[![DepShield Badge](https://depshield.sonatype.org/badges/sonatype-nexus-community/nexus-repository-helm/depshield.svg)](https://depshield.github.io)

> **Huzzah!** Helm is now part of Nexus Repository Manager. Version 3.21 includes the Helm plugin by default. 
>The plugin source code is now in [nexus-public](https://github.com/sonatype/nexus-public) in [nexus-repository-helm](https://github.com/sonatype/nexus-public/tree/master/plugins/nexus-repository-helm).

> **Filing issues:** Upgrade to the latest version of Nexus Repository Manager 3, to get the latest fixes and improvements, before filing any issues or feature requests at https://issues.sonatype.org/.

> **Upgrading:** If you are using a version prior to 3.21 and upgrade to a newer version you will not be able to install the community plugin. 
>No other changes are required and your existing data will remain intact.

# Table Of Contents
* [Release notes](https://help.sonatype.com/display/NXRM3/2020+Release+Notes#id-2020ReleaseNotes-RepositoryManager3.21.0)
* [Developing](#developing)
   * [Requirements](#requirements)
   * [Building](#building)
* [Using Helm with Nexus Repository Manger 3](#using-helm-with-nexus-repository-manager-3)
* [Installing the plugin](#installing-the-plugin)
   * [Permanent Install](#permanent-reinstall)
* [The Fine Print](#the-fine-print)
* [Getting Help](#getting-help)

## Developing

### Requirements

* [Apache Maven 3.3.3+](https://maven.apache.org/install.html)
* [Java 8](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)
* Network access to https://repository.sonatype.org/content/groups/sonatype-public-grid

Also, there is a good amount of information available at [Bundle Development](https://help.sonatype.com/display/NXRM3/Bundle+Development)

### Building

To build the project and generate the bundle use Maven

    mvn clean package

If everything checks out, the bundle for Helm should be available in the `target` folder.

In the examples below, substitute `<helm_version>` with the current version of the helm format plugin.

#### Build with Docker

`docker build -t nexus-repository-helm:<helm_version> .`

#### Run as a Docker container

`docker run -d -p 8081:8081 --name nexus nexus-repository-helm:<helm_version>` 

For further information like how to persist volumes check out [the GitHub repo for our official image](https://github.com/sonatype/docker-nexus3).

After allowing some time to spin up, the application will be available from your browser at http://localhost:8081.

To read the generated admin password for your first login to the web UI, you can use the command below against the running docker container:

      docker exec -it nexus cat /nexus-data/admin.password && echo
      
  Once logged into the application UI as `admin` using the generated password, you may also want to 
  turn on ""Enable anonymous access"" when prompted by the setup wizard.     

## Using Helm With Nexus Repository Manager 3

[We have detailed instructions on how to get started here!](https://help.sonatype.com/repomanager3/formats/helm-repositories)

## Compatibility with Nexus Repository Manager 3 Versions

The table below outlines what version of Nexus Repository the plugin was built against

| Plugin Version        | Nexus Repository Version |
|-----------------------|--------------------------|
| v0.0.6                | 3.13.0-01                |
| v0.0.7                | 3.14.0-04                |
| v0.0.8                | 3.15.2-01                |
| v0.0.9                | 3.16.2-01                |
| v0.0.10               | 3.17.0-01                |
| v0.0.11               | 3.18.0-01                |
| v0.0.12               | 3.18.0-01                |
| v0.0.13               | 3.18.0-01                |
| v1.0.2 In product     | 3.21.0+                  |
All released versions can be found [here](https://github.com/sonatype-nexus-community/nexus-repository-helm/releases).

## Features Implemented In This Plugin 

| Feature | Implemented          |
|---------|----------------------|
| Proxy   | :heavy_check_mark:   |
| Hosted  | :heavy_check_mark:   |
| Group   |                      |
  
## Installing The Plugin
In Nexus Repository Manager 3.21+ `Helm` format is already included. So there is no need to install it. But if you want to reinstall the plugin with your improvements then following instructions will be useful. <br> <b>Note:</b> Using an unofficial version of the plugin is not supported by the Sonatype Support team.  

### Permanent Reinstall

* Copy the new bundle into `<nexus_dir>/system/org/sonatype/nexus/plugins/nexus-repository-helm/<helm_version>/nexus-repository-helm-<helm_version>.jar`
* Edit `<nexus_dir>/system/org/sonatype/nexus/assemblies/nexus-cma-feature/3.x.y/nexus-cma-feature-3.x.y-features.xml`  changing helm to your build version (examples, the actual lines surrounding may vary):

   ```
         <feature version=""3.a.b"">nexus-repository-p2</feature>
         <feature version=""<helm_version>"">nexus-repository-helm</feature>
         <feature version=""3.x.y.xy"">nexus-repository-raw</feature>
     </feature>
   ```
   And
   ```
    <feature name=""nexus-repository-helm"" description=""org.sonatype.nexus.plugins:nexus-repository-helm"" version=""<helm_version>"">
        <details>org.sonatype.nexus.plugins:nexus-repository-helm</details>
        <bundle>mvn:org.sonatype.nexus.plugins/nexus-repository-helm/<helm_version></bundle>
    </feature>
   ```
This will cause the plugin to be loaded and started with each startup of Nexus Repository Manager.

NOTE: The file location changed in version 3.21. For older versions, edit these files:
* If you are using OSS edition, make these mods in: `<nexus_dir>/system/com/sonatype/nexus/assemblies/nexus-oss-feature/3.x.y/nexus-oss-feature-3.x.y-features.xml`
* If you are using PRO edition, make these mods in: `<nexus_dir>/system/com/sonatype/nexus/assemblies/nexus-pro-feature/3.x.y/nexus-pro-feature-3.x.y-features.xml`

Additionally, prior to 3.21 the lines did not exist so they'd need to be added instead of edited.

## The Fine Print

Starting from version 3.21+ the `Helm` plugin is supported by Sonatype, but still is a contribution of ours
to the open source community (read: you!)

Phew, that was easier than I thought. Last but not least of all:

Have fun creating and using this plugin and the Nexus platform, we are glad to have you here!

## Getting help

Looking to contribute to our code but need some help? There's a few ways to get information:

* If using Nexus Repository Manager 3.21+ or later please file any issues at https://issues.sonatype.org/.
* Chat with us on [Gitter](https://gitter.im/sonatype/nexus-developers)
* Check out the [Nexus3](http://stackoverflow.com/questions/tagged/nexus3) tag on Stack Overflow
* Check out the [Nexus Repository User List](https://groups.google.com/a/glists.sonatype.com/forum/?hl=en#!forum/nexus-users)
"
113,renaudcerrato/appengine-maven-repository,Java,"# appengine-maven-repository

Private Maven repositories hosted on Google App-Engine, backed by Google Cloud Storage, supporting HTTP Basic authentication and minimalistic user access control deployed in less than 5 minutes.

   * [Why ?](#why-)
   * [Installation](#installation)
      * [Prerequisites](#prerequisites)
      * [Configuration](#configuration)
      * [Deployment](#deployment)
   * [Usage](#usage)
   * [Limitations](#limitations)
   * [License](#license)
   
# Why ?

Private Maven repositories shouldn't cost you [an arm and a leg](https://www.cloudrepo.io/pricing.html), nor requires you to become a [Linux Sys-Admin](https://inthecheesefactory.com/blog/how-to-setup-private-maven-repository/en) to setup, and should ideally be **zero maintenance** and **costs nothing**.

Thanks to Google App-Engine's [free quotas](https://cloud.google.com/appengine/docs/quotas), you'll benefits for free of:

* 5GB of storage
* 1GB of daily incoming bandwidth
* 1GB of daily outgoing bandwidth
* 20,000+ storage ops per day

Moreover, no credit card is required to benefits of the quotas above.

# Installation

## Prerequisites

### Create a new Project
First of all, you'll need to go to your [Google Cloud console](https://console.cloud.google.com/projectselector/appengine/create?lang=java&st=true) to create a new App Engine application: 

![](https://i.imgur.com/SD1WwP3.png)

As soon as your project is created, a default [Google Cloud storage bucket](https://console.cloud.google.com/storage/browser) has been automatically created for you which provides the first 5GB of storage for free.

### Setup the Google Cloud SDK

Follow the [official documentation](https://cloud.google.com/sdk/docs/) to install the latest Google Cloud SDK. As a shorthand, you'll find below the Ubuntu/Debian instructions:


```bash
$ export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)""
$ echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
$ curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
$ sudo apt-get update && sudo apt-get install google-cloud-sdk
```

Do not forget to install the `app-engine-java` [component](https://cloud.google.com/sdk/docs/components#external_package_managers). If you installed the Google Cloud SDK using the instructions above:

```bash
$ sudo apt-get install google-cloud-sdk-app-engine-java
```

As a last step, configure the `gcloud` command line environment and select your newly created App Engine project when requested to do so:

```bash
$ gcloud init
$ gcloud auth application-default login
```

## Configuration

Clone (or [download](https://github.com/renaudcerrato/appengine-maven-repository/archive/master.zip)) the source code:

```bash
$ git clone https://github.com/renaudcerrato/appengine-maven-repository.git
```

Update [`WEB-INF/users.txt`](src/main/webapp/WEB-INF/users.txt) to declare users, passwords and permissions:

```ini
# That file declares your users - using basic authentication.
# Minimalistic access control is provided through the following permissions: write, read, or list.
# Syntax is:
# <username>:<password>:<permission>
# (use '*' as username and password to declare anonymous users)
admin:l33t:write
john:j123:read
donald:coolpw:read
guest:guest:list
```
> The `list` permission allows users to list the content of the repository but prohibits downloads. The `read` permission allows downloads (and implies `list`). The `write` permission allows uploads (and implies `read`).

> Anonymous users are supported by using ""*"" for both username and password. For example, `*:*:read` will allow anonymous read access. 

## Deployment

Once you're ready to go live, just push the application to Google App-Engine:

```bash
$ cd appengine-maven-repository
$ ./gradlew appengineDeploy
```

And voilà! Your private Maven repository is hosted and running at the following address:

`https://<your-project-id>.appspot.com`

# Usage

You'll find some usage examples in the [examples](examples) folder. There's absolutely no extra steps required to fetch and/or deploy Maven artifacts to your repository: simply use your favorite Maven tools as you're used to do. 

An example deploying artifacts using the maven plugin for Gradle:

```gradle
apply plugin: 'java'
apply plugin: 'maven'

...

uploadArchives {
    repositories {
        mavenDeployer {
            repository(url: ""https://<your-project-id>.appspot.com"") {
                authentication(userName: ""admin"", password: ""s3curepa55w0rd"")
            }
            pom.version = ""1.0.0""
            pom.artifactId = ""test""
            pom.groupId = ""com.example""
        }
    }
}
```

Using the above plugin, deploying artifacts to your repository is as simple as:

```bash
$ ./gradlew upload
```

In the other way, accessing password protected Maven repositories using Gradle only requires you to specify the `credentials` closure:

```gradle
repositories {
    ...
    maven {
        credentials {
            username 'user'
            password 'YouCantGuess'
        }
        url ""https://<your-project-id>.appspot.com""
    }
}
```

> Ensure you do NOT commit credentials with your code. With Gradle, you can achieve this by amending the above examples using the approach specified [here](http://stackoverflow.com/a/12751665/752167) of moving your creds to `~/.gradle/gradle.properties` and only referring to the variable names within your build.

# Limitations

Google App-Engine HTTP requests are limited to 32MB - and thus, any artifacts above that limit can't be hosted.

# File Management (Google Cloud File Storage)

The provided examples only provide a few gradle files that upload an installed local maven artifact. Sometimes there might be need to perform more file functions not available from gradle such as deletion.
The [documentation](https://cloud.google.com/storage/docs/uploading-objects) includes a button that launches a browser based file browser which presents a browser interface to perform file management.

# License

```
Copyright 2018 Cerrato Renaud

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```
"
114,dotnet/core,Shell,"﻿# .NET Core Home

The dotnet/core repository is a good starting point for .NET Core.

The latest major release is [.NET 5.0](release-notes/5.0/README.md). The latest patch updates are listed in [.NET Core release notes](release-notes/README.md).

## .NET Core Releases

* [Download the latest .NET Core SDK](https://dotnet.microsoft.com/download/dotnet/5.0)
* [.NET Core releases](releases.md)
* [.NET Core daily builds](daily-builds.md)

## Learn about .NET Core

* [Learn about .NET Core](https://docs.microsoft.com/dotnet/core)
* [.NET Core Roadmap](https://github.com/dotnet/core/blob/main/roadmap.md)
* [Learn about the .NET platform](https://docs.microsoft.com/dotnet/standard/)
* [.NET Core release notes](https://github.com/dotnet/core/blob/main/release-notes/README.md)
* [.NET Core Announcements](https://github.com/dotnet/announcements)
* [.NET Core blog](https://blogs.msdn.microsoft.com/dotnet/tag/net-core/)

## Getting help

* [File an issue](Documentation/core-repos.md)
* [Ask on Stack Overflow](https://stackoverflow.com/questions/tagged/.net-core)
* [Contact Microsoft Support](https://support.microsoft.com/contactus/)
* [VS Developer Community Portal](https://developercommunity.visualstudio.com/) for .NET Framework feedback (or via [Report a Problem](https://aka.ms/vs-rap) tool)

## How to Engage, Contribute and Provide Feedback

The .NET Core team encourages [contributions](https://github.com/dotnet/runtime/blob/master/CONTRIBUTING.md), both issues and PRs. The first step is finding the [.NET Core repository](Documentation/core-repos.md) that you want to contribute to.

Check the [product roadmap](https://aka.ms/dotnet-product-roadmap) to see what the team is focusing on.

### Community

This project uses the [.NET Foundation Code of Conduct](https://dotnetfoundation.org/code-of-conduct) to define expected conduct in our community.
Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting a project maintainer at conduct@dotnetfoundation.org.

## .NET Foundation

The .NET Core platform is part of the [.NET Foundation](https://www.dotnetfoundation.org).

## Licenses

.NET Core repos typically use either the [MIT](LICENSE.TXT) or
[Apache 2](https://www.apache.org/licenses/LICENSE-2.0) licenses for code.
Some projects license documentation and other forms of content under
[Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/).

See specific [repos](Documentation/core-repos.md) to understand the license used.
"
115,mateodelnorte/meta,JavaScript,"[![Build Status](https://travis-ci.com/mateodelnorte/meta.svg?branch=master)](https://travis-ci.com/mateodelnorte/meta)
[![npm version](https://badge.fury.io/js/meta.svg)](https://badge.fury.io/js/meta)
<img src=""https://img.shields.io/github/release-date/mateodelnorte/meta.svg"" alt=""Latest Release Date"" />

<span class=""badge-daviddm""><a href=""https://www.npmjs.com/package/meta"" title=""View the status of this project's dependencies on NPM""><img src=""https://img.shields.io/david/mateodelnorte/meta.svg"" alt=""Dependency Status"" /></a></span>
<span class=""badge-daviddmdev""><a href=""https://www.npmjs.com/package/meta"" title=""View the status of this project's development dependencies on DavidDM""><img src=""https://img.shields.io/david/dev/mateodelnorte/meta.svg"" alt=""Dev Dependency Status"" /></a></span>

<span class=""badge-npmdownloads""><a href=""https://npmjs.org/package/meta"" title=""View this project on NPM""><img src=""https://img.shields.io/npm/dm/meta.svg"" alt=""NPM downloads"" /></a></span>
<span><img src=""https://img.shields.io/github/contributors/mateodelnorte/meta.svg"" alt=""Contributors"" /></span>
<span class=""badge-daviddmdev""><a href=""https://gitter.im/meta-repos-ftw/Lobby"" title=""Discuss meta on Gitter""><img src=""https://img.shields.io/gitter/room/mateodelnorte/meta.svg"" alt=""Gitter"" /></a></span>

# meta

meta is a tool for managing multi-project systems and libraries. It answers the conundrum of choosing between a mono repo or many repos by saying ""both"", with a meta repo!

meta is powered by plugins that wrap common commands, letting you execute them against some or all of the repos in your solution at once. meta is built on [loop](https://github.com/mateodelnorte/loop), and as such inherits loops ability to easily target a particular set of directories for executing a common command (eg `meta git status --include-only dir1,dir2`. See [loop](https://github.com/mateodelnorte/loop) for more available options).

meta is packaged with a few of these core plugins by default: https://github.com/mateodelnorte/meta/blob/master/package.json#L63-L66

## Why meta?

- clone a many-project architecture in one line
- give every engineer on your team the same project setup, regardless of where it's cloned
- npm / yarn install against all your projects at once
- execute arbitrary commands against many repos to manage your projects
- super simple plugin architecture using commander.js
- easily wrap commands for working with any platform, not just Node!
- meta repo keeps code in per project repos, benefiting deployment and reuse
- use the same tools you always use. no strange side effects of git submodules or subtree
- give different teams different slices of your architecture, with multiple metarepos!
- use `meta project migrate` to migrate mono-repos to a meta repo consisting of many repos

# getting started

## installing

`npm i -g meta` will install a `meta` command on your system.

## initializing a new meta project

To create a new meta project:

1.  create a new directory for your meta project `mkdir my-meta-repo`
2.  initialize a new git repository in your new dir: `cd my-meta-repo && git init`
3.  initialize your new repository as a meta repo: `meta init`

meta will have created a .meta file to hold references to any child repositories you add.

4.  (a) to create a new project, use `meta project create [folder] [repo url]`
    (b) to import an existing project, use `meta project import [folder] [repo url]`

for each project added, meta will update your .gitignore file and the .meta file with references to the new child repo

[![asciicast](https://asciinema.org/a/d3nnfgv3n0vj2omzsl33l8um6.png)](https://asciinema.org/a/d3nnfgv3n0vj2omzsl33l8um6)

You can now perform commands against all of the repositories that make up your meta repository by using `meta exec`.

For example, to list all of the files in each project:

```
meta exec ""ls -la""
```

## cloning an existing meta project

To clone an existing meta repo, rather than `git clone` like you are used to, simply execute `meta git clone [meta repo url]` instead. `meta` will clone your meta repo and all child repositories at once.

```
meta git clone git@github.com:mateodelnorte/meta.git
```

[![asciicast](https://asciinema.org/a/2rkev7pu41cv51a0bajwnxu7s.png)](https://asciinema.org/a/2rkev7pu41cv51a0bajwnxu7s)

## Getting meta project updates

If you are working on a team and another members adds a project to the meta repository, to get the project, run `meta git update`.

```sh
# get new .meta file
git pull origin master

# clone missing projects
meta git update
```

# working with meta

## meta exec

The most basic way to interact with meta repositories is to use the `meta exec` command. This will let you run any command against the projects that make up your meta repo.

```
meta exec ""git checkout master""
```

In many cases, that is enough. There are also special cases where the functionality provided by the initial tool wasn't quite meta-y enough, and for those, there are plugins.

Even meta-exec, itself is a plugin, but it comes with meta by default.

## plugins

All meta functionality is contributed by plugins - node modules that begin with `meta-` and are either installed globally or in your meta repo's node_modules directory. We recommend you install them as devDependencies in your meta repo's package.json. Plugins add additional sub commands to meta, and can leverage [loop](https://github.com/mateodelnorte/loop) or [meta-loop](https://github.com/mateodelnorte/meta-loop) to easily execute a common command against your meta repo and all child repos.

Here's how easy it is to install `meta-npm` as a plugin, and gain the ability to `meta npm install` all your repos at once:

[![asciicast](https://asciinema.org/a/8iqph5ju6j00drxpknbj6lnm6.png)](https://asciinema.org/a/8iqph5ju6j00drxpknbj6lnm6)

Going deeper - meta plugins are able to wrap common commands for a friendly user experience, such as `meta npm install`. They are also able to extend the native tool's capabilities. For example, `git update` is not a git command, but `meta git update` will clone any repos that exist in your .meta file that aren't cloned locally - a problem that doesn't exist with a single git repo.

You shouldn't have much new syntax to memorize for some crazy new utilities nobody knows about. For instance, if you want to check the `git status` of all your repositories at once, you can just type `meta git status`:

[![asciicast](https://asciinema.org/a/83lg1tvqz9gwynixq5nhwsm2k.png)](https://asciinema.org/a/83lg1tvqz9gwynixq5nhwsm2k)

In the case a command has not been wrapped with a plugin, just use `meta exec` instead.

### Available Plugins

- [meta-init](https://github.com/mateodelnorte/meta-init)
- [meta-project](https://github.com/mateodelnorte/meta-project)
- [meta-git](https://github.com/mateodelnorte/meta-git)
- [meta-exec](https://github.com/mateodelnorte/meta-exec)
- [meta-gh](https://github.com/mateodelnorte/meta-gh)
- [meta-loop](https://github.com/mateodelnorte/meta-loop)
- [meta-npm](https://github.com/mateodelnorte/meta-npm)
- [meta-yarn](https://github.com/mateodelnorte/meta-yarn)
- [meta-template](https://github.com/patrickleet/meta-template)

### Third-party Plugins

- [meta-bump](https://github.com/patrykzurawik/meta-bump)
- [meta-release](https://github.com/alqh/meta-release)
- [meta-search](https://www.npmjs.com/package/meta-search)

### Available Templates

- [meta-plugin](https://github.com/patrickleet/meta-template-meta-plugin)

# Usage Scenarios

## Product Development Team

Your product consists of multiple applications and services. As the project lead, you can use `meta` to group together the projects so every developer is able to `meta git clone` a single project to get everything they need for development.

Furthermore, you could add a `docker-compose` file at this root level to run all of the services and applications:

```
version: '3.7'

services:

  app1:
    image: app1
    build:
      context: projects/app1
    ports:
    - 1234:1234
    env_file: projects/app1/.env

  app2:
    image: app2
    build:
      context: projects/app2
    ports:
    - 1234:1234
    env_file: projects/app2/.env

  service1:
    image: service1
    build:
      context: projects/service1
    ports:
    - 1236:1234
    env_file: projects/service1/.env

  service1:
    image: service1
    build:
      context: src/service2
    ports:
    - 1237:1234
    env_file: src/service2/.env
```

The meta repo is a good place for things like this, including scripts and a `Makefile` that are responsible for meta things, like gettings secrets for each project, like `.env` files for local development.

Take this example `Makefile` at the root of a meta repo:

```Makefile
onboard:
	meta exec ""make setup""

setup: install-tools get-secrets

install-tools:
  echo ""add install scripts here""

get-secrets:
	echo ""get secrets via SOPS/Vault/however and cp into appropriate projects""
```

The command `make onboard` would start the setup task in the root and all of the child directories.

Each project can then contain a `Makefile` like so:

```
setup:
  npm ci
  npm run dev
```

To get new projects up and running you can give them the instructions:

```
meta git clone git@github.com/yourorg/metaproject
cd metaproject
make onboard
```

And they would have a fully running dev environment.

## Developing a Library with many modules

Meta itself is developed with meta. This way you have a monorepo like feel while developing, but with individual components with their own release cycles.

It takes advantage of `npm link`, just like tools like Lerna do.

Using `meta npm link && meta npm link --all` enables a good development experience by creating symlinks so each project uses the development version of any other project in the meta repo:

```sh
# install meta
npm i -g meta

# clone and enter the meta repo
meta git clone git@github.com:mateodelnorte/meta.git
cd ./meta

# install plugins
npm install

# run install for all child repos
meta npm install

# create symlinks to/from all child repos
meta npm link --all

# link meta itself globally
npm link
```

There is admittedly now the problem of updating each repository to use the newly published versions of each other. For this, we recommend using a tool like Renovate, Dependabot, or Greenkeeper.

See this article for an example: [Bring In The Bots, And Let Them Maintain Our Code!](https://hackernoon.com/bring-in-the-bots-and-let-them-maintain-our-code-gh3s33n9)

## Migrating a Monorepo to many repos

'meta project migrate' helps you move from a monorepo to a meta repo by moving directories from
your existing repo into separate child repos, with git history intact. These are then referenced in
your '.meta' file and cloned, making the operation transparent to your codebase.

For example, given the following monorepo structure:

```
- monorepo-base
  - project-a
  - project-b
  - project-c
```

Create git repos for `project-a`, `project-b`, and `project-c`, then run:

```
cd monorepo-base
meta init
meta project migrate project-a git@github.com/yourorg/project-a
meta project migrate project-b git@github.com/yourorg/project-b
meta project migrate project-c git@github.com/yourorg/project-c
```

This will keep the git history of each subproject in tact, using some git magic:

- Explanation: https://help.github.com/en/articles/splitting-a-subfolder-out-into-a-new-repository
- Implementation: https://github.com/mateodelnorte/meta-project/blob/master/lib/splitSubtree.js

### How it works

1. Migrate will first create a copy of your project in a temporary directory and replace the remote
   'origin' with the provided <childRepoUrl>
1. It will split the history from <destFolder> and push to the provided <childRepo>:
   https://help.github.com/en/articles/splitting-a-subfolder-out-into-a-new-repository
1. Next <destFolder> is removed from your monorepo, and then cloned back into the same location.

In the eyes of the monorepo, the only thing that has changed is the .meta file, however, <destFolder> now also has it's own distinct history.

### Migration Phase

If you need the monorepos structure to stay in tact for any extended duration, such as supporting legacy CI systems, you can stop here.

While in this 'migration' phase, you need to commit to the child directory's git history as well as the monorepo's git history. These commits can literally be made twice by cd-ing around or both can be made at once using 'meta git commit'.

### Finishing the Migration

When the monorepo no longer needs to be maintained you can simply add the migrated project to your '.gitignore'.

This will cause changes to only be tracked in the child repo, rather than both, such as during the migration phase.

# FAQs

## How can I create a group of repositories, to, for example, run npm install on only node projects?

There are two ways to do this:

1. Meta repos can contain other meta repos. Make smaller groups of repos that only contain projects with commands that will be executed together. This is commonly not an option such as in migrating legacy monorepos.
1. Use a `Makefile` to declare the groups, and use `make` commands:

```
NODE_APPS=app1,service1,app2,service2

node-install:
	meta npm install --include-only $(NODE_APPS)
```

Then you can run `make node-install`

## Can I run things in parallel?

Yes.

```
meta exec ""npm ci"" --parallel
```

Output is even grouped nicely together for you at the end! :)

## How to escape expressions

If you try to evaluate an expression run in meta exec, you'll notice that the expression is evaluated before being run in the target projects.

```sh
➜  meta exec ""echo `pwd`"" --include-only=plugins/meta-loop

plugins/meta-loop:
/Users/patrickleet/dev/mateodelnorte/meta
plugins/meta-loop ✓
```

In these cases, simply escape the expression so that it is not executed until being run against each project rather than ahead of time:

```sh
➜  meta exec ""echo \`pwd\`"" --include-only=plugins/meta-loop

plugins/meta-loop:
/Users/patrickleet/dev/mateodelnorte/meta/plugins/meta-loop
plugins/meta-loop ✓
```

Or...

```sh
➜  meta exec ""echo \$(pwd)"" --include-only=plugins/meta-loop 

plugins/meta-loop:
/Users/patrickleet/dev/mateodelnorte/meta/plugins/meta-loop
plugins/meta-loop ✓
```

# Developing meta locally

The best way to get started is to do the following:

```
npm i -g meta
meta git clone git@github.com:mateodelnorte/meta.git
cd ./meta
npm install
meta npm install
meta npm link --all
npm link
```

This will clone the meta project, `meta`, enter the directory, and then use `meta` to perform `npm install`, `npm link --all` in each directory listed in `projects` of the `.meta` JSON configuration file, and link meta itself to be used as a global command.

You can then write your command and test using `./bin/meta git gh [subcommand]`.

You can run the above as a single command:

```
meta git clone git@github.com:mateodelnorte/meta.git && cd ./meta && npm i && meta npm install && meta npm link --all && npm link
```

Yarn lovers can do the same:

```
npm i -g meta
meta git clone git@github.com:mateodelnorte/meta.git
cd ./meta
yarn
meta yarn install
meta yarn link --all
yarn link
```

Or

```
meta git clone git@github.com:mateodelnorte/meta.git && cd ./meta && yarn && meta yarn install && meta yarn link --all && yarn link
```

See discussion [here](https://github.com/mateodelnorte/meta/issues/8) for more details

## More resources

- [Mono-repo or multi-repo? Why choose one, when you can have both? by @patrickleet](https://medium.com/@patrickleet/mono-repo-or-multi-repo-why-choose-one-when-you-can-have-both-e9c77bd0c668)
- [Developing a plugin for meta by @patrickleet](https://medium.com/@patrickleet/developing-a-plugin-for-meta-bd2e9c39882d)
"
116,zendframework/zendframework,,"![Logo](https://raw.githubusercontent.com/zendframework/zf2/234b554f2ca202095aea32e4fa557553f8849664/resources/ZendFramework-logo.png)

# Welcome to the *Zend Framework 3.0* Release!

## RELEASE INFORMATION

*Zend Framework 3.0.1dev*

This is the first maintenance release for the Zend Framework 3 series.

DD MMM YYYY

### UPDATES IN 3.0.1

Please see [CHANGELOG.md](CHANGELOG.md).

### SYSTEM REQUIREMENTS

Zend Framework 3 requires PHP 5.6 or later; we recommend using the
latest PHP version whenever possible.

### INSTALLATION

We no longer recommend installing this package directly. The package is a
metapackage that aggregates all components (and/or integrations) originally
shipped with Zend Framework; in most cases, you will want a subset, and these
may be installed separately; see https://docs.zendframework.com/ for a list of
available packages and installation instructions for each.

We recommend using either the zend-mvc skeleton application:

```bash
$ composer create-project zendframework/skeleton-application project
```

or the Expressive skeleton application:

```bash
$ composer create-project zendframework/zend-expressive-skeleton project
```

The primary use case for installing the entire framework is when upgrading from
a version 2 release.

If you decide you still want to install the entire framework:

```console
$ composer require zendframework/zendframework
```

#### GETTING STARTED

A great place to get up-to-speed quickly is the [Zend Framework
QuickStart](https://docs.zendframework.com/tutorials/getting-started/overview/).

The QuickStart covers some of the most commonly used components of ZF.
Since Zend Framework is designed with a use-at-will architecture and
components are loosely coupled, you can select and use only those
components that are needed for your project.

#### MIGRATION

For detailed information on migration from v2 to v3, please [read our Migration
Guide](https://docs.zendframework.com/tutorials/migration/to-v3/overview/).

### COMPONENTS

This package is a metapackage aggregating the following components:

- [zend-authentication](https://github.com/zendframework/zend-authentication)
- [zend-barcode](https://github.com/zendframework/zend-barcode)
- [zend-cache](https://github.com/zendframework/zend-cache)
- [zend-captcha](https://github.com/zendframework/zend-captcha)
- [zend-code](https://github.com/zendframework/zend-code)
- [zend-config](https://github.com/zendframework/zend-config)
- [zend-console](https://github.com/zendframework/zend-console)
- [zend-crypt](https://github.com/zendframework/zend-crypt)
- [zend-db](https://github.com/zendframework/zend-db)
- [zend-debug](https://github.com/zendframework/zend-debug)
- [zend-di](https://github.com/zendframework/zend-di)
- [zend-diactoros](https://github.com/zendframework/zend-diactoros)
- [zend-dom](https://github.com/zendframework/zend-dom)
- [zend-escaper](https://github.com/zendframework/zend-escaper)
- [zend-eventmanager](https://github.com/zendframework/zend-eventmanager)
- [zend-feed](https://github.com/zendframework/zend-feed)
- [zend-file](https://github.com/zendframework/zend-file)
- [zend-filter](https://github.com/zendframework/zend-filter)
- [zend-form](https://github.com/zendframework/zend-form)
- [zend-http](https://github.com/zendframework/zend-http)
- [zend-hydrator](https://github.com/zendframework/zend-hydrator)
- [zend-i18n](https://github.com/zendframework/zend-i18n)
- [zend-i18n-resources](https://github.com/zendframework/zend-i18n-resources)
- [zend-inputfilter](https://github.com/zendframework/zend-inputfilter)
- [zend-json](https://github.com/zendframework/zend-json)
- [zend-json-server](https://github.com/zendframework/zend-json-server)
- [zend-loader](https://github.com/zendframework/zend-loader)
- [zend-log](https://github.com/zendframework/zend-log)
- [zend-mail](https://github.com/zendframework/zend-mail)
- [zend-math](https://github.com/zendframework/zend-math)
- [zend-memory](https://github.com/zendframework/zend-memory)
- [zend-mime](https://github.com/zendframework/zend-mime)
- [zend-modulemanager](https://github.com/zendframework/zend-modulemanager)
- [zend-mvc](https://github.com/zendframework/zend-mvc)
- [zend-mvc-console](https://github.com/zendframework/zend-mvc-console)
- [zend-mvc-form](https://github.com/zendframework/zend-mvc-form)
- [zend-mvc-i18n](https://github.com/zendframework/zend-mvc-i18n)
- [zend-mvc-plugins](https://github.com/zendframework/zend-mvc-plugins)
- [zend-navigation](https://github.com/zendframework/zend-navigation)
- [zend-paginator](https://github.com/zendframework/zend-paginator)
- [zend-permissions-acl](https://github.com/zendframework/zend-permissions-acl)
- [zend-permissions-rbac](https://github.com/zendframework/zend-permissions-rbac)
- [zend-progressbar](https://github.com/zendframework/zend-progressbar)
- [zend-psr7bridge](https://github.com/zendframework/zend-psr7bridge)
- [zend-serializer](https://github.com/zendframework/zend-serializer)
- [zend-server](https://github.com/zendframework/zend-server)
- [zend-servicemanager](https://github.com/zendframework/zend-servicemanager)
- [zend-servicemanager-di](https://github.com/zendframework/zend-servicemanager-di)
- [zend-session](https://github.com/zendframework/zend-session)
- [zend-soap](https://github.com/zendframework/zend-soap)
- [zend-stdlib](https://github.com/zendframework/zend-stdlib)
- [zend-stratigility](https://github.com/zendframework/zend-stratigility)
- [zend-tag](https://github.com/zendframework/zend-tag)
- [zend-test](https://github.com/zendframework/zend-test)
- [zend-text](https://github.com/zendframework/zend-text)
- [zend-uri](https://github.com/zendframework/zend-uri)
- [zend-validator](https://github.com/zendframework/zend-validator)
- [zend-view](https://github.com/zendframework/zend-view)
- [zend-xml2json](https://github.com/zendframework/zend-xml2json)
- [zend-xmlrpc](https://github.com/zendframework/zend-xmlrpc)
- [zendxml](https://github.com/zendframework/zendxml)

### CONTRIBUTING

If you wish to contribute to Zend Framework, please read the
[CONTRIBUTING.md](.github/CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](.github/CODE_OF_CONDUCT.md) files.

### QUESTIONS AND FEEDBACK

Online documentation can be found at https://docs.zendframework.com/.
Questions that are not addressed in the manual should be directed to the
relevant repository, as linked above.

If you find code in this release behaving in an unexpected manner or
contrary to its documented behavior, please create an issue with the relevant
repository, as linked above.

## Reporting Potential Security Issues

If you have encountered a potential security vulnerability in Zend Framework,
please report it to us at [zf-security@zend.com](mailto:zf-security@zend.com).
We will work with you to verify the vulnerability and patch it.

When reporting issues, please provide the following information:

- Component(s) affected
- A description indicating how to reproduce the issue
- A summary of the security vulnerability and impact

We request that you contact us via the email address above and give the project
contributors a chance to resolve the vulnerability and issue a new release prior
to any public exposure; this helps protect Zend Framework users and provides
them with a chance to upgrade and/or update in order to protect their
applications.

For sensitive email communications, please use
[our PGP key](https://framework.zend.com/zf-security-pgp-key.asc).

### LICENSE

The files in this archive are released under the Zend Framework license.
You can find a copy of this license in [LICENSE.md](LICENSE.md).

### ACKNOWLEDGEMENTS

The Zend Framework team would like to thank all the
[contributors](https://github.com/zendframework/zendframework/contributors) to
the Zend Framework project; our corporate sponsor, Zend Technologies / Rogue
Wave Software; and you, the Zend Framework user.

Please visit us sometime soon at http://framework.zend.com.
"
117,mesosphere/universe,Mustache,"# Mesosphere Universe

| Build | Status |
|---|---|
|CI   | [![Build Status](https://teamcity.mesosphere.io/app/rest/builds/buildType:(id:Oss_Universe_Ci)/statusIcon?guest=1)](https://teamcity.mesosphere.io/viewType.html?buildTypeId=Oss_Universe_Ci&guest=1)|
| Universe Server | [![Build Status](https://teamcity.mesosphere.io/app/rest/builds/buildType:(id:Oss_Universe_UniverseServer)/statusIcon?guest=1)](https://teamcity.mesosphere.io/viewType.html?buildTypeId=Oss_Universe_UniverseServer&guest=1)|

Mesosphere Universe registry of packages made available for DC/OS Clusters.

#### Table of Contents
* [Universe Purpose](#universe-purpose)
  * [Library Dependencies](#library-dependencies)
* [Publish a Package](#publish-a-package-1)
  * [Creating a Package](#creating-a-package)
    * [`package.json`](#packagejson)
      * [`.minDcosReleaseVersion`](#mindcosreleaseversion)
    * [`config.json`](#configjson)
    * [`marathon.json.mustache`](#marathonjsonmustache)
    * [`command.json`](#commandjson)
    * [`resource.json`](#resourcejson)
      * [Docker Images](#docker-images)
      * [Images](#images)
      * [CLI Resources](#cli-resources)
  * [Submit your Package](#submit-your-package)
* [Repository Consumption](#repository-consumption-1)
  * [Universe Server](#universe-server)
    * [Build Universe Server locally](#build-universe-server-locally)
    * [Run Universe Server](#run-universe-server)
  * [Consumption Protocol](#consumption-protocol)
  * [Supported DC/OS Versions](#supported-dcos-versions)


## Universe Purpose
You can publish and store packages in the Universe repository. The packages can then be consumed by DC/OS. This git repo facilitates these three necessary functions - to publish, store and consume packages. You can publish and store packages in the Universe repository. The packages can then be consumed by DC/OS. If you are new to Universe and Packages, this [Get Started Guide](docs/tutorial/GetStarted.md) is highly recommended.

### Library dependencies
* [jq](https://stedolan.github.io/jq/download/) is installed in your environment.
* `python3` is installed in your environment (minimum python3.5).
* Docker is installed in your environment.

### Publish a Package

To publish a package to Universe, fork this repo and open a Pull Request. A set of automated builds will be run against
the Pull Request to ensure the modifications made in the PR leave the Universe well formed.
See [Creating a Package](#creating-a-package) for details.

### Registry of Packages

The registry of published packages is maintained as the contents of this repo in the `repo/packages` directory. As of
repository version `3.0` multiple packaging versions are allowed to co-exist in the same repository. Validation of
packages are coordinated based on the packaging version defined in `package.json`.

### Repository Consumption

In order for published packages to be consumed and installed in a DC/OS Cluster the Universe Server needs to be built
and run in a location accessible by the DC/OS Cluster. See [Universe Server](#universe-server) for details on
building the Universe artifacts and Server.

## Publish a Package

### Creating a Package

Each package has its own directory, with one subdirectory for each package revision. Each package revision directory
contains the set of files necessary to create a consumable package that can be used by a DC/OS Cluster to install
the package.
```
└── repo/package/F/foo
    ├── 0
    │   ├── command.json
    │   ├── config.json
    │   ├── marathon.json.mustache
    │   ├── resource.json
    │   └── package.json
    ├── 1
    │   ├── command.json
    │   ├── config.json
    │   ├── marathon.json.mustache
    │   ├── resource.json
    │   └── package.json
    └── ...
```


#### `package.json`

|Packaging Version|   |
|-----------------|---|
|2.0|required|
|3.0|required|
|4.0|required|

Every package in Universe must have a `package.json` file which specifies the high level metadata about the package.

Currently, a package can specify one of three values for `.packagingVersion`
either `2.0` or `3.0` or `4.0`; which version is declared
will dictate which other files are required for the complete package as well as the schema(s) all the files must
adhere to. Below is a snippet that represents a version `4.0` package.

See [`repo/meta/schema/package-schema.json`](repo/meta/schema/package-schema.json) for the full json schema outlining
what properties are available for each corresponding version of a package.

```json
{
  ""packagingVersion"": ""4.0"",
  ""name"": ""foo"",
  ""version"": ""1.2.3"",
  ""tags"": [""mesosphere"", ""framework""],
  ""maintainer"": ""help@bar.io"",
  ""description"": ""Does baz."",
  ""scm"": ""https://github.com/bar/foo.git"",
  ""website"": ""http://bar.io/foo"",
  ""framework"": true,
  ""upgradesFrom"": [""1.2.2""],
  ""downgradesTo"": [""1.2.2""],
  ""minDcosReleaseVersion"": ""1.10"",
  ""postInstallNotes"": ""Have fun foo-ing and baz-ing!"",
  ""licenses"": [{""name"": ""My license"", ""url"": ""http://example.com/license_url""}]
}
```

For the first version of the package, add this line to the beginning of `preInstallNotes`: ```This DC/OS Service is currently in preview. There may be bugs, incomplete features, incorrect documentation, or other discrepancies. Preview packages should never be used in production!``` It will be removed once the package has been tested and used by the community.

###### `.minDcosReleaseVersion`

|Packaging Version|   |
|-----------------|---|
|2.0|not supported|
|3.0|optional|
|4.0|optional|

Introduced in `packagingVersion` `3.0`, `.minDcosReleaseVersion` can be specified as a property of `package.json`.
When `.minDcosReleaseVersion` is specified the package will only be made available to DC/OS clusters with a DC/OS
Release Version greater than or equal to (`>=`) the value specified.

For example, `""minDcosReleaseVersion"" : ""1.8""` will prevent the package from being installed on clusters older than DC/OS 1.8.

###### `.upgradesFrom`

|Packaging Version|   |
|-----------------|---|
|2.0|not supported|
|3.0|not supported|
|4.0|optional|

Introduced in `packagingVersion` `4.0`, `.upgradesFrom` can be specified as a property of `package.json`.
When `.upgradesFrom` is specified this indicates to users that the package is able to upgrade from any of
the versions listed in the property. It is the resposibility of the package creator to make sure that this
is indeed the case.

###### `.downgradesTo`

|Packaging Version|   |
|-----------------|---|
|2.0|not supported|
|3.0|not supported|
|4.0|optional|

Introduced in `packagingVersion` `4.0`, `.downgradesTo` can be specified as a property of `package.json`.
When `.downgradesTo` is specified this indicates to users that the package is able to downgrade to any of
the versions listed in the property. It is the resposibility of the package creator to make sure that this
is indeed the case.

#### `config.json`

|Packaging Version|   |
|-----------------|---|
|2.0|optional|
|3.0|optional|
|4.0|optional|

This file describes the configuration properties supported by the package, represented as a
[json-schema](http://spacetelescope.github.io/understanding-json-schema/). Each property can specify whether or not it
is required, a default value, as well as some basic validation.

Users can then [override specific values](https://docs.mesosphere.com/1.7/usage/services/config/) at
installation time by passing an options file to the DC/OS CLI or by setting config values through the
DC/OS UI (since DC/OS 1.7).

```json
{
  ""type"": ""object"",
  ""properties"": {
    ""foo"": {
      ""type"": ""object"",
      ""properties"": {
        ""baz"": {
          ""type"": ""integer"",
          ""description"": ""How many times to do baz."",
          ""minimum"": 0,
          ""maximum"": 16,
          ""required"": false,
          ""default"": 4
        }
      },
      ""required"": [""baz""]
    }
  },
  ""required"": [""foo""]
}
```


#### `marathon.json.mustache`

|Packaging Version|   |
|-----------------|---|
|2.0|required|
|3.0|optional|
|4.0|optional|

This file is a [mustache template](http://mustache.github.io/) that when rendered will create a
[Marathon](http://github.com/mesosphere/marathon) app definition capable of running your service.

Variables in the mustache template will be evaluated from a union object created by merging three objects in the
following order:

1. Defaults specified in `config.json`

2. User supplied options from either the DC/OS CLI or the DC/OS UI

3. The contents of `resource.json`

```json
{
  ""id"": ""foo"",
  ""cpus"": 1.0,
  ""mem"": 1024,
  ""instances"": 1,
  ""args"": [""{{{foo.baz}}}""],
  ""container"": {
    ""type"": ""DOCKER"",
    ""docker"": {
      ""image"": ""{{resource.assets.container.docker.foo23b1cfe8e04a}}"",
      ""network"": ""BRIDGE"",
      ""portMappings"": [
        {
          ""containerPort"": 8080,
          ""hostPort"": 0,
          ""servicePort"": 0,
          ""protocol"": ""tcp""
        }
      ]
    }
  }
}
```

See the
[Marathon API Documentation](https://mesosphere.github.io/marathon/docs/rest-api.html)
for more detailed instruction on app definitions.

#### `command.json`

|Packaging Version|   |
|-----------------|---|
|2.0|optional|
|3.0|optional **[Deprecated]**|
|4.0|not supported|

As of `packagingVersion` `3.0`, `command.json` is deprecated in favor of the `.cli` property of `resource.json`.
See [CLI Resources](#cli-resources) for details.

Describes how to install the package's CLI via pip, the Python package manager. This document represents the
format of a Pip requirements file where each element in the array is a line in the requirements file.

```json
{
  ""pip"": [
    ""https://pypi.python.org/packages/source/f/foo/foo-1.2.3.tar.gz""
  ]
}
```

Packaging version 4.0 does not support command.json. The presence of command.json in the
directory will fail the universe validation.

#### `resource.json`

|Packaging Version|   |
|-----------------|---|
|2.0|optional|
|3.0|optional|
|4.0|optional|

This file contains all of the externally hosted resources (E.g. Docker images, HTTP objects and
images) needed to install the application.

See [`repo/meta/schema/v2-resource-schema.json`](repo/meta/schema/v2-resource-schema.json) and
[`repo/meta/schema/v3-resource-schema.json`](repo/meta/schema/v3-resource-schema.json) for the full
json schema outlining what properties are available for each corresponding version of a package.

```json
{
  ""images"": {
    ""icon-small"": ""http://some.org/foo/small.png"",
    ""icon-medium"": ""http://some.org/foo/medium.png"",
    ""icon-large"": ""http://some.org/foo/large.png"",
    ""screenshots"": [
      ""http://some.org/foo/screen-1.png"",
      ""http://some.org/foo/screen-2.png""
    ]
  },
  ""assets"": {
    ""uris"": {
      ""log4j-properties"": ""http://some.org/foo/log4j.properties""
    },
    ""container"": {
      ""docker"": {
        ""23b1cfe8e04a"": ""some-org/foo:1.0.0""
      }
    }
  }
}
```

##### Docker Images

For the Docker image, please use the image ID for the referenced image. You can find this by
pulling the image locally and running `docker images some-org/foo:1.0.0`.

##### Images

While `images` is an optional field, it is highly recommended you include icons and screenshots
in `resource.json` and update the path definitions accordingly. Specifications are as follows:

* `icon-small`: 48px (w) x 48px (h)
* `icon-medium`: 96px (w) x 96px (h)
* `icon-large`: 256px (w) x 256px (h)
* `screenshots[...]`: 1200px (w) x 675px (h)

**NOTE:** To ensure your service icons look beautiful on retina-ready displays,
please supply 2x versions of all icons. No changes are needed to
`resource.json` - simply supply an additional icon file with the text `@2x` in
the name before the file extension.
For example, the icon `icon-cassandra-small.png` would have a retina-ready
alternate image named `icon-cassandra-small@2x.png`.

##### CLI Resources

|Packaging Version|   |
|-----------------|---|
|2.0|not supported|
|3.0|optional|
|4.0|optional|

The new `.cli` property allows for a package to configure native CLI subcommands for several platforms and
architectures.

```json
{
  ""cli"":{
    ""binaries"":{
      ""darwin"":{
        ""x86-64"":{
          ""contentHash"":[
            { ""algo"": ""sha256"", ""value"": ""..."" }
          ],
          ""kind"": ""executable"",
          ""url"":""https://some.org/foo/1.0.0/cli/darwin/dcos-foo""
        }
      },
      ""linux"":{
        ""x86-64"":{
          ""contentHash"":[
            { ""algo"":""sha256"", ""value"":""..."" }
          ],
          ""kind"":""executable"",
          ""url"":""https://some.org/foo/1.0.0/cli/linux/dcos-foo""
        }
      },
      ""windows"":{
        ""x86-64"":{
          ""contentHash"":[
            { ""algo"":""sha256"", ""value"":""..."" }
          ],
          ""kind"":""executable"",
          ""url"":""https://some.org/foo/1.0.0/cli/windows/dcos-foo""
        }
      }
    }
  }
}
```

### Submit your Package

Developers are invited to publish a package containing their DC/OS Service by submitting a Pull Request targeted at
the `version-3.x` branch of this repo.

Full Instructions:

1. Fork this repo and clone the fork:

  ```bash
  git clone https://github.com/<user>/universe.git /path/to/universe
  ```

2. Run the verification and build script:

  ```bash
  scripts/build.sh
  ```

3. Verify all build steps completed successfully
4. Ensure the license field in package.json is completed. Without a license attribution we cannot accept pull requests.
5. Submit a pull request against the `version-3.x` branch with your changes. Every pull request opened will have a set
   of automated verifications run against it. These automated verification are reported against the pull request using
   the GitHub status API. All verifications must pass in order for a pull request to be eligible for merge.

6. Respond to manual review feedback provided by the DC/OS Community.
  * Each Pull Request to Universe will also be manually reviewed by a member of the DC/OS Community. To ensure your
    package is able to be made available to users as quickly as possible be sure to respond to the feedback provided.
7. Add a getting started example of how to install and use the DC/OS package. To add the example, fork the [`examples`](https://github.com/dcos/examples) repo and send in a pull request. Re-use the format from the existing examples there.


## Repository Consumption

In order for Universe to be consumed by DC/OS the build process needs to be run to create the Universe Server. This section describes how to test a package before releasing it to public Universe. See [Local Universe](https://docs.mesosphere.com/latest/administering-clusters/deploying-a-local-dcos-universe/) for running universe server on air-gapped clusters.

### Universe Server

Universe Server is a new component introduce alongside `packagingVersion` `3.0`. In order for Universe to be able to
provide packages for many versions of DC/OS at the same time, it is necessary for a server to be responsible for serving
the correct set of packages to a cluster based on the cluster's version.

All Pull Requests opened for Universe and the `version-3.x` branch will have their Docker image built and published
to the DockerHub image [`mesosphere/universe-server`](https://hub.docker.com/r/mesosphere/universe-server/).
In the artifacts tab of the build results you can find `docker/server/marathon.json` which can be used to run the
Universe Server for testing in your DC/OS cluster.  For each Pull Request, click the details link of the ""Universe Server
Docker image"" status report to view the build results.

#### Build Universe Server locally

1. Validate and build the Universe artifacts
  ```bash
  scripts/build.sh
  ```

2. Build the Universe Server Docker image
  ```bash
  DOCKER_IMAGE=""my-org/my-image"" DOCKER_TAG=""my-package"" docker/server/build.bash
  ```
  This will create a Docker image `universe-server:my-package` and `docker/server/target/marathon.json` on your local machine

3. If you would like to publish the built Docker image, run
  ```bash
  DOCKER_IMAGE=""my-org/my-image"" DOCKER_TAG=""my-package"" docker/server/build.bash publish
  ```

#### Run Universe Server

Using the `marathon.json` that is created when building Universe Server we can run a Universe Server in our DC/OS
Cluster which can then be used to install packages.

Run the following commands to configure DC/OS to use the custom Universe Server (DC/OS 1.8+):

```bash
dcos marathon app add marathon.json
dcos package repo add --index=0 dev-universe http://universe.marathon.mesos:8085/repo
```

For DC/OS 1.7, a different URL must be used:

```bash
dcos marathon app add marathon.json
dcos package repo add --index=0 dev-universe http://universe.marathon.mesos:8085/repo-1.7
```

### Consumption Protocol

A DC/OS Cluster can be configured to point to multiple Universe Servers; each Universe Server will be fetched via
HTTPS or HTTP. When a DC/OS Cluster attempts to fetch the package set from a Universe Server, the Universe Server
will provide ONLY those packages which can be run on the cluster.

For example:
A DC/OS 1.6.1 Cluster will only receive packages with a `minDcosReleaseVersion` less than or equal to (`<=`) `1.6.1`
in the format the DC/OS Cluster expects.

```
 +----------------------+   +-----------------------+
 │public universe server│   │private universe server│
 +----------------------+   +-----------------------+
                http \         / http
                      \       /
                       \     /
                       +-----+           +--------+
                       │DC/OS│-----------│Marathon│
                       +-----+    http   +--------+
```

### Supported DC/OS Versions
Currently Universe Server provides support for the following versions of DC/OS

| DC/OS Release Version | Support Level |
|-----------------------|---------------|
| 1.6.1                 | Deprecated    |
| 1.7                   | Deprecated    |
| 1.8                   | Full Support  |
| 1.9                   | Full Support  |
| 1.10                  | Full Support  |
| 1.11                  | Full Support  |
| 1.12                  | Full Support  |
| 1.13                  | Full Support  |
| 2.0                   | Full Support  |
"
118,kodi-community-addons/repository.marcelveldt,Python,"# repository.marcelveldt
Kodi repository for beta versions of marcelveldt's addons

**To install you can :** 

* Download zip file outside kodi 
https://github.com/kodi-community-addons/repository.marcelveldt/raw/master/repository/repository.marcelveldt/repository.marcelveldt-1.0.3.zip
* Then install from zip and point to your downloaded zip on your device 



OR


* Add https://kodi-community-addons.github.io/repository.marcelveldt/ as a Kodi file source (file manager)  and give it a name such as ""marcelveldt repo""
* Then you can install the repo zip from that file source. 
 *Install from zip > choose the file source you just made > select the repo zip to install*


To Browse repo content in browser:
https://github.com/kodi-community-addons/repository.marcelveldt/repository
"
119,sonatype-nexus-community/nexus-repository-import-scripts,Shell,"# Nexus Repository Import Scripts
## Wut?
These are bare bones bash scripts to import a Nexus 2 Maven, NuGet or npm repository (and likely other file system based repos)
into Nexus Repository 3.
### Wut does it do?
Imports artifacts into a Nexus Repository 3 Maven2, NuGet or npm hosted repo.
### Wut does it not do?
Literally anything else. You want security? Better set it up yourself.
## How do I use it?
* Maven
  * cd rootdirectorywithallyourartifacts
  * ./mavenimport.sh -u admin -p admin123 -r http://localhost:8084/repository/maven-releases/
  * Watch a bunch of verbose output from curl
  * If need be, change -u to user, -p to password, and -r (I bet you'll have to change this) to the repo you want to upload in to
* NuGet
  * cd rootdirectorywithallyournugetpackages
  * ./nugetimport.sh -k APIKEYFROMNEXUS - r http://localhost:8084/repository/nuget-hosted/
  * Watch the money roll in and the haters start askin
  * You'll need to obtain your APIKEY for Nexus Repository, and obviously set -r to the repo path you want to use
* npm
  * npm login --registry http://localhost:8084/repository/npm-internal/
  * cd rootdirectorythatcontainsallnpmmadness
  * ./npmimport.sh -r http://localhost:8084/repository/npm-internal/
  * Watch a bunch of stuff prolly fail because it has extra build steps, figure those out and then remediate if you really care
  * Set -r and --registry to the NPM hosted repo you plan to use
## Like it?
Great, buy me a beer.
"
120,leonvanbokhorst/RepositoryPatternEntityFramework,C#,
121,packal/repository,,"Packal Repository
==========

This repository hosts Workflows for Alfred 2.

All files in here are pushed automatically from http://packal.org. All are user-contributed and have been subjected to a virus scan.

You can clone this repository, but it should be fairly big. We recommend using the website to discover which Workflows are relevant for you.

We do not accept pull requests for any of the files because, well, they're user contributed and not developed here.

This repository has three functions:
 1. To be the primary host for these Workflows (all download links from http://packal.org point here).
 2. To be the remote source for the Packal Workflow's updating abilities (still in development).
 3. To be a backup in case the Packal server goes down (all workflow files are also on the webserver).
"
122,jdf76/repository,HTML,
123,hacker-walker/repository,,"# repository

repository


🇨🇳[简体中文](./README.md) | 🇺🇸English | [更新日志](https://gitee.com/Hacker-walker/repository)
[![author](https://img.shields.io/badge/author-walker-blue.svg)](https://wuxf.cn)
[![JDK](https://img.shields.io/badge/JDK-1.8-orange.svg)](https://github.com/Hacker-walker/repository)
[![QQ群](https://img.shields.io/badge/chat-walkerQQ%E7%BE%A4-yellow.svg)](https://jq.qq.com/?_wv=1027&k=5mjexzD)
<a href=""https://github.com/hacker-walker/repository""><img alt=""star"" src=""https://img.shields.io/github/stars/hacker-walker/repository.svg?label=Stars&style=social""/></a>
[![star](https://gitee.com/Hacker-walker/repository/badge/star.svg?theme=dark)](https://gitee.com/Hacker-walker/repository/stargazers)


## catalogue

- [Java](#java)
    - [基础](#基础)
    - [Java8](#Java8)
    - [JDK](#jdk)
    - [JVM](#jvm)
    - [I/O](#io)
    - [编程规范](#编程规范)
- [数据库](#数据库)
    - [MySQL](#mysql)
    - [Redis](#redis)
- [Network](#Network)
    - [Network](#Networks)
- [explain ](#explain )
## Java

### 基础

* [一 java基础知识](Java/Java基础知识.md)
* [二 J2EE基础知识](Java/J2EE基础知识.md)
* [多线程系列](Java/basis/多线程系列.md)
* [设计模式](Java/basis/设计模式.md)

### Java8 

* [新特性总结](Java/Java8Tutorial.md)
* [学习资源推荐](Java/Java8教程推荐.md)

### JDK 

* [接口规范](Java/jdk/JDK8接口规范-静态、默认方法.md)
* [监控和故障处理](Java/jdk/JDK监控和故障处理工具总结.md)

### JVM

* [一 Java内存区域](Java/jvm/Java内存区域.md)
* [二 类加载过程](Java/jvm/类加载过程.md)

### I/O

* [一 BIO,NIO,AIO 总结 ](Java/io/BIO-NIO-AIO.md)
* [二 Java IO与NIO系列文章](Java/io/Java_IO与NIO.md)

### 编程规范

- [Java 编程规范](Java/Java编程规范.md)

## 数据库

### MySQL

* [MySQL 学习与面试](Mysql/MySQL.md)
* [MySQL Index](Mysql/MySQL Index.md)
* [MySQL高性能优化规范](Mysql/MySQL高性能优化规范.md)
* [一千行MySQL命令](Mysql/一千行MySQL命令.md)
* [事务隔离级别](Mysql/事务隔离级别.md)

### Redis

* [Redis 总结](Redis/Redis.md)
* [Redis持久化](Redis/Redis持久化.md)
* [RedLock分布式锁](Redis/Redlock分布式锁.md)
* [如何做可靠的分布式锁](Redis/如何做可靠的分布式锁，Redlock真的可行么.md)

## Network

### Networks

* [计算机网络](Network/计算机网络.md)
* [Https](Network/HTTPS中的TLS.md)
* [计算机网络知识总结](Network/计算机网络知识总结.md)

## explain 

### introduce

*  **对于 Java For beginners：** This document is intended to provide you with a more detailed learning path to give you a sense of the overall body of knowledge of Java.In addition, some of the articles in this article are good practices for you to learn and review your Java knowledge；
*  **对于非 Java 初学者来说：** This document is better for reviewing your knowledge, preparing for the interview, and figuring out which questions to focus on.
要搞清楚这个道理：Know the interviews ahead of time, not so you can memorize them, but so you can focus on them.

Markdown 格式参考：[Github Markdown格式](https://guides.github.com/features/mastering-markdown/)，表情素材来自：[EMOJI CHEAT SHEET](https://www.webpagefx.com/tools/emoji-cheat-sheet/)。

利用 docsify 生成文档部署在 Github pages: [docsify 官网介绍](https://docsify.js.org/#/)

### About the transfer

If you need to reprint some articles from this warehouse to your own blog, please note the original address.

### article source 

This project is mainly used to collect and organize my notes in study. Most articles are from the official account below <br>
![公众号](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/167598cd2e17b8ec.png) <br>
If you need to learn more, keep an eye on the princess!!! <br>

Note:<br>
Most of the articles in this treasure house are original works of various great gods, I just sort them out and put them here for everyone to learn together!<br>
If there are similar, pure coincidence!!!

### WeChat Official Accounts:Jin Xia Nian hua
![个人公众号](https://images.gitee.com/uploads/images/2019/0610/161454_710bf233_2127888.png)"
124,drinfernoo/repository.example,Python,"## Video version of this tutorial now available!
[How to make a Kodi Repository!](https://youtu.be/EgFtVW0MRrs)

# BASIC - How to setup for hosting on GitHub Pages

In order to follow this tutorial, first fork this repository, and then clone your newly forked copy locally.

First, you'll need to edit the `addon.xml` file within the `/repository.example` folder with your chosen add-on ID, a version number, and your username (or whatever you'd like) for `provider`, as seen on line 2:

```XML
<addon id=""ADDON_ID_HERE"" name=""REPO_NAME_HERE"" version=""VERSION_NUMBER_HERE"" provider-name=""YOUR_USERNAME_HERE"">
```

You also need to replace `YOUR_USERNAME_HERE` and `REPOSITORY_NAME_HERE` with your GitHub username and this repository's name, respectively, as seen on lines 5-7:

```XML
<info compressed=""false"">https://raw.githubusercontent.com/YOUR_USERNAME_HERE/REPOSITORY_NAME_HERE/master/zips/addons.xml</info>
<checksum>https://raw.githubusercontent.com/YOUR_USERNAME_HERE/REPOSITORY_NAME_HERE/master/zips/addons.xml.md5</checksum>
<datadir zip=""true"">https://raw.githubusercontent.com/YOUR_USERNAME_HERE/REPOSITORY_NAME_HERE/master/zips/</datadir>
```

You should also change the summary and description of your repository, as seen on lines 11-12:

```XML
<summary>REPO_NAME_HERE</summary>
<description>DESCRIPTION OF YOUR REPO HERE</description>
```

While not required, it is also recommended to replace `icon.png` and `fanart.jpg` in the `/repository.example` folder with art relevant to your repository or the add-ons contained within. `icon.png` should be 512x512 px, and `fanart.jpg` should be 1920x1080 px, or a similar ratio.

To build the repository, first rename the `/repository.example` folder to match whatever add-on ID you chose earlier. Place the add-on source folders for whichever add-ons you'd like to be contained in your Kodi repo in the main folder of this repository, and run `_repo_xml_generator.py`. 

This will create zips of all of the desired add-ons, and place them in the `zips` folder, along with a generated `addons.xml` and `addons.xml.md5`. Copy the zip file of your repository, located at `/zips/ADDON_ID_HERE/ADDON_ID_HERE-VERSION_NUMBER_HERE.zip`,
and paste it into the `/repo` folder.

Inside the `/repo` folder, edit the link inside `index.html` to reflect your add-on's filename, as seen on line 1:

```HTML
<a href=""ADDON_ID_HERE-VERSION_NUMBER_HERE.zip"">ADDON_ID_HERE-VERSION_NUMBER_HERE.zip</a>
```

After committing and pushing these changes to your repo, go to the ""Settings"" section for this repository on GitHub. In the first box, labeled ""Repository name"", change your repository's name. Generally, GitHub Pages repositories are named `YOUR_USERNAME_HERE.github.io`,  but it can be whatever you'd like.
Next, scroll down to the ""GitHub Pages"" section, choose the `master` branch as the source, and click ""Save"".

After that, you should be all done!

If you named this repository `YOUR_USERNAME_HERE.github.io`, your file manager source will be:

`https://YOUR_USERNAME_HERE.github.io/repo/`

And if you named it something else, it will be:

`https://YOUR_USERNAME_HERE.github.io/REPOSITORY_NAME_HERE/repo/`

# ADVANCED - How to set up for hosting without GitHub Pages

If you want to host your Kodi repo on a different host besides GitHub Pages, simply download this repository as a `.zip`, and unzip it , rather than forking and cloning it. Continue to follow the rest of the setup procedure, except for the setting up of GitHub Pages. The only differences will be in your `addon.xml` file (lines 5-7), as it will need to reference yourhost, rather than GitHub:

```XML
<info compressed=""false"">https://YOUR_HOST_URL_HERE/zips/addons.xml</info>
<checksum>https://YOUR_HOST_URL_HERE/zips/addons.xml.md5</checksum>
<datadir zip=""true"">https://YOUR_HOST_URL_HERE/zips/</datadir>
```

And upload the contents of this repository to your host. It is **very important** that `YOUR_HOST_URL_HERE` is the URL to *this* folder.

After doing so, your file manager source will be:

`https://YOUR_HOST_URL_HERE/repo/`





"
125,AdiChat/Repository-Hunter,Ruby,"# Repository Hunter :octocat: <br>

[![Join the chat at https://gitter.im/AdiChat/Repository-Hunter](https://badges.gitter.im/AdiChat/Repository-Hunter.svg)](https://gitter.im/AdiChat/Repository-Hunter?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![License](https://img.shields.io/badge/license-CC0--1.0-orange.svg)](https://img.shields.io/badge/license-CC0--1.0-orange.svg)<br>

### Making GitHub:octocat: more socially engaging 🎮 and fun 🍥 for all 👦 + 👧 + 👴 + 👶 + 🐮 + 🐦 + 🐱 ...<br>

![alt text](https://github.com/AdiChat/Repository-Hunter/blob/master/Preview/repository_hunter_1.3.gif "" The view of the application"")<br>

The application provides 6 kinds of services: 
* **Believe it or not**
* **Fun Facts**
* **Embeddable GitHub Contribution graph**
* **Compare two GitHub profiles**
* **Find the best coder at an organization**
* **Profile Presentation**

* The first feature is 🎉 **Believe it or not**🎉. It explores some of the mysterious corners at GitHub such as:
  * A man who has been committing for over **100 years**.
  * A **16-year-old issue** on 9-year-old GitHub using 12-year-old Git
  * A man with **no contribution graph**.
  * A human with a name that extends vertically indefinitely and much more. 

* The second engaging feature is the 🎉 **_Fun Facts_** 🎉. This meets the curiosity of a human to peek into the progress/ work accomplished by other users. Several engaging facts are presented which when observed closely provides deep insights into the mind of our fellow GitHub users. Some instances of engaging features are the list of most commented issues in GitHub, the list of oldest issues in GitHub, some of the most popular developers in GitHub and others. Some deep insights are:
   * The most commented issue has around **16565 comments** but the sad part is that it is a result of automation. 
   * The oldest unresolved bug is around **_15 years old_**. 
   * The most popular user is _Linus Torvalds_ with over **_46700 followers_** whereas he follows none (contrary to the assumption that ""_`more users I follow, the more users will follow me`_"") . 
   * A seemingly authentic issue has over **1915 comments** which is a discussion💭 over iCloudin, which is a tool to bypass iCloud and another issue addressing the same issue has over **1279 comments**. The strange part is there are only around 130 participants, a small number compared to other issues.
   * An issue over a project that won China 🌍 2014 State Science and Technology Prizes has over **1426 comments** with over **997 participants**. <br>
   and observations continue forever. ▶️ 

* The third one is 🎉 **_Embeddable GitHub Contribution graph_** 📅. This allows anyone to embed their contribution graph in any web application. Get your graph today! The best part is the even if your graph has been removed as happened with _Ciro Santilli_ several years ago, still, you will be able to **see your long lost graph** and save it for warm memories. Here is my saved contribution graph: <br>
![alt text](https://github.com/AdiChat/Repository-Hunter/blob/master/Preview/adichat.jpg "" The view of the contribution graph"")<br>
Check out your graph and embed it today: **[here](http://repository-hunter.herokuapp.com/contribution)** 

* The fourth feature is 🎉 **GitHub Compare**. You can compare two GitHub accounts and find out who is :medal_sports:stronger💪 ? Yes, I know comparing two entities is not morally correct yet it is quite fun to see who's ahead? Take this as a disclaimer. 

* The fifth feature is 🎉 **GitHub master of the Organization** :1st_place_medal:. You can find the ranking of all public members of an organization and thus, there is an entity that dominates the ranking and hence, is the best 🏆 coder in the organization.

The top five contributors at **Google** are:

| Rank | Score | User |
|:----:|:-----:|:----:|
| 1 | 169.84 | [keyboardsurfer](https://github.com/keyboardsurfer) :1st_place_medal: 
| 2 | 136.01 | [spf13](https://github.com/spf13) 
| 3 | 126.47 | [philipwalton](https://github.com/philipwalton) 
| 4 | 119.82 | [igrigorik](https://github.com/igrigorik) 
| 5 | 119.42 | [jverkoey](https://github.com/jverkoey) 

See the full list [here](https://github.com/AdiChat/Repository-Hunter/wiki/Rank-List-at-Google). Take a look at the ranking list of **DuckDuckGo** [here](https://github.com/AdiChat/Repository-Hunter/wiki/Rank-List-at-DuckDuckGo).

* The sixth one is a 🎉 **user profile presentation service** :person_fencing: where the public profile details such as some users following the concerned user, some users being followed by the concerned user, the language usage distribution and some public information is displayed in an exciting way. Check your profile [today](http://repository-hunter.herokuapp.com/). 

#### Use the app [here](http://repository-hunter.herokuapp.com/)<br>

New exciting features coming soon. 🎉 

Consider giving this repository a 🌟 if you enjoyed it. <br>
"
126,dobbelina/repository.dobbelina,Python,"# Cumination

<img src=""https://user-images.githubusercontent.com/46063764/103461711-a9eb6280-4d20-11eb-983b-516b022cbbf5.png"" width=""300"" align=""right"">

Cumination release

Welcome to Cumination!

Matrix and Leia compatible release

Backup favorites from UWC can be imported



---
All credit to the fantastic people at reddit who has provided the fixes, and especially jdoedev & 12asdfg12 & camilt


and Whitecream & holisticdioxide who are the original author/s of the addon.

For auto-updates, download repository.dobbelina-1.0.3.zip here https://dobbelina.github.io

**Please post in the ""Issues"" section if you can contribute to fix broken**

**sites/catchers, or make a pull request.**

[![GitHub Issues Open](https://github-basic-badges.herokuapp.com/issues/dobbelina/repository.dobbelina.svg)]()

[![GitHub Commits](https://github-basic-badges.herokuapp.com/commits/dobbelina/repository.dobbelina.svg)]()



"
127,tugberkugurlu/GenericRepository,C#,"> :warning: **THIS REPOSITORY IS NOT MAINTAINED ANYMORE. IF YOU WOULD LIKE TO VOLUNTEER TO BE THE MAINTAINER, PLEASE [CONTACT ME](https://twitter.com/tourismgeek).**

This little project contains a Generic Repository implementation for several data access platforms such as Entity Framework.

You can find more information about the main idea of this pattern and the usage on ASP.NET MVC along with Unit Testing by visiting the below links:

 - [Generic Repository Pattern - Entity Framework, ASP.NET MVC and Unit Testing Triangle][1]
 - [How to Work With Generic Repositories on ASP.NET MVC and Unit Testing Them By Mocking][2]

# How to Install

You can directly install this little project from [Nuget][6]. There are two packages:

**[GenericRepository][7]**

    PM> Install-Package GenericRepository

Generic Repository Insrastructure For .NET Applications

**[GenericRepository.EF][8]**

    PM> Install-Package GenericRepository.EF

Generic Repository DbContext Implementation

# Releases

Under the [master][3] branch, you can find the latest stable release of this project.

Also, you can see the most up-to-date project under [develop][4] branch. Repository under develop branch is possibly the unstable version of the project.

You can also find all the releases under [Tags][5] section.

  [1]: http://www.tugberkugurlu.com/archive/generic-repository-pattern-entity-framework-asp-net-mvc-and-unit-testing-triangle
  [2]: http://www.tugberkugurlu.com/archive/how-to-work-with-generic-repositories-on-asp-net-mvc-and-unit-testing-them-by-mocking
  [3]: https://github.com/tugberkugurlu/GenericRepository
  [4]: https://github.com/tugberkugurlu/GenericRepository/tree/develop
  [5]: https://github.com/tugberkugurlu/GenericRepository/tags
  [6]: http://nuget.org
  [7]: https://nuget.org/packages/GenericRepository
  [8]: https://nuget.org/packages/GenericRepository.EF
"
128,alexandre-spieser/mongodb-generic-repository,C#,"# MongoDbGenericRepository

An example of generic repository implementation using the MongoDB C# Sharp 2.0 driver (async)

Now available as a nuget package:
https://www.nuget.org/packages/MongoDbGenericRepository/

Covered by 400+ integration tests and counting.

The MongoDbGenericRepository is also used in [AspNetCore.Identity.MongoDbCore](https://github.com/alexandre-spieser/AspNetCore.Identity.MongoDbCore).

# Support This Project

If you have found this project helpful, either as a library that you use or as a learning tool, please consider buying Alex a coffee: <a href=""https://www.buymeacoffee.com/zeitquest"" target=""_blank""><img height=""40px"" src=""https://cdn.buymeacoffee.com/buttons/default-orange.png"" alt=""Buy Me A Coffee"" style=""max-height: 51px;width: 150px !important;"" ></a>

# Worth Knowing

This package sets the `MongoDefaults.GuidRepresentation` to `MongoDB.Bson.GuidRepresentation.Standard` by default, instead of the default driver setting of `MongoDB.Bson.GuidRepresentation.CSharpLegacy`. This can cause issues if you have been using the driver on an existing application previously or if you are using CosmosDB.

You can override this behaviour to enforce legacy behaviour in your app Startup routine like so :

`MongoDbContext.SetGuidRepresentation(MongoDB.Bson.GuidRepresentation.CSharpLegacy)`. More info [here](https://github.com/alexandre-spieser/mongodb-generic-repository/issues/7).

# Usage examples

This repository is meant to be inherited from. 

You are responsible for managing its lifetime, it is advised to setup this repository as a singleton.

Here is an example of repository usage, where the TestRepository is implementing 2 custom methods:

```csharp
    public interface ITestRepository : IBaseMongoRepository
    {
        void DropTestCollection<TDocument>();
        void DropTestCollection<TDocument>(string partitionKey);
    }
    
    public class TestRepository : BaseMongoRepository, ITestRepository
    {
        public TestRepository(string connectionString, string databaseName) : base(connectionString, databaseName)
        {
        }

        public void DropTestCollection<TDocument>()
        {
            MongoDbContext.DropCollection<TDocument>();
        }

        public void DropTestCollection<TDocument>(string partitionKey)
        {
            MongoDbContext.DropCollection<TDocument>(partitionKey);
        }
    }
```
If all your documents have the same type of `Id`, you can use the more specific `BaseMongoRepository<TKey>` where `TKey` is the type of the `Id` of your documents.
```csharp
    public class TestTKeyRepository<TKey> : BaseMongoRepository<TKey>, ITestRepository<TKey> where TKey : IEquatable<TKey>
    {
        const string connectionString = ""mongodb://localhost:27017/MongoDbTests"";
        private static readonly ITestRepository<TKey> _instance = new TestTKeyRepository<TKey>(connectionString);
        /// <inheritdoc />
        private TestTKeyRepository(string connectionString) : base(connectionString)
        {
        }
    }
```

## Instantiation

The repository can be instantiated like so:

```csharp
ITestRepository testRepository = new TestRepository(connectionString, ""MongoDbTests"");
ITestRepository<TKey> testTKeyRepository = new TestTKeyRepository<TKey>(connectionString);
```

If you prefer to reuse the same MongoDb database across your application, you can use the `MongoDatabase` from the MongoDb driver implementing the `IMongoDatabase` interface:

```csharp
var client = new MongoClient(connectionString);
var mongoDbDatabase = Client.GetDatabase(databaseName);
ITestRepository testRepository = new TestRepository(mongoDbDatabase);
```

## Adding documents
To add a document, its class must inherit from the `Document` class,  implement the `IDocument` or `IDocument<TKey>` interface:

```csharp
    public class MyDocument : Document
    {
        public MyDocument()
        {
            Version = 2; // you can bump the version of the document schema if you change it over time
        }
        public string SomeContent { get; set; }
    }
```

The `IDocument` and `IDocument<TKey>` interfaces can be seen below:

```csharp
    /// <summary>
    /// This class represents a basic document that can be stored in MongoDb.
    /// Your document must implement this class in order for the MongoDbRepository to handle them.
    /// </summary>
    public interface IDocument
    {
        Guid Id { get; set; }
        int Version { get; set; }
    }
    
    /// <summary>
    /// This class represents a basic document that can be stored in MongoDb.
    /// Your document must implement this class in order for the MongoDbRepository to handle them.
    /// </summary>
    public interface IDocument<TKey> where TKey : IEquatable<TKey>
    {
        /// <summary>
        /// The Primary Key, which must be decorated with the [BsonId] attribute 
        /// if you want the MongoDb C# driver to consider it to be the document ID.
        /// </summary>
        [BsonId]
        TKey Id { get; set; }
        /// <summary>
        /// A version number, to indicate the version of the schema.
        /// </summary>
        int Version { get; set; }
    }
```

## Partitioned collections
This repository also allows you to partition your document across multiple collections, this can be useful if you are running a SaaS application and want to keep good performance.

To use partitioned collections, you must define your documents using the PartitionedDocument class, which implements the IPartitionedDocument interface:
```csharp
    public class MyPartitionedDocument : PartitionedDocument
    {
        public MyPartitionedDocument(string myPartitionKey) : base(myPartitionKey)
        {
            Version = 1;
        }
        public string SomeContent { get; set; }
    }
```

This partitioned key will be used as a prefix to your collection name.
The collection name is derived from the name of the type of your document, is set to camel case, and is pluralized using a class taken from Humanizer (https://github.com/Humanizr/Humanizer).

```csharp
var myDoc = new MyPartitionedDocument(""myPartitionKey"");
_testRepository.AddOne(myDoc);
```

The above code will generate a collection named `myPartitionKey-myPartitionedDocuments`.

## CollectionName Attribute
It is now possible to change the collection name by using the `CollectionName` attribute:

```csharp
    [CollectionName(""MyCollectionName"")]
    public class MyDocument : Document
    {
        public MyDocument()
        {
            Version = 2;
        }
        public string SomeContent { get; set; }
    }
```
Documents of this type will be inserted into a collection named ""MyCollectionName"".

## Index Management
From version 1.3.8 the `MongoDbGenericRepository` implements the `IBaseMongoRepository_Index` and  `IBaseMongoRepository_Index<TKey>` interfaces. 
This exposes the functionality to programmatically manage indexes against your collections in a generic fashion.

The following methods are exposed and fully integration tested:
+ CreateAscendingIndexAsync
+ CreateDescendingIndexAsync
+ CreateCombinedTextIndexAsync
+ CreateHashedIndexAsync
+ CreateTextIndexAsync
+ DropIndexAsync
+ GetIndexesNamesAsync

Usage examples:  
```csharp
	string expectedIndexName = $""myCustomIndexName"";
	var option = new IndexCreationOptions
	{
		Name = expectedIndexName
	};
	// Act
	// create a text index against the Version property of the document.
	var result = await SUT.CreateTextIndexAsync<T, TKey>(x => x.Version, option, PartitionKey);

	// Assert
	var listOfIndexNames = await SUT.GetIndexesNamesAsync<T, TKey>(PartitionKey);
	Assert.Contains(expectedIndexName, listOfIndexNames);

	// Cleanup
	await SUT.DropIndexAsync<T, TKey>(expectedIndexName, PartitionKey);
```

Please refer to the IntegrationTests (NET45) and CoreIntegrationTests (netstandard2.0) projects for more usage examples.
The `CoreIntegrationTests.Infrastructure.MongoDbTKeyDocumentTestBase<T, TKey>` test class is a good start.

## Author
**Alexandre Spieser**

## License
mongodb-generic-repository is under MIT license - http://www.opensource.org/licenses/mit-license.php

The MIT License (MIT)

Copyright (c) 2016-2019 Alexandre Spieser

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

==============================================================================

Inflector (https://github.com/srkirkland/Inflector)
The MIT License (MIT)
Copyright (c) 2013 Scott Kirkland

==============================================================================

Humanizer (https://github.com/Humanizr/Humanizer)
The MIT License (MIT)
Copyright (c) 2012-2014 Mehdi Khalili (http://omar.io)

==============================================================================

## Copyright
Copyright © 2019
"
129,Guilouz/repository.guilouz,Python,"## Welcome to my Repository !


### • Changes for Estuary MOD for Kodi 17 Krypton : [Here](https://raw.githubusercontent.com/Guilouz/repository.guilouz/master/skin.estuary.mod/changelog.txt)


### • Changes for Estuary MOD V2 for Kodi 18 Leia : [Here](https://raw.githubusercontent.com/Guilouz/repository.guilouz/master/skin.estuary.modv2/changelog.txt)


### • Changes for Movie Genres Icons for Estuary MOD V2 : [Here](https://raw.githubusercontent.com/Guilouz/repository.guilouz/master/resource.images.moviegenreicons.estuarymod/changelog.txt)


### • Installation :

- In Kodi go to ""Settings / Add-ons / Install from Zip file"".
- Select downloaded ZIP file.


### • Download my Repository :

[ ![Download](http://i.imgur.com/CYTD9na.png) ](https://github.com/Guilouz/repository.guilouz/raw/master/_repo/repository.guilouz/repository.guilouz-1.0.3.zip)

### • Donation :

If you like my work and would like to buy me a beer or a coffee, I would appreciate :)

[ ![Download](http://i.imgur.com/zNN1EYG.png) ](https://www.paypal.me/CyrilGuislain)

### • Support on Kodi forum :

[ ![Download](http://i.imgur.com/hA3nKiV.png) ](https://forum.kodi.tv/showthread.php?tid=306757)"
130,jitsi/jitsi-maven-repository,,"Jitsi Maven Repository

Bellow is a basic script that describes the basic workflow of deploying a module
and updating the Jitsi Maven Repository.

```sh
# We use the packet-logging module as an example. Change accordingly.
MODULE_NAME=jitsi-packetlogging
MODULE_HOME=$HOME/src/jitsi/jitsi/m2/$MODULE_NAME
REPO_HOME=$HOME/src/jitsi/jitsi-maven-repository

# Deploy the module.
cd $MODULE_HOME/
mvn deploy -DaltDeploymentRepository=jmrs::default::file://$REPO_HOME/snapshots

# Update the maven repository.
cd $REPO_HOME/
git pull -r origin master
git add snapshots/
git commit -m ""Updates $MODULE_NAME.""
git push origin master
```
"
131,freeall/create-repository,JavaScript,"# create-repository

Easily set up a new github repository. Reads the name/description from the package.json file if it's present. Sets origin upstream if it's not already set.

```
npm install create-repository -g
```

## Usage

`create-repository` will try to read `package.json` and use the name and description properties.

```
$ create-repository
```

You can also pass values for name and description.

```
$ create-repository --name my-new-project --description ""That's all I have to say about that""
```

## License

MIT"
132,acoomans/ACCodeSnippetRepositoryPlugin,Objective-C,"# ACCodeSnippetRepositoryPlugin

ACCodeSnippetRepositoryPlugin is a Xcode plugin for seemless synchronization of snippets with a git repository.

The snippets are synchronized as human-readable text (and not an obscure _.codesnippet_ plist).

[![Build Status](https://api.travis-ci.org/acoomans/ACCodeSnippetRepositoryPlugin.png)](https://api.travis-ci.org/acoomans/ACCodeSnippetRepositoryPlugin.png)

If you want to know more about snippets in Xcode, I recommend reading [Xcode Snippets on NSHipster](http://nshipster.com/xcode-snippets/).

Want to try the plugin with an existing public repository? Try [acoomans](https://github.com/acoomans/xcode-snippets) or [mattt](https://github.com/mattt/Xcode-Snippets.git)'s snippets.


## Install

1. Build the project to install the plugin. The plugin is installed in `/Library/Application Support/Developer/Shared/Xcode/Plug-ins/ACCodeSnippetRepository.xcplugin`.

2. Restart Xcode for the plugin to be activated.

Alternatively, install through [Alcatraz](https://github.com/supermarin/Alcatraz) plugin manager.


## Configuration

There should be a `Plug-ins` item in the xcode menu:

![screenshots](Screenshots/screenshot01.png)

First configure the plugin by forking a remote repository:

![screenshots](Screenshots/screenshot02.png)

When you fork, all snippets from the repository (with the right format, see below _Format_) will be imported in Xcode. Your existing snippets, that do not belong to any repository yet, will not be affected.

After forking, you will given the choice to import (user) snippets from Xcode to the repository.

Also, you can remove the system snippets if you don't use them.

In case of any problem (see below, _Bugs and limitations_), you can go to the user snippets folder and backup your user snippets. Note that user snippets are automatically backed up before any fork.


## Usage

Use snippets like you always do in Xcode. Snippets are automatically added/edited/removed and pushed to the git repository.

![screenshots](Screenshots/screenshot03.png)

### Format 

The snippets are text files with a _.m_ file extension, in the following format:

* a header, containing:
	* the title
	* description
	* various Xcode-related information in a _Key: Value_ format (keys ordered alphabetically)
* the code

If your snippet does not contain any header, the header will be added, committed and pushed when imported.

**Note**: do not remove the Xcode-related information as they are needed for synchronization.

**Note**: if you manually create a snippet by copying an existing one, change or remove the _IDECodeSnippetIdentifier_ as Xcode does not like having two snippets with the same identifier (will crash).

![screenshots](Screenshots/screenshot04.png)

### Synchronization

Every change in a snippet is commited and pushed independently.

Snippets are synchronized (pulled) with the remote repository is done every 10 minutes (if the option is enabled) or when the `Update snippets` item was chosen in the menu.

In case of conflict during the merge, the remote repository has priority.


## Bugs and limitations

Probably a lot. 

Backup your snippets and your git repository. Expect bugs, crashes, synchronization issues, data and hair loss.


## Contributing

### Architecture

First a quick word on the architecture:

The plugin does some swizzling with the _IDECodeSnippetRepository_ class from _IDECodeSnippetLibrary.ideplugin_ (_/Applications/Xcode.app/Contents/PlugIns/IDECodeSnippetLibrary.ideplugin_).

_IDECodeSnippetRepository_ is modified so to make it possible to attach data stores (_ACCodeSnippetDataStoreProtocol_).

Here's a colourful schematic to make you even more confused:

![screenshots](Documentation/architecture.jpg)

### Pull requests

If you want to contribute, send me a pull request.

### Improvements

Ideas of improvements:

- synchronize _.codesnippet_ plists
- multiple data stores (in progress but need to be careful when synchronizing)
- github's gist support
"
133,singhsanket143/CppCompetitiveRepository,C++,"# CppCompetitiveRepository
All the competitive programming codes done arranged by judges
"
134,omarabid/Self-Hosted-WordPress-Plugin-repository,PHP,"# Self-Hosted-WordPress-Plugin-repository

Create your own self-hosted WordPress Plugin repository for pushing automatic updates.

For integration with Composer, please use [wp-autoupdate](https://github.com/wpplex/wp-autoupdate)

## Quick Start

1) Place the `wp_autoupdate.php` file somewhere in your plugin directory and require it.
```php
require_once( 'wp_autoupdate.php' );
```
2) Hook the [init](https://codex.wordpress.org/Plugin_API/Action_Reference/init) function to initiatilize the update function when your plugin loads. Best put in your main `plugin.php` file:
```php
	function snb_activate_au()
	{
		// set auto-update params
		$plugin_current_version = '<your current version> e.g. ""0.6""';
		$plugin_remote_path     = '<remote path to your update server> e.g. http://update.example.com';
		$plugin_slug            = plugin_basename(__FILE__);
		$license_user           = '<optional license username>';
		$license_key            = '<optional license key>';

		// only perform Auto-Update call if a license_user and license_key is given
		if ( $license_user && $license_key && $plugin_remote_path )
		{
			new wp_autoupdate ($plugin_current_version, $plugin_remote_path, $plugin_slug, $license_user, $license_key);
		}
	}

	add_action('init', 'snb_activate_au');
```

The `license_user` and `license_key` fields are optional. You can use these to implement an auto-update functionility for specified customers only. It's left to the developer to implement this if needed.

Note that it's possible to store certain settings as a Wordpress [option](https://codex.wordpress.org/Options_API) like the `plugin_remote_path` version.
If you do so, you can use `get_option()` to get fields like `plugin_remote_path`, `license_user`, `license_key` directly from your plugin. This increases maintainability.

3) Create your server back-end to handle the update requests. You are fee to implement this any way you want, with any framework you want. 
The idea is that when Wordpress loads your plugin, it will check the given remote path to see if an update is availabe through the returned transient. For a basic implementation see the example below. 

Note however this example does not provide any protection or security, it serves as a demonstration purpose only.

```php
if (isset($_POST['action'])) {
  switch ($_POST['action']) {
    case 'version':
      echo '1.1';
      break;
    case 'info':
      $obj                = new stdClass();
      $obj->slug          = 'plugin.php';
      $obj->plugin_name   = 'plugin.php';
      $obj->new_version   = '1.1';
      $obj->requires      = '3.0';
      $obj->tested        = '3.3.1';
      $obj->downloaded    = 12540;
      $obj->last_updated  = '2012-01-12';
      $obj->sections      = array(
          'description'     => 'The new version of the Auto-Update plugin',
          'another_section' => 'This is another section',
          'changelog'       => 'Some new features'
      );
      $obj->download_link = 'http://localhost/repository/update.zip';
      echo serialize($obj);
    case 'license':
      echo 'false';
      break;
  }
} else {
    header('Cache-Control: public');
    header('Content-Description: File Transfer');
    header('Content-Type: application/zip');
    readfile('update.zip');
}
```

4) Make sure the `download_link` points to a `*.zip` file that holds the new version of your plugin. This `*.zip` file must have the same name as your WordPress plugin does. Also the `*.zip` file must NOT contain the plugin files directly, but must have a subfolder with the same name as your plugin to make WordPress play nicely with it.
e.g.:
```php
my-plugin.zip
     │
     └ my-plugin
           │
           ├ my-plugin.php
           ├ README.txt
           ├ uninstall.php
           ├ index.php
           ├ ..
           └ etc.
```

# More information 

You could find detailed explanation and example of usage [here](http://code.tutsplus.com/tutorials/a-guide-to-the-wordpress-http-api-automatic-plugin-updates--wp-25181)
"
135,Vashiel/repository.adulthideout,Python,"# AdultHideout
XXX Porn Adult Addon for Kodi.<br />
The one and only official site for my Repo is this Github Page.<br />
If you find my Repo somewhere else, he/she has nothing to do with me. 

# What it is
Adulthideout is my first try to make a Kodi XXX Addon. This addon was actually made for one Page to add the ""play from here"" function that is missing in nearly all similiar addons, but somehow turned into something bigger. 
# FAQ
Q: How can i remove the Pin?<br />
A: There is no Pin or Password protection in Adulthideout and there never will be.<br />
You are not using original Kodi and/or original AdultHideout.<br />
Most of the time Kodi bundles or addon bundles are the Problem.

Just deinstall Kodi and reinstall it from it's original source https://kodi.tv/download and follow the intructions on this Page to install AdultHideot. Password gone!

Q: Can you add site [addSitenamehere] to AdultHideout?<br />
A: No! There are 30 Sites in Adulthideout and little bit of everything for anyone. AND it's very hard to keep those sites runnig. If we have to remove a Site...maybe.

Q: Is there a develop version of Adulthideout on github?<br />
A: No, not really. But i upload my beta test Version always to https://a-hideout.000webhostapp.com/addons/ with fixes before releasing Adulthideout.

    
# Installation for Kodi 16
1. Download Repo https://github.com/Vashiel/repository.adulthideout/releases/download/Repo/repository.adulthideout-1.0.1.zip
2. Start Kodi
3. Goto ""system> addons> install from zip"" and choose the downloaded Repo zip file.
4. Wait for message repo has been installed
5. Go to addons and choose install from repo (or ""get more"" on older versions of Kodi)
6. Choose adult hideout and click video, then adult hideout
7. Wait till adulthideout has been installed . 
8. You will find Adulthideout now under ""Video - Addons""
9. Have Fun :)

#Video installation instructions on Youtube
For those who still have problems with the installation, i just uploaded Intructions on Youtube. Link below.
https://youtu.be/R8kcLXioEP0

# NEW!!! Installation for Kodi 17 or higher with Estuary skin
#New instalattion tutorial on youtube.

If you can't copy the repo zip file to your Kodi Device, here is a alternative way to install AdultHideout.
https://youtu.be/bFuwYpAcOXE

# Contact me
Easy way is to write me via Github or adulthideout@yandex.com (don't check that one very often)
"
136,notepad-plus-plus/notepad-plus-plus,C++,"What is Notepad++ ?
===================

[![GitHub release](https://img.shields.io/github/release/notepad-plus-plus/notepad-plus-plus.svg)](../../releases/latest)
&nbsp;&nbsp;&nbsp;&nbsp;[![Appveyor build status](https://ci.appveyor.com/api/projects/status/github/notepad-plus-plus/notepad-plus-plus?branch=master&svg=true)](https://ci.appveyor.com/project/donho/notepad-plus-plus)
&nbsp;&nbsp;&nbsp;&nbsp;[![Join the disscussions at https://community.notepad-plus-plus.org/](https://notepad-plus-plus.org/assets/images/NppCommunityBadge.svg)](https://community.notepad-plus-plus.org/)
&nbsp;&nbsp;&nbsp;&nbsp;[![Join the chat at https://gitter.im/notepad-plus-plus/notepad-plus-plus](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/notepad-plus-plus/notepad-plus-plus?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

Notepad++ is a free (free as in both ""free speech"" and ""free beer"") source code
editor and Notepad replacement that supports several programming languages and
natural languages. Running in the MS Windows environment, its use is governed by
[GPL License](LICENSE).

See the [Notepad++ official site](https://notepad-plus-plus.org/) for more information.

Notepad++ Release Key
---------------------
_Since the release of version 7.6.5 Notepad++ is signed using GPG with the following key:_

- **Signer:** Notepad++
- **E-mail:** don.h@free.fr
- **Key ID:** 0x8D84F46E
- **Key fingerprint:** 14BC E436 2749 B2B5 1F8C 7122 6C42 9F1D 8D84 F46E
- **Key type:** RSA 4096/4096
- **Created:** 2019-03-11
- **Expiries:** 2024-03-11

https://github.com/notepad-plus-plus/notepad-plus-plus/blob/master/nppGpgPub.asc


Supported OS
------------

All the Windows systems still supported by Microsoft are supported by Notepad++. However, not all Notepad++ users can or want to use the newest system. Here is the [Supported systems information](SUPPORTED_SYSTEM.md) you may need in case you are one of them.




Build Notepad++
---------------

Please follow [build guide](BUILD.md) to build Notepad++ from source.


Contribution
------------

Code contribution is welcome. Here are some [rules](CONTRIBUTING.md) that your should follow to make your contribution accepted easily. 

[Notepad++ Contributors](https://github.com/notepad-plus-plus/notepad-plus-plus/graphs/contributors)

"
137,crcms/repository,PHP,"## CRCMS Repository

[![Latest Stable Version](https://poser.pugx.org/crcms/repository/v/stable)](https://packagist.org/packages/crcms/repository)
[![License](https://poser.pugx.org/crcms/repository/license)](https://packagist.org/packages/crcms/repository)
[![StyleCI](https://github.styleci.io/repos/75898581/shield?branch=master)](https://github.styleci.io/repos/75898581)

A specialized data provider layer, based on ORM, only as a data provider, the main role is to separate the coupling before the Controller and Model

## Install

You can install the package via composer:

```bash
composer require crcms/repository
```

## Laravel

If your version is less than 5.5 please modify ``config / app.php``

```php
'providers' => [
    CrCms\Repository\RepositoryServiceProvider::class,
]

```

If you'd like to make configuration changes in the configuration file you can pubish it with the following Aritsan command:
```bash
php artisan vendor:publish --provider=""CrCms\Repository\RepositoryServiceProvider""
```

## Commands

```bash
php artisan make:repository TestRepository --model TestModel
```
```bash
php artisan make:magic TestMagic
```

## Example

### QueryMagic
```php

use CrCms\Repository\AbstractMagic;
use CrCms\Repository\Contracts\QueryRelate;

class TestMagic extends AbstractMagic
{
    /**
     * @param QueryRelate $queryRelate
     * @param int $id
     * @return QueryRelate
     */
    protected function byName(QueryRelate $queryRelate, string $name)
    {
        return $queryRelate->where('name', $name);
    }

    /**
     * @param QueryRelate $queryRelate
     * @param string $title
     * @return QueryRelate
     */
    protected function byTitle(QueryRelate $queryRelate, string $title)
    {
        return $queryRelate->where('title', 'like', ""%{$title}%"");
    }

    /**
     * @param QueryRelate $queryRelate
     * @param int $id
     * @return QueryRelate
     */
    protected function byId(QueryRelate $queryRelate, int $id)
    {
        return $queryRelate->where('id', $id);
    }
    
    /**
     * @param QueryRelate $queryRelate
     * @param array $sort
     * @return QueryRelate
     */
    protected function bySort(QueryRelate $queryRelate, array $sort)
    {
        return $queryRelate->orderByArray($sort);
    }
}
```

### Repository
```php
class TestRepository extends AbstractRepository
{
    /**
     * @var array
     */
    protected $guard = [
        'id', 'title','other'
    ];

    /**
     * @return TestModel
     */
    public function newModel(): TestModel
    {
        return app(TestModel::class);
    }

    /**
     * @param int $perPage
     * @return LengthAwarePaginator
     */
    public function paginate(AbstractMagic $magic = null, int $perPage = 15): LengthAwarePaginator
    {
        return $this->whenMagic($magic)->where('built_in', 1)->orderBy($this->getModel()->getKeyName(), 'desc')->paginate($perPage);
    }

    /**
     * @param int $name
     * @param int $title
     */
    public function updateName(string $name, string $title)
    {
        $this->where('name', $name)->update(['title' => $title]);
    }
    
}
```

### Guard Or Scenes

Usually we need to filter the incoming parameter values when adding or modifying and querying the data, and retain the required parameter values.

Guard and scenes are born for this

```php
class TestRepository extends AbstractRepository
{
    /**
     * @var array
     */
    protected $scenes = [
        'create' => ['sort', 'added_at'],
        'modify' => ['sort', 'published_at']
    ];
    
    /**
     * @var array
     */
    protected $guard = [
        'id', 'title', 'other'
    ];
}

$testRepository->create($data, 'create'); //OR
$testRepository->setCurrentScene('create')->create($data); //OR
$testRepository->setGuard(['sort', 'added_at'])->create($data); 

```

```php
class TestMagic extends AbstractMagic
{
    /**
     * @var array
     */
    protected $scenes = [
        'frontend' => ['name'],
        'backend' => ['title']
    ];
    
    /**
     * @var array
     */
    protected $guard = [
        'title',
    ];

    /**
     * @param QueryRelate $queryRelate
     * @param int $id
     * @return QueryRelate
     */
    protected function byName(QueryRelate $queryRelate, string $name)
    {
        return $queryRelate->where('name', $name);
    }

    /**
     * @param QueryRelate $queryRelate
     * @param string $title
     * @return QueryRelate
     */
    protected function byTitle(QueryRelate $queryRelate, string $title)
    {
        return $queryRelate->where('title', 'like', ""%{$title}%"");
    }
}

$testRepository->magic(new TestMagic($data, 'frontend'))->paginate(); //OR
$testRepository->magic((new TestMagic($data))->setCurrentScene('frontend'))->paginate(); //OR
$testRepository->magic((new TestMagic($data))->setGuard(['title']))->paginate(); //OR->create($data);

```

**Note: when guard and scenes are both present, scenes has a higher priority. If scenes is empty, it will use guard.**

### Listener

```php
TestRepository::observer(TestListener::class);

TestListener {

    public function creating(TestRepository $repository, array $data)
    {
		//append the value to be written
		$repository->addData('append_data','value');
		
		//rewrite all values written
		$repository->setData(['key'=>'value']);
    }    
    
    public function created(TestRepository $repository, TestModel $model)
    {
    }    

    public function updating(TestRepository $repository, array $data)
    {
    }    
    
    public function updated(TestRepository $repository, TestModel $model)
    {
    }
    
    public function deleting(TestRepository $repository, array $ids)
    {
    }
    
    public function deleted(TestRepository $repository, Collection $models)
    {
    }
}
```

### Cache

```php
class TestRepository {

    public function do(User $user)
    {
        return $this->byIntId($user->id);
    }
}

$repository = new TestRepository;

```

#### store
```php
$repository->cache()->do(new User);
```

#### forget
```php
$repository->cache()->forget('do')
```

#### flush
```php
$repository->cache()->flush()
```


### Repository Methods
```php
public function all(): Collection;
```
```php
public function get(): Collection;
``` 
```php
public function pluck(string $column, string $key = ''): Collection;
```
```php
public function max(string $column): int;
```
```php
public function count(string $column = '*'): int;
```
```php
public function avg($column): int;
```
```php
public function sum(string $column): int;
```
```php
public function chunk(int $limit, callable $callback): bool;
```
```php
public function valueOfString(string $key, string $default = ''): string;
```
```php
public function valueOfInt(string $key, int $default = 0): int;
```
```php
public function increment(string $column, int $amount = 1, array $extra = []): int;
```
```php
public function decrement(string $column, int $amount = 1, array $extra = []): int;
```
```php
public function delete(): int;
```
```php
public function deleteByStringId(string $id): int;
```
```php
public function deleteByIntId(int $id): int;
```
```php
public function deleteByArray(array $ids): int;
```
```php
public function paginate(int $perPage = 15) : LengthAwarePaginator;
```
```php
public function create(array $data) : Model;
```
```php
public function update(array $data): int;
```
```php
public function updateByIntId(array $data, int $id) : Model;
```
```php
public function updateByStringId(array $data, string $id) : Model;
```
```php
public function byIntId(int $id);
```
```php
public function byStringId(string $id);
```
```php
public function byIntIdOrFail(int $id) : Model;
```
```php
public function byStringIdOrFail(string $id) : Model;
```
```php
public function oneByString(string $field, string $value): Model;
```
```php
public function oneByInt(string $field, int $value): Model;
```
```php
public function oneByStringOrFail(string $field, string $value) : Model;
```
```php
public function oneByIntOrFail(string $field, int $value) : Model;
```
```php
public function first();
```
```php
public function firstOrFail() : Model;
```    

### QueryRelate Methods

```php
public function select(array $column = ['*']): QueryRelate;
```
```php
public function selectRaw(string $expression, array $bindings = []): QueryRelate;
```
```php
public function skip(int $limit): QueryRelate;
```
```php
public function take(int $limit): QueryRelate;
```
```php
public function groupBy(string $column): QueryRelate;
```
```php
public function groupByArray(array $columns): QueryRelate;
```
```php
public function orderBy(string $column, string $sort = 'desc'): QueryRelate;
```
```php
public function orderByArray(array $columns): QueryRelate;
```
```php
public function distinct(): QueryRelate;
```
```php
public function where(string $column, string $operator = '=', string $value = ''): QueryRelate;
```
```php
public function whereClosure(\Closure $callback): QueryRelate;
```
```php
public function orWhereClosure(\Closure $callback): QueryRelate;
```
```php
public function orWhere(string $column, string $operator = '=', string $value = ''): QueryRelate;
```    
```php
public function whereBetween(string $column, array $between): QueryRelate;
```
```php
public function orWhereBetween(string $column, array $between): QueryRelate;
```    
```php
public function whereRaw(string $sql, array $bindings = []): QueryRelate;
```
```php
public function orWhereRaw(string $sql, array $bindings = []): QueryRelate;
```

```php
public function orWhereNotBetween($column, array $between): QueryRelate;
```    

```php
public function whereExists(\Closure $callback): QueryRelate;
```

```php
public function orWhereExists(\Closure $callback): QueryRelate;
```

```php
public function whereNotExists(\Closure $callback): QueryRelate;
```

```php
public function orWhereNotExists(\Closure $callback): QueryRelate;
```
```php
public function whereIn(string $column, array $values): QueryRelate;
```
```php
public function orWhereIn(string $column, array $values): QueryRelate;
```
```php
public function whereNotIn(string $column, array $values): QueryRelate;
```    

```php
public function orWhereNotIn(string $column, array $values): QueryRelate;
```    
```php
public function whereNull(string $column): QueryRelate;
```
```php
public function orWhereNull(string $column): QueryRelate;
```
```php
public function whereNotNull(string $column): QueryRelate;
```
```php
public function orWhereNotNull(string $column): QueryRelate;
```
```php
public function raw(string $sql): QueryRelate;
```    
```php
public function from(string $table): QueryRelate;
```
```php
public function join(string $table, string $one, string $operator = '=', string $two = ''): QueryRelate;
```
```php
public function joinClosure(string $table, \Closure $callback): QueryRelate;
```
```php
public function leftJoin(string $table, string $first, string $operator = '=', string $two = ''): QueryRelate;
```
```php
public function leftJoinClosure(string $table, \Closure $callback): QueryRelate;
```
```php
public function rightJoin(string $table, string $first, string $operator = '=', string $two = ''): QueryRelate;
```
```php
public function rightJoinClosure(string $table, \Closure $callback): QueryRelate;
```
```php
public function callable(callable $callable): QueryRelate;
```
```php
public function wheres(array $wheres): QueryRelate;
```    
```php
public function union(QueryRelate $queryRelate): QueryRelate;
```
```php
public function magic(QueryMagic $queryMagic): QueryRelate;
```
```php
public function whenMagic(?QueryMagic $queryMagic = null): QueryRelate;
```
```php
public function with(string $relation): QueryRelate;
```
```php
public function withArray(array $relations): QueryRelate;
```
```php
public function without(string $relation): QueryRelate;
```
```php
public function withoutArray(array $relations): QueryRelate;
```
```php
public function having(string $column, $operator = null, $value = null): QueryRelate;
```
```php
public function orHaving(string $column, $operator = null, $value = null): QueryRelate;
```
```php
public function havingRaw(string $sql, array $bindings = []): QueryRelate;
```
```php
public function orHavingRaw(string $sql, array $bindings = []): QueryRelate;
```
```php
public function lockForUpdate(): QueryRelate;
```
```php
public function sharedLock(): QueryRelate;
```


## License
[MIT license](https://opensource.org/licenses/MIT)
"
138,scipy/scipy,Python,".. raw:: html

    <p>
      <h1>
        <a href=""https://docs.scipy.org/doc/scipy/reference/""><img valign=""middle"" src=""doc/source/_static/scipyshiny_small.png"" height=""50"" height=""50"" alt=""SciPy logo""/></a>
        SciPy
      </h1>
    </p>

.. image:: https://img.shields.io/circleci/project/github/scipy/scipy/master.svg?label=CircleCI
  :target: https://circleci.com/gh/scipy/scipy

.. image:: https://dev.azure.com/scipy-org/SciPy/_apis/build/status/scipy.scipy?branchName=master
  :target: https://dev.azure.com/scipy-org/SciPy/_build/latest?definitionId=1?branchName=master

.. image:: https://github.com/scipy/scipy/workflows/macOS%20tests/badge.svg?branch=master
  :target: https://github.com/scipy/scipy/actions?query=workflow%3A%22macOS+tests%22

.. image:: https://img.shields.io/pypi/dm/scipy.svg?label=Pypi%20downloads
  :target: https://pypi.org/project/scipy/

.. image:: https://img.shields.io/conda/dn/conda-forge/scipy.svg?label=Conda%20downloads
  :target: https://anaconda.org/conda-forge/scipy

.. image:: https://codecov.io/gh/scipy/scipy/branch/master/graph/badge.svg
  :target: https://codecov.io/gh/scipy/scipy

.. image:: https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg
  :target: https://stackoverflow.com/questions/tagged/scipy

.. image:: https://img.shields.io/badge/DOI-10.1038%2Fs41592--019--0686--2-blue
  :target: https://www.nature.com/articles/s41592-019-0686-2

SciPy (pronounced ""Sigh Pie"") is an open-source software for mathematics,
science, and engineering. It includes modules for statistics, optimization,
integration, linear algebra, Fourier transforms, signal and image processing,
ODE solvers, and more.

- **Website:** https://docs.scipy.org/doc/scipy/reference/
- **Documentation:** https://docs.scipy.org/
- **Mailing list:** https://scipy.org/scipylib/mailing-lists.html
- **Source code:** https://github.com/scipy/scipy
- **Contributing:** https://scipy.github.io/devdocs/dev/index.html
- **Bug reports:** https://github.com/scipy/scipy/issues
- **Code of Conduct:** https://scipy.github.io/devdocs/dev/conduct/code_of_conduct.html
- **Report a security vulnerability:** https://tidelift.com/docs/security
- **Citing in your work:** https://www.scipy.org/citing.html

SciPy is built to work with
NumPy arrays, and provides many user-friendly and efficient numerical routines,
such as routines for numerical integration and optimization. Together, they
run on all popular operating systems, are quick to install, and are free of
charge. NumPy and SciPy are easy to use, but powerful enough to be depended
upon by some of the world's leading scientists and engineers. If you need to
manipulate numbers on a computer and display or publish the results, give
SciPy a try!

For the installation instructions, see `our install
guide <https://scipy.github.io/devdocs/getting_started.html#installation>`__.


Call for Contributions
----------------------

We appreciate and welcome contributions. Small improvements or fixes are always appreciated; issues labeled as ""good
first issue"" may be a good starting point. Have a look at `our contributing
guide <http://scipy.github.io/devdocs/dev/hacking.html>`__.

Writing code isn’t the only way to contribute to SciPy. You can also:

- review pull requests
- triage issues
- develop tutorials, presentations, and other educational materials
- maintain and improve `our website <https://github.com/scipy/scipy.org>`__
- develop graphic design for our brand assets and promotional materials
- help with outreach and onboard new contributors
- write grant proposals and help with other fundraising efforts

If you’re unsure where to start or how your skills fit in, reach out! You can
ask on the mailing list or here, on GitHub, by leaving a
comment on a relevant issue that is already open.

If you are new to contributing to open source, `this
guide <https://opensource.guide/how-to-contribute/>`__ helps explain why, what,
and how to get involved.

.. image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A
  :target: https://numfocus.org
"
139,LeonWuV/FE-blog-repository,,"### 公众号
欢迎大家关注博主的公众号：<strong>猿人说事</strong>
<p align=""center"">
  <img src=""http://storage.360buyimg.com/cdn-upload/yuanRenQR83057a63644441fda8a095ae68c574c5.jpg"" alt=""猿人说事-logo"" width=""150px"" height=""150px""/>
  <br>
</p>

### :bug: 有问题请留言
各位同学可以在issues中提问，无论是实际项目中遇到的问题，或者是技术问题都可以， 大家一起解决:100: :+1: :smile:。

### :book: 文章列表 -- leon


#### :art: css基础
1. [css基础 -- CSS中两种放大zoom和scale的区别.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/css%E5%9F%BA%E7%A1%80/CSS%E4%B8%AD%E4%B8%A4%E7%A7%8D%E6%94%BE%E5%A4%A7zoom%E5%92%8Cscale%E7%9A%84%E5%8C%BA%E5%88%AB.md)
2. [css基础 -- css3自定义滚动条样式写法.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/css%E5%9F%BA%E7%A1%80/css3%E8%87%AA%E5%AE%9A%E4%B9%89%E6%BB%9A%E5%8A%A8%E6%9D%A1%E6%A0%B7%E5%BC%8F%E5%86%99%E6%B3%95.md)
3. [css基础 -- 单冒号before和双冒号before的区别.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/css%E5%9F%BA%E7%A1%80/css%E5%9F%BA%E7%A1%80--%E5%8D%95%E5%86%92%E5%8F%B7before%E5%92%8C%E5%8F%8C%E5%86%92%E5%8F%B7before%E7%9A%84%E5%8C%BA%E5%88%AB.md)
4. [css基础 -- 深入理解opacity和rgba的区别.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/css%E5%9F%BA%E7%A1%80/css%E5%9F%BA%E7%A1%80--%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3opacity%E5%92%8Crgba%E7%9A%84%E5%8C%BA%E5%88%AB.md)
5. [css基础 -- 深入理解弹性盒flex布局.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/css%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E7%9B%92flex%E5%B8%83%E5%B1%80.md)
6. [css基础 -- 用百分比布局时，子元素那些属性值取决于于父元素的高那些属性取决于宽.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/css%E5%9F%BA%E7%A1%80/%E7%94%A8%E7%99%BE%E5%88%86%E6%AF%94%E5%B8%83%E5%B1%80%E6%97%B6%EF%BC%8C%E5%AD%90%E5%85%83%E7%B4%A0%E9%82%A3%E4%BA%9B%E5%B1%9E%E6%80%A7%E5%80%BC%E5%8F%96%E5%86%B3%E4%BA%8E%E4%BA%8E%E7%88%B6%E5%85%83%E7%B4%A0%E7%9A%84%E9%AB%98%E9%82%A3%E4%BA%9B%E5%B1%9E%E6%80%A7%E5%8F%96%E5%86%B3%E4%BA%8E%E5%AE%BD.md)


#### js基础
1. [js基础 -- Date.parse()与Date.getTime()方法详解.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/Date.parse()%E4%B8%8EDate.getTime()%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3.md)
2. [js基础 -- JSON.parse() 报错Unexpected token o in JSON at position 1.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/JSON.parse()%20%E6%8A%A5%E9%94%99Unexpected%20token%20o%20in%20JSON%20at%20position%201.md)
3. [js基础 -- javaScript数据类型你都弄明白了吗？绝对干货.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/javaScript%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%BD%A0%E9%83%BD%E5%BC%84%E6%98%8E%E7%99%BD%E4%BA%86%E5%90%97%EF%BC%9F%E7%BB%9D%E5%AF%B9%E5%B9%B2%E8%B4%A7.md)
4. [js基础 -- js中setTimeout问题.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/js%E5%9F%BA%E7%A1%80--setTimeout%E9%97%AE%E9%A2%98.md)
5. [js基础 -- 如何判断浏览器标签页是隐藏或者显示状态.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/js%E5%9F%BA%E7%A1%80--%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E6%B5%8F%E8%A7%88%E5%99%A8%E6%A0%87%E7%AD%BE%E9%A1%B5%E6%98%AF%E9%9A%90%E8%97%8F%E6%88%96%E8%80%85%E6%98%BE%E7%A4%BA%E7%8A%B6%E6%80%81.md)
6. [js基础 -- 获取浏览器当前页面的滚动条高度的兼容写法.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/js%E5%9F%BA%E7%A1%80--%E8%8E%B7%E5%8F%96%E6%B5%8F%E8%A7%88%E5%99%A8%E5%BD%93%E5%89%8D%E9%A1%B5%E9%9D%A2%E7%9A%84%E6%BB%9A%E5%8A%A8%E6%9D%A1%E9%AB%98%E5%BA%A6%E7%9A%84%E5%85%BC%E5%AE%B9%E5%86%99%E6%B3%95.md)
7. [js基础 -- js编码的实用技巧(一).md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/js%E7%BC%96%E7%A0%81%E7%9A%84%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7(%E4%B8%80).md)
8. [js基础 -- js编码的实用技巧(二).md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/js%E7%BC%96%E7%A0%81%E7%9A%84%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7(%E4%B8%80).md)
9. [js基础 --  slice（）与splice（）的用法和区别你清楚吗？.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/slice%EF%BC%88%EF%BC%89%E4%B8%8Esplice%EF%BC%88%EF%BC%89%E7%9A%84%E7%94%A8%E6%B3%95%E5%92%8C%E5%8C%BA%E5%88%AB%E4%BD%A0%E6%B8%85%E6%A5%9A%E5%90%97%EF%BC%9F.md)
10. [js基础 -- 图片上传时实现本地预览功能的原理.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0%E6%97%B6%E5%AE%9E%E7%8E%B0%E6%9C%AC%E5%9C%B0%E9%A2%84%E8%A7%88%E5%8A%9F%E8%83%BD%E7%9A%84%E5%8E%9F%E7%90%86.md)
11. [js基础 -- 在iframe标签中如何操作父页面中的元素、方法、变量.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/%E5%A6%82%E4%BD%95%E6%93%8D%E4%BD%9Ciframe%E7%88%B6%E9%A1%B5%E9%9D%A2%E4%B8%AD%E7%9A%84%E5%85%83%E7%B4%A0%E3%80%81%E6%96%B9%E6%B3%95%E3%80%81%E5%8F%98%E9%87%8F.md)
12. [js基础 -- window.btoa和window.atob使用详解](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/js%E5%9F%BA%E7%A1%80%20--%20window.btoa%E5%92%8Cwindow.atob%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3.md)
13. [js基础--测试随机数的概率是否相等](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/js%E5%9F%BA%E7%A1%80--%E6%B5%8B%E8%AF%95%E9%9A%8F%E6%9C%BA%E6%95%B0%E7%9A%84%E6%A6%82%E7%8E%87%E6%98%AF%E5%90%A6%E7%9B%B8%E7%AD%89.md)
14. [js基础--将内存中的数据保存为文件下载到本地](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/js%E5%9F%BA%E7%A1%80--%E5%B0%86%E5%86%85%E5%AD%98%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BF%9D%E5%AD%98%E4%B8%BA%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD%E5%88%B0%E6%9C%AC%E5%9C%B0.md)
15. [js基础--数据类型检测的相关知识](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80/js%E5%9F%BA%E7%A1%80--%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%A3%80%E6%B5%8B%E7%9A%84%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86.md)

#### js基础进阶
1. [js基础进阶 -- 关于Array.prototype.slice.call(arguments) 的思考.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80%E8%BF%9B%E9%98%B6/js%E5%9F%BA%E7%A1%80%E8%BF%9B%E9%98%B6--%E5%85%B3%E4%BA%8EArray.prototype.slice.call(arguments)%20%E7%9A%84%E6%80%9D%E8%80%83.md)
2. [js基础进阶 -- 函数柯里化carrying.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80%E8%BF%9B%E9%98%B6/js%E5%9F%BA%E7%A1%80%E8%BF%9B%E9%98%B6--%E5%87%BD%E6%95%B0%E6%9F%AF%E9%87%8C%E5%8C%96carrying.md)
3. [js基础进阶--从ajax到fetch的理解](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80%E8%BF%9B%E9%98%B6/js%E5%9F%BA%E7%A1%80%E8%BF%9B%E9%98%B6--%E4%BB%8Eajax%E5%88%B0fetch%E7%9A%84%E7%90%86%E8%A7%A3.md)
4. [js基础进阶--promise和setTimeout执行顺序的问题](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E5%9F%BA%E7%A1%80%E8%BF%9B%E9%98%B6/js%E5%9F%BA%E7%A1%80%E8%BF%9B%E9%98%B6--promise%E5%92%8CsetTimeout%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%E7%9A%84%E9%97%AE%E9%A2%98.md)


#### js简单算法
1. [js简单算法 -- js中的几种随机排序方案.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E7%AE%80%E5%8D%95%E7%AE%97%E6%B3%95/js%E4%B8%AD%E7%9A%84%E5%87%A0%E7%A7%8D%E9%9A%8F%E6%9C%BA%E6%8E%92%E5%BA%8F%E6%96%B9%E6%A1%88.md)
2. [js简单算法 -- js中的快速排序.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E7%AE%80%E5%8D%95%E7%AE%97%E6%B3%95/js%E4%B8%AD%E7%9A%84%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F.md)
3. [js简单算法 -- js获取一个字符串中个数最多的字母.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E7%AE%80%E5%8D%95%E7%AE%97%E6%B3%95/js%E8%8E%B7%E5%8F%96%E4%B8%80%E4%B8%AA%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E4%B8%AA%E6%95%B0%E6%9C%80%E5%A4%9A%E7%9A%84%E5%AD%97%E6%AF%8D.md)

#### js设计模式
1. [js模块化的发展历程.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/js%E6%A8%A1%E5%9D%97%E5%8C%96%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B.md)
2. [JavaScript设计模式--观察者模式（发布者-订阅者模式）.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/js%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/JavaScript%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F--%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%8F%91%E5%B8%83%E8%80%85-%E8%AE%A2%E9%98%85%E8%80%85%E6%A8%A1%E5%BC%8F%EF%BC%89.md)

#### vue
1. [--save-dev 与 --save的区别.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/--save-dev%20%E4%B8%8E%20--save%E7%9A%84%E5%8C%BA%E5%88%AB.md)
2. [vue.js开发神器devtools的安装方法.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/vue.js%E5%BC%80%E5%8F%91%E7%A5%9E%E5%99%A8devtools%E7%9A%84%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95.md)
3. [vue中v-if和v-show的区别.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/vue%E4%B8%ADv-if%E5%92%8Cv-show%E7%9A%84%E5%8C%BA%E5%88%AB.md)
4. [vue中如何实现样式之间的切换.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/vue%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%A0%B7%E5%BC%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E5%88%87%E6%8D%A2.md)
5. [vue数据绑定数组，改变元素时不更新view问题.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/vue%E6%95%B0%E6%8D%AE%E7%BB%91%E5%AE%9A%E6%95%B0%E7%BB%84%EF%BC%8C%E6%94%B9%E5%8F%98%E5%85%83%E7%B4%A0%E6%97%B6%E4%B8%8D%E6%9B%B4%E6%96%B0view%E9%97%AE%E9%A2%98.md)
6. [我们为什么要用vue，他解决了什么问题，如何使用它？（转载）.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/%E6%88%91%E4%BB%AC%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8vue%EF%BC%8C%E4%BB%96%E8%A7%A3%E5%86%B3%E4%BA%86%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%8C%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%EF%BC%9F%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89.md)
7. [vue -- 如何去掉url默认的锚点#](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/vue%20--%20%E5%A6%82%E4%BD%95%E5%8E%BB%E6%8E%89url%E9%BB%98%E8%AE%A4%E7%9A%84%E9%94%9A%E7%82%B9%23.md)
8. [vue -- vue-router的history模式打包后页面空白的解决方案](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/vue%20--%20vue-router%E7%9A%84history%E6%A8%A1%E5%BC%8F%E6%89%93%E5%8C%85%E5%90%8E%E9%A1%B5%E9%9D%A2%E7%A9%BA%E7%99%BD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md)
9. [《深入浅出Vue.js》作者谈前端框架的“御剑之道”](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/%E3%80%8A%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAVue.js%E3%80%8B%E4%BD%9C%E8%80%85%E8%B0%88%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6%E7%9A%84%E2%80%9C%E5%BE%A1%E5%89%91%E4%B9%8B%E9%81%93%E2%80%9D.md)
10. [You are using the runtime-only build of Vue where the template compiler is not available. Either pre-compile the templates into render functions, or use the compiler-included build](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/You%20are%20using%20the%20runtime-only%20build%20of%20Vue%20where%20the%20template%20compiler%20is%20not%20a.md)
11. [vue -- Cannot set property 'render' of undefined解决方法](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/vue%20--%20Cannot%20set%20property%20'render'%20of%20undefined%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95.md)
12. [vue -- 父组件通过$refs获取子组件的值和方法](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/vue%20--%20%E7%88%B6%E7%BB%84%E4%BB%B6%E9%80%9A%E8%BF%87%24refs%E8%8E%B7%E5%8F%96%E5%AD%90%E7%BB%84%E4%BB%B6%E7%9A%84%E5%80%BC%E5%92%8C%E6%96%B9%E6%B3%95.md)
13. [vue -- 非父子组件传值，事件总线（eventbus）的使用方式](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/vue%20--%20%E9%9D%9E%E7%88%B6%E5%AD%90%E7%BB%84%E4%BB%B6%E4%BC%A0%E5%80%BC%EF%BC%8C%E4%BA%8B%E4%BB%B6%E6%80%BB%E7%BA%BF%EF%BC%88eventbus%EF%BC%89%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F.md)
14. [vue -- foreach not a function 或者map not a function的解决办吧](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/vue%20--%20foreach%20not%20a%20function%20%E6%88%96%E8%80%85map%20not%20a%20function%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E5%90%A7.md)
15. [vue -- vue-i18n国际化使用简单教程](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/vue%20--%20vue-i18n%E5%9B%BD%E9%99%85%E5%8C%96%E4%BD%BF%E7%94%A8%E7%AE%80%E5%8D%95%E6%95%99%E7%A8%8B.md)
16. [vue--为什么data属性必须是一个函数](https://github.com/LeonWuV/FE-blog-repository/blob/master/vue/vue--%E4%B8%BA%E4%BB%80%E4%B9%88data%E5%B1%9E%E6%80%A7%E5%BF%85%E9%A1%BB%E6%98%AF%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0.md)
16. <a href=""/vue/Vue报错 -- the 'scope' attribute for scoped slots have been deprecated and replaced by 'slot-scope'.md"">Vue报错 -- the 'scope' attribute for scoped slots have been deprecated and replaced by 'slot-scope'</a>

#### jQuery
1. [ie11 Object doesn't support property or method 'attachEvent'.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/jQuery/ie11%20Object%20doesn't%20support%20property%20or%20method%20'attachEvent'.md)
2. [jQuery如何增加、修改、删除一个jQuery对象的class类.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/jQuery/jQuery%E5%A6%82%E4%BD%95%E5%A2%9E%E5%8A%A0%E3%80%81%E4%BF%AE%E6%94%B9%E3%80%81%E5%88%A0%E9%99%A4%E4%B8%80%E4%B8%AAjQuery%E5%AF%B9%E8%B1%A1%E7%9A%84class%E7%B1%BB.md)

#### es6
1. [es6 -- function](https://github.com/LeonWuV/FE-blog-repository/blob/master/es6/es6--function.md)
2. [es6 -- Array.from()函数的用法](https://github.com/LeonWuV/FE-blog-repository/blob/master/es6/es6%20--%20Array.from()%E5%87%BD%E6%95%B0%E7%9A%84%E7%94%A8%E6%B3%95.md)
3. [es6 -- 默认参数Default，不定参数Rest，扩展运算符Spread详解](https://github.com/LeonWuV/FE-blog-repository/blob/master/es6/es6%20--%20%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0Default%EF%BC%8C%E4%B8%8D%E5%AE%9A%E5%8F%82%E6%95%B0Rest%EF%BC%8C%E6%89%A9%E5%B1%95%E8%BF%90%E7%AE%97%E7%AC%A6Spread%E8%AF%A6%E8%A7%A3.md)

### webpack
1. [webpack -- 打包时如何将资源和图片引用绝对路径改为相对路径.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/webpack/webpack%20--%20%E6%89%93%E5%8C%85%E6%97%B6%E5%A6%82%E4%BD%95%E5%B0%86%E8%B5%84%E6%BA%90%E5%92%8C%E5%9B%BE%E7%89%87%E5%BC%95%E7%94%A8%E7%BB%9D%E5%AF%B9%E8%B7%AF%E5%BE%84%E6%94%B9%E4%B8%BA%E7%9B%B8%E5%AF%B9%E8%B7%AF%E5%BE%84.md)
2. [webpack-dev-server启动时报错.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/webpack/webpack-dev-server%E5%90%AF%E5%8A%A8%E6%97%B6%E6%8A%A5%E9%94%99.md)
3. [webpack -- 关于proxyTable的配置在开发环境和生产环境中的原理解析.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/webpack/webpack%20--%20%E5%85%B3%E4%BA%8EproxyTable%E7%9A%84%E9%85%8D%E7%BD%AE%E5%9C%A8%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%92%8C%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90.md)
4. [webpack -- require和import机制.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/webpack/webpack%20--%20require%E5%92%8Cimport%E6%9C%BA%E5%88%B6.md)
5. [webpack -- 简单的使用总结.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/webpack/webpack%20--%20%E7%AE%80%E5%8D%95%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93.md)
6. [Missing space before function parentheses 报错终极解决方案](https://github.com/LeonWuV/FE-blog-repository/blob/master/webpack/Missing%20space%20before%20function%20parentheses%20%E6%8A%A5%E9%94%99%E7%BB%88%E6%9E%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md)

### nodejs
1. [commonjs,es6模块的编写规则，适用于node，webpack，rollup.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/nodejs/commonjs%2Ces6%E6%A8%A1%E5%9D%97%E7%9A%84%E7%BC%96%E5%86%99%E8%A7%84%E5%88%99%EF%BC%8C%E9%80%82%E7%94%A8%E4%BA%8Enode%EF%BC%8Cwebpack%EF%BC%8Crollup.md)
2. [nodejs入门（一）](https://github.com/LeonWuV/FE-blog-repository/blob/master/nodejs/nodejs%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89.md)
3. [nodejs -- node-sass安装失败的解决方案](https://github.com/LeonWuV/FE-blog-repository/blob/master/nodejs/nodejs%20--%20node-sass%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md)
4. [nodejs -- Node Sass does not yet support your current environment解决办法](https://github.com/LeonWuV/FE-blog-repository/blob/master/nodejs/nodejs%20--%20Node%20Sass%20does%20not%20yet%20support%20your%20current%20environment%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95.md)
5. [npm常见命令](https://github.com/LeonWuV/FE-blog-repository/blob/master/nodejs/npm%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4.md)
6. [nodejs--自动重启工具nodemon简介](https://github.com/LeonWuV/FE-blog-repository/blob/master/nodejs/nodejs--%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AF%E5%B7%A5%E5%85%B7nodemon%E7%AE%80%E4%BB%8B.md)

#### 前端优化
1. [关于重绘（repaint）和回流（reflow）的理解.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/%E5%89%8D%E7%AB%AF%E4%BC%98%E5%8C%96/%E5%85%B3%E4%BA%8E%E9%87%8D%E7%BB%98%EF%BC%88repaint%EF%BC%89%E5%92%8C%E5%9B%9E%E6%B5%81%EF%BC%88reflow%EF%BC%89%E7%9A%84%E7%90%86%E8%A7%A3.md)

#### 开发bug集合
1. [bug集合js1--Unexpected token o in JSON at position 1](https://github.com/LeonWuV/FE-blog-repository/blob/master/%E5%BC%80%E5%8F%91bug%E9%9B%86%E5%90%88/bug%E9%9B%86%E5%90%88js1--Unexpected%20token%20o%20in%20JSON%20at%20position%201.md)

#### 微信小程序
1. [微信小程序错误：WAService.js 3 navigateTo fail url.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E9%94%99%E8%AF%AF%EF%BC%9AWAService.js%203%20navigateTo%20fail%20url.md)

#### 一些插件的API详解
1. [回到顶部的插件topjs，支持自定义，使用简单，带动画效果.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/%E6%8F%92%E4%BB%B6API%E8%AF%A6%E8%A7%A3/%E8%87%AA%E5%B7%B1%E5%86%99%E4%BA%86%E4%B8%80%E4%B8%AA%E5%9B%9E%E5%88%B0%E9%A1%B6%E9%83%A8%E7%9A%84%E6%8F%92%E4%BB%B6topjs%EF%BC%8C%E6%94%AF%E6%8C%81%E8%87%AA%E5%AE%9A%E4%B9%89%EF%BC%8C%E4%BD%BF%E7%94%A8%E7%AE%80%E5%8D%95%EF%BC%8C%E5%B8%A6%E5%8A%A8%E7%94%BB%E6%95%88%E6%9E%9C.md)

#### 码农工具
1. [cmder 添加到右键菜单时提示 Access is denied解决办法.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/%E7%A0%81%E5%86%9C%E5%B7%A5%E5%85%B7/cmder%20%E6%B7%BB%E5%8A%A0%E5%88%B0%E5%8F%B3%E9%94%AE%E8%8F%9C%E5%8D%95%E6%97%B6%E6%8F%90%E7%A4%BA%20Access%20is%20denied%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95.md)
2. [vs code 常用插件.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/%E7%A0%81%E5%86%9C%E5%B7%A5%E5%85%B7/vs%20code%20%E5%B8%B8%E7%94%A8%E6%8F%92%E4%BB%B6.md)
3. [一个比tree命令结果更加优美的目录结构小工具treer.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/%E7%A0%81%E5%86%9C%E5%B7%A5%E5%85%B7/%E4%B8%80%E4%B8%AA%E6%AF%94tree%E5%91%BD%E4%BB%A4%E7%BB%93%E6%9E%9C%E6%9B%B4%E5%8A%A0%E4%BC%98%E7%BE%8E%E7%9A%84%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E5%B0%8F%E5%B7%A5%E5%85%B7treer.md)
4. [Prettier的三种使用场景和使用方法](https://github.com/LeonWuV/FE-blog-repository/blob/master/%E7%A0%81%E5%86%9C%E5%B7%A5%E5%85%B7/Prettier%E7%9A%84%E4%B8%89%E7%A7%8D%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E5%92%8C%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.md)

#### git
1. [git bash、cmder 下操作文件及文件夹相关命令.md](https://github.com/LeonWuV/FE-blog-repository/blob/master/git/git%20bash%E3%80%81cmder%20%E4%B8%8B%E6%93%8D%E4%BD%9C%E6%96%87%E4%BB%B6%E5%8F%8A%E6%96%87%E4%BB%B6%E5%A4%B9%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4.md)
2. [git -- git emoji列表（github commit 前面的小icon）](https://github.com/LeonWuV/FE-blog-repository/blob/master/git/git%20--%20git%20emoji%E5%88%97%E8%A1%A8%EF%BC%88github%20commit%20%E5%89%8D%E9%9D%A2%E7%9A%84%E5%B0%8Ficon%EF%BC%89.md)
3. [git--git tag相关命令和实践记录](https://github.com/LeonWuV/FE-blog-repository/blob/master/git/git--git%20tag%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4%E5%92%8C%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95.md)

#### less
1. [Class constructor FileManager cannot be invoked without 'new'](https://github.com/LeonWuV/FE-blog-repository/blob/master/less/Class%20constructor%20FileManager%20cannot%20be%20invoked%20without%20'new'.md)


#### hexo
1. [hexo系列问题之我们换了电脑怎么办](https://github.com/LeonWuV/FE-blog-repository/blob/master/hexo/hexo%E7%B3%BB%E5%88%97%E9%97%AE%E9%A2%98%E4%B9%8B%E6%88%91%E4%BB%AC%E6%8D%A2%E4%BA%86%E7%94%B5%E8%84%91%E6%80%8E%E4%B9%88%E5%8A%9E.md)
2. [hexo系列问题之部署到github时会删掉README文件](https://github.com/LeonWuV/FE-blog-repository/blob/master/hexo/hexo%E7%B3%BB%E5%88%97%E9%97%AE%E9%A2%98%E4%B9%8B%E9%83%A8%E7%BD%B2%E5%88%B0github%E6%97%B6%E4%BC%9A%E5%88%A0%E6%8E%89README%E6%96%87%E4%BB%B6.md)

#### linux
1. <a href=""/linux/Mac如何生成并配置多个ssh秘钥.md"">Mac如何生成并配置多个ssh秘钥</a>

#### angular
1. [angular.foreach 的使用方法](https://github.com/LeonWuV/FE-blog-repository/blob/master/angular/angular.foreach%20%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.md)

#### 杂谈与思考
1. [你get了无数技能，为啥却一事无成](https://github.com/LeonWuV/FE-blog-repository/blob/master/%E6%9D%82%E8%B0%88%E4%B8%8E%E6%80%9D%E8%80%83/%E4%BD%A0get%E4%BA%86%E6%97%A0%E6%95%B0%E6%8A%80%E8%83%BD%EF%BC%8C%E4%B8%BA%E5%95%A5%E5%8D%B4%E4%B8%80%E4%BA%8B%E6%97%A0%E6%88%90.md)
2. [专业程序员的七个特质](https://github.com/LeonWuV/FE-blog-repository/blob/master/%E6%9D%82%E8%B0%88%E4%B8%8E%E6%80%9D%E8%80%83/%E4%B8%93%E4%B8%9A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E4%B8%83%E4%B8%AA%E7%89%B9%E8%B4%A8.md)
3. [提高程序员编程能力的有效方法](https://github.com/LeonWuV/FE-blog-repository/blob/master/%E6%9D%82%E8%B0%88%E4%B8%8E%E6%80%9D%E8%80%83/%E6%8F%90%E9%AB%98%E7%A8%8B%E5%BA%8F%E5%91%98%E7%BC%96%E7%A8%8B%E8%83%BD%E5%8A%9B%E7%9A%84%E6%9C%89%E6%95%88%E6%96%B9%E6%B3%95.md)

"
140,DSpace/DSpace,Java,"
# DSpace

[![Build Status](https://github.com/DSpace/DSpace/workflows/Build/badge.svg)](https://github.com/DSpace/DSpace/actions?query=workflow%3ABuild)

[DSpace Documentation](https://wiki.lyrasis.org/display/DSDOC/) |
[DSpace Releases](https://github.com/DSpace/DSpace/releases) |
[DSpace Wiki](https://wiki.lyrasis.org/display/DSPACE/Home) |
[Support](https://wiki.lyrasis.org/display/DSPACE/Support)

DSpace open source software is a turnkey repository application used by more than
2,000 organizations and institutions worldwide to provide durable access to digital resources.
For more information, visit http://www.dspace.org/

***
:warning: **Work on DSpace 7 has begun on our `main` branch.** This means that there is NO user interface on this `main` branch. DSpace 7 will feature a new, unified [Angular](https://angular.io/) user interface, along with an enhanced, rebuilt REST API. The latest status of this work can be found on the [DSpace 7 Working Group](https://wiki.lyrasis.org/display/DSPACE/DSpace+7+Working+Group) page.  Additionally, the codebases can be found in the following places:
  * DSpace 7 REST API work is occurring on the [`main` branch](https://github.com/DSpace/DSpace/tree/main/dspace-server-webapp) of this repository.
     * The REST Contract is at https://github.com/DSpace/Rest7Contract
  * DSpace 7 Angular UI work is occurring at https://github.com/DSpace/dspace-angular

**If you would like to get involved in our DSpace 7 development effort, we welcome new contributors.** Just join one of our meetings or get in touch via Slack. See the [DSpace 7 Working Group](https://wiki.lyrasis.org/display/DSPACE/DSpace+7+Working+Group) wiki page for more info.

**If you are looking for the ongoing maintenance work for DSpace 6 (or prior releases)**, you can find that work on the corresponding maintenance branch (e.g. [`dspace-6_x`](https://github.com/DSpace/DSpace/tree/dspace-6_x)) in this repository.
***

## Downloads

The latest release of DSpace can be downloaded from the [DSpace website](http://www.dspace.org/latest-release/) or from [GitHub](https://github.com/DSpace/DSpace/releases).

Past releases are all available via GitHub at https://github.com/DSpace/DSpace/releases

## Documentation / Installation

Documentation for each release may be viewed online or downloaded via our [Documentation Wiki](https://wiki.lyrasis.org/display/DSDOC/).

The latest DSpace Installation instructions are available at:
https://wiki.lyrasis.org/display/DSDOC7x/Installing+DSpace

Please be aware that, as a Java web application, DSpace requires a database (PostgreSQL or Oracle)
and a servlet container (usually Tomcat) in order to function.
More information about these and all other prerequisites can be found in the Installation instructions above.

## Running DSpace 7 in Docker
See [Running DSpace 7 with Docker Compose](dspace/src/main/docker-compose/README.md)

## Contributing

DSpace is a community built and supported project. We do not have a centralized development or support team,
but have a dedicated group of volunteers who help us improve the software, documentation, resources, etc.

We welcome contributions of any type. Here's a few basic guides that provide suggestions for contributing to DSpace:
* [How to Contribute to DSpace](https://wiki.lyrasis.org/display/DSPACE/How+to+Contribute+to+DSpace): How to contribute in general (via code, documentation, bug reports, expertise, etc)
* [Code Contribution Guidelines](https://wiki.lyrasis.org/display/DSPACE/Code+Contribution+Guidelines): How to give back code or contribute features, bug fixes, etc.
* [DSpace Community Advisory Team (DCAT)](https://wiki.lyrasis.org/display/cmtygp/DSpace+Community+Advisory+Team): If you are not a developer, we also have an interest group specifically for repository managers. The DCAT group meets virtually, once a month, and sends open invitations to join their meetings via the [DCAT mailing list](https://groups.google.com/d/forum/DSpaceCommunityAdvisoryTeam).

We also encourage GitHub Pull Requests (PRs) at any time. Please see our [Development with Git](https://wiki.lyrasis.org/display/DSPACE/Development+with+Git) guide for more info.

In addition, a listing of all known contributors to DSpace software can be
found online at: https://wiki.lyrasis.org/display/DSPACE/DSpaceContributors

## Getting Help

DSpace provides public mailing lists where you can post questions or raise topics for discussion.
We welcome everyone to participate in these lists:

* [dspace-community@googlegroups.com](https://groups.google.com/d/forum/dspace-community) : General discussion about DSpace platform, announcements, sharing of best practices
* [dspace-tech@googlegroups.com](https://groups.google.com/d/forum/dspace-tech) : Technical support mailing list. See also our guide for [How to troubleshoot an error](https://wiki.lyrasis.org/display/DSPACE/Troubleshoot+an+error).
* [dspace-devel@googlegroups.com](https://groups.google.com/d/forum/dspace-devel) : Developers / Development mailing list

Great Q&A is also available under the [DSpace tag on Stackoverflow](http://stackoverflow.com/questions/tagged/dspace)

Additional support options are at https://wiki.lyrasis.org/display/DSPACE/Support

DSpace also has an active service provider network. If you'd rather hire a service provider to
install, upgrade, customize or host DSpace, then we recommend getting in touch with one of our
[Registered Service Providers](http://www.dspace.org/service-providers).

## Issue Tracker

DSpace uses GitHub to track issues:
* Backend (REST API) issues: https://github.com/DSpace/DSpace/issues
* Frontend (User Interface) issues: https://github.com/DSpace/dspace-angular/issues

## Testing

### Running Tests

By default, in DSpace, Unit Tests and Integration Tests are disabled. However, they are
run automatically by [GitHub Actions](https://github.com/DSpace/DSpace/actions?query=workflow%3ABuild) for all Pull Requests and code commits.

* How to run both Unit Tests (via `maven-surefire-plugin`) and Integration Tests (via `maven-failsafe-plugin`):
  ```
  mvn install -DskipUnitTests=false -DskipIntegrationTests=false
  ```
* How to run _only_ Unit Tests:
  ```
  mvn test -DskipUnitTests=false
  ```
* How to run a *single* Unit Test
  ```
  # Run all tests in a specific test class
  # NOTE: failIfNoTests=false is required to skip tests in other modules
  mvn test -DskipUnitTests=false -Dtest=[full.package.testClassName] -DfailIfNoTests=false

  # Run one test method in a specific test class
  mvn test -DskipUnitTests=false -Dtest=[full.package.testClassName]#[testMethodName] -DfailIfNoTests=false
  ```
* How to run _only_ Integration Tests
  ```
  mvn install -DskipIntegrationTests=false
  ```
* How to run a *single* Integration Test
  ```
  # Run all integration tests in a specific test class
  # NOTE: failIfNoTests=false is required to skip tests in other modules
  mvn install -DskipIntegrationTests=false -Dit.test=[full.package.testClassName] -DfailIfNoTests=false

  # Run one test method in a specific test class
  mvn install -DskipIntegrationTests=false -Dit.test=[full.package.testClassName]#[testMethodName] -DfailIfNoTests=false
  ```
* How to run only tests of a specific DSpace module
  ```
  # Before you can run only one module's tests, other modules may need installing into your ~/.m2
  cd [dspace-src]
  mvn clean install

  # Then, move into a module subdirectory, and run the test command
  cd [dspace-src]/dspace-server-webapp
  # Choose your test command from the lists above
  ```

## License

DSpace source code is freely available under a standard [BSD 3-Clause license](https://opensource.org/licenses/BSD-3-Clause).
The full license is available in the [LICENSE](LICENSE) file or online at http://www.dspace.org/license/
"
141,peter-evans/repository-dispatch,TypeScript,"# Repository Dispatch
[![CI](https://github.com/peter-evans/repository-dispatch/workflows/CI/badge.svg)](https://github.com/peter-evans/repository-dispatch/actions?query=workflow%3ACI)
[![GitHub Marketplace](https://img.shields.io/badge/Marketplace-Repository%20Dispatch-blue.svg?colorA=24292e&colorB=0366d6&style=flat&longCache=true&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAM6wAADOsB5dZE0gAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAAERSURBVCiRhZG/SsMxFEZPfsVJ61jbxaF0cRQRcRJ9hlYn30IHN/+9iquDCOIsblIrOjqKgy5aKoJQj4O3EEtbPwhJbr6Te28CmdSKeqzeqr0YbfVIrTBKakvtOl5dtTkK+v4HfA9PEyBFCY9AGVgCBLaBp1jPAyfAJ/AAdIEG0dNAiyP7+K1qIfMdonZic6+WJoBJvQlvuwDqcXadUuqPA1NKAlexbRTAIMvMOCjTbMwl1LtI/6KWJ5Q6rT6Ht1MA58AX8Apcqqt5r2qhrgAXQC3CZ6i1+KMd9TRu3MvA3aH/fFPnBodb6oe6HM8+lYHrGdRXW8M9bMZtPXUji69lmf5Cmamq7quNLFZXD9Rq7v0Bpc1o/tp0fisAAAAASUVORK5CYII=)](https://github.com/marketplace/actions/repository-dispatch)

A GitHub action to create a repository dispatch event.

## Usage

```yml
      - name: Repository Dispatch
        uses: peter-evans/repository-dispatch@v1
        with:
          token: ${{ secrets.REPO_ACCESS_TOKEN }}
          event-type: my-event
```

### Action inputs

| Name | Description | Default |
| --- | --- | --- |
| `token` | (**required**) A `repo` scoped GitHub [Personal Access Token](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token). See [token](#token) for further details. | |
| `repository` | The full name of the repository to send the dispatch. | `github.repository` (current repository) |
| `event-type` | (**required**) A custom webhook event name. | |
| `client-payload` | JSON payload with extra information about the webhook event that your action or workflow may use. | `{}` |

#### `token`

This action creates [`repository_dispatch`](https://developer.github.com/v3/repos/#create-a-repository-dispatch-event) events.
The default `GITHUB_TOKEN` does not have scopes to do this so a `repo` scoped [PAT](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token) created on a user with `write` access to the target repository is required.
If you will be dispatching to a public repository then you can use the more limited `public_repo` scope.

## Example

Here is an example setting all of the input parameters.

```yml
      - name: Repository Dispatch
        uses: peter-evans/repository-dispatch@v1
        with:
          token: ${{ secrets.REPO_ACCESS_TOKEN }}
          repository: username/my-repo
          event-type: my-event
          client-payload: '{""ref"": ""${{ github.ref }}"", ""sha"": ""${{ github.sha }}""}'
```

Here is an example `on: repository_dispatch` workflow to receive the event.
Note that repository dispatch events will only trigger a workflow run if the workflow is committed to the default branch (usually `master`).

```yml
name: Repository Dispatch
on:
  repository_dispatch:
    types: [my-event]
jobs:
  myEvent:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
        with:
          ref: ${{ github.event.client_payload.ref }}
      - run: echo ${{ github.event.client_payload.sha }}
```

### Dispatch to multiple repositories

You can dispatch to multiple repositories by using a [matrix strategy](https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#jobsjob_idstrategymatrix). In the following example, after the `build` job succeeds, an event is dispatched to three different repositories.

```yml
jobs:
  build:
    # Main workflow job that builds, tests, etc.

  dispatch:
    needs: build
    strategy:
      matrix:
        repo: ['my-org/repo1', 'my-org/repo2', 'my-org/repo3']
    runs-on: ubuntu-latest
    steps:
      - name: Repository Dispatch
        uses: peter-evans/repository-dispatch@v1
        with:
          token: ${{ secrets.REPO_ACCESS_TOKEN }}
          repository: ${{ matrix.repo }}
          event-type: my-event
```

## Client payload

The GitHub API allows a maximum of 10 top-level properties in the `client-payload` JSON.
If you use more than that you will see an error message like the following.

```
No more than 10 properties are allowed; 14 were supplied.
```

For example, this payload will fail because it has more than 10 top-level properties.

```yml
client-payload: ${{ toJson(github) }}
```

To solve this you can simply wrap the payload in a single top-level property.
The following payload will succeed.

```yml
client-payload: '{""github"": ${{ toJson(github) }}}'
```

Additionally, there is a limitation on the total data size of the `client-payload`. A very large payload may result in a `client_payload is too large` error.

## License

[MIT](LICENSE)
"
142,JustryDeng/CommonRepository,Java,
143,kisslinux/community,Shell,"|/
|\ISS                                                           https://k1ss.org
________________________________________________________________________________


Community Repository
________________________________________________________________________________

The community repository for KISS Linux.


Documentation
________________________________________________________________________________

- https://k1ss.org/package-system
- https://k1ss.org/package-manager
- https://k1ss.org/wiki/kiss/style-guide
- https://k1ss.org/software


Maintenance
________________________________________________________________________________

KISS follows a maintainer model when it comes to package ownership. Only the
maintainer of a package has the ability to push changes to said package. Any
issues must be reported to the maintainer directly.

The maintainer's details can be found via 'git log' or the 'kiss maintainer'
command. If the maintainer cannot be reached via email, open an issue in this
repository on GitHub.

If the maintainer does not respond within a reasonable amount of time, the
package will be orphaned and ownership will be given to someone else.


Submitting Pull Requests
________________________________________________________________________________

Pull requests should contain only a single package. This makes the review
process easier and allows for individual packages to be merged without waiting
on reviews for others.

Please fill out the pull request template as well.


Commit Style
________________________________________________________________________________

Contributions to this repository should adhere to the following commit style.

+------------------------------------------------------------------------------+
| Adding a new package                                                         |
+------------------------------------------------------------------------------+
|                                                                              |
|   $ git commit -m ""pkg_name: new package at pkg_version""                     |
|                                                                              |
+------------------------------------------------------------------------------+
| Updating an existing package                                                 |
+------------------------------------------------------------------------------+
|                                                                              |
|   $ git commit -m ""pkg_name: pkg_version""                                    |
|   $ git commit -m ""pkg_name: bumped to pkg_version""                          |
|                                                                              |
+------------------------------------------------------------------------------+
| Miscellaneous changes                                                        |
+------------------------------------------------------------------------------+
|                                                                              |
|   $ git commit -m ""pkg_name: other changes""                                  |
|   $ git commit -m ""pkg_name: added missing dep_name dependency""              |
|   $ git commit -m ""pkg_name: fixed incorrect bin dir""                        |
|   $ git commit -m ""pkg_name: fixed build failure ...""                        |
|                                                                              |
+------------------------------------------------------------------------------+


"
144,Alfresco/alfresco-repository,Java,"## This project has now been Archived

The alfresco-core, alfresco-data-model, alfresco-repository and alfresco-remote-api projects have been archived with their code incorporated into [alfresco-community-repo]( https://github.com/Alfresco/alfresco-community-repo) to simply ongoing development. The same artifacts are still produced by the new project. It also has a branch used as the basis of each of ACS 6 Enterprise release. For more information, set the new project’s README.md file.

### Alfresco Repository
[![Build Status](https://travis-ci.com/Alfresco/alfresco-repository.svg?branch=master)](https://travis-ci.com/Alfresco/alfresco-repository)

Repository is a library packaged as a jar file which is part of [Alfresco Content Services Repository](https://community.alfresco.com/docs/DOC-6385-project-overview-repository).
The library contains the following:
* DAOs and SQL scripts
* Various Service implementations
* Utility classes

### Building and testing 
The project can be built by running Maven command:
~~~
mvn clean install
~~~
The tests are combined in test classes split by test type or Spring application context used in the test, see classes in _src/test/java/org/alfresco_. All of these classes as well as individual tests can be run by specifying the test class name and a set of DB connection properties, for example:
~~~
mvn clean test -Dtest=SomeRepoTest -Ddb.driver=org.postgresql.Driver -Ddb.name=alfresco -Ddb.url=jdbc:postgresql:alfresco -Ddb.username=alfresco -Ddb.password=alfresco
~~~

### Artifacts
The artifacts can be obtained by:
* downloading from [Alfresco repository](https://artifacts.alfresco.com/nexus/content/groups/public)
* getting as Maven dependency by adding the dependency to your pom file:
~~~
<dependency>
  <groupId>org.alfresco</groupId>
  <artifactId>alfresco-repository</artifactId>
  <version>version</version>
</dependency>
~~~
and Alfresco Maven repository:
~~~
<repository>
  <id>alfresco-maven-repo</id>
  <url>https://artifacts.alfresco.com/nexus/content/groups/public</url>
</repository>
~~~
The SNAPSHOT version of the artifact is **never** published.

### Contributing guide
Please use [this guide](CONTRIBUTING.md) to make a contribution to the project.
"
145,nasa/Common-Metadata-Repository,Clojure,"# Common Metadata Repository

Visit the CMR at [https://earthdata.nasa.gov/about/science-system-description/eosdis-components/common-metadata-repository](https://earthdata.nasa.gov/about/science-system-description/eosdis-components/common-metadata-repository)

## About

The Common Metadata Repository (CMR) is an earth science metadata repository
for [NASA](https://www.nasa.gov/) [EOSDIS](https://earthdata.nasa.gov) data. The CMR
Search API provides access to this metadata.

## Client-facing Components

- Search
  - Allows the user to search by collections, granules, and concepts with a
    myriad of different query types
  - API Docs: https://cmr.earthdata.nasa.gov/search/site/search_api_docs.html

- Ingest
  - Ingest refers to the process of validating, inserting, updating, or
    deleting metadata in the CMR system. It affects only the metadata for the
    specific Data Partner. The CMR allows Data Partners to ingest metadata.
    records through a RESTful API
  - API Docs: https://cmr.earthdata.nasa.gov/ingest/site/ingest_api_docs.html

- Access Control
  - Access Control Lists (ACLs) are the mechanism which grants users
    access to perform different operations in the CMR. CMR ACLs follow the same
    design as ECHO ACLs, which are a superset of the generic ACL
    design pattern used in many other systems. An ACL is a
    mapping of actors (subjects) to resources (object) to operations
    (predicate).
  - Two quick examples of a CMR ACL could be:
    - All registered users have READ access to ASTER data
    - A provider's operations team may ingest data for that provider
  - API Docs: https://cmr.earthdata.nasa.gov/access-control/site/access_control_api_docs.html

## Our Development Environment

- Mac OSX
- Atom: https://atom.io/
- Proto-Repl: https://atom.io/packages/proto-repl
  - Installed and configured according to this guide: https://git.io/atom_clojure_setup

## Prerequisites

- Java 1.8.0 (a.k.a. JAVA8) only; higher versions are not currently supported.
- Leiningen (https://leiningen.org) 2.5.1 or above.
  - We've had success with Homebrew and with the install script on the
    Leiningen website.
- Ruby (used to support two legacy apps)
- Maven (https://maven.apache.org/install.html)
    - Mac OS X devs can use `brew install maven`
    - Linux devs can use `sudo apt-get install maven`
- GCC and libc
- Docker

## Obtaining the Code

You can get the CMR source code by cloning the repository from Github:

```
$ git clone git@github.com:nasa/Common-Metadata-Repository.git cmr
```

## Building and Running the CMR

The CMR is a system consisting of many services. The services can run
individually or in a single process. Running in a single process makes
local development easier because it avoids having to start many different
processes. The dev-system project allows the CMR to run from a single REPL
or Jar file. If you're developing a client against the CMR you can build and
run the entire CMR with no external dependencies from this Jar file and use
that instance for local testing. The sections below contain instructions for
running the CMR as a single process or as many processes.

#### Using the `cmr` CLI Tool

This project has its own tool that is able to do everything from initial setup to
running builds and tests on the CI/CD infrastructure. To use the tool
as we do below, be sure to run the following from the top-level CMR directory:

```
export PATH=$PATH:`pwd`/bin
source resources/shell/cmr-bash-autocomplete
```

(If you use a system shell not compatible with Bash, we'll accept a PR with
auto-complete for it.)

To make this change permanent:

```
echo ""export PATH=\$PATH:`pwd`/bin"" >> ~/.profile
echo ""source `pwd`/resources/shell/cmr-bash-autocomplete"" >> ~/.profile
```

#### Oracle Dependencies

Even if you're not going to develop against a local Oracle database,
you still need to have the Oracle libraries locally installed to use the
CMR.

Here are the steps to do so:

1. Ensure you have installed on your system the items listed above in the
   ""Prerequisites"" section.
1. Download the Oracle JDBC JAR files into `./oracle-lib/support` by
   following instructions in `./oracle-lib/README.md`. (The CMR must have these
   libraries to build but it does not depend on Oracle DB when running
   locally. It uses a local in-memory database by default.) If you're reading this
   guide on the web, [here is a handy link to the instructions.](https://github.com/nasa/Common-Metadata-Repository/tree/master/oracle-lib)
1. With the JAR files downloaded to the proper location, you're now ready
   to install them for use by the CMR:"" `cmr install oracle-libs`

#### Building and Running CMR Dev System in a REPL with CMR CLI tool

1. `cmr setup profile` and then update the new `./dev-system/profiles.clj` file.
   it will look something like this:
   ``` clojure
   {:dev-config {:env {:cmr-metadata-db-password ""<YOUR PASSWORD HERE>""
                       :cmr-sys-dba-password ""<YOUR PASSWORD HERE>""
                       :cmr-bootstrap-password ""<YOUR PASSWORD HERE>""
                       :cmr-ingest-password ""<YOUR PASSWORD HERE>""
                       :cmr-urs-password ""<YOUR PASSWORD HERE>""}}}
   ```

2. `cmr setup dev`
3. `cmr start repl`
4. Once given a Clojure prompt, run `(reset)`

Note that the `reset` action could potentially take a while, not only due to
the code reloading for a large number of namespaces, but for bootstrapping
services as well as starting up worker threads.

#### Building and Running CMR Dev System from a Jar

Assuming you have already run the above steps (namely `cmr setup dev`), to
build and run the default CMR development system (`dev-system`) from a
`.jar` file:

1. `cmr build uberjars`
2. `cmr build all`
3. `cmr start uberjar dev-system` will run the dev-system as a background task

See CMR Development Guide to read about specifying options and setting
environment variables

#### Building and Running separate CMR Applications

The following will build every application but will put each jar into the
appropriate `target` directory for each application. The command shown in step
3 is an example. For the proper command to start up each application, see the
`Applications` section below. Note: You only need to complete steps 1 and 2 once.

1. `cmr build uberjar APP`
2. `cmr run uberjar APP`

Where `APP` is any supported CMR app. You can touble-tap the `TAB` key on
your keyboard to get the `cmr` tool to show you the list of available apps
after entering `uberjar` in each step above.

Note: building uberjars will interfere with your repl. If you want to use your repl post-build you will need to,
`rm -f ./dev-system/target/`

## Checking Dependencies, Static Analysis, and Tests

There are several `lein` plugins within the CMR for performing
various tasks either at individual subproject levels or at the top-level for
all subprojects.

#### Dependency Versions

To check for up-to-date versions of all project dependencies, you can use
`cmr test versions PROJ`, where `PROJ` is any CMR sub-project under the
top-level directory.

You may run the same command without a project to check for all projects:
`cmr test versions`.

Note that this command fails with the first project that fails. If many
subprojects are failing their dependency version checks and you wish to see
them all, you may use your system shell:

```sh
for PROJ in `ls -1d */project.clj|xargs dirname`
do
  ""Checking $PROJ ...""
  cmr test versions $PROJ
  cd - &> /dev/null
done
```

#### Dependency Ambiguities and `.jar` File Conflicts

To see if the JVM is having problems resolving which version of a
dependency to use, you can run `cmr test dep-tree PROJ`. To perform this
against all projects: `cmr test dep-trees`.

#### Static Analysis and Linting

To perform static analysis and linting for a project, you can run
`cmr test lint PROJ`. As above with dependency version checking, by
not passing a project, you can run for all projects: `cmr test lint`.

#### Dependency Vulnerability Scanning

You can see if your currently installed version of CMR has any reported Common Vulnerabilities and Exploits (CVEs) by running the helpful alias `lein check-sec` that you can use in each application, or at the root folder to scan all CMR apps together.

You will find the vulnerability summary in `./target/dependency-check-report.html` in each application.

#### Testing CMR

Test files in CMR should follow the naming convention of ending in `-test`.

There are two modes of testing the CMR:

* From the REPL
* Utilizing the CI/CD script to run against an Oracle database

For the first, the steps are as follows:

1. Ensure you have set up your development environment in `dev-system`
2. If you have built any `.jar` files, run `cmr clean PROJ` (for a given
   project) or `cmr clean` to clean all projects.
3. Start the REPL: `cmr start repl`
4. Once in the REPL, start the in-memory services: `(reset)`
5. Run the tests: `(run-all-tests)` or `(run-all-tests-future)`

You have the option of substituting the last step with `(run-suites)`. This
uses a third-party tool to display clear test results which are
easier copy/paste should you want to run them on an individual basis.
These results also contain easier to read
exception messages/stacktraces. Here's an excerpt:

```
   cmr.system-int-test.ingest.provider-ingest-test
     update-provider-test
       assertion 1 ........................................................ [OK]
       assertion 2 ........................................................ [OK]
       assertion 3 ........................................................ [OK]
       assertion 4 ........................................................ [OK]
     delete-provider-test
       assertion 1 ........................................................ [OK]
       assertion 2 ........................................................ [OK]
       assertion 3 ........................................................ [OK]
       assertion 4 ........................................................ [OK]
       assertion 5 ........................................................ [OK]
       assertion 6 ........................................................ [OK]
       assertion 7 ........................................................ [OK]
       assertion 8 ........................................................ [OK]
       assertion 9 ........................................................ [OK]
```

For non-terminal based dev, depending upon your IDE/editor, you may have
shortcuts available to you for starting/restarting the services and/or running the
tests. To find out what these are you can contact a CMR core dev.

To run the tests against an Oracle database, we recommend that
you use an Oracle VM built for this purpose. You will also need
configuration and authentication information that will be set as environment
variables. Be sure to contact a CMR core dev for this information.

To run only certain types of tests, you may run the following:

##### Unit Tests

``` sh
lein modules utest
```

##### Integration Tests

If running CMR with the in-memory database (default)
``` sh
lein modules itest --skip-meta :oracle

```

If running CMR with an external database
``` sh
lein modules itest --skip-meta :in-memory-db
```

If you want to run tests against Oracle, bring up the Oracle VM and execute
the following to create the users and run the migrations:

``` sh
cmr setup db
```

Then, in the CMR REPL:

```clj
user=> (reset :db :external)
...
user=> (run-all-tests)
...
```

Those tests will take much longer to run than when done with the in-memory
database (~25m vs. ~6m). To switch back to using the in-memory database,
call `(reset :db :in-memory)`.

There is also a different, optional test runner you can use. For more details
see the docstring for `run-suites` in `dev-system/dev/user.clj`.
It will contain usage instructions
#### Testing in the CI Environment

Throughout the modules, in the `project.clj` files there are additional `lein` aliseses for 
executing the tests in the CI/CD environment. They are 
* ci-itest
* ci-utest

These run the integration and unit tests, respectively, in the CI environment. The difference
between `itest` and `ci-itest` or `utest` and `ci-utest` are the settings passed to the
kaocha test runner.

In the CI environment, color is omitted, and certain tests that require an internal memory
database are excluded. The aliases may be used locally as well. 

To see the differnce in detail, inspect the `tests.edn` files for each module to see the
profile in use in the CI environment. Kaocha supports the use of profiles so more may 
be added as necessary.

### Test Development

CMR uses the [Kaocha](https://github.com/lambdaisland/kaocha) test library.
It provides plugins and grouping capabilities. Tests are organized in each module
with the standard being `:unit` and `:integration`.

Not all modules will contain `:integration` tests.

## Code structure

The CMR comprises several small services called microservices. These are
small purposed-based services that do a small set of things well.

- For more reading on microservices: https://martinfowler.com/articles/microservices.html

### The Microservices

Each microservice has a `README` file in its root directory, which provides a
short overview of the service's functionality. There are many main
applications, as well as several libraries and support applications.

#### Applications:

- access-control-app
  - The mechanism which grants users access to perform different
    operations in the CMR. It also maintains groups and access control rules.
    Note that ECHO and URS provide user access as an external dependency.
    The mock-echo application implements both of the necessary interfaces
    for local testing.
  - Main method: cmr.access_control.runner

- bootstrap-app
  - Contains APIs for performing various bulk actions in the CMR
  - Main method: cmr.bootstrap.runner
  - See `/bootstrap-app/README.md` for a list of lein and uberjar commands

- dev-system
  - An app that combines the separate microservices of the CMR into a single
  application. We use this to simplify development
  - Main method: cmr.dev_system.runner

- indexer-app
  - This handles indexing collections, granules, and tags in Elasticsearch
  - Maintains the set of indexes in elasticsearch for each concept
  - Main method: cmr.indexer.runner

- ingest-app
  - The Ingest app handles collaborating with metadata db and indexer systems.
  This maintains the lifecycle of concepts coming into the CMR
  - Main method: cmr.ingest.runner

- search-app
  - Provides a public search API for concepts in the CMR
  - Main method: cmr.search.runner

- search-relevancy-test
  - Tests to measure and report the effectiveness of CMR's search relevancy algorithm

- system-int-test
  - Black-box, system-level tests to ensure functionality of the CMR

- virtual-product-app
  - Adds the concept of Virtual Products to the CMR. Virtual Products represent
  products that a provider generates on demand from users. This takes place when
   a user places an order or downloads a product through a URL
  - Main method: cmr.virtual_product.runner

- metadata-db-app
  - A database that maintains revisioned copies of metadata for the CMR
  - Main method: cmr.metadata_db.runner

- mock-echo-app
  - This mocks out the ECHO REST API and the URS API as well. Its purpose is to
  make it easier to integration test the CMR system without having to run a full
   instance of ECHO. It will only provide the parts necessary to enable
   integration testing. You should not expect a perfect or complete
   implementation of ECHO.
  - Main method: cmr.mock_echo.runner

#### Libraries:

- acl-lib
  - Contains utilities for retrieving and working with ACLs

- common-app-lib
  - Contains utilities used within many CMR applications

- common-lib
  - Provides common utility code for CMR projects

- elastic-utils-lib
  - A library that handles most of the interfacing with Elasticsearch

- es-spatial-plugin
  - An Elasticsearch plugin that enables spatial search within elastic

- oracle-lib
  - Contains utilities for connecting to and manipulating data in Oracle

- orbits-lib
  - Clojure wrapper of a Ruby implementation of the Backtrack Orbit Search
    Algorithm (BOSA)

- message-queue-lib
  - A library for interfacing with RabbitMQ, AWS SQS, and an in-memory message queue

- spatial-lib
  - The spatial libraries provide utilities for working with spatial areas in the CMR

- transmit-lib
  - The Transmit Library defines functions for invoking CMR services

- umm-lib
  - This is the old source of UMM schemas and translation code. Since the
    advent of umm-spec-lib we are planning to remove it

- umm-spec-lib
  - The UMM Spec lib contains JSON schemas that define the Unified Metadata
    Model, as well as mappings to other supported formats, and code to migrate
    collections between any supported formats.

## Further Reading

- CMR Client Partner User Guide: https://wiki.earthdata.nasa.gov/display/ED/CMR+Client+Partner+User+Guide
- CMR Data Partner User Guide: https://wiki.earthdata.nasa.gov/display/CMR/CMR+Data+Partner+User+Guide
- CMR Client Developer Forum: https://wiki.earthdata.nasa.gov/display/CMR/CMR+Client+Developer+Forum

## License

Copyright © 2014-2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.
"
146,vim/vim,Vim script,"[![Vim Logo](https://github.com/vim/vim/raw/master/runtime/vimlogo.gif)](https://www.vim.org)

[![Github Build status](https://github.com/vim/vim/workflows/GitHub%20CI/badge.svg)](https://github.com/vim/vim/actions?query=workflow%3A%22GitHub+CI%22) [![Travis Build Status](https://travis-ci.com/vim/vim.svg?branch=master)](https://travis-ci.com/github/vim/vim) [![Appveyor Build status](https://ci.appveyor.com/api/projects/status/o2qht2kjm02sgghk?svg=true)](https://ci.appveyor.com/project/chrisbra/vim) [![Cirrus Build Status](https://api.cirrus-ci.com/github/vim/vim.svg)](https://cirrus-ci.com/github/vim/vim) [![Coverage Status](https://codecov.io/gh/vim/vim/coverage.svg?branch=master)](https://codecov.io/gh/vim/vim?branch=master) [![Coverity Scan](https://scan.coverity.com/projects/241/badge.svg)](https://scan.coverity.com/projects/vim) [![Language Grade: C/C++](https://img.shields.io/lgtm/grade/cpp/g/vim/vim.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/vim/vim/context:cpp) [![Debian CI](https://badges.debian.net/badges/debian/testing/vim/version.svg)](https://buildd.debian.org/vim) [![Packages](https://repology.org/badge/tiny-repos/vim.svg)](https://repology.org/metapackage/vim) [![Fossies codespell report](https://fossies.org/linux/test/vim-master.tar.gz/codespell.svg)](https://fossies.org/linux/test/vim-master.tar.gz/codespell.html)

<sub>For translations of this README see the end.</sub>


## What is Vim? ##

Vim is a greatly improved version of the good old UNIX editor
[Vi](https://en.wikipedia.org/wiki/Vi).  Many new
features have been added: multi-level undo, syntax highlighting, command line
history, on-line help, spell checking, filename completion, block operations,
script language, etc.  There is also a Graphical User Interface (GUI)
available.  Still, Vi compatibility is maintained, those who have Vi ""in the
fingers"" will feel at home.
See [`runtime/doc/vi_diff.txt`](runtime/doc/vi_diff.txt) for differences with
Vi.

This editor is very useful for editing programs and other plain text files.
All commands are given with normal keyboard characters, so those who can type
with ten fingers can work very fast.  Additionally, function keys can be
mapped to commands by the user, and the mouse can be used.

Vim runs under MS-Windows (XP, Vista, 7, 8, 10), macOS, Haiku, VMS and almost
all flavours of UNIX.  Porting to other systems should not be very difficult.
Older versions of Vim run on MS-DOS, MS-Windows 95/98/Me/NT/2000, Amiga DOS,
Atari MiNT, BeOS, RISC OS and OS/2.  These are no longer maintained.

For Vim9 script see [README_VIM9](README_VIM9.md).

## Distribution ##

You can often use your favorite package manager to install Vim.  On Mac and
Linux a small version of Vim is pre-installed, you still need to install Vim
if you want more features.

There are separate distributions for Unix, PC, Amiga and some other systems.
This `README.md` file comes with the runtime archive.  It includes the
documentation, syntax files and other files that are used at runtime.  To run
Vim you must get either one of the binary archives or a source archive.
Which one you need depends on the system you want to run it on and whether you
want or must compile it yourself.  Check http://www.vim.org/download.php for
an overview of currently available distributions.

Some popular places to get the latest Vim:
* Check out the git repository from [GitHub](https://github.com/vim/vim).
* Get the source code as an [archive](https://github.com/vim/vim/releases).
* Get a Windows executable from the
[vim-win32-installer](https://github.com/vim/vim-win32-installer/releases) repository.



## Compiling ##

If you obtained a binary distribution you don't need to compile Vim.  If you
obtained a source distribution, all the stuff for compiling Vim is in the
[`src`](/src) directory.  See [`src/INSTALL`](src/INSTALL) for instructions.


## Installation ##

See one of these files for system-specific instructions.  Either in the
[READMEdir directory](./READMEdir/) (in the repository) or
the top directory (if you unpack an archive):

	README_ami.txt		Amiga
	README_unix.txt		Unix
	README_dos.txt		MS-DOS and MS-Windows
	README_mac.txt		Macintosh
	README_haiku.txt	Haiku
	README_vms.txt		VMS

There are other `README_*.txt` files, depending on the distribution you used.


## Documentation ##

The Vim tutor is a one hour training course for beginners.  Often it can be
started as `vimtutor`.  See `:help tutor` for more information.

The best is to use `:help` in Vim.  If you don't have an executable yet, read
[`runtime/doc/help.txt`](/runtime/doc/help.txt). 
It contains pointers to the other documentation files.
The User Manual reads like a book and is recommended to learn to use
Vim.  See `:help user-manual`.


## Copying ##

Vim is Charityware.  You can use and copy it as much as you like, but you are
encouraged to make a donation to help orphans in Uganda.  Please read the file
[`runtime/doc/uganda.txt`](runtime/doc/uganda.txt)
for details (do `:help uganda` inside Vim).

Summary of the license: There are no restrictions on using or distributing an
unmodified copy of Vim.  Parts of Vim may also be distributed, but the license
text must always be included.  For modified versions a few restrictions apply.
The license is GPL compatible, you may compile Vim with GPL libraries and
distribute it.


## Sponsoring ##

Fixing bugs and adding new features takes a lot of time and effort.  To show
your appreciation for the work and motivate Bram and others to continue
working on Vim please send a donation.

Since Bram is back to a paid job the money will now be used to help children
in Uganda.  See [`runtime/doc/uganda.txt`](runtime/doc/uganda.txt).  But
at the same time donations increase Bram's motivation to keep working on Vim!

For the most recent information about sponsoring look on the Vim web site:
	http://www.vim.org/sponsor/


## Contributing ##

If you would like to help making Vim better, see the
[CONTRIBUTING.md](/CONTRIBUTING.md) file.


## Information ##

The latest news about Vim can be found on the Vim home page:
	http://www.vim.org/

If you have problems, have a look at the Vim documentation or tips:
	http://www.vim.org/docs.php
	http://vim.wikia.com/wiki/Vim_Tips_Wiki

If you still have problems or any other questions, use one of the mailing
lists to discuss them with Vim users and developers:
	http://www.vim.org/maillist.php

If nothing else works, report bugs directly:
	Bram Moolenaar <Bram@vim.org>


## Main author ##

Send any other comments, patches, flowers and suggestions to:
	Bram Moolenaar <Bram@vim.org>


This is `README.md` for version 8.2 of Vim: Vi IMproved.


## Translations of this README ##

[Korean](https://github.com/cjw1359/opensource/blob/master/Vim/README_ko.md)
"
147,brunohbrito/MongoDB-RepositoryUoWPatterns,C#,"# ASP.NET Core - MongoDB Repository & Unit of Work Pattern
<img src=""https://www.brunobrito.net.br/content/images/2019/02/capa-social.jpg"" width=""480"" />

Example App - This demo shows how to simple implement repository and unit of work patterns for MongoDB .Net driver.

* **Server Side**: ASP.NET Core

Pre reqs

* Net.Core 3.1

Techs:

* ASP.NET Core
* Swagger
* ServiceStack.Core

# How to run

1. Download code
  * Open folder
  * Open a terminal
  * docker-compose up --build


More about on [ASP.NET Core - MongoDB Repository Pattern & Unit Of Work](https://www.brunobrito.net.br/aspnet-core-mongodb-unit-of-work/) - Portuguese only
"
148,ulyaoth/repository,Shell,"### Ulyaoth Repository - END OF LIFE!!
https://ulyaoth.com/news/ulyaoth-repository-end-of-life/

Ulyaoth repository is no longer maintained and removed.


### Ulyaoth Repository
https://www.ulyaoth.net/resources/categories/repository.1/

### Available packages:
* apr
* banana
* filebeat
* fuse
* fuse-s3fs
* go
* hhvm
* haproxy
* hiawatha
* httpdiff
* jsvc
* kafka
* keepalived
* kibana
* logstash
* logstash-Forwarder
* lua
* mbed tls
* monkey
* nginx
* nginx (modsecurity
* nginx (naxsi)
* nginx (pagespeed)
* nginx (passenger)
* openssl
* packetbeat
* redis
* solr
* spotify
* tengine
* tomcat
* tomcat native
* topbeat
* ulyaoth
* varnish
* vegeta
* wolfssl
* zookeeper
"
149,ziglibs/repository,Zig,"# Zig Package Repository

This is one community-maintained repository of zig packages.

## Contributions
If you have an activly maintained package, feel free to create a PR that adds your package to the repository! If you feel like it, you're also free to add other peoples packages!

### Adding new packages

The repo provides a convenience tool to create new packages. Just run `zig build add` to get a prompt for package informations. The tool will verify that the entered information is vaguely correct and follows the format rules, also checks if a package with that name already exists and if the tags are all valid.

### Verification

![Repository Validation](https://github.com/ziglibs/repository/workflows/Repository%20Validation/badge.svg?event=push)

This repository will use the CI to verify if all PRs keep the database consistent. If you want to locally test this before doing the PR, just call `zig build verify` in the root folder.

## Repository structure

The repository contains two major data sets: *packages* and *tags*.

*Tags* are just groups of packages, each package can have zero or more tags assigned. *Packages* are basically a link to *any* git repository paired with a root source file which is required for the package to be imported.

### `packages/`
A folder containing a single file per package. Each file is a json file following this structure:
```json
{
  ""author"": ""<author>"",
  ""description"": ""<description>"",
  ""git"": ""<url>"",
  ""root_file"": ""<path>"",
  ""tags"": [
    ""<tag>"", ""<tag>""
  ]
}
```

The fields have the following meaning:
- `author` is the name (or nickname) of the package author
- `description` is a short description of the package
- `git` is a path to the git repository where the package can be fetched
- `root_file` is an absolute path in unix style (path segments separated by `/`) to the root file of the package. This is what should be used for `std.build.Pkg.path`
- `tags` is an array of strings where each item is the name of a tag in the folder `tags/`. Tags are identified by their file name (without extension) and will group the packages

### `tags/`
A folder containing a single file per tag. Each file is a json file following this structure:
```json
{
  ""description"": ""<text>""
}
```

The fields have the following meaning:
- `description` is a short description of what kind of packages can be found in this group.

### `tools/`

This folder contains the sources of the verification tools and other nice things.
"
150,megamit/repository,JavaScript,
151,awes-io/repository,PHP,"<p align=""center"">
    <a href=""https://www.awes.io/?utm_source=github&utm_medium=repository"" target=""_blank"" rel=""noopener noreferrer"">
        <img width=""100"" src=""https://static.awes.io/promo/Logo_sign_color.svg"" alt=""Awes.io logo"">
    </a>
</p>

<h1 align=""center"">Repository</h1>

<p align=""center"">Repository Pattern in Laravel. The package allows to filter by request out-of-the-box, as well as to integrate customized criteria and any kind of filters.</p>

<p align=""center"">
    <a href=""https://www.awes.io/?utm_source=github&amp;utm_medium=shields"">
        <img src=""https://repo.pkgkit.com/4GBWO/awes-io/repository/badges/master/coverage.svg"" alt=""Coverage report"" >
    </a>
    <a href=""https://www.awes.io/?utm_source=github&amp;utm_medium=shields"">
        <img src=""https://www.pkgkit.com/4GBWO/awes-io/repository/version.svg"" alt=""Last version"" >
    </a>
    <a href=""https://www.awes.io/?utm_source=github&amp;utm_medium=shields"">
        <img src=""https://repo.pkgkit.com/4GBWO/awes-io/repository/badges/master/build.svg"" alt=""Build status"" >
    </a>
    <a href=""https://www.awes.io/?utm_source=github&amp;utm_medium=shields"">
        <img src=""https://www.pkgkit.com/4GBWO/awes-io/repository/downloads.svg"" alt=""Downloads"" >
    </a>
    <a href=""https://www.awes.io/?utm_source=github&amp;utm_medium=shields"">
        <img src=""https://img.shields.io/github/license/awes-io/repository.svg"" alt=""License"" />
    </a>
    <a href=""https://www.awes.io/?utm_source=github&amp;utm_medium=shields"">
        <img src=""https://www.pkgkit.com/4GBWO/awes-io/repository/status.svg"" alt=""CDN Ready"" /> 
    </a>
    <a href=""https://www.awes.io/?utm_source=github&amp;utm_medium=shields"" target=""_blank"">
        <img src=""https://static.pkgkit.com/badges/laravel.svg"" alt=""laravel"" />
    </a>
    <a href=""https://www.awes.io/?utm_source=github&amp;utm_medium=shields"">
        <img src=""https://img.shields.io/github/last-commit/awes-io/repository.svg"" alt=""Last commit"" />
    </a>
    <a href=""https://github.com/awes-io/awes-io"">
        <img src=""https://ga-beacon.appspot.com/UA-134431636-1/awes-io/repository"" alt=""Analytics"" />
    </a>
    <a href=""https://www.pkgkit.com/?utm_source=github&amp;utm_medium=shields"">
        <img src=""https://www.pkgkit.com/badges/hosted.svg"" alt=""Hosted by Package Kit"" />
    </a>
    <a href=""https://www.patreon.com/join/awesdotio"">
        <img src=""https://static.pkgkit.com/badges/patreon.svg"" alt=""Patreon"" />
    </a>
</p>

##
<p align=""center"">
    <img src=""https://static.awes.io/github/repository-cover.png"" alt=""Repository Laravel"" />
</p>


## Table of Contents

- <a href=""#installation"">Installation</a>
- <a href=""#configuration"">Configuration</a>
- <a href=""#overview"">Overview</a>
- <a href=""#usage"">Usage</a>
    - <a href=""#create-a-model"">Create a Model</a>
    - <a href=""#create-a-repository"">Create a Repository</a>
    - <a href=""#use-built-in-methods"">Use built-in methods</a>
    - <a href=""#create-a-criteria"">Create a Criteria</a>
    - <a href=""#scope-filter-and-order"">Scope, Filter, and Order</a>
    - <a href=""#artisan-commands"">Artisan Commands</a>
- <a href=""#testing"">Testing</a>

## Installation

Via Composer

``` bash
$ composer require awes-io/repository
```

The package will automatically register itself.

## Configuration

First publish config:

```bash
php artisan vendor:publish --provider=""AwesIO\Repository\RepositoryServiceProvider"" --tag=""config""
```

```php
// $repository->smartPaginate() related parameters
'smart_paginate' => [
    // name of request parameter to take paginate by value from
    'request_parameter' => 'limit',
    // default paginate by value
    'default_limit' => 15,
    // max paginate by value
    'max_limit' => 100,
]
```

## Overview


##### Package allows you to filter data based on incoming request parameters:

```
https://example.com/news?title=Title&custom=value&orderBy=name_desc
```

It will automatically apply built-in constraints onto the query as well as any custom scopes and criteria you need:

```php
protected $searchable = [
    // where 'title' equals 'Title'
    'title',
];

protected $scopes = [
    // and custom parameter used in your scope
    'custom' => MyScope::class,
];
```

```php
class MyScope extends ScopeAbstract
{
    public function scope($builder, $value, $scope)
    {
        return $builder->where($scope, $value)->orWhere(...);
    }
}
```

Ordering by any field is available:

```php
protected $scopes = [
    // orderBy field
    'orderBy' => OrderByScope::class,
];
```

Package can also apply any custom criteria:

```php
return $this->news->withCriteria([
    new MyCriteria([
        'category_id' => '1', 'name' => 'Name'
    ])
    ...
])->get();
```

## Usage

### Create a Model

Create your model:

```php
namespace App;

use Illuminate\Database\Eloquent\Model;

class News extends Model 
{
    ...
}
```

### Create a Repository

Extend it from `AwesIO\Repository\Eloquent\BaseRepository` and provide `entity()` method to return full model class name:

```php
namespace App;

use AwesIO\Repository\Eloquent\BaseRepository;

class NewsRepository extends BaseRepository
{
    public function entity()
    {
        return News::class;
    }
}
```

### Use built-in methods

```php
use App\NewsRepository;

class NewsController extends BaseController 
{
    protected $news;

    public function __construct(NewsRepository $news)
    {
        $this->news = $news;
    }
    ....
}
```

Execute the query as a ""select"" statement or get all results:

```php
$news = $this->news->get();
```

Execute the query and get the first result:

```php
$news = $this->news->first();
```

Find a model by its primary key:

```php
$news = $this->news->find(1);
```

Add basic where clauses and execute the query:

```php
$news = $this->news->->findWhere([
        // where id equals 1
        'id' => '1',
        // other ""where"" operations
        ['news_category_id', '<', '3'],
        ...
    ]);
```

Paginate the given query:

```php
$news = $this->news->paginate(15);
```

Paginate the given query into a simple paginator:

```php
$news = $this->news->simplePaginate(15);
```

Paginate the given query by 'limit' request parameter:

```php
$news = $this->news->smartPaginate();
```

Add an ""order by"" clause to the query:

```php
$news = $this->news->orderBy('title', 'desc')->get();
```

Save a new model and return the instance:

```php
$news = $this->news->create($request->all());
```

Update a record:

```php
$this->news->update($request->all(), $id);
```

Delete a record by id:

```php
$this->news->destroy($id);
```

Attach models to the parent:

```php
$this->news->attach($parentId, $relationship, $idsToAttach);
```

Detach models from the relationship:

```php
$this->news->detach($parentId, $relationship, $idsToDetach);
```

Find model or throw an exception if not found:

```php
$this->news->findOrFail($id);
```

Execute the query and get the first result or throw an exception:

```php
$this->news->firstOrFail();
```

### Create a Criteria

Criteria are a way to build up specific query conditions.

```php
use AwesIO\Repository\Contracts\CriterionInterface;

class MyCriteria implements CriterionInterface {

    protected $conditions;
    
    public function __construct(array $conditions)
    {
        $this->conditions = $conditions;
    }

    public function apply($entity)
    {
        foreach ($this->conditions as $field => $value) {
            $entity = $entity->where($field, '=', $value);
        }
        return $entity;
    }
}
```

Multiple Criteria can be applied:

```php
use App\NewsRepository;

class NewsController extends BaseController 
{
    protected $news;

    public function __construct(NewsRepository $news)
    {
        $this->news = $news;
    }

    public function index()
    {
        return $this->news->withCriteria([
            new MyCriteria([
                'category_id' => '1', 'name' => 'Name'
            ]),
            new WhereAdmin(),
            ...
        ])->get();
    }
}
```

### Scope, Filter and Order

In your repository define which fields can be used to scope your queries by setting `$searchable` property.

```php
protected $searchable = [
    // where 'title' equals parameter value
    'title',
    // orWhere equals
    'body' => 'or',
    // where like
    'author' => 'like',
    // orWhere like
    'email' => 'orLike',
];
```

Search by searchables:

```php
public function index($request)
{
    return $this->news->scope($request)->get();
}
```

```
https://example.com/news?title=Title&body=Text&author=&email=gmail
```

Also several serchables enabled by default:

```php
protected $scopes = [
    // orderBy field
    'orderBy' => OrderByScope::class,
    // where created_at date is after
    'begin' => WhereDateGreaterScope::class,
    // where created_at date is before
    'end' => WhereDateLessScope::class,
];
```

```php
$this->news->scope($request)->get();
```

Enable ordering for specific fields by adding `$orderable` property to your model class:

```php
public $orderable = ['email'];
```

```
https://example.com/news?orderBy=email_desc&begin=2019-01-24&end=2019-01-26
```

`orderBy=email_desc` will order by email in descending order, `orderBy=email` - in ascending

You can also build your own custom scopes. In your repository override `scope()` method:

```php
public function scope($request)
{
    // apply build-in scopes
    parent::scope($request);

    // apply custom scopes
    $this->entity = (new NewsScopes($request))->scope($this->entity);

    return $this;
}
```

Create your `scopes` class and extend `ScopesAbstract`

```php
use AwesIO\Repository\Scopes\ScopesAbstract;

class NewsScopes extends ScopesAbstract
{
    protected $scopes = [
        // here you can add field-scope mappings
        'field' => MyScope::class,
    ];
}
```

Now you can build any scopes you need:

```php
use AwesIO\Repository\Scopes\ScopeAbstract;

class MyScope extends ScopeAbstract
{
    public function scope($builder, $value, $scope)
    {
        return $builder->where($scope, $value);
    }
}
```

### Artisan Commands

Package provides useful artisan command:

```bash
php artisan repository:generate Models/Order --scope=Search
```

#### It'll generate several classes for ```App\Models\Order```:

Main repository: ```App\Repositories\Orders\OrdersRepository```

Main scopes class: ```App\Repositories\Orders\Scopes\OrdersScopes```

Individual search scope class: ```App\Repositories\Orders\Scopes\SearchOrdersScope```

## Testing

The coverage of the package is <a href=""https://www.awes.io/?utm_source=github&amp;utm_medium=shields""><img src=""https://repo.pkgkit.com/4GBWO/awes-io/repository/badges/master/coverage.svg"" alt=""Coverage report""></a>.
                                   
You can run the tests with:

```bash
composer test
```

## Contributing

Please see [contributing.md](contributing.md) for details and a todolist.

## Credits

- [Galymzhan Begimov](https://github.com/begimov)
- [All Contributors](contributing.md)

## License

[MIT](http://opensource.org/licenses/MIT)
"
152,green-fox-academy/git-lesson-repository,HTML,"# git-lesson-repository
An example repository for the command line workshop in tools and basics module.

For the problems see the workshop: https://github.com/greenfox-academy/teaching-materials/edit/master/workshop/command-line
"
153,thinkgem/repository,Shell,"repository
=========="
154,darcyclarke/Repo.js,,"Repo.js
=======

Repo.js is a jQuery plugin that lets you easily embed a Github repo onto your site. This is great for other plugin or library authors that want to showcase the contents of a repo on their project pages. 

Repo.js uses [Markus Ekwall](https://twitter.com/#!/mekwall)'s [jQuery Vangogh](https://github.com/mekwall/jquery-vangogh) plugin for styling of file contents. Vangogh, subsequently, utilizes [highlight.js](https://github.com/isagalaev/highlight.js), written by [Ivan Sagalaev](https://github.com/isagalaev) for syntax highlighting.

## Example Usage

```js
$('body').repo({ user: 'darcyclarke', name: 'Repo.js' })
```

You can also reference a specific branch if you want:

```js
$('body').repo({ user: 'jquery', name: 'jquery', branch: 'strip_iife' })
```

"
155,repology/repology-updater,Python,"# Repology

![CI](https://github.com/repology/repology-updater/workflows/CI/badge.svg)
[![codecov](https://codecov.io/gh/repology/repology-updater/branch/master/graph/badge.svg)](https://codecov.io/gh/repology/repology-updater)

Repology is a service which monitors *a lot* of package repositories
and other sources and aggregates data on software package versions,
reporting new releases and packaging problems.

This repository contains Repology updater code, a backend service
which updates the repository information. See also the
[web application](https://github.com/repology/repology-webapp) code.

## Dependencies

  - [Python](https://www.python.org/) 3.9+
  - Python module [Jinja2](http://jinja.pocoo.org/)
  - Python module [libversion](https://pypi.python.org/pypi/libversion) (also requires [libversion](https://github.com/repology/libversion) C library)
  - Python module [psycopg2](http://initd.org/psycopg/)
  - Python module [pyyaml](http://pyyaml.org/)
  - Python module [xxhash](https://github.com/ifduyue/python-xxhash)
  - [PostgreSQL](https://www.postgresql.org/) 13.0+
  - PostgreSQL extension [libversion](https://github.com/repology/postgresql-libversion)

Needed for fetching/parsing repository data:

  - Python module [requests](http://python-requests.org/)
  - Python module [rubymarshal](https://github.com/d9pouces/RubyMarshal)
  - Python module [lxml](http://lxml.de/)
  - Python module [rpm](http://rpm.org/) (comes with RPM package manager)
  - Python module [jsonslicer](https://pypi.org/project/jsonslicer/)
  - Python module [pyparsing](https://github.com/pyparsing/pyparsing)
  - Python module [protobuf](https://github.com/protocolbuffers/protobuf)
  - Python module [zstandard](https://pypi.org/project/zstandard/)
  - Python module sqlite3 (part of Python, sometimes packaged separately)
  - [git](https://git-scm.com/)
  - [rsync](https://rsync.samba.org/)
  - [subversion](https://subversion.apache.org/)

### Development dependencies

Optional, for doing HTML validation when running tests:
  - Python module [pytidylib](https://pypi.python.org/pypi/pytidylib) and [tidy-html5](http://www.html-tidy.org/) library

Optional, for checking schemas of configuration files:
  - Python module [voluptuous](https://pypi.python.org/pypi/voluptuous)

Optional, for python code linting:
  - Python module [flake8](https://pypi.python.org/pypi/flake8)
  - Python module [flake8-builtins](https://pypi.python.org/pypi/flake8-builtins)
  - Python module [flake8-import-order](https://pypi.python.org/pypi/flake8-import-order)
  - Python module [flake8-quotes](https://pypi.python.org/pypi/flake8-quotes)
  - Python module [mypy](http://mypy-lang.org/)

## Running

### Preparing

Since repology rules live in separate repository you'll need to
clone it first. The location may be arbitrary, but `rules.d`
subdirectory is what default configuration file points to, so
using it is the most simple way.

```shell
git clone https://github.com/repology/repology-rules.git rules.d
```

### Configuration

First, you may need to tune settings which are shared by all repology
utilities, such as directory for storing downloaded repository state
or DSN (string which specifies how to connect to PostgreSQL database).
See `repology.conf.default` for default values, create `repology.conf`
in the same directory to override them (don't edit `repology.conf.default`!)
or specify path to alternative config via `REPOLOGY_SETTINGS`
environment variable, or override settings via command line.

By default, repology uses `./_state` directory for storing raw and parsed
repository data and `repology/repology/repology` database/user/password
on localhost.

### Creating the database

For the following steps you'll need to set up the database. Ensure
PostgreSQL server is up and running, and execute the following
commands to create the database for repology:

```shell
psql --username postgres -c ""CREATE DATABASE repology""
psql --username postgres -c ""CREATE USER repology WITH PASSWORD 'repology'""
psql --username postgres -c ""GRANT ALL ON DATABASE repology TO repology""
psql --username postgres --dbname repology -c ""CREATE EXTENSION pg_trgm""
psql --username postgres --dbname repology -c ""CREATE EXTENSION libversion""
```

in the case you want to change the credentials, don't forget to add
actual ones to `repology.conf`.

Next you can create database schema (tables, indexes etc.) and at the
same time test that the database is accessible with the following command:

```shell
./repology-update.py --initdb
```

### Fetching/updating repository data

The database is now ready to be filled with data. Typical Repology
update cycle consists of multiple steps, but in most cases you'll need
to just run all of them:

```shell
./repology-update.py --fetch --fetch --parse --database --postupdate
```

  - `--fetch` tells the utility to fetch raw repository data
    (download files, scrape websites, clone git repos) into state
    directory. Note that it won't refetch (update) data unless
    it's specified twice.
  - `--parse` enables parsing downloaded data into internal format
    which is also saved into state directory.
  - `--database` pushes processed package data into the database.
  - `--postupdate` runs additional database processing such as
    calculating summaries and updating feeds. It's separate from
    `--database` because it can be ran in background, parallelly
    to the following fetch/update cycle.

## Documentation

  - How to extend or fix [rules](https://github.com/repology/repology-rules/blob/master/README.md) for package matching
  - How repology [compares versions](https://github.com/repology/libversion/blob/master/doc/ALGORITHM.md)

## Author

  - [Dmitry Marakasov](https://github.com/AMDmi3) <amdmi3@amdmi3.ru>

## License

GPLv3 or later, see [COPYING](COPYING).
"
156,Yet-Another-Series/Yet_Another_Algorithms_Repository,C++,"![APM](https://img.shields.io/apm/l/vim-mode)
![GitHub contributors](https://img.shields.io/github/contributors/AshishOhri/Yet_Another_Algorithms_Repository)
![GitHub pull requests](https://img.shields.io/github/issues-pr/AshishOhri/Yet_Another_Algorithms_Repository)
![GitHub issues](https://img.shields.io/github/issues/AshishOhri/Yet_Another_Algorithms_Repository)
[![CodeFactor](https://www.codefactor.io/repository/github/Yet-Another-Series/yet_another_algorithms_repository/badge)](https://www.codefactor.io/repository/github/ashishohri/yet_another_algorithms_repository)
![GitHub forks](https://img.shields.io/github/forks/AshishOhri/Yet_Another_Algorithms_Repository?style=social)
# Yet_Another_Algorithms_Repository
<i>Beginners are welcome to contribute to this repository!</i>
<p align=""center"">
 <a href=""https://hacktoberfest.digitalocean.com/"">
<img src='images/hacktoberfest-img.svg' width=""400"" height=""400"" align=""center"">
  </a>
</p>
:tada: :sparkles:<i>Contribute now and get a chance to earn a free tee!:sparkles: :tada:<br>
For more information follow this link : https://hacktoberfest.digitalocean.com/</i>

## Getting Started

* This repository is **beginner friendly** and anyone can add new algorithms and data structures. 
* Please **follow** [File and Folder Naming Guidelines](https://github.com/AshishOhri/Yet_Another_Algorithms_Repository/blob/master/CONTRIBUTING.md#File-and-Folder-Naming-Guidelines).
* Also, everyone is free to contribute in any of the the languages mentioned in the <a href=""https://github.com/AshishOhri/Yet_Another_Algorithms_Repository/blob/master/CONTRIBUTING.md"">Contributing Guidelines</a>.

### Prerequisites

Anyone with a knack for programming is free to push!


## Contributing Guidelines

Please read [CONTRIBUTING.md](https://github.com/AshishOhri/Yet_Another_Algorithms_Repository/blob/master/CONTRIBUTING.md) for details on how to get started and submit pull requests to us.


## Authors

<table>
  <tr>
    <td align=""center""><a href=""https://github.com/iam-Shashank""><img src=""https://avatars0.githubusercontent.com/u/34963991?s=460&v=4"" width=""100px;"" alt=""Shashank Agrawal""/><br /><sub><b>Shashank Agrawal</b></sub></a></td>
    <td align=""center""><a href=""https://github.com/AshishOhri""><img src=""https://avatars1.githubusercontent.com/u/44030156?s=460&v=4"" width=""100px;"" alt=""Ashish Ohri""/><br /><sub><b>Ashish Ohri</b></sub></a></td>
    <td align=""center""><a href=""https://github.com/subhahu123""><img src=""https://avatars0.githubusercontent.com/u/34541684?s=460&v=4"" width=""100px;"" alt=""Subhahu Jain""/><br /><sub><b>Subhahu Jain</b></sub></a></td>
  </tr>
 </table>
 
 ## Maintainers
 
 <table>
  <tr>
    <td align=""center""><a href=""https://github.com/anujji1999""><img src=""https://avatars1.githubusercontent.com/u/33349650?s=460&v=4"" width=""100px;"" alt=""Anuj Gupta""/><br /><sub><b>Anuj Gupta</b></sub></a></td>
   <td align=""center""><a href=""https://github.com/Nikunj-Aggarwal""><img src=""https://avatars3.githubusercontent.com/u/42604363?s=400&v=4"" width=""100px;"" alt=""Nikunj Aggarwal""/><br /><sub><b>Nikunj Aggarwal</b></sub></a></td>
   <td align=""center""><a href=""https://github.com/sammjainn""><img src=""https://avatars3.githubusercontent.com/u/46999417?s=400&v=4"" width=""100px;"" alt=""Samriddhi Jain""/><br /><sub><b>Samriddhi Jain</b></sub></a></td>
  </tr>
 </table>

## Contributors

See also the list of [contributors](https://github.com/AshishOhri/Yet_Another_Algorithms_Repository/graphs/contributors) who participated in this project.

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgments

* <a href=""https://github.com/all-contributors"">all-contributors</a>
* <a href=""https://gist.github.com/PurpleBooth/109311bb0361f32d87a2"">PupleBooth</a>
"
157,OAI/OpenAPI-Specification,JavaScript,"# The OpenAPI Specification

![Build Status](https://github.com/OAI/OpenAPI-Specification/workflows/validate-markdown/badge.svg)

![](https://avatars3.githubusercontent.com/u/16343502?v=3&s=200)


The OpenAPI Specification is a community-driven open specification within the [OpenAPI Initiative](https://www.openapis.org/), a Linux Foundation Collaborative Project.

The OpenAPI Specification (OAS) defines a standard, programming language-agnostic interface description for HTTP APIs, which allows both humans and computers to discover and understand the capabilities of a service without requiring access to source code, additional documentation, or inspection of network traffic. When properly defined via OpenAPI, a consumer can understand and interact with the remote service with a minimal amount of implementation logic. Similar to what interface descriptions have done for lower-level programming, the OpenAPI Specification removes guesswork in calling a service.

Use cases for machine-readable API definition documents include, but are not limited to: interactive documentation; code generation for documentation, clients, and servers; and automation of test cases. OpenAPI documents describe an APIs services and are represented in either YAML or JSON formats. These documents may either be produced and served statically or be generated dynamically from an application.

The OpenAPI Specification does not require rewriting existing APIs. It does not require binding any software to a service – the service being described may not even be owned by the creator of its description. It does, however, require the capabilities of the service be described in the structure of the OpenAPI Specification. Not all services can be described by OpenAPI – this specification is not intended to cover every possible style of HTTP APIs, but does include support for [REST APIs](https://en.wikipedia.org/wiki/Representational_state_transfer). The OpenAPI Specification does not mandate a specific development process such as design-first or code-first. It does facilitate either technique by establishing clear interactions with a HTTP API.

This GitHub project is the starting point for OpenAPI. Here you will find the information you need about the OpenAPI Specification, simple examples of what it looks like, and some general information regarding the project.

## Current Version - 3.1.0

The current version of the OpenAPI specification is [OpenAPI Specification 3.1.0](versions/3.1.0.md).

### Previous Versions

This repository also contains all [previous versions](versions).

Each folder in this repository, such as [examples](examples) and [schemas](schemas), should contain folders pertaining to the current and previous versions of the specification.

## See It in Action

If you just want to see it work, check out the [list of current examples](examples).

## Tools and Libraries

Looking to see how you can create your own OpenAPI definition, present it, or otherwise use it? Check out the growing
[list of implementations](IMPLEMENTATIONS.md).

## Participation

The current process for development of the OpenAPI Specification is described in 
[Development Guidelines](DEVELOPMENT.md).
Development of the next version of the OpenAPI Specification is guided by the [Technical Steering Committee (TSC)](https://www.openapis.org/participate/how-to-contribute/governance#TDC). This group of committers bring their API expertise, incorporate feedback from the community, and expand the group of committers as appropriate. All development activity on the future specification will be performed as features and merged into this branch. Upon release of the future specification, this branch will be merged to `main`.

The TSC holds weekly web conferences to review open pull requests and discuss open issues related to the evolving OpenAPI Specification. Participation in weekly calls and scheduled working sessions is open to the community. You can view the [TSC calendar online](https://openapi.groups.io/g/tsc/calendar), and import it to your calendar using the [iCal link](https://openapi.groups.io/g/tsc/ics/1105671/1995679554/feed.ics).

The OpenAPI Initiative encourages participation from individuals and companies alike. If you want to participate in the evolution of the OpenAPI Specification, consider taking the following actions:

* Review the [current specification](versions/3.1.0.md). The human-readable markdown file _is the source of truth_ for the specification.
* Review the [development](DEVELOPMENT.md) process so you understand how the spec is evolving.
* Check the [issues](https://github.com/OAI/OpenAPI-Specification/issues) and [pull requests](https://github.com/OAI/OpenAPI-Specification/pulls) to see if someone has already documented your idea or feedback on the specification. You can follow an existing conversation by subscribing to the existing issue or PR.
* Create an issue to describe a new concern. If possible, propose a solution.

Not all feedback can be accommodated and there may be solid arguments for or against a change being appropriate for the specification.

## Licensing

See: [License (Apache-2.0)](https://github.com/OAI/OpenAPI-Specification/blob/main/LICENSE)

![Analytics](https://ga-beacon.appspot.com/UA-831873-42/readme.md?pixel)
"
158,touchgfx/touchgfx-open-repository,,"# <img src=""http://touchgfx.com/static/touchgfx_logo_open_rep_small.png"" width=""500"" height=""135"">

TouchGFX documentation can be found at [touchgfx.com](https://support.touchgfx.com) and TouchGFX Community can be found at [st.com](https://community.st.com/s/topic/0TO0X0000003iw6WAA/touchgfx).


"
159,technojam/Ultimate_Algorithms_Repository,C++,"# DATA STRUCTURE AND ALGORITHMS(C++)

## DISCLAMER
   1. Knowing a programming language and DSA are two different things .

      eg. you might be knowing python , c, c++ , java etc but if you know DSA then you can implement them in any programming language with minor syntatic modifications.

   2. This Repository does not intend to spoon feed , it can be used for studying in the right direction , but the efforts should be solely yours .

## WHY ?
  1. If you want to crack the interviews and get into the product based companies.
  2. Nearly everything functional in computer science is an implementation of data structure along with certain algorithms
  3. If you love to solve the real-world complex problems.


## ROADMAP
  1. 1-D Arrays
  2. 2-D Arrays
  3. Array ADT
  4. Character Arrays
  5. Strings
  6. Pointers
  7. Dynamic Memory Allocation
  8. STL - ALGO'S
  9. Sorting Algorithms
  10. BINARY SEARCH
  11. Vectors
  12. Bit Manipulation
  13. Number Theory
  14. Recursion
  15. Backtracking
  16. Space and Time Complexities
  17. OOP'S
  18. Linked List
  19. Stacks
  20. Queues
  21. Deque
  22. Binary Tree
  23. Binary Search Tree
  24. Heaps
  25. Hashing
  26. Greedy Algorithms
  27. Dynamic Programming
  28. Graph
  29. Segment Tree/Fenwick Tree


  # Contributing

  When contributing to this repository, please first discuss the change you wish to make via issue,
  email, or any other method with the owners of this repository before making a change.

  Please note we have a code of conduct, please follow it in all your interactions with the project.

  ## Pull Request Process

  1. Ensure that every PR is linked with an issue, all standalone PRs will be rejected by the maintainers.
  2. Discuss all the features and requirements in issue section before sending an PR.
  3. It would be really appreciated if you try to look into previous created issue instead of a new one.
  4. Use proper template and be describe all the changes that you are addressing in a PR.
  5. Add relevant comments explaining what the code is all about (if possible write the Time and Space complexities)
  6. Any type pf plagrised code or fishy code will not be merged.
  5. Once you follow above points and your PR gets approved by more than 2 reviewers it will be merged by the maintainers.

  ## Code of Conduct

  ### Our Pledge

  In the interest of fostering an open and welcoming environment, we as
  contributors and maintainers pledge to making participation in our project and
  our community a harassment-free experience for everyone, regardless of age, body
  size, disability, ethnicity, gender identity and expression, level of experience,
  nationality, personal appearance, race, religion, or sexual identity and
  orientation.

  ### Our Standards

  Examples of behavior that contributes to creating a positive environment
  include:

  * Using welcoming and inclusive language
  * Being respectful of differing viewpoints and experiences
  * Gracefully accepting constructive criticism
  * Focusing on what is best for the community
  * Showing empathy towards other community members

  Examples of unacceptable behavior by participants include:

  * The use of sexualized language or imagery and unwelcome sexual attention or
  advances
  * Trolling, insulting/derogatory comments, and personal or political attacks
  * Public or private harassment
  * Publishing others' private information, such as a physical or electronic
    address, without explicit permission
  * Other conduct which could reasonably be considered inappropriate in a
    professional setting
    
   ## Contact Info
   
   Feel free to contact us to discuss any issues, questions, or comments.
   Our contact info can be found on our [GitHub page](https://github.com/technojam).

 
 
"
160,ioBroker/ioBroker.repositories,JavaScript,"# ioBroker.repositories

This is github project for storage of latest and stable repositories.

## Update of the version in stable
1. Be sure that the version is tested in forum by users or you fix the critical bug with that.
2. Delete the versionTime if exists

## Add a new adapter to the latest repository
1. Fork this repo and clone your fork
2. Run `npm i`
3. Run `npm run addToLatest -- --name <adapter-name> --type <adapter-type>`  
    (replace `<adapter-name>` with your adapter's name (without 'iobroker.' prefix) and `<adapter-type>` with the adapter type)
4. Push a commit with the changes to `sources-dist.json`
5. Create a PR

## Requirements for adapter to get added to the latest repository

*already required for latest repository*

1. Your github repository must have name ""ioBroker.<adaptername>"". **B** is capital in ""ioBroker"", but in the package.json the *name* must be low case, because npm does not allow upper case letters. Your repository must have ""topics"". Add these with `Manage topics`.
2. Do not use in the title the words ""ioBroker"" or ""Adapter"". It is clear anyway, that it is adapter for ioBroker.
3. *title* in io-package.json (common) is simple short name of adapter in english. *titleLang* is object that consist short names in many languages. *Lang* ist not german `Länge`, but english `LANGuages`.
4. Adapter needs to have a README.md with description, detail information and changelog. English is mandatory. Other languages are welcome. See [Example of README.md](#example-of-readme-md).

   **In README.md, there must be a link to the device or the manufacturer's website. Devices must have a photo. Services do not require a photo, but are still welcome.**
5. Adapter must have a predefined license.
6. Please remove www, widgets and docs directories (admin/tab_m.html, admin/custom_m.html) if not used.
7. Adapter needs to have at least Adapter basic testing (installing, running) using Travis-CI (optionally and Appveyor). More information in Forum from apollon77 (Just take from other adapters the samples)
8. Define one of the types in io-package.json. See details [here](#types)
9. Define one of the connection types (if applied) in io-package.json. See details [here](#connection-types)
10. All states must have according [valid roles](https://github.com/ioBroker/ioBroker/blob/master/doc/STATE_ROLES.md#state-roles) (and not just ""state"")
11. Include ""author"" in io-package.json and ""authors"" in io-package.json. See [here](#authors).
12. Adapter needs to be available as package on npm. See [How to publish on npm](#how-to-publish-on-npm)
13. iobroker organisation must be added as owner to npm package. [Why and how to do that.](#add-owner-to-packet)
14. Add your adapter into the list (first latest and after that into stable, when tested).
   Examples of entries you can find [here](#samples).
15. No new adapters will be accepted to repo without admin3 Configuration dialog. Admin2 dialog is optional!
16. Check and Follow the Coding best practices listed below

*Note:* you can watch the video about it (only german) on [youtube](https://www.youtube.com/watch?v=7N8fsJcAdlE)
*Note:* There is a helper https://adapter-check.iobroker.in/ to check many points automatically. Just place your github adapter repo there, e.g `https://github.com/ioBroker/ioBroker.admin` and press enter or on the check button.

## Development and Coding best practices
* Best use the adapter creator (https://github.com/ioBroker/create-adapter) or get a fresh relevant version from the Template Repository (https://github.com/ioBroker/ioBroker.template) to start coding to always get the latest basic version and also updates. You should not always copy basic files from former adapters!
* Do not copy a package.json or io-package.json after an installation because some fields might have been added on installation! e.g. io-package with common.installedFrom eds to be removed
* **Use the Adapter Checker and fix all issues shown there: https://adapter-check.iobroker.in/**
* Respect Object and state definitions, types and roles Values not defined here should not be used. Discussions about missing roles or types are welcome:
  * https://github.com/ioBroker/ioBroker.docs/blob/master/docs/en/dev/objectsschema.md#object-types
  * https://github.com/ioBroker/ioBroker.docs/blob/master/docs/en/dev/stateroles.md
* Only commit .vscode, .idea or other IDE files/helper directories to GitHub if there is a need to. This is to prevent other users settings to interfere with yours or make PRs more complex because of this.
* If you do not need onState/ObjectChange/Message please do not implement it
* if you need to store passwords please encrypt them in Admin! You can check e.g. Apollon77/iobroker.meross for example code in index_m.html and main.js
* add all editable fields from index_m.html to io-package native with their default values
* **You need to make sure to clean up ALL resources in ""unload"". Clear all Timers, Intervals, close serial ports and servers and end everything. Else this will break the compact mode**
* **Please test in compact mode!** Especially starting, running, stopping adapter and verify that nothing runs any longer and no logs are triggered and also a new start works.
* Be careful with ""setObject"" because it overwrites the object and (especially in js-controller < 2.2) custom settings like history may be removed by this! Use setObjectNotExists or read the object to detect if it exists and use extendObject to update.
* get familiar with the ""ack"" concept of ioBroker. Adapters normally set all ""final"" values with ack=true and these are mostly ignored in onStateChange handlers. ack=false are commands that normally are handled by Adapters.
* Do not use process.exit() because this breaks compact mode. Use adapter.terminate() if the method is available.
* If you consider using a scheduling library in conjunction with external/cloud services then consider the potential consequences! If your adapter becomes successfull then all users will do their calls to the external service in the exact same second. This can become a DOS stile ""attack"" to that server with bad consequences. Additioanlly to that using a Scheduling library just to implement intervals is overkill :-) setInterval/setTimeout should be completely sufficiant AND has the good side effect that requests are not done all at the same second, but start when the adapter starts.
* When using Intervals together with external communication think about timeout and error cases - an interval triggers the next call also if the last has not finished. So requests might pile up and you DOS the external API. A better practice might be to use setTimeout and set at the end of one call for the next call
* If you use ""connections"" to other systems (Websockets, MQTT, TCP, Serial or other) please also implement the info.connection state (directly create objects by including in io-package) and set the connection value accordingly. Using this enables Admin to differentiate the status between green (ok, running), yellow (basically running but not connected) and red (not running).
* Consider and understand the asynchronous nature of JavaScript and make sure to know what will happen in parallel and what makes more sence to be sequencially! It is ok to use callbacks or Promises/async/await - the latter makes it more easy to understand and control how your code really flows.
* Consider using ESLint or other JavaScript code and type checker to see errors in your code before releasing a new version.
* **Please activate adapter testing with at least package- and integration-tests on Travis-CI** GitHub Actions are not enough at the moment because they do not allow us to get an easy overview, especially when we want to see how our adapters behave with new nodejs versions.
* The adapter testing using Travis and/or GitHub Actions is not for us - it is for you! Please check it after pushing changes to GitHub and before telling it to users or publish an NPM package. If testing is ""red"" you should check the testing log to see whats broken.
* If you like to increase testing you can start implementing adapter specific tests that always run when you push changes to GitHub.
* You can/should use https://translator.iobroker.in/ to auto translate all relevant texts into all needed languages by providing the english text
* If an adapter instance want to generate an object structure it should use objects from the type device, channel or folder to define sub-structures and provide objects of type state only on the last ""level"". Different levels can be separated by a ""."". An object of the type ""state"" should never have more objecte below it. The allowed field for the relevant object types are documented in https://github.com/ioBroker/ioBroker.docs/blob/master/docs/en/dev/objectsschema.md#core-concept

## Add a new adapter to the stable repository
1. Fork this repo and clone your fork
2. Run `npm i`
3. Run `npm run addToStable -- --name <adapter-name> --version <stable-version>`  
    (replace `<adapter-name>` with your adapter's name and `<stable-version>` with the version that should be added to the stable repo)
4. Push a commit with the changes to `sources-dist-stable.json`
5. Create a PR

### Requirements for adapter to get added to the stable repository

Additionally to all above listed points:

15. Forum thread with question to test the adapter.
16. Some feedback on [forum](http://forum.iobroker.net).
17. **Important** Discovery function! If device can be found automatically (USB, IP) it must be implemented in discovery adapter.

## How-to
### How to publish on npm

https://docs.npmjs.com/getting-started/publishing-npm-packages

### Add owner to packet
We are really happy, that other developers are contributing to ioBroker. But some of them with the time lost the enthusiasms and stop support and maintain the adapter.

There is no problem with github repository. We can just fork it and maintain it in our organisation, but the situation with **npm** is different.

If some name is blocked (e.g. iobroker.rpi) we cannot publish the changed adapter under the same name, we must change the name to e.g. iobroker.rpi2.

Than we must change the ioBroker repositories and the user must install the new adapter and migrate the old settings and objects into new adapter.

This is not suitable.

Because of that we ask you to give ioBroker organisation publish rights to update the npm package. We will use it only in emergency or if author do not react to our requests.

To add the ioBroker organisation to npm packet, you must write following, after the packet is published:

```npm access grant read-write iobroker:developers iobroker.<adaptername>```

If the command does not work just add bluefox as owner.

```npm owner add bluefox iobroker.<adaptername>```

### Example of README.md

https://github.com/ioBroker/ioBroker.admin/blob/master/README.md

Much better is to write documentation separately for every language like here:

https://github.com/ioBroker/ioBroker.admin/blob/master/io-package.json#L171

and the files are here https://github.com/ioBroker/ioBroker.admin/tree/master/docs

And make the link in your readme file to this files, like here: https://github.com/ioBroker/ioBroker.javascript/blob/master/README.md

### Licenses
Following licenses are used now in ioBroker project:

* MIT (used for most of adapters and core)
* Apache 2.0
* CC-BY
* OFL-1.1
* EPL
* LGPL
* GPLv3
* GPLv2
* CC-BY-NC
* CC-BY-NC-SA

You can choose the suitable license here: https://ufal.github.io/public-license-selector/

Of course you can add your own licenses, even WTFPL.

You must of course take in count the licenses of components, that used in your adapter. E.g. if you use main packet under GPLv2 license, you cannot make CC-BY-NC from that.

### Testing
See how testing is implemented on ioBroker.template:
 - https://github.com/ioBroker/ioBroker.template/tree/master/test
 - https://github.com/ioBroker/ioBroker.template/blob/master/package.json#L39
 - Activate tests on travis-ci.org: https://github.com/mbonaci/mbo-storm/wiki/Integrate-Travis-CI-with-your-GitHub-repo
 - Activate appveyor (for windows) if applicable: https://www.appveyor.com/

You can find some help in this [PDF](http://forum.iobroker.net/download/file.php?id=11259) (Only german) See **Adapter Testing** Section.

### Types
The io-package.json must have attribute type in common part.
An example can be seen [here](https://github.com/ioBroker/ioBroker.template/blob/1e48d01e69c9ad15c70ab8dced572a4d6882ae0d/io-package.json#L76):

- `alarm` - security of home, car, boat, ...
- `climate-control` - climate, heaters, air filters, water heaters, ...
- `communication` - deliver data for other services via RESTapi, websockets
- `date-and-time` - schedules, calendars, ...
- `energy` - energy metering
- `metering` - other, but energy metering (water, gas, oil, ...)
- `garden` - mower, springs, ...
- `general` - general purpose adapters, like admin, web, discovery, ...
- `geoposition` - geo-positioning. These adapters delivers or accepst the position of other objects or persons.
- `health` - heart pulse, blood pressure, body weight, ...
- `hardware` - different multi-purpose hardware, arduino, esp, bluetooth, ...
- `household` - vacuum-cleaner, kitchen devices, ...
- `infrastructure` - Network, printers, phones, NAS, ...
- `iot-systems` - Other comprehensive smarthome systems (software and hardware)
- `lighting` - light
- `logic` - rules, scripts, parsers, scenes, ...
- `messaging` - these adapters send and receive messages from message services: telegram, email, whatsapp, ...
- `misc-data` - export/import of some unsorted information, contacts, systeminfo, gazoline prises, share curses, currents (EUR=>USD), ...
- `multimedia` - TV, AV Receivers, TV play boxes, Android/apple TV boxes, multi-room music, IR controls, speech input/output, ...
- `network` - ping, network detectors, UPnP, ...
- `protocols` - Communication protocols: MQTT,
- `storage` - logging, data protocols, SQL/NoSQL DBs, file storage, ...
- `utility` - different help adapters. Like backup, export/import
- `vehicle` - cars 
- `visualization` - visualisation, like vis, material, mobile
- `visualization-icons` - icons for visualisation
- `visualization-widgets` - iobroker.vis widgets
- `weather` - weather info, air quality, environment statistics

You can see the types of existing adapters [here](http://download.iobroker.net/list.html#sortCol=type&sortDir=0) and try to find the similar one.

### Connection types
If your adapter control some device/car/house the adapter could be connected with with various methods and receives data via different protocols.

Define `connectionType` in `common` part of `io-package.json` as:
- `local` - if the communication with device do not requie cloud access.
- `cloud` - if the communication is via cloud.

Define `dataSource` in `common` as:
- `poll` - Querying the status means that an update may be noticed later.
- `push` - ioBroker will be notified when a new status is available.
- `assumption` - The status of the device cannot be determined. ioBroker takes status based on last ioBroker command.

#### Defined categories for non-repo adapters
* pilight -	 iot-systems
* samsung2016 -	multimedia
* scriptgui	- logic
* viessmann	- climate-control
* vuplus - multimedia

### Authors
Please define following attributes in package.json :
- https://github.com/ioBroker/ioBroker.template/blob/master/JavaScript/package.json#L5 (Only one author)
- https://github.com/ioBroker/ioBroker.template/blob/master/JavaScript/package.json#L9 (Many contributors)
- https://github.com/ioBroker/ioBroker.template/blob/master/JavaScript/io-package.json#L32 (Same here, but you can set many authors/contributors if desired)

### Samples
For **latest** (sources-dist.json):

```
  ""admin"": {
    ""meta"": ""https://raw.githubusercontent.com/ioBroker/ioBroker.admin/master/io-package.json"",
    ""icon"": ""https://raw.githubusercontent.com/ioBroker/ioBroker.admin/master/admin/admin.png"",
    ""type"": ""general""
  },
```

For **stable** (sources-dist-stable.json):

```
  ""admin"": {
    ""meta"": ""https://raw.githubusercontent.com/ioBroker/ioBroker.admin/master/io-package.json"",
    ""icon"": ""https://raw.githubusercontent.com/ioBroker/ioBroker.admin/master/admin/admin.png"",
    ""type"": ""general"",
    ""version"": ""2.0.7""
  },
```

*Note*: stable has always specific version.
"
161,philtabor/Youtube-Code-Repository,Jupyter Notebook,"# Youtube-Code-Repository
Repository for all the code from my youtube channel
You can find me at https://youtube.com/MachineLearningWithPhil <br>

<h2> Kaggle/Venus-Volcanoes </h2>

My crude implementation of a convolutional neural network to perform image classification on data gathered <br>
by the Magellan spacecraft. The data is horribly skewed, as most images do not contain a volcano. <br>
This means we'll have to do some creative data engineering for our model training. <br>
Please note that in the test set, 84.1% of the data is ""no volcano"", and our model returns <br>
an accuracy of around 88%, which is better than a model that outputs straight 0s for predictions. <br>

You can check out the video for this at https://youtu.be/Ki-xOKydQrY <br>
You can find the data for this project at https://www.kaggle.com/fmena14/volcanoesvenus/home
<h2> ReinforcementLearning/DeepQLearning </h2>

My implementation of the Deep Q learning algorithm in PyTorch. Here we teach the algorithm to play the game of space invaders. I haven't had enough time to train this model yet, as it takes quite some time even on my 1080Ti / i7 7820k @ 4.4 GHz. I'll train
longer and provide a video on how well it does, at a later time.

The blog post talking about how Deep Q learning works can be found at http://www.neuralnet.ai/coding-a-deep-q-network-in-pytorch/ <br>
Video for this is at https://www.youtube.com/watch?v=RfNxXlO6BiA&t=2s



<h2> CNN.py </h2>

Simple implementation of a convolutional neural network in TensorFlow, version 1.5. <br>
Video tutorial on this code can be found here https://youtu.be/azFyHS0odcM <br>
Achieves accuracy of 98% after 10 epochs of training <br>
Requires data from http://yann.lecun.com/exdb/mnist/ <br>

<h2> ReinforcementLearning/blackJack-no-es.py </h2>

Implementation of Monte Carlo control without exploring starts in the blackjack environment from the OpenAI gym. <br>
Video tutorial on this code can be found at https://youtu.be/e8ofon3sg8E <br>
Algorithm trains for 1,000,000 games and produces a win rate of around 42%, loss rate of 52% and draw rate of 6% <br>

<h2> ReinforcementLearning/blackJack-off-policy.py </h2>

Implementation of off policy Monte Carlo control in the blackjack environment from the OpenAI gym. <br>
Video tutorial on this code can be found at https://youtu.be/TvO0Sa-6UVc <br>
Algorithm trains for 1,000,000 games and produces a win rate of around 29%, loss rate of 66% and draw rate of 5% <br>

<h2> ReinforcementLearning/cartpole_qlearning.py </h2>

Implementation of the Q learning algorithm for the cart pole problem. Code is based on the course by lazy programmer,  <br>
which you can find here <a href=""https://github.com/lazyprogrammer/machine_learning_examples/blob/master/rl/q_learning.py""> here </a>  <br>
Video tutorial on this code can be found at https://youtu.be/ViwBAK8Hd7Q <br>

<h2> ReinforcementLearning/doubleQLearning.py </h2>

Implementation of the double Q learning algorithm in the cart pole environment. This is based on my course on  <br>
reinforcement learning, which you can find at <a href=""https://github.com/philtabor/Reinforcement-Learning-In-Motion/tree/master/Unit-8-The-Mountaincar""> this repo </a> <br>
Video tutorial on this code can be found https://youtu.be/Q99bEPStnxk <br>

<h2> ReinforcementLearning/sarsa.py </h2>

Implementation of the SARSA algorithm in the cart pole environment. This is based on my course on reinforcement learning,  
which can be found <a href=""https://github.com/philtabor/Reinforcement-Learning-In-Motion/tree/master/Unit-7-The-Cartpole""> here </a> <br>
Video tutorial on this code can be found at https://youtu.be/P9XezMuPfLE <br>
"
162,dustinschultz/scf-config-repository,,
163,RobThree/MongoRepository,C#,"# ![Logo](https://raw.githubusercontent.com/RobThree/MongoRepository/master/mongorepositorylogo.png) Project Description

An easy to use library to use MongoDB with .NET. It implements a Repository pattern on top of Official MongoDB C# driver. This project is now available as a [NuGet](https://www.nuget.org) package for your convenience. If you're new to NuGet, [check it out](http://docs.nuget.org/); it's painless, easy and fast. You can find this project by [searching for MongoRepository](https://www.nuget.org/packages?q=mongorepository) in NuGet (or [simply clicking here](http://nuget.org/packages/MongoRepository)).

Check the [documentation](https://github.com/RobThree/MongoRepository/wiki/Documentation) for a step-by-step example and more advanced usage.

## Example:

```c#
// The Entity base-class is provided by MongoRepository
// for all entities you want to use in MongoDb
public class Customer : Entity 
{
        public string FirstName { get; set; }
        public string LastName { get; set; }
}

public class CustomerRepoTest
{
        public void Test()
        {
            var repo = new MongoRepository<Customer>();

            // adding new entity
            var newCustomer = new Customer {
                FirstName = ""Steve"",
                LastName = ""Cornell""
            };

            repo.Add(newCustomer);

            // searching
            var result = repo.Where(c => c.FirstName == ""Steve"");

            // updating 
            newCustomer.LastName = ""Castle"";
            repo.Update(newCustomer);
        }
}
```
[<img src=""http://i.imgur.com/2yf60gf.png"" alt=""Productivity Visual Studio add-in for C#, VB.NET, XML, XAML, ASP.NET and more"">](http://www.jetbrains.com/resharper/features/index.html)
"
164,cnych/kubeapp,Python,"# k8s-repo
Some commonly used kubernetes app 🎉🎉🎉~~~


"
165,grafana/grafana-plugin-repository,JavaScript,"# Grafana Plugin Repository

This repository lists all officially supported Grafana plugins. Plugins in this repository are listed on [Grafana.com](https://grafana.com/grafana/plugins), and can be [installed](https://grafana.com/docs/grafana/latest/plugins/installation/) locally using [Grafana CLI](https://grafana.com/docs/grafana/latest/administration/cli/#plugins-commands) or by [Grafana Cloud](https://grafana.com/products/cloud/) users.

We review all plugins before they are published. This means that it may take some time before we can review your plugin.

Here's a few things you can do to help us review your plugin faster.

- Validate your plugin release with [this plugin validator](http://plugin-validator.grafana.net)
- Use the [GitHub workflows](https://github.com/grafana/plugin-workflows) to automate your plugin release
- Read the [Review Guidelines](http://docs.grafana.org/plugins/developing/plugin-review-guidelines/) before submitting your plugin. These guidelines determine if the plugin is ready to be published or not.
- If possible, for datasource plugins please provide a description on how to set up a simple test environment. A docker container or simple install script helps speed up the review process a lot.

To submit a plugin for review:

1. Fork this repository
1. Add your plugin to `repo.json`
1. Create a pull request

> **Note:** Commercial plugins require a plugin subscription to be published. Commercial plugin subscriptions help us fund continued development of our open source platform and software. See the [terms](https://grafana.com/terms) for more details.

## Add a plugin to `repo.json`

To publish a plugin, add a new entry to the `plugins` array in [repo.json](repo.json).

Here's an example of a plugin release:

```json
{
  ""id"": ""briangann-gauge-panel"",
  ""type"": ""panel"",
  ""url"": ""https://github.com/briangann/grafana-gauge-panel"",
  ""versions"": [
    {
      ""version"": ""0.0.8"",
      ""url"": ""https://github.com/briangann/grafana-gauge-panel"",
      ""download"": {
        ""any"": {
          ""url"": ""https://github.com/briangann/grafana-gauge-panel/releases/download/v0.0.8/briangann-gauge-panel-0.0.8.zip"",
          ""md5"": ""782c973460f330287b7efca5486aa015""
        }
      }
    }
  ]
}
```

### Plugin Release Schema

| Property   | Description                                              |
|------------|----------------------------------------------------------|
| `id`       | Plugin ID. Needs to match the plugin ID in `plugin.json` |
| `type`     | Plugin type, e.g. `panel`, `datasource`, or `app`        |
| `url`      | URL to the plugin's GitHub project page                  |
| `versions` | List of all published versions of the plugin             |

### Plugin Version Schema

| Property   | Description                                                 |
|------------|-------------------------------------------------------------|
| `version`  | Plugin version. Needs to match the version in `plugin.json` |
| `url`      | URL to the plugin's GitHub project page                     |
| `download` | Download information.                                       |

### Plugin Download Schema

| Property   | Description                                                      |
|------------|------------------------------------------------------------------|
| `url`      | URL to a ZIP archive containing a production build of the plugin |
| `md5`      | MD5 check sum of the ZIP archive                                 |

## Resources

- [Plugin Development](https://grafana.com/docs/grafana/latest/developers/plugins/)
- [Sign a plugin](https://grafana.com/docs/grafana/latest/developers/plugins/sign-a-plugin)
- [Package a plugin](https://grafana.com/docs/grafana/latest/developers/plugins/package-a-plugin)
- [plugin.json Schema](https://grafana.com/docs/grafana/latest/developers/plugins/metadata/)
- [6 tips for improving your Grafana plugin before you publish](https://grafana.com/blog/2021/01/21/6-tips-for-improving-your-grafana-plugin-before-you-publish/)
"
166,dragonslayerx/Competitive-Programming-Repository,C++,"# Competitive-Programming-Repository
Collection of algorithms and data structures in C++ used widely in Competitive programming contests. 

### The following topics are covered:

#### Range Updates and Queries
* **Range Aggregate Queries** :
  * *Binary Indexed Trees (BIT)* :
    * [Point Update Range Query](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/binary_indexed_tree.cpp)
    * [Range Update Range Query](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/binary_indexed_tree_range_query_range_update.cpp)
    * [Order Statistic Query](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/binary_indexed_tree_order_stat.cpp)
    * [2D Binary Indexed Trees](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/binary_indexed_tree_2D.cpp)
  * *Segment Trees (SegTree)* :
    * [Point Update Range Query](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/segment_tree_range_query_point_update.cpp) 
    * [Fast Iterative Segtrees](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/segment_trees_interative_fast.cpp)
    * [Range Update Point Query - Lazy Propogation](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/segment_tree_range_query_range_update_lazy_propogation.cpp)
    * [Max subsegment sum in range](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/segment_tree_custom_merge_function.cpp)
    * [2D Segment Trees](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/segment_tree_2D.cpp)
    * [Dynamic Segment Trees - Insertion/Deletion between elements](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/segment_tree_dynamic_using_treaps.cpp)
    * [Dynamic Segment Trees - Reverse a segment](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/segment_tree_dynamic_reverse_subarray_using_treap.cpp)
  * *Merge Sort Trees* :
    * [Merge sort trees](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/merge_sort_trees.cpp)
    * [Merge sort trees - Order Statistics](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/merge_sort_trees_order_stat_query.cpp)  
  * *Sparse Table* :
    * [Range Minimum Query](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/range_minimum_query_sparse_table.cpp)
  * *Mo Algorithm* :
    * [Mo Algorithm - Arrays](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/mo_algorithm_offline_range_query.cpp)
* **Dynamic Programming** :
  * *Dynamic Programming Templates* :
    * [Digit DP / Bitwise DP](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/dynamic_programming_templates.cpp)
  * *Standard DP Problems* :
    * [Longest Increasing Subsequence](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/longest_increasing_subsequence_lis_binary_search.cpp)
    * [Longest Palindromic Subsequence](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/palindrome_longest_subsequence.cpp)
    * [Levenstein Distance / Edit Distance](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/edit_distance_levenstein_dynamic_programming.cpp)
* **Graphs** :
  * *Single Source Shortest Path Algorithms* :
    * [Dijkstra in dense graphs](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/dijsktra_dense_graphs.cpp)
    * [Dijkstra using priority queue](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/dijkstra_using_priority_queue.cpp)
    * [Kth Shortest Path between Nodes using Dijkstra](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/kth_shortest_path_between_nodes_graph.cpp)
    * [Bellman Ford Negative cycle detection](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/bellman_ford.cpp)
  * *All Pair shortest path* :
    * [Using Binary exponentiation](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/all_pair_shortest_path_binary_exponentation.cpp)
    * [Floyd Warshall](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/all_pair_shortest_path_floyd_warshall.cpp)
  * *Cycle Detection* :
    * [Cycle detection in Undirected/Directed Graphs](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/cycle_detection_in_graph.cpp)
  * *Minimum Spanning tree* :
    * [Kruskal Algorithm](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/kruskal_min_spanning_tree.cpp)
  * *Topological Sort / Strongly Connected Component* :
    * [Topological Sort](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/topological_sort_kosaraju.cpp)
    * [Strongly Connected Component](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/strongly_connected_components_kosaraju.cpp) 
  * *Maxflow/Matching* :
    * [Hopkroft Karp Max Matching](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/max_bipartite_matching_hopcroft_karp.cpp)
    * [Dinic Max Flow](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/max_flow_network_dinic_algorithm.cpp)
  * *Misc* :
    * [Bridges in Graph](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/bridges_in_graph.cpp)
    * [Connectivity](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/isConnected_using_bfs.cpp)
    * [Bipartite Check](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/non_bipartite_check.cpp)
* **Trees** :
    * *Ancestor queries* :
        * [Lowest Common Ancestor](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/lowest_common_ancestor_lca.cpp)
        * [Kth Ancestor](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/kth_ancestor_tree.cpp)
    * *Path queries* :
        * [Sparse Table *](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/trees_path_query_sparse_tables.cpp)
        * [Heavy Light Decomposition Weighted Nodes](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/heavy_light_decomposition_wieghted_vertices(hld).cpp)
        * [Heavy Light Decomposition Weighted Edges](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/heavy_light_decomposition_weighted_edges%20(hld).cpp)
    * *Misc* :
        * [Diameter of Tree](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/tree_diameter.cpp)
        * [Preorder/Postorder stamps, Is it in Subtree?](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/tree_dfs_preorder_postorder_isInSubtree.cpp) 
* **Binary Exponentiation** :
   * [Calculate n^m using Binary exponentiation](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/power_binary_exponentiation.cpp)
   * [Solving Linear Recurrences](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/linear_recurrence_matrix_exponentiation.cpp)
* **Strings** :
   * *String Algorithms* :
       * [Z Algorithm](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/Z_algorithm_max_prefix_match.cpp)
       * [KMP algorithm](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/KMP.cpp)
       * [Rolling String Hashing](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/string_hashing.cpp)
       * [Rolling String Hashing for Dynamic Strings](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/string_hashing_dynamic_segment_trees.cpp)
   * *String Data Structures* :
       * [Suffix Array *](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/untested-codes/suffix_array.cpp)
* **Sorting** :
  * [Merge Sort](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/merge_sort_count_inversion.cpp)
  * [Quick Select](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/quick_select_order_stat_linear.cpp)
* **Fast Input/Output, String/Integer Conversion** :
   * [Fast Input/Output](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/fast_readInt_writeInt_function.cpp)
   * [String/Integer Conversion](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/int2string_string2int.cpp)
* **Misc. Data Structures** :
   * [Disjoint Set](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/disjoint_set.cpp)
   * [Disjoint Set (Supports Undo Operation)](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/disjoint_set_with_undo_operation.cpp)
   * [Max/Min Priority Queue with update](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/heap_using_multiset_max_min_insert_erase_update.cpp)
   * [Binary Trie for xor maximization/minimization](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/binary_trie_max_xor.cpp)
   * [Bigint *](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/bigint_library.cpp)
   * [Augmented Binary Tree for order statistics and rank query](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/orderstat_rank_query_augmented_bst.cpp)
   * [Monotone Priority Queue](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/monotone_priority_queue.cpp)
   * [Trie](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/trie_insertion_deleteion.cpp)
* **Persistent Data Structures** :
  * *Persistent Segment Trees (SegTree)* :
    * [Persistent Segment Tree](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/segment_tree_persistent.cpp)
    * [Persistent Segment Tree - Order Statistics](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/segment_tree_persistent_order_stat.cpp)
* **Number Theory Algorithms** :
  * *Primality Check* :
      * [Fermat's Primality Check](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/primality_check_fermat.cpp)
  * *Sieve* :
      * [Sieve of Eratosthenes](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/prime_sieve.cpp)
      * [Segmented Sieve for large primes](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/segmented_sieve_large_primes.cpp)
      * [Counting Prime Factors](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/prime_factor_count.cpp)
  * *Polynomial Multiplication* :
      * [Fast Fourier Tranform](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/fast_fourier_transform_fft.cpp)
      * [Karatsuba](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/karatsuba_polynomial_multiplication.cpp)
  * *Misc* :
      * [Combinatorial and Catalan - Factorial preprocessing](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/factorial_preprocessing.cpp)
      * [Mobeius Function](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/mobeius_function.cpp)
      * [Euler Totient Function](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/euler_phi_euler_totient_function.cpp)
      * [Lucas Theorm - Combinatorics](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/lucas_combinatorics.cpp)  
* **Computational Geometry** :
  * [Convex Hull](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/convexhull.cpp)
* **Misc** :
  * [Sum of floor(x) with x=1:n](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/aggreate_sqrt_distinct_values.cpp)
  * [Sum of cyclic functions *](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/aggregate_cyclic_function.cpp)
  * [Closest larger element before/after every element](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/closest_max_element_before_after_index_using_stack.cpp)
  * [Multiply Long Long Integers](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/multiply_longlong_integers.cpp)
  * [Multiply Long Long Integers - Overflow detection](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/multiply_detect_overflow.cpp)
  * [Scanline - Merge intersecting intervals](https://github.com/dragonslayerx/Competitive-Programming-Repository/blob/master/src/scanline_merge_overlapping_intervals.cpp)
  
  
      
      
  
  
   
   


















"
167,nurkiewicz/spring-data-jdbc-repository,Java,"[![Build Status](https://secure.travis-ci.org/nurkiewicz/spring-data-jdbc-repository.png?branch=master)](https://travis-ci.org/nurkiewicz/spring-data-jdbc-repository) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.nurkiewicz.jdbcrepository/jdbcrepository/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.nurkiewicz.jdbcrepository/jdbcrepository)

# Spring Data JDBC generic DAO implementation

----

### Check out [jirutka/spring-data-jdbc-repository](https://github.com/jirutka/spring-data-jdbc-repository) fork that is actively developed and maintained. This repository is no longer supported. 

----

The purpose of this project is to provide generic, lightweight and easy to use DAO implementation for relational databases based on [`JdbcTemplate`](http://static.springsource.org/spring/docs/3.0.x/api/org/springframework/jdbc/core/JdbcTemplate.html) from [Spring framework](http://www.springsource.org/spring-framework), compatible with Spring Data umbrella of projects.

## Design objectives

* Lightweight, fast and low-overhead. Only a handful of classes, **no XML, annotations, reflection**
* **This is not full-blown ORM**. No relationship handling, lazy loading, dirty checking, caching
* CRUD implemented in seconds
* For small applications where JPA is an overkill
* Use when simplicity is needed or when future migration e.g. to JPA is considered
* Minimalistic support for database dialect differences (e.g. transparent paging of results)

## Features

Each DAO provides built-in support for:

* Mapping to/from domain objects through [`RowMapper`](http://static.springsource.org/spring/docs/3.0.x/api/org/springframework/jdbc/core/RowMapper.html) abstraction
* Generated and user-defined primary keys
* Extracting generated key
* Compound (multi-column) primary keys
* Immutable domain objects
* Paging (requesting subset of results)
* Sorting over several columns (database agnostic)
* Optional support for *many-to-one* relationships
* Supported databases (continuously tested):
	* MySQL
	* PostgreSQL
	* H2
	* HSQLDB
	* Derby
	* MS SQL Server (2008, 2012)
	* Oracle 10g / 11g (9i should work too)
	* ...and most likely many others
* Easily extendable to other database dialects via [`SqlGenerator`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/main/java/com/nurkiewicz/jdbcrepository/sql/SqlGenerator.java) class.
* Easy retrieval of records by ID

## API

Compatible with Spring Data [`PagingAndSortingRepository`](http://static.springsource.org/spring-data/data-commons/docs/current/api/org/springframework/data/repository/PagingAndSortingRepository.html) abstraction, **all these methods are implemented for you**:

```java
public interface PagingAndSortingRepository<T, ID extends Serializable> extends CrudRepository<T, ID> {
			 T  save(T entity);
	Iterable<T> save(Iterable<? extends T> entities);
			 T  findOne(ID id);
		boolean exists(ID id);
	Iterable<T> findAll();
		   long count();
		   void delete(ID id);
		   void delete(T entity);
		   void delete(Iterable<? extends T> entities);
		   void deleteAll();
	Iterable<T> findAll(Sort sort);
		Page<T> findAll(Pageable pageable);
	Iterable<T> findAll(Iterable<ID> ids);
}
```

`Pageable` and `Sort` parameters are also fully supported, which means you get **paging and sorting by arbitrary properties for free**. For example say you have `userRepository` extending `PagingAndSortingRepository<User, String>` interface (implemented for you by the library) and you request 5th page of `USERS` table, 10 per page, after applying some sorting:

```java
Page<User> page = userRepository.findAll(
	new PageRequest(
		5, 10, 
		new Sort(
			new Order(DESC, ""reputation""), 
			new Order(ASC, ""user_name"")
		)
	)
);
```

Spring Data JDBC repository library will translate this call into (PostgreSQL syntax):

```sql
SELECT *
FROM USERS
ORDER BY reputation DESC, user_name ASC
LIMIT 50 OFFSET 10
```

...or even (Derby syntax):

```sql
SELECT * FROM (
	SELECT ROW_NUMBER() OVER () AS ROW_NUM, t.*
	FROM (
		SELECT * 
		FROM USERS 
		ORDER BY reputation DESC, user_name ASC
		) AS t
	) AS a 
WHERE ROW_NUM BETWEEN 51 AND 60
```

No matter which database you use, you'll get `Page<User>` object in return (you still have to provide `RowMapper<User>` yourself to translate from [`ResultSet`](http://docs.oracle.com/javase/7/docs/api/java/sql/ResultSet.html) to domain object). If you don't know Spring Data project yet, [`Page<T>`](http://static.springsource.org/spring-data/commons/docs/current/api/org/springframework/data/domain/Page.html) is a wonderful abstraction, not only encapsulating `List<T>`, but also providing metadata such as total number of records, on which page we currently are, etc.

## Reasons to use

* You consider migration to JPA or even some NoSQL database in the future.

	Since your code will rely only on methods defined in [`PagingAndSortingRepository`](http://static.springsource.org/spring-data/data-commons/docs/current/api/org/springframework/data/repository/PagingAndSortingRepository.html) and [`CrudRepository`](http://static.springsource.org/spring-data/data-commons/docs/current/api/org/springframework/data/repository/CrudRepository.html) from [Spring Data Commons](http://www.springsource.org/spring-data/commons) umbrella project you are free to switch from [`JdbcRepository`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/main/java/com/nurkiewicz/jdbcrepository/JdbcRepository.java) implementation (from this project) to: [`JpaRepository`](http://static.springsource.org/spring-data/data-jpa/docs/current/api/org/springframework/data/jpa/repository/JpaRepository.html), [`MongoRepository`](http://static.springsource.org/spring-data/data-mongodb/docs/current/api/org/springframework/data/mongodb/repository/MongoRepository.html), [`GemfireRepository`](http://static.springsource.org/spring-data-gemfire/docs/current/api/org/springframework/data/gemfire/repository/GemfireRepository.html) or [`GraphRepository`](http://static.springsource.org/spring-data/data-graph/docs/current/api/org/springframework/data/neo4j/repository/GraphRepository.html). They all implement the same common API. Of course don't expect that switching from JDBC to JPA or MongoDB will be as simple as switching imported JAR dependencies - but at least you minimize the impact by using same DAO API.

* You need a fast, simple JDBC wrapper library. JPA or even [MyBatis](http://blog.mybatis.org/) is an overkill

* You want to have full control over generated SQL if needed

* You want to work with objects, but don't need lazy loading, relationship handling, multi-level caching, dirty checking... You need [CRUD](http://en.wikipedia.org/wiki/Create,_read,_update_and_delete) and not much more

* You want to by [*DRY*](http://en.wikipedia.org/wiki/Don't_repeat_yourself)

* You are already using Spring or maybe even [`JdbcTemplate`](http://static.springsource.org/spring/docs/3.0.x/api/org/springframework/jdbc/core/JdbcTemplate.html), but still feel like there is too much manual work

* You have very few database tables

## Getting started

For more examples and working code don't forget to examine [project tests](https://github.com/nurkiewicz/spring-data-jdbc-repository/tree/master/src/test/java/com/nurkiewicz/jdbcrepository).

### Prerequisites

Maven coordinates:

```xml
<dependency>
	<groupId>com.nurkiewicz.jdbcrepository</groupId>
	<artifactId>jdbcrepository</artifactId>
	<version>0.4</version>
</dependency>
```

This project is available under maven central repository.

Alternatively you can [download source code as ZIP](https://github.com/nurkiewicz/spring-data-jdbc-repository/tags).

---

In order to start your project must have `DataSource` bean present and transaction management enabled. Here is a minimal MySQL configuration:

```java
@EnableTransactionManagement
@Configuration
public class MinimalConfig {

	@Bean
	public PlatformTransactionManager transactionManager() {
		return new DataSourceTransactionManager(dataSource());
	}

	@Bean
	public DataSource dataSource() {
		MysqlConnectionPoolDataSource ds = new MysqlConnectionPoolDataSource();
		ds.setUser(""user"");
		ds.setPassword(""secret"");
		ds.setDatabaseName(""db_name"");
		return ds;
	}

}
```

### Entity with auto-generated key

Say you have a following database table with auto-generated key (MySQL syntax):

```sql
CREATE TABLE COMMENTS (
	id INT AUTO_INCREMENT,
	user_name varchar(256),
	contents varchar(1000),
	created_time TIMESTAMP NOT NULL,
	PRIMARY KEY (id)
);
```

First you need to create domain object [`User`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/test/java/com/nurkiewicz/jdbcrepository/repositories/User.java) mapping to that table (just like in any other ORM):

```java
public class Comment implements Persistable<Integer> {

	private Integer id;
	private String userName;
	private String contents;
	private Date createdTime;

	@Override
	public Integer getId() {
		return id;
	}

	@Override
	public boolean isNew() {
		return id == null;
	}
	
	//getters/setters/constructors/...
}
```

Apart from standard Java boilerplate you should notice implementing [`Persistable<Integer>`](http://static.springsource.org/spring-data/commons/docs/current/api/org/springframework/data/domain/Persistable.html) where `Integer` is the type of primary key. `Persistable<T>` is an interface coming from Spring Data project and it's the only requirement we place on your domain object.

Finally we are ready to create our [`CommentRepository`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/test/java/com/nurkiewicz/jdbcrepository/repositories/CommentRepository.java) DAO:

```java
@Repository
public class CommentRepository extends JdbcRepository<Comment, Integer> {

	public CommentRepository() {
		super(ROW_MAPPER, ROW_UNMAPPER, ""COMMENTS"");
	}

	public static final RowMapper<Comment> ROW_MAPPER = //see below

	private static final RowUnmapper<Comment> ROW_UNMAPPER = //see below

	@Override
	protected <S extends Comment> S postCreate(S entity, Number generatedId) {
		entity.setId(generatedId.intValue());
		return entity;
	}
}
```

First of all we use [`@Repository`](http://static.springsource.org/spring/docs/3.0.x/api/org/springframework/stereotype/Repository.html) annotation to mark DAO bean. It enables persistence exception translation. Also such annotated beans are discovered by CLASSPATH scanning.

As you can see we extend `JdbcRepository<Comment, Integer>` which is the central class of this library, providing implementations of all `PagingAndSortingRepository` methods. Its constructor has three required dependencies: `RowMapper`, [`RowUnmapper`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/main/java/com/nurkiewicz/jdbcrepository/RowUnmapper.java) and table name. You may also provide ID column name, otherwise default `""id""` is used.

If you ever used `JdbcTemplate` from Spring, you should be familiar with [`RowMapper`](http://static.springsource.org/spring/docs/3.0.x/api/org/springframework/jdbc/core/RowMapper.html) interface. We need to somehow extract columns from `ResultSet` into an object. After all we don't want to work with raw JDBC results. It's quite straightforward:

```java
public static final RowMapper<Comment> ROW_MAPPER = new RowMapper<Comment>() {
	@Override
	public Comment mapRow(ResultSet rs, int rowNum) throws SQLException {
		return new Comment(
				rs.getInt(""id""),
				rs.getString(""user_name""),
				rs.getString(""contents""),
				rs.getTimestamp(""created_time"")
		);
	}
};
```

`RowUnmapper` comes from this library and it's essentially the opposite of `RowMapper`: takes an object and turns it into a `Map`. This map is later used by the library to construct SQL `CREATE`/`UPDATE` queries:

```java
private static final RowUnmapper<Comment> ROW_UNMAPPER = new RowUnmapper<Comment>() {
	@Override
	public Map<String, Object> mapColumns(Comment comment) {
		Map<String, Object> mapping = new LinkedHashMap<String, Object>();
		mapping.put(""id"", comment.getId());
		mapping.put(""user_name"", comment.getUserName());
		mapping.put(""contents"", comment.getContents());
		mapping.put(""created_time"", new java.sql.Timestamp(comment.getCreatedTime().getTime()));
		return mapping;
	}
};
```

If you never update your database table (just reading some reference data inserted elsewhere) you may skip `RowUnmapper` parameter or use [`MissingRowUnmapper`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/main/java/com/nurkiewicz/jdbcrepository/MissingRowUnmapper.java).

Last piece of the puzzle is the `postCreate()` callback method which is called after an object was inserted. You can use it to retrieve generated primary key and update your domain object (or return new one if your domain objects are immutable). If you don't need it, just don't override `postCreate()`.

Check out [`JdbcRepositoryGeneratedKeyTest`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/test/java/com/nurkiewicz/jdbcrepository/JdbcRepositoryGeneratedKeyTest.java) for a working code based on this example.

> By now you might have a feeling that, compared to JPA or Hibernate, there is quite a lot of manual work. However various JPA implementations and other ORM frameworks are notoriously known for introducing significant overhead and manifesting some learning curve. This tiny library intentionally leaves some responsibilities to the user in order to avoid complex mappings, reflection, annotations... all the implicitness that is not always desired.

> This project is not intending to replace mature and stable ORM frameworks. Instead it tries to fill in a niche between raw JDBC and ORM where simplicity and low overhead are key features.

### Entity with manually assigned key

In this example we'll see how entities with user-defined primary keys are handled. Let's start from database model:

```java
CREATE TABLE USERS (
	user_name varchar(255),
	date_of_birth TIMESTAMP NOT NULL,
	enabled BIT(1) NOT NULL,
	PRIMARY KEY (user_name)
);
```

...and [`User`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/test/java/com/nurkiewicz/jdbcrepository/repositories/User.java) domain model:

```java
public class User implements Persistable<String> {

	private transient boolean persisted;

	private String userName;
	private Date dateOfBirth;
	private boolean enabled;

	@Override
	public String getId() {
		return userName;
	}

	@Override
	public boolean isNew() {
		return !persisted;
	}

	public void setPersisted(boolean persisted) {
		this.persisted = persisted;
	}

	//getters/setters/constructors/...

}
```

Notice that special `persisted` transient flag was added. Contract of [`CrudRepository.save()`](http://static.springsource.org/spring-data/data-commons/docs/current/api/org/springframework/data/repository/CrudRepository.html#save(S)) from Spring Data project requires that an entity knows whether it was already saved or not (`isNew()`) method - there are no separate `create()` and `update()` methods. Implementing `isNew()` is simple for auto-generated keys (see `Comment` above) but in this case we need an extra transient field. If you hate this workaround and you only insert data and never update, you'll get away with return `true` all the time from `isNew()`.

And finally our DAO, [`UserRepository`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/test/java/com/nurkiewicz/jdbcrepository/repositories/UserRepository.java) bean:

```java
@Repository
public class UserRepository extends JdbcRepository<User, String> {

	public UserRepository() {
		super(ROW_MAPPER, ROW_UNMAPPER, ""USERS"", ""user_name"");
	}

	public static final RowMapper<User> ROW_MAPPER = //...

	public static final RowUnmapper<User> ROW_UNMAPPER = //...

	@Override
	protected <S extends User> S postUpdate(S entity) {
		entity.setPersisted(true);
		return entity;
	}

	@Override
	protected <S extends User> S postCreate(S entity, Number generatedId) {
		entity.setPersisted(true);
		return entity;
	}
}
```

`""USERS""` and `""user_name""` parameters designate table name and primary key column name. I'll leave the details of mapper and unmapper (see [source code]((https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/test/java/com/nurkiewicz/jdbcrepository/repositories/UserRepository.java))). But please notice `postUpdate()` and `postCreate()` methods. They ensure that once object was persisted, `persisted` flag is set so that subsequent calls to `save()` will update existing entity rather than trying to reinsert it.

Check out [`JdbcRepositoryManualKeyTest`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/test/java/com/nurkiewicz/jdbcrepository/JdbcRepositoryManualKeyTest.java) for a working code based on this example.

### Compound primary key

We also support compound primary keys (primary keys consisting of several columns). Take this table as an example:

```sql
CREATE TABLE BOARDING_PASS (
	flight_no VARCHAR(8) NOT NULL,
	seq_no INT NOT NULL,
	passenger VARCHAR(1000),
	seat CHAR(3),
	PRIMARY KEY (flight_no, seq_no)
);
```

I would like you to notice the type of primary key in `Persistable<T>`:

```java
public class BoardingPass implements Persistable<Object[]> {

	private transient boolean persisted;

	private String flightNo;
	private int seqNo;
	private String passenger;
	private String seat;

	@Override
	public Object[] getId() {
		return pk(flightNo, seqNo);
	}

	@Override
	public boolean isNew() {
		return !persisted;
	}

	//getters/setters/constructors/...

}
```

Unfortunately library does not support small, immutable value classes encapsulating all ID values in one object (like JPA does with [`@IdClass`](http://docs.oracle.com/javaee/6/api/javax/persistence/IdClass.html)), so you have to live with `Object[]` array. Defining DAO class is similar to what we've already seen:

```java
public class BoardingPassRepository extends JdbcRepository<BoardingPass, Object[]> {
	public BoardingPassRepository() {
		this(""BOARDING_PASS"");
	}

	public BoardingPassRepository(String tableName) {
		super(MAPPER, UNMAPPER, new TableDescription(tableName, null, ""flight_no"", ""seq_no"")
		);
	}

	public static final RowMapper<BoardingPass> ROW_MAPPER = //...

	public static final RowUnmapper<BoardingPass> UNMAPPER = //...

}
```
Two things to notice: we extend `JdbcRepository<BoardingPass, Object[]>` and we provide two ID column names just as expected: `""flight_no"", ""seq_no""`. We query such DAO by providing both `flight_no` and `seq_no` (necessarily in that order) values wrapped by `Object[]`:

```java
BoardingPass pass = boardingPassRepository.findOne(new Object[] {""FOO-1022"", 42});
```

No doubts, this is cumbersome in practice, so we provide tiny helper method which you can statically import:

```java
import static com.nurkiewicz.jdbcrepository.JdbcRepository.pk;
//...

BoardingPass foundFlight = boardingPassRepository.findOne(pk(""FOO-1022"", 42));
```

Check out [`JdbcRepositoryCompoundPkTest`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/test/java/com/nurkiewicz/jdbcrepository/JdbcRepositoryCompoundPkTest.java) for a working code based on this example.

### Transactions

This library is completely orthogonal to transaction management. Every method of each repository requires running transaction and it's up to you to set it up. Typically you would place `@Transactional` on service layer (calling DAO beans). I don't recommend [placing `@Transactional` over every DAO bean](http://stackoverflow.com/questions/8993318/what-is-the-right-way-to-use-spring-mvc-with-hibernate-in-dao-sevice-layer-arch).

## Caching

Spring Data JDBC repository library is not providing any caching abstraction or support. However adding `@Cacheable` layer on top of your DAOs or services using [caching abstraction in Spring](http://static.springsource.org/spring/docs/3.1.0.RELEASE/spring-framework-reference/html/cache.html) is quite straightforward. See also: [*@Cacheable overhead in Spring*](http://nurkiewicz.blogspot.no/2013/01/cacheable-overhead-in-spring.html).

## Contributions

..are always welcome. Don't hesitate to [submit bug reports](https://github.com/nurkiewicz/spring-data-jdbc-repository/issues) and [pull requests](https://github.com/nurkiewicz/spring-data-jdbc-repository/pulls).

### Testing

This library is continuously tested using Travis ([![Build Status](https://secure.travis-ci.org/nurkiewicz/spring-data-jdbc-repository.png?branch=master)](https://travis-ci.org/nurkiewicz/spring-data-jdbc-repository)). Test suite consists of 60+ distinct tests each run against 8 different databases: MySQL, PostgreSQL, H2, HSQLDB and Derby + MS SQL Server and Oracle tests not run as part of CI.

When filling [bug reports](https://github.com/nurkiewicz/spring-data-jdbc-repository/issues) or submitting new features please try including supporting test cases. Each [pull request](https://github.com/nurkiewicz/spring-data-jdbc-repository/pulls) is automatically tested on a separate branch.

### Building

After forking the [official repository](https://github.com/nurkiewicz/spring-data-jdbc-repository) building is as simple as running:

```bash
$ mvn install
```

You'll notice plenty of exceptions during JUnit test execution. This is normal. Some of the tests run against MySQL and PostgreSQL available only on Travis CI server. When these database servers are unavailable, whole test is simply *skipped*:

```
Results :

Tests run: 484, Failures: 0, Errors: 0, Skipped: 295
```

Exception stack traces come from root [`AbstractIntegrationTest`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/test/java/com/nurkiewicz/jdbcrepository/AbstractIntegrationTest.java).

## Design

Library consists of only a handful of classes, highlighted in the diagram below ([source](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/main/docs/yuml.txt)):

![UML diagram](https://raw.github.com/nurkiewicz/spring-data-jdbc-repository/master/src/main/docs/classes.png)

[`JdbcRepository`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/main/java/com/nurkiewicz/jdbcrepository/JdbcRepository.java) is the most important class that implements all [`PagingAndSortingRepository`](http://static.springsource.org/spring-data/data-commons/docs/current/api/org/springframework/data/repository/PagingAndSortingRepository.html) methods. Each user repository has to extend this class. Also each such repository must at least implement [`RowMapper`](http://static.springsource.org/spring/docs/3.0.x/api/org/springframework/jdbc/core/RowMapper.html) and [`RowUnmapper`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/main/java/com/nurkiewicz/jdbcrepository/RowUnmapper.java) (only if you want to modify table data).

SQL generation is delegated to [`SqlGenerator`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/main/java/com/nurkiewicz/jdbcrepository/sql/SqlGenerator.java). [`PostgreSqlGenerator.`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/main/java/com/nurkiewicz/jdbcrepository/sql/PostgreSqlGenerator.java) and [`DerbySqlGenerator`](https://github.com/nurkiewicz/spring-data-jdbc-repository/blob/master/src/main/java/com/nurkiewicz/jdbcrepository/sql/DerbySqlGenerator.java) are provided for databases that don't work with standard generator.

## Changelog

### 0.4.1

* Fixed [*Standalone Configuration and CDI Implementation*](https://github.com/nurkiewicz/spring-data-jdbc-repository/issues/10)

### 0.4

* Repackaged: `com.blogspot.nurkiewicz` -> `com.nurkiewicz`

### 0.3.2

* First version available in Maven central repository
* Upgraded Spring Data Commons 1.6.1 -> 1.8.0

### 0.3.1

* Upgraded Spring dependencies: 3.2.1 -> 3.2.4 and 1.5.0 -> 1.6.1
* Fixed [#5 Allow manually injecting JdbcOperations, SqlGenerator and DataSource](https://github.com/nurkiewicz/spring-data-jdbc-repository/issues/5)

### 0.3

* Oracle 10g / 11g support (see [pull request](https://github.com/nurkiewicz/spring-data-jdbc-repository/pull/3))
* Upgrading Spring dependency to 3.2.1.RELEASE and [Spring Data Commons](http://www.springsource.org/spring-data/commons) to 1.5.0.RELEASE (see [#4](https://github.com/nurkiewicz/spring-data-jdbc-repository/issues/4)).

### 0.2

* MS SQL Server 2008/2012 support (see [pull request](https://github.com/nurkiewicz/spring-data-jdbc-repository/pull/2))

### 0.1

* Initial revision ([announcement](http://nurkiewicz.blogspot.no/2013/01/spring-data-jdbc-generic-dao.html))

## License
This project is released under version 2.0 of the [Apache License](http://www.apache.org/licenses/LICENSE-2.0) (same as [Spring framework](https://github.com/SpringSource/spring-framework)).
"
168,magento-hackathon/composer-repository,HTML,
169,gamedilong/anes-repository,,"# anes-repository
## 插件源码地址  
* https://github.com/gamedilong/anes
## rom 仓库

* 该仓库是小霸王插件的，远程资源仓库，内容主要在list.json中维护，文档持续完善中。
* 有其他问题意见也可以加qq群 858843908 沟通交流
* 插件地址 https://marketplace.visualstudio.com/items?itemName=gamedilong.anes

## 资源更新 
* 如果有想要添加得资源，可以在群里反馈，也可以fork分支PR上来，测试通过后会尽快发布
* 修改方式，在list.json中添加配置即可
示例
```
[
...
{
    ""name"":""坦克大战"",
    ""url"":""http://11.down.thenightblindness.com:8000/fc/Battle.City(J).zip"",
    ""fileName"":""Battle.City(J).zip"",
    ""nesName"":""Battle City (J).nes""
}
...
]
name remote中展示名称
url 资源下载链接
fileName  与url最后得资源名称保持一致
nesName  该压缩文件解压后的nes内容完整名称 (压缩文件名，与文件名不一定一致)
```

## FAQ
* **下载一直在99%**   该问题已经升级了一个0.02的版本有遇到的同学可以更新下插件，另外最好删除一下C盘用户文件夹下的.anes文件重新打开即可。新的第三方如果没有挂掉的话，应该是可以正常下载了。
* **兼容性问题** 目前支持vscode版本 1.47.0+
* **remote列表不显示** 原因可能是github的资源列表没用下载成功，可以删掉(C:\Users\XXXX\.anes)目录，然后关闭vscode重新打开会重新下载。如果依然没有，可直接下载  https://github.com/gamedilong/anes-repository/archive/master.zip  将内容解压的C:\Users\XXXX\.anes\remote下
 最终目录结构如下 C:\Users\XXX\.anes\remote\anes-repository-master  
 注 : mac目录 为 /User/XXX/.anes
* **本地rom打开黑屏问题** 使用的jsnes模拟器，部分rom不支持。后续如果找到好的mapper适配方案，会优先适配issue里反馈的比较多的rom。 
"
170,collabH/repository,Shell,"![img.png](./img/logo.png)

# repository

[![License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT/)

[![Stargazers over time](https://starchart.cc/collabH/repository.svg)](#)

## 概述

* 个人学习知识库涉及到数据仓库建模、实时计算、大数据、Java、算法等。

## RoadMap

![roadMap](./roadmap/roadmap.jpg)

## 基础能力

### 数据结构

### 分布式理论

* [分布式架构](base/分布式理论/分布式架构.md)

### 计算机理论

* [LSM存储模型](base/计算机理论/LSM存储模型.md)

### Scala

* [ScalaOverView](./base/scala/ScalaOverView.md)

### JVM

### Java

#### JDK源码

#### todo

## 算法

* [算法题解](base/algorithm/算法题解.md)

## BigData

### zeppelin

* [zeppelin](bigdata/zeppelin/Zeppelin.xmind)

### datalake

#### iceberg

* [IceBerg整合Flink](bigdata/datalake/iceberg/icebergWithFlink.md)

### rocksDB

* [rocksDB概述](bigdata/rocksdb/RocksdbOverview.md)

### Hadoop

* 广义上的Hadoop生态圈的学习笔记，主要记录HDFS、MapReduce、Yarn相关读书笔记及源码分析等。

#### HDFS

* [Hadoop快速入门](bigdata/hadoop/Hadoop快速开始.xmind)
* [HDFSOverView](bigdata/hadoop/HDFS/HDFSOverView.xmind)
* [Hadoop广义生态系统](bigdata/hadoop/Hadoop广义生态系统.xmind)
* [Hadoop高可用配置](bigdata/hadoop/Hadoop高可用配置.md)
* [HadoopCommon分析](bigdata/hadoop/HDFS/Common包解析.md)
* [HDFS集群相关管理](bigdata/hadoop/HDFS/HDFS集群管理.md)
* [HDFS Shell](bigdata/hadoop/HDFS/HDFS%20Shell命令.md)

#### MapReduce

* [分布式处理框架MapReduce](bigdata/hadoop/MapReduce/分布式处理框架MapReduce.md)
* [MapReduce概览](bigdata/hadoop/MapReduce/MapReduceOverView.xmind)
* [MapReduce调优](bigdata/hadoop/MapReduce/MapReduce调优.xmind)
* [MapReduce数据相关操作](bigdata/hadoop/MapReduce/MapReduce数据操作.md)
* [MapReduce输入输出剖析](bigdata/hadoop/MapReduce/MapReduce输入输出剖析.md)
* [MapReduce的工作机制](bigdata/hadoop/MapReduce/MapReduce的工作原理剖析.md)

#### Yarn

* [Yarn快速入门](bigdata/hadoop/Yarn/YARN快速入门.md)

#### 高可用配置

* [Hadoop高可用配置](bigdata/hadoop/Hadoop高可用配置.md)

### Canal

* [CanalOverView](bigdata/canal/CanalOverView.md)

### Debezium

* [DebeziumOverView](bigdata/debezium/DebeziumOverView.md)
* [Debezium踩坑](bigdata/debezium/Debezium踩坑.xmind)
* [Debezium监控系统搭建](bigdata/debezium/Debezium监控系统搭建.md)
* [Debezium使用改造](bigdata/debezium/Debezium使用改造.md)

### Hive

* [HiveOverwrite](bigdata/hive/HiveOverwrite.md)
* [Hive SQL](bigdata/hive/Hive%20SQL.xmind)
* [Hive调优指南](bigdata/hive/Hive调优指南.xmind)
* [Hive踩坑解决方案](bigdata/hive/Hive踩坑解决方案.xmind)
* [Hive编程指南读书笔记](bigdata/hive/hive编程指南)
* [Hive Shell Beeline](bigdata/hive/Hive%20Shell和Beeline命令.md)
* [Hive分区表和分桶表](bigdata/hive/Hive分区表和分桶表.md)

### Spark

* 主要包含Spark相关书籍读书笔记、Spark核心组件分析、Spark相关API实践以及Spark生产踩坑等。

* [Spark基础入门](bigdata/spark/Spark基础入门.xmind)
* [SparkOnDeploy](bigdata/spark/SparkOnDeploy.md)
* [Spark调度系统](bigdata/spark/Spark调度系统.md)
* [Spark计算引擎和Shuffle](bigdata/spark/Spark计算引擎和Shuffle.md)
* [Spark存储体系](bigdata/spark/Spark存储体系.md)
* [Spark大数据处理读书笔记](bigdata/spark/Spark大数据处理读书笔记.xmind)

#### Spark Core

* [SparkCore](bigdata/spark/spark%20core/Spark%20Core.xmind)
* [SparkOperator](bigdata/spark/spark%20core/Spark%20Operator.xmind)
* [SparkConnector](bigdata/spark/spark%20core/Spark%20Connector.xmind)

#### Spark SQL

* [SparkSQLAPI](bigdata/spark/spark%20sql/Spark%20SQL%20API.xmind)
* [SparkSQL](bigdata/spark/spark%20sql/Spark%20SQL.xmind)
* [SparkSQL API](bigdata/spark/spark%20sql/SparkSQL%20API.md)

#### Spark Streaming

* [SparkStreaming](bigdata/spark/spark%20streaming/Spark%20Steaming.xmind)
* [SparkStreaming整合Flume](bigdata/spark/spark%20streaming/SparkStreaming整合Flume.md)

#### 源码解析

* [从浅到深剖析Spark源码](bigdata/spark/从浅到深剖析Spark源码.md)
* [源码分析系列](bigdata/spark/源码分析)

### Zookeeper

* [Zookeeper原理和参数配置](bigdata/zookeeper/ZookeeperOverView.md)
* [Zookeeper操作与部署](bigdata/zookeeper/Zookeeper操作与部署.md)

### Flume

* [Flume快速入门](bigdata/flume/FlumeOverwrite.md)
* [Flume对接Kafka](bigdata/flume/Flume对接Kafka.md)

### Kafka

* [kafka概览](bigdata/kafka/KafkaOverView.xmind)
* [基本概念](bigdata/kafka/基本概念.md)
* [kafka监控](bigdata/kafka/Kafka监控.md)
* [生产者源码剖析](bigdata/kafka/生产者源码剖析.md)
* [消费者源码剖析](bigdata/kafka/消费者源码剖析.md)
* [kafkaShell](bigdata/kafka/KafkaShell.xmind)
* [kafka权威指南读书笔记](bigdata/kafka/kafka权威指南)
* [深入理解Kafka读书笔记](bigdata/kafka/深入理解Kafka)

### HBase

* [HBase概览](bigdata/hbase/HBaseOverview.md)
* [HBaseShell](bigdata/hbase/HBase%20Shell.xmind)
* [HBaseJavaAPI](bigdata/hbase/HBase%20Java%20API.xmind)
* [HBase整合MapReduce](bigdata/hbase/HBase整合第三方组件.md)
* [HBase过滤器](bigdata/hbase/Hbase过滤器.md)

### Sqoop

* [SqoopOverview](bigdata/sqoop/SqoopOverview.md)
* [Sqoop实战操作](bigdata/sqoop/Sqoop实战操作.md)

### DolphinScheduler

* [DolphinScheduler快速开始](bigdata/scheduler/DolphinScheduler快速开始.md)

### Flink

* 主要包含对Flink文档阅读的总结和相关Flink源码的阅读，以及Flink新特性记录等等

#### Core

* [FlinkOverView](bigdata/flink/core/FlinkOverview.md)
* [CheckPoint机制](bigdata/flink/core/Checkpoint机制.md)  
* [TableSQLOverview](bigdata/flink/core/TableSQLOverview.md)
* [DataStream API](bigdata/flink/core/FlinkDataStream%20API.xmind)
* [ProcessFunction API](bigdata/flink/core/ProcessFunction%20API.xmind)
* [Data Source](bigdata/flink/core/Data%20Source.xmind)
* [Table API](bigdata/flink/core/TABLE%20API.xmind)
* [Flink SQL](bigdata/flink/core/FlinkSQL.xmind)
* [Flink Hive](bigdata/flink/core/Flink%20Hive.xmind)
* [Flink CEP](bigdata/flink/core/Flink%20Cep.xmind)
* [Flink Function](bigdata/flink/core/Flink%20Function.xmind)
* [DataSource API](bigdata/flink/core/Data%20Source.xmind)

#### SourceCode

* [FlinkCheckpoint源码分析](bigdata/flink/sourcecode/FlinkCheckpoint源码分析.md)
* [FlinkSQL源码解析](bigdata/flink/sourcecode/FlinkSQL源码解析.md)
* [Flink内核源码分析](bigdata/flink/sourcecode/Flink内核源码分析.md)  
* [Flink网络流控及反压](bigdata/flink/sourcecode/Flink网络流控及反压.md)
* [TaskExecutor内存模型原理深入](bigdata/flink/sourcecode/TaskExecutor内存模型原理深入.md)
* [Flink窗口实现应用](bigdata/flink/sourcecode/Flink窗口实现应用原理.md)
* [Flink运行环境源码解析](bigdata/flink/sourcecode/Flink运行环境源码解析.md)
* [FlinkTimerService机制分析](bigdata/flink/sourcecode/FlinkTimerService机制分析.md)
* [StreamSource源解析](bigdata/flink/sourcecode/StreamSource源解析.md)
* [Flink状态管理与检查点机制](bigdata/flink/sourcecode/Flink状态管理与检查点机制.xmind)

#### Book
##### Flink内核原理与实现

* [1-3章读书笔记](bigdata/flink/books/Flink内核原理与实现/1-3章读书笔记.xmind)
* [第4章时间与窗口](bigdata/flink/books/Flink内核原理与实现/第4章时间与窗口.xmind)
* [5-6章读书笔记](bigdata/flink/books/Flink内核原理与实现/5-6章读书笔记.xmind)

#### Feature

* [Flink1.12新特性](bigdata/flink/feature/Flink1.12新特性.md)

#### Practice

* [Flink踩坑指南](bigdata/flink/practice/Flink踩坑.xmind)
* [记录一次Flink反压问题](bigdata/flink/practice/记录一次Flink反压问题.md)  
* [Flink SQL实践调优](bigdata/flink/practice/Flink%20SQL调优.xmind)

#### Connector

* [自定义Table Connector](bigdata/flink/connector/自定义TableConnector.md)

#### monitor

* [搭建Flink任务指标监控系统](bigdata/flink/monitor/搭建Flink任务指标监控系统.md)

### olap

* 主要核心包含Kudu、Impala相关Olap引擎，生产实践及论文记录等。

#### Presto

* [presto概述](bigdata/olap/presto/PrestoOverview.md)

#### clickhouse

* [ClickHouse快速入门](bigdata/olap/clickhouse/ClickHouseOverView.md)
* [ClickHouse表引擎](bigdata/olap/clickhouse/ClickHouse表引擎.xmind)

#### Druid

* [Druid概述](bigdata/olap/druid/DruidOverView.md)

#### Kylin

* [Kylin概述](bigdata/olap/kylin/KylinOverWrite.md)

#### Kudu

* [KuduOverView](bigdata/olap/kudu/KuduOverView.md)
* [Kudu表和Schema设计](bigdata/olap/kudu/KuduSchemaDesgin.md)
* [KuduConfiguration](bigdata/olap/kudu/KuduConfiguration.md)
* [Kudu原理分析](bigdata/olap/kudu/Kudu原理分析.md)
* [Kudu踩坑](bigdata/olap/kudu/Kudu踩坑.xmind)
* [Kudu存储结构架构图](bigdata/olap/kudu/Kudu存储结构)

##### paper

* [Kudu论文阅读](bigdata/olap/kudu/paper/KuduPaper阅读.md)

#### Impala

* [ImpalaOverView](bigdata/olap/impala/ImpalaOverView.md)

## 数据仓库

* [数据建模](datawarehouse/DataModeler.md)
* [数据仓库建模](datawarehouse/数据仓库建模.xmind)
* [数据仓库](datawarehouse/数据仓库实战.md)
* [基于Flink的实时数仓建设](datawarehouse/基于Flink的实时数仓建设.md)
* [自研数据中台设计](datawarehouse/数据中台设计/数据中台设计.md)

### 读书笔记

* [数据中台读书笔记](datawarehouse/数据中台读书笔记.md)

## devops

* [shell命令](devops/Shell学习.xmind)
* [Linux命令](devops/Linux学习.xmind)
* [openshift基础命令](devops/k8s-openshift客户端命令使用.md)

## maven

* [maven骨架制作](devops/maven/制作maven骨架.md)
* [maven命令](devops/maven/Maven命令.md)

## 服务监控

* [Prometheus](servicemonitor/Prometheus/Prometheus实战.md)

## mac

* [iterm2](mac/iterm2)"
171,kodi-czsk/repository,Python,
172,sonatype/nexus-oss,,"# Moved

Merged into https://github.com/sonatype/nexus-public

Downloadable bundles here: https://www.sonatype.com/download-oss-sonatype
"
173,JustryDeng/PublicRepository,Java,
174,GitIndonesia/awesome-indonesia-repo,,"# Awesome Indonesia Repo

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome/)
[![TravisCI](https://api.travis-ci.org/GitIndonesia/awesome-indonesia-repo.svg)](https://travis-ci.org/GitIndonesia/awesome-indonesia-repo)
[![Gitter](https://img.shields.io/:chat-on_gitter-ED2067.svg)](https://gitter.im/GitIndonesia/awesome-indonesia-repo)
[![Country](https://img.shields.io/badge/country-indonesia-blue.svg)](#)

A curated list of amazingly awesome repository created and contributed by Indonesian people.<br>
It aims to inspire what has been made by Indonesian people and hopefully can give you encouragement to contribute in Open-source software.

Inspired by [sindresorhus/awesome](https://github.com/sindresorhus/awesome)

## Contributing

Please see [CONTRIBUTING](CONTRIBUTING.md) for details.

## Table of Contents

- [Awesome Indonesia Repo](#awesome-indonesia-repo)
  - [Contributing](#contributing)
  - [Table of Contents](#table-of-contents)
  - [Android](#android)
  - [Awesome List](#awesome-list)
  - [Books](#books)
  - [Content Management System](#content-management-system)
  - [Databases](#databases)
  - [Framework](#framework)
  - [Front-end Development](#front-end-development)
  - [Game](#game)
  - [Go](#go)
  - [Haskell](#haskell)
  - [Rust](#rust)
  - [JavaScript](#javascript)
  - [Java](#java)
  - [Angular](#angular)
  - [Express](#express)
  - [Vue](#vue)
  - [Gridsome Starter](#gridsome-starter)
  - [Jekyll Theme](#jekyll-theme)
  - [Learn](#learn)
  - [Linux Software](#linux-software)
  - [PHP](#php)
  - [CodeIgniter](#codeigniter)
  - [Laravel](#laravel)
  - [Slim](#slim)
  - [Symfony](#symfony)
  - [Yii](#yii)
  - [Plugin](#plugin)
  - [Python](#python)
- [R](#r)
  - [Natural Language Processing](#natural-language-processing)
  - [Miscellaneous](#miscellaneous)
  - [Community](#community)
  - [License](#license)

## Android

- [Android StepsView](https://github.com/anton46/Android-StepsView) - A simple progress steps view on Android.
- [WhatsApp ProfileCollapsingToolbar](https://github.com/anton46/WhatsApp-ProfileCollapsingToolbar) - An Android custom animation view that looks like WhastApp Profile screen style.
- [Ziliun React Native](https://github.com/sonnylazuardi/ziliun-react-native) - Ziliun article reader android app built with React Native.
- [React Tomato Timer](https://github.com/sonnylazuardi/react-tomato-timer) - A simple timer for a more productive way to work and study built with ReactJS.
- [Compressor](https://github.com/zetbaitsu/Compressor) - An android image compression library.
- [android-base-mvp](https://github.com/derohimat/android-base-mvp) - Android Base MVP Concept with RXJava, Dagger, Event Bus, Retrofit, Glide, OkHTTP.
- [RxDownloader](https://github.com/esafirm/RxDownloader) - An Rx wrapper for Download Manager in Android.
- [ImageEffectFilter](https://github.com/mnafian/ImageEffectFilter) - Android image filter processing sample using layer drawable and effect factory android.
- [React Native Credit Card](https://github.com/sonnylazuardi/react-native-credit-card) - React native credit card display component.
- [Android Color Picker](https://github.com/yukuku/ambilwarna) - To customize the color of some background, text, or maybe for a painting application.
- [ImagePicker](https://github.com/esafirm/android-image-picker) - A configureable library to select image from gallery and camera without the hassle.
- [CafeBar](https://github.com/danimahardhika/cafebar) - An upgraded Snackbar for Android that provides more options and easy to use.
- [CandyBar](https://github.com/danimahardhika/candybar-library) - Android icon pack dashboard library.
- [WallpaperBoard](https://github.com/danimahardhika/wallpaperboard) - Android wallpaper dashboard library.
- [Nike+ Running - React Native](https://github.com/sonnylazuardi/react-native-nike-running) - UI & UX Prototype of redesigned Nike+ Running App.
- [Subtitle CollapsingToolbarLayout](https://github.com/HendraAnggrian/collapsingtoolbarlayout-subtitle) - Standard CollapsingToolbarLayout with subtitle support.
- [Benih](https://github.com/zetbaitsu/Benih) - A standard template for Android apps by Zelory
- [YuanaItemSettingView](https://github.com/andhikayuana/YuanaItemSettingView) - Customizable Item Setting View for Android
- [DraggableView](https://github.com/hyuwah/DraggableView) - Android library to easily make floating draggable view (like Tokopedia's lucky egg or Bukalapak's maudikode).
- [Android Image Picker](https://github.com/WendyYanto/android-image-picker) - Non-blocking image gallery library built using kotlin and coroutine to help user to choose image (single or multi)
- [TicTacToe](https://github.com/WendyYanto/android-tic-tac-toe) - Android tictactoe game built using kotlin
- [SheenValidator](https://github.com/rizmaulana/SheenValidator) - Android library to make form validation easier
- [SmartMarker](https://github.com/utsmannn/SmartMarker) - Smart helper for marker movement in Google Maps and Mapbox

## Awesome List

- [Awesome Indonesia Repo](https://github.com/GitIndonesia/awesome-indonesia-repo) - A curated list of amazingly awesome repository created by Indonesian people.
- [Awesome CodeIgniter](https://github.com/codeigniter-id/awesome-codeigniter) - A list of awesome CodeIgniter core, helpers, hooks, language, libraries, and third party.
- [Engineering Blogs](https://github.com/sumodirjo/engineering-blogs) - A curated list of engineering blog of startup and enterprise company.
- [NLP Bahasa Indonesia](https://github.com/sastrawi/nlp-bahasa-indonesia) - A curated list of Indonesian Natural Language Processing resources.
- [Awesome Indonesia Telegram Groups](https://github.com/hendisantika/List-All-Programming-Telegram-Group) - List of All Programming Telegram Group in Indonesia.
- [Awesome Guidelines](https://github.com/Kristories/awesome-guidelines) - A curated list of high quality coding style conventions and standards.
- [Belajar ngoding dalam bahasa Indonesia](https://github.com/amuritna/belajar-ngoding-bhs-indo) - A list of Indonesian resources for learning to code.

## Books

- [Free Programming Books Indonesia](https://github.com/EbookFoundation/free-programming-books/blob/master/free-programming-books-id.md) - Freely available programming books with Indonesia Language.
- [Buku Git](https://github.com/endymuhardin/buku-git) - Buku Version Control menggunakan Git.
- [E-Library MDC](https://github.com/MuhammadiyahDeveloperClub/E-Library) - E-Library yang berisi kumpulan E-Book tentang dunia IT.
- [JVM Handbook](https://github.com/bliblidotcom/jvm-handbook) - Buku pembahasan detil tentang cara kerja dan internal JVM.

## Content Management System

- [Lentera](https://github.com/laravel-indonesia/lentera) - Learning Management from Nusantara.
- [Pusaka CMS](https://github.com/codepolitanlab/pusakacms) - File-based CMS build with CodeIgniter Framework.
- [Elybin CMS](https://github.com/elybin/ElybinCMS) - Sistem manajemen konten gratis yang berfokus kepada kegunaan, kecepatan, dan kemudahan.
- [Fiyo CMS](https://github.com/FiyoCMS/FiyoCMS) - CMS Open Source with Easy Design, Fast Load, and SEO Friendly.
- [HTMLy](https://github.com/danpros/htmly) - HTMLy is an Databaseless or Flat-File Blogging Platform prioritizes simplicity and speed written in PHP.
- [PopojiCMS](https://github.com/PopojiCMS/PopojiCMS) - Buat Sendiri Rasa Webmu.

## Databases

- [Wilayah Administratif Indonesia](https://github.com/edwardsamuel/Wilayah-Administratif-Indonesia) - Data Provinsi, Kota/Kabupaten, Kecamatan, dan Kelurahan/Desa di Indonesia.

## Framework

- [Panada](https://github.com/panada/Panada) - High performance PHP development framework, yet simple.
- [Puko Framework](https://github.com/Velliz/pukoframework) - MVC framework for quick PHP application development.
- [IGOS Nusantara SDK](https://github.com/ignsdk/ignsdk-qt) - A SDK designed specifically for application developers in IGOS Nusantara.
- [EmeraldBox](https://github.com/femmerling/EmeraldBox) - EmeraldBox is a boilerplate framework for developing python web applications with database access.
- [Scarlets](https://github.com/ScarletsFiction/Scarlets) - Multipurpose and high performance PHP framework
- [ScarletsFrame](https://github.com/ScarletsFiction/ScarletsFrame) - A frontend framework that can help you write a simple web structure
- [Brutal PHP Framework](https://github.com/sonyarianto/brutal-php-framework) - A simple PHP framework for vanilla PHP programmer with taste of Twig and Symfony Routing. Database and other component is up to programmer to decide.

## Front-end Development

- [Bootstrap Markdown](https://github.com/toopay/bootstrap-markdown) - Bootstrap plugin for markdown editing.
- [Marka](https://github.com/fians/marka) - Beautiful transformable icons built for the web.
- [Waves](https://github.com/fians/Waves) - Click effect inspired by Google's Material Design.
- [React Komik](https://github.com/sonnylazuardi/react-komik) - ReactJS based comic strip creator using fabric.js canvas rendering.
- [Stisla](https://github.com/stisla/stisla) - A Free Bootstrap Admin Template which will help you to speed up your project and design your own dashboard UI

## Game

- [Word Chain](https://github.com/sonnylazuardi/wordchain) - Multiplayer word chaining game using AngularJS and firebase.
- [KatanyaGomoku](https://github.com/mgilangjanuar/KatanyaGomoku) - Modification Game of Tic Tac Toe using Java Languange.

## Go

- [unzipall](https://github.com/gedex/unzipall) - Unzip all zip files in src directory to dst directory.
- [Inflector](https://github.com/gedex/inflector) - Inflector pluralizes and singularizes English nouns.
- [Imaginative Go](https://github.com/sonyarianto/imaginative-go) - Web to learn Go.
- [Snowboard](https://github.com/bukalapak/snowboard) - API blueprint parser and renderer.
- [Shiori](https://github.com/go-shiori/shiori) - Simple bookmark manager built with Go.
- [Proctor](https://github.com/gojek/proctor) - A Developer-Friendly Automation Orchestrator.
- [GoTral](https://github.com/codenoid/GoTral) - Go cenTralized config, for distributed software
- [File.io Clone](https://github.com/codenoid/file.io) - File.io clone, file sharing with expiration

## Haskell

- [juancuk-hs](https://github.com/bejoistic/juancuk-hs) - Onion vanity URL generator written in Haskell
- [blockell](https://github.com/bejoistic/blockell) - Experimental blockchain implementation using Haskell

## Rust

- [Fido](https://github.com/codenoid/Fido) - Distributed Storage, Easily distribute your data accross disk/node

## JavaScript
- [PandaJS](https://github.com/rizki4106/pandajs) - pandajs is a library for create REST SERVER
- [PhantomJS](https://github.com/ariya/phantomjs) - Scriptable Headless WebKit.
- [jQuery Calx](https://github.com/xsanisty/jquery-calx) - jQuery plugin for creating formula-based calculation.
- [JVFloat.js](https://github.com/maman/JVFloat.js) - jQuery / Zepto plugin to emulate Matt D. Smith's floating placeholder text.
- [Meteoris2](https://github.com/radiegtya/meteoris2) - Realtime Javascript Boilerplate base on MeteorJS Framework.
- [SimpleExcel.js](https://github.com/faisalman/simple-excel-js) - Client-side script to easily parse / convert / write any Microsoft Excel.
- [UAParser.js](https://github.com/faisalman/ua-parser-js) - Lightweight JavaScript-based User-Agent string parser.
- [jQuery Awesome Sosmed Share Button](https://github.com/bachors/jQuery-Awesome-Sosmed-Share-Button) - Awesome social media button with share count.
- [Kinetic](https://github.com/ariya/kinetic) - Kinetic Scrolling with JavaScript.
- [Esprima](https://github.com/jquery/esprima) - ECMAScript parsing infrastructure for multipurpose analysis.
- [PHUNT](https://github.com/Kristories/phunt) - Product Hunt Command Line Client.
- [React Simple PWA](https://github.com/BosNaufal/react-simple-pwa) - Simple Progressive Web App Built with React Js.
- [Soya Next](https://github.com/traveloka/soya-next) - An opinionated configured Next.js framework.
- [Angkot](https://github.com/widatama/angkot) - A searchable Jakarta’s public transportation routes.
- [Iyem](https://github.com/lukluk/iyem) - Simple implementation multi thread in nodejs
- [react-electron-starter](https://github.com/hikmahgumelar/react-electron-starter) - A starter for create apps with Electron and ReactJS
- [cra-universal](https://github.com/antonybudianto/cra-universal) - Create React App companion for universal app. No eject, zero config, full HMR, and more
- [PodengJS](https://github.com/slaveofcode/podeng) - Simple JSON value normalization to make everything gone right.
- [SFDatabase-js](https://github.com/ScarletsFiction/SFDatabase-js) - A database library for Browser and Nodejs.
- [SFMediaStream](https://github.com/ScarletsFiction/SFMediaStream) - HTML5 media streamer library for playing music, video, playlist, or even live streaming microphone & camera with node server.
- [Serberries](https://github.com/ScarletsFiction/Serberries) - Nodejs live webserver that able to reload your script when running.
- [SFFileSystem](https://github.com/ScarletsFiction/SFFileSystem) - A filesystem library for saving files inside the browser storage.
- [SFIntercom](https://github.com/ScarletsFiction/SFIntercom) - Communication between browser's tabs with same domain.
- [YouTube Musical Spectrum](https://github.com/mfcc64/youtube-musical-spectrum) - A browser extension that offers audio visualization on your YouTube page with nice musical notes.
- [Simple Web Storage](https://github.com/sutanlab/simple-webstorage) - Lightweight utilities that can make easier to access application storage in client browser.
- [Museum Indonesia CLI](https://github.com/maulana20/museum-id) - Open source Show Data Museum in Indonesia via Command-CLI.
- [Data Wilayah](https://github.com/codenoid/Data-Wilayah.js) - Indonesia territory data in Javascript, so you don't backend/database

## Java

- [AndKasir](https://github.com/andriawan/AndKasir) - AndKasir merupakan Aplikasi Open Source berbasis Desktop yang dibangun dengan teknologi Java untuk kebutuhan retail perusahaan menengah dalam mengatur aktifitas barang masuk dan keluar.

## Angular

- [Angular Starter](https://github.com/antonybudianto/angular-starter) - Gulp Angular Starter using TypeScript.
- [Angular Webpack Starter](https://github.com/antonybudianto/angular-webpack-starter) - Angular Webpack Starter with AoT compilation, Lazy-loading, Tree-shaking, and Hot Module Reload.
- [Angular Parse Typescript](https://github.com/aacassandra/aac-parsetypescript) - This package makes it easy for typescript users for access the parse database via httpclient.

## Express

- [Express4 Bootstrap Starter](https://github.com/hengkiardo/express4-bootstrap-starter) - Lightweight Bootstrap NodeJS Apps Build Using ExpressJS 4.

## Vue

- [Vue 2 Starter](https://github.com/BosNaufal/vue2-starter) - Simple Vue 2 Starter for single page application with Vuex and Vue Router.
- [Vue 2 Simplert](https://github.com/mazipan/vue2-simplert) - Vue 2 Simple Alert Component (SweetAlert Inspired).
- [Vue SoundCloud](https://github.com/mul14/vue-soundcloud) - Vuejs + SoundCloud demo app.
- [Vue Autocomplete](https://github.com/BosNaufal/vue-autocomplete) - Autocomplete Component for Vue.js.
- [Vue Mini Shop](https://github.com/BosNaufal/vue-mini-shop) - Mini Online Shop Built With Vue.js.
- [Vue Loading Bar](https://github.com/BosNaufal/vue-loading-bar) - Youtube Like Loading Bar Component for Vue.js.
- [Vue 2 Loading Bar](https://github.com/BosNaufal/vue2-loading-bar) - Simplest Youtube Like Loading Bar Component For Vue 2.
- [Vue Simple PWA](https://github.com/BosNaufal/vue-simple-pwa) - Simple Progressive Web App Built with Vue.js.
- [Explore GitHub](https://github.com/mazipan/explore-github) - VueJS 2 GitHub Explorer Using API v3.
- [Salat Time](https://github.com/widatama/salat-time) - Location based daily salat schedule.
- [Tic Tac Vue](https://github.com/widatama/tic-tac-vue) - Tic tac toe game implemented with Vue.js.
- [Guess the Number](https://github.com/widatama/guess-the-number) - A single player guess the number game made with Vue.js.

## Gridsome Starter


## Jekyll Theme

- [JekMDL](https://github.com/tigefa4u/jekmdl) - Jekyll theme use Material Design Lite.
- [Mangan](https://github.com/dikiaap/mangan) - Geek theme for Jekyll.
- [Jekyll Klisé](https://github.com/piharpi/jekyll-klise) - Theme for running a personal site or blog, light and dark mode support.
- [Stack Problem](https://github.com/agusmakmun/agusmakmun.github.io) - Free and open-source Jekyll theme.
- [Jekyll Starter Blog](https://github.com/sutanlab/jekyll-starter-blog) - Awesome and Beautiful Jekyll Starter kit.

## Learn

- [Belajar Git](https://github.com/endymuhardin/belajarGit) - Tutorial Git dalam Bahasa Indonesia.
- [Ruby Basic](https://github.com/xinuc/ruby_basic) - Learning material for Ruby Programming Language, written in Bahasa Indonesia.
- [Docs vuejs id](https://github.com/vuejs-id/docs) - Dokumentasi Vue.js Bahasa Indonesia.
- [PUEBI Daring](https://github.com/ivanlanin/puebi) - PUEBI Daring adalah versi web ramah gawai dari PUEBI Permendikbud 50/2015.
- [TarungLab DDP1](https://github.com/laymonage/TarungLabDDP1) - Kumpulan materi pembelajaran dasar-dasar pemrograman dalam bahasa Python.
- [Basic Go](https://github.com/novalagung/dasarpemrogramangolang) - Best E-Book Learn Go in Bahasa.

## Linux Software

- [KBBI Qt](https://github.com/bgli/kbbi-qt) - KBBI Qt adalah aplikasi Kamus Besar Bahasa Indonesia berbasis GUI.
- [Frost Plank Theme](https://github.com/dikiaap/frost-plank-theme) - Quite dark Plank theme.
- [Blitz CLI](https://github.com/sonnylazuardi/blitz-cli) - A command line app to check cgv blitz movie seats.
- [Manokwari](https://github.com/BlankOn/manokwari) - A desktop shell for GNOME 3 with GTK+ and HTML5 frontend.
- [AndKamus](https://github.com/andriawan/AndKamus) - Aplikasi kamus Bahasa Indonesia - Bahasa Inggris berbasis CLI dikembangkan dengan C++.

## PHP

- [PHP-BOOTSTRAP](https://github.com/Harazaki/PHP-BOOTSTRAP) - Simple design templates framework together with the famous bootstrap.
- [Simple PHP Excel](https://github.com/faisalman/simple-excel-php) - Easily parse / convert / write any Microsoft Excel.
- [Membership PHP Indonesia](https://github.com/phpindonesia/phpindonesia.or.id-membership2) - Membership Application for PHP Indonesia.
- [playCMS](https://github.com/antonraharja/playSMS) - Web-based Mobile Portal System that it can be made to fit to various services such as an SMS gateway.
- [Terbilang](https://github.com/mul14/terbilang-php) - Convert numbers into words in Indonesian language.
- [SLiMS 8 Akasia](https://github.com/slims/slims8_akasia) - SLiMS is free open source software for library resources management and administration.
- [cURL Lib](https://github.com/andhikayuana/curl-lib) - Simple Wrapper for cURL
- [ovoid](https://github.com/lintangtimur/ovoid) - Unofficial OVO API Wrapper
- [MIKHMON - MikroTik Hotspot Monitor](https://github.com/laksa19/mikhmonv3)- MikroTik Hotspot Monitor adalah aplikasi berbasis web (MikroTik API PHP class) untuk membantu manajemen Hotspot MikroTik. Khususnya MikroTik yang tidak mendukung User Manager.
- [Pub/Sub Redis for PHP](https://github.com/RioRizkyRainey/pubsub-redis-php) - Implementasi Publisher-Subscriber Pattern pada PHP menggunakan Redis
- [PHP-Spellchecker](https://github.com/RioRizkyRainey/PHP-Spellchecker) - Spellchecker atau Typo detector menggunakan PHP. Tersedia juga assets dictionary untuk Bahasa Indonesia

## CodeIgniter

- [Gas ORM](https://github.com/toopay/gas-orm) - A lighweight and easy-to-use ORM for CodeIgniter.
- [SunQA](https://github.com/SunDi3yansyah/SunQA) - Simple App Question Answer like Stackoverflow.
- [CodeIgniter Websocket Apache Secure](https://github.com/aacassandra/CodeIgniter-Websocket-Apache-Secure) - Combination of CodeIgniter + Ratchet Websocket with Secure Connection.

## Laravel

- [Avatar](https://github.com/laravolt/avatar) - Plug and play avatar, turn initial name into beautiful avatar.
- [Pingpong Admin](https://github.com/pingpong-labs/admin) - Laravel 5 Admin.
- [Absis](https://github.com/smpn1smg/absis) - Sistem Akademik K13/KTSP Berbasis Web.
- [AppRocket PreProject](https://github.com/rawaludin/approcket-preproject) - Contoh aplikasi CRUD dengan Laravel 5.2.
- [Laravel Circuit Breaker](https://github.com/rymanalu/laravel-circuit-breaker) - Circuit Breaker pattern implementation in Laravel 5.
- [Laravel Simple Uploader](https://github.com/rymanalu/laravel-simple-uploader) - Simple file uploader for Laravel 5.
- [Laravel 5 Model Factory Generator](https://github.com/rymanalu/factory-generator) - Generate a new model factory using Artisan command.
- [Laravolt Indonesia](https://github.com/laravolt/indonesia) - Package Laravel yang berisi data Provinsi, Kabupaten/Kota, dan Kecamatan/Desa di Indonesia.
- [Semantic Form](https://github.com/laravolt/semantic-form) - Semantic UI form builder, for Laravel.
- [CRUD Booster](https://github.com/crocodic-studio/crudbooster) - Easy Admin Dashboard & CRUD generator for Laravel.
- [Dokularavel](https://github.com/crocodic-studio/dokularavel) - DOKU Payment Gateway Library for Laravel.
- [Free PMO](https://github.com/nafiesl/free-pmo) - Project management software for freelancers or agencies, built with Laravel 5.
- [LaraCed (Laravel Creator-Editor-Destroyer)](https://github.com/RioRizkyRainey/LaraCed) - This package automatically inserts/updates creator, editor and destroyer on your table migrations.
- [Indoregion](https://github.com/azishapidin/indoregion) - Package Data Provinsi, Kabupaten/Kota, Kecamatan/Distrik dan Desa/Kelurahan di Indonesia.

## Slim

- [Slim Starter](https://github.com/xsanisty/SlimStarter) - Starter Application built on Slim Framework in MVC (and HMVC) environment.
- [Boilerplate Slim 3](https://github.com/zhiephie/boilerplate-slim3) - Boilerplate for getting started with Slim Framework.

## Symfony

- [Symfonian Indonesia AdminBundle](https://github.com/SymfonyId/AdminBundle) - Admin Bundle with Symfony Framework.
- [Tania](https://github.com/Tanibox/tania) - An open source farming management system.

## Yii

- [Yii2 Admin](https://github.com/mdmsoft/yii2-admin) - Auth manager for Yii2 (RBAC Manager).

## Plugin

- [WP Slack](https://github.com/gedex/wp-slack) - This plugin allows you to send notifications to Slack channels when certain events in WordPress occur.
- [Minimalist](https://github.com/dikiaap/minimalist) - A Material Colorscheme Darker for Vim.
- [mpv-discordRPC](https://github.com/noaione/mpv-discordRPC) - A mpv media player lua script for Discord RPC integration

## Python

- [WordGraph](https://github.com/tistaharahap/WordGraph) - Weighting word frequency graph.
- [fasttext](https://github.com/salestock/fastText.py) - A Python interface for Facebook fastText.
- [HSR](https://github.com/pyk/hsr) - Hand signals recognition using Convolutional Neural Network implemented in TensorFlow.
- [KBBI Python](https://github.com/laymonage/kbbi-python) - Modul Python untuk mengambil entri sebuah kata/frase dalam KBBI Daring.
- [Excel2api](https://github.com/FerdinaKusumah/excel2api) - Convert your excel data as api.
- [Simple Face Recognition](https://github.com/FerdinaKusumah/face-recognition-webservice) - Simple face recognition with example.
- [Corona data api](https://github.com/FerdinaKusumah/corona-api) - Corona data api with realtime data.

# R

- [KalkulatorPajak](https://github.com/yht/KalkulatorPajak) - Indonesian Tax Calculator

## Natural Language Processing

- [Sastrawi](https://github.com/sastrawi/sastrawi) - High quality stemmer library for Indonesian Language.
- [PySastrawi](https://github.com/har07/PySastrawi) - Python port of high quality stemmer library for Indonesian Language.
- [Indonesian NLP resources](https://github.com/kmkurn/id-nlp-resource) - A list of Indonesian NLP resources related to language modeling, sentiment analysis, and more.

## Miscellaneous

- [Mac OSX Lion Theme](https://github.com/SunDi3yansyah/mac-osx-lion-theme) - (Web) Static site theme with Mac OSX Lion style.
- [Indonesian AdBlock Rules](https://github.com/ABPindo/indonesianadblockrules) - Compiled ABP rules to block various ads in Indonesian based site.
- [Indonesia on Top Regional Repositories](https://github.com/lorey/top-regional-repositories/blob/master/countries/indonesia.md) - Popular repositories in Indonesia.
- [Topojson of Indonesia](https://github.com/tvalentius/Indonesia-topojson) - TopoJSON of Indonesia (Kota/Kabupaten Level), useful for data visualisation.
- [OpenRetail](https://github.com/rudi-krsoftware/open-retail) - Perangkat lunak open source yang dikembangkan khusus untuk bidang usaha ritel, grosir, toko bangunan, toko komputer, toko buku, counter hp, pos, point of sale, kasir dan bidang usaha lainnya yang sejenis.

## Community

- [Komunitas Git Indonesia](https://github.com/GitIndonesia)
- [PHP Indonesia](https://github.com/phpindonesia)
- [CodeIgniter Indonesia](https://github.com/codeigniter-id)
- [Laravel Indonesia](https://github.com/laravel-indonesia)
- [Id-Laravel](https://github.com/id-laravel)
- [Symfonian Indonesia](https://github.com/SymfonyId)
- [Komunitas Pengguna Go Indonesia](https://github.com/golang-id)
- [Python Indonesia](https://github.com/id-python)
- [Jakarta JavaScript User Group](https://github.com/jakartajs)
- [Bali JavaScript Community](https://github.com/balijs)
- [React JS Indonesia](https://github.com/reactjs-id)
- [Nginx Indonesia](https://github.com/NginxID)
- [Asosiasi Programmer Indonesia](https://github.com/aprogsi)
- [Django Indonesia](https://github.com/django-id)
- [Haskell Indonesia](https://github.com/haskell-id)
- [Indonesian Ruby Community](https://github.com/id-ruby)
- [Kotlin Indonesia](https://github.com/KotlinID)
- [Nodejs Indonesia](https://github.com/nodejs-indonesia)
- [Hugo Indonesia](https://github.com/gohugoid)
- [Lombokdev](https://github.com/LombokDev)
- [Lombokjs](https://github.com/lombokjs)
- [JemberDev](https://github.com/jember-dev)
- [Banyuwangi Dev](https://github.com/banyuwangidev)
- [Data Science Indonesia](https://github.com/datascienceid)
- [Kalimantan Selatan Javascript User Group](https://github.com/KalselJS)
- [BekasiJS](https://github.com/bekasijs)
- [SurabayaJS](https://github.com/SurabayaJS)
- [ReversingID](https://github.com/ReversingID)
- [Komunitas Programmer Semarang](https://programmer-semarang.com)

## License

![Creative Commons License](https://licensebuttons.net/l/by/4.0/88x31.png)

This work is licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).
"
175,SharpRepository/SharpRepository,C#,"![sharp repository logo](https://user-images.githubusercontent.com/6349515/28491141-7b600e46-6eeb-11e7-8c4c-d6139479c18e.png)

What is SharpRepository?
--------------------------------

SharpRepository is a generic repository written in C# which includes support for various relational, 
document and object databases including Entity Framework, RavenDB, MongoDb and Db4o. SharpRepository includes Xml and
InMemory repository implementations as well. SharpRepository offers built-in caching options for AppFabric, 
Memcached and standard System.Runtime.Caching. SharpRepository also supports Specifications, FetchStrategies, 
Batches and Traits. 

How do I get started?
--------------------------------
Check out the [getting started guide](https://github.com/SharpRepository/SharpRepository/wiki/Getting-started). When you're done there, review the SharpRepository.Samples, SharpRepository.Samples.MvcCore, SharpRepository.Samples.MVC5, SharpRepository.Tests.Integration and SharpRepository.Tests 
project for additional sample usage and implementation details.

Compatibility Issues
--------------------------------
- All packages support .NET Framework 4.6 and .NET Standard 2.0. A good part of them supports .NET Standard 1.3
- CouchDB Repository is not compatible with CouchDB 2.0.0 (removed temporary views support)

Running tests
--------------------------------

Integration tests uses all implementations. In order to avoid failing tests and long timeouts you have to install:
- CouchDB 1.x (not 2.x)
- SQL Server Compact
- MongoDB

We notice timeouts and long test discovery in VS2017 and timeouts in ""dotnet test"". 
The best way is use nunit3 console you can get console here https://github.com/nunit/nunit-console/releases and add installation folder in your path
After that from your project folder you can run all tests with: 
```
nunit3-console.exe "".\SharpRepository.Samples\bin\Debug\net461\SharpRepository.Samples.dll"" "".\SharpRepository.Tests\bin\Debug\net461\SharpRepository.Tests.dll"" "".\SharpRepository.Tests.Integration\bin\Debug\net461\SharpRepository.Tests.Integration.dll""
```


Have Questions?
--------------------------------

* https://github.com/SharpRepository/SharpRepository/issues
* mail to sharprepository@googlegroups.com
* https://groups.google.com/d/forum/sharprepository
* open a question on stackoverflow.com with sharp-repository tag https://stackoverflow.com/questions/tagged/sharp-repository


"
176,jenkins-infra/repository-permissions-updater,Groovy,"Repository Permissions Updater
==============================

About
-----

The Jenkins project hosts Maven artifacts such as core and plugin releases on [Artifactory](https://repo.jenkins-ci.org/).

Its permissions system is independent of GitHub's, and we limit which users (identified by the Jenkins LDAP account, same as wiki and JIRA) are allowed to upload which artifacts.

This repository contains both the definitions for Artifactory upload permissions in [YAML format](https://en.wikipedia.org/wiki/YAML), as well as the tool that synchronizes them to Artifactory.

**Note:** These permissions are specifically for _uploading_ artifacts to the Jenkins project's Maven repository. It is independent of GitHub repository permissions. You may have one without the other. Typically, you'll either have both, or just the GitHub repository access.

Requesting Permissions
----------------------

**Prerequisite**: You need to have logged in once to [Artifactory](https://repo.jenkins-ci.org/) with your Jenkins community account (this is the same as the account you would use to login to Jira) before you can be added to a permissions target.

To request upload permissions to an artifact (typically a plugin), [file a PR](https://help.github.com/articles/creating-a-pull-request/) editing the appropriate YAML file, and provide a reference that shows you have commit permissions, or have an existing committer to the plugin comment on your PR, approving it.
See [this page](https://jenkins.io/doc/developer/plugin-governance/managing-permissions/) for more information.

Managing Permissions
--------------------

The directory `permissions/` contains a set of files, one per plugin or artifact, that define the permissions for the respective artifacts. Files typically have a `component`, `plugin`, or `pom` prefix for organization purposes:

* `plugin` is used for Jenkins plugins.
* `pom` is used for parent POMs and everything else consisting of just a POM file.
* `component` is used for everything else, usually libraries.

These prefixes, like the rest of the file name, have no semantic meaning and just help in organizing these files. 

Each file contains the following in [YAML format](https://en.wikipedia.org/wiki/YAML):

- A `name` (typically mirrored in the file name), this is also the `artifactId` of the Maven artifact.
- A `github` field indicating the GitHub organization and repository which is expected to produce these artifacts.
- A set of paths, usually just one. These correspond to the full Maven coordinates (`groupId` and `artifactId`) used for the artifact. Since Jenkins plugins can change group IDs and are still considered the same artifact, multiple entries are possible.
- A set of user names (Jenkins community user accounts in LDAP, the same as used for wiki and JIRA) allowed to upload this artifact to Artifactory. This set can be empty, which means nobody is currently allowed to upload the plugin in question (except Artifactory admins). This can happen for plugins that haven't seen releases in several years, or permission cleanups.

Example file:

```yaml
---
name: ""p4""
github: ""jenkinsci/p4-plugin""
paths:
- ""org/jenkins-ci/plugins/p4""
developers:
- ""p4paul""
```

* `p4` (lines 2 and 5): `artifactId`
* `p4-plugin` (line 3): GitHub repository name
* `org/jenkins-ci` (line 5): `groupId` (with slashes replacing periods)
* `p4paul` (line 7): Jenkins community account user name

### Adding a new plugin

Create a new YAML file similar to existing files.

### Adding a new uploader to an existing plugin

Edit the `developers` list in the YAML file for the plugin.

### Deprecating a plugin

Remove the YAML file. The next synchronization will remove permissions for the plugin.

### Renaming a plugin

Rename and edit the existing permissions file, changing both `name` and the last `path` component.

### Changing a plugin's `groupId`

Change the `paths` to match the new Maven coordinates, or, if further uploads for the old coordinates are expected, add a new list entry.

Managing Security Process
-------------------------

The Jenkins project acts as a primary contact point for security researchers seeking to report security vulnerabilities in Jenkins and Jenkins plugins ([learn more](https://jenkins.io/security/)).

We generally assign reported issues to maintainers after a basic sanity check ([learn more](https://www.jenkins.io/security/for-maintainers/)).
Through additional metadata in the YAML file described above, you can define who else should be contacted in the event of a report being received and is authorized to make decisions regarding security updates, e.g. scheduling.
Add a section like the following to your plugin's YAML file:

```yaml
security:
  contacts:
    jira: some_user_name
    email: security@acme.org
```

Given the above example, we will primarily assign any security issue in Jira to `some_user_name` and send an email notification to `security@acme.org` to establish contact.
Regular maintainers are added to the issue as well to give visibility and allow participation/discussion.
This means that specifying a Jira security contact is only useful when it's an account not already listed as maintainer.
Either of `jira` and `email` is optional.

Please note that we generally reject email contacts due to the additional overhead in reaching out via email.
Unless you represent a large organization with dedicated security team that needs to be involved in the coordination of a release, but is not otherwise part of plugin maintenance, please refrain from requesting to be contacted via email.

Managing Issue Trackers
-----------------------

The YAML metadata files in this repository also hold information about issue trackers.
This is used to make issue trackers more accessible, e.g. by adding them to the UI of Jenkins or on https://plugins.jenkins.io.

### Declaring Issue Trackers

The top-level `issues` key contains a sorted list of issue tracker references in descending order of preference.

For GitHub issues, a GitHub repository must be specified as the value for the `github` key, and the value must start with `jenkinsci/`, followed by the repository name.
For Jira, a component name or ID must be specified as the value for the `jira` key.
The component name is easier to read, while the component ID is resilient in case of component renames.

Either kind of issue tracker supports the `report:` boolean value that controls whether new issues should be reported in this issue tracker.
The default is `true`.

A complete example with two trackers:

```yaml
issues:
  - github: 'jenkinsci/configuration-as-code-plugin' # The preferred issue tracker
  - jira: 'configuration-as-code-plugin' # A secondary issue tracker is the Jira component 'configuration-as-code-plugin'
    report: no # No new issues should be reported here
```

When GitHub Issues is used, there would be some duplicated content in the file (between `github` and `issues` entries) which can be resolved by using a YAML reference.
Example:

```yaml
github: &GH 'jenkinsci/configuration-as-code-plugin' # Declare a reference
issues:
  - github: *GH # Use the reference
```

### Consuming Issue Trackers

A file `issues.index.json` is generated when the tool is executed, containing a map from component names to a list of issue trackers.
Only plugins are expected to specify an issue tracker here.

If a plugin does not have a corresponding key in this map, the tool did not consider it for inclusion.
If a plugin has a corresponding key in this map but an empty list of issue trackers, no issue trackers are known or supported.

Each issue tracker entry will have the following keys:

* `type`: Currently `jira` (meaning issues.jenkins.io) or `github` (meaning GitHub issues)
* `reference` contains a `type`-specific identifier string that provides additional information how issues are tracked; for Jira it is the component name or ID and for GitHub Issues it is the `orgname/reponame` String.
* `viewUrl` is a URL to a human-readable overview page. This value may not exist if no valid URL could be determined.
* `reportUrl` is a URL where users can report issues. This value may not exist if no valid URL could be determined, or new issues should not be reported in this tracker.

The list is sorted in descending order of preference.
The first issue tracker in the list with a `reportUrl` should be presented as the primary (or only) option for reporting issues.
Further issue trackers are mostly provided as a reference, e.g. when listing existing issues, although a different issue tracker with `reportUrl` can be linked if users provide a preference for a specific kind of issue tracker.

Usage
-----

To see how to run this tool to synchronize Artifactory permission targets with the definitions in this repository, see `Jenkinsfile`.

The following Java system properties can be used to customize the tool's behavior:

* `dryRun` - Set to `true` to generate the API payloads without submitting them. No modifications will be executed.
* `development` - Set to `true` during tool development to ensure production data is not overridden. This will have the following effects:
  - Permissions are only granted to deploy to the `snapshots` repository (rather than both `snapshots` and `releases`)
  - A different, non-colliding set of prefixes (unless overridden, see below) is used.
* `definitionsDir` - Path to directory containing permission definitions YAML files, defaults to `./permissions`.
* `artifactoryUserNamesJsonListUrl` - URL to a list containing known Artifactory user names, any permissions assigned to a user not on that list will cause the tool to abort
* `artifactoryApiTempDir` - Path to directory (that will be created) where this tool stores Artifactory permissions API JSON payloads, defaults to `./json`.
* `artifactoryObjectPrefix` - Override the prefix for groups and permission targets managed (created, updated, removed) using the tool.
  If unspecified, the value will be `generatedv2-` by default, or `generateddev-` in _development mode_.
* `artifactoryUrl` - URL to Artifactory, defaults to `https://repo.jenkins-ci.org`
* `artifactoryTokenMinutesValid` - How long authentication tokens to Artifactory for CD enabled repos should be valid for, default `240` (4 hours).
  Regular script execution frequency needs to be aligned with this.
* `gitHubSecretNamePrefix` - Prefix for secrets sent to GitHub repos.
  If unspecified, the value will be `MAVEN_` by default, or `DEV_MAVEN_` in _development mode_.
* `jiraUserNamesJsonListUrl` - URL to a list containing known Jira user names of (potential) maintainers.
  This is essentially a workaround to reduce the number of individual user lookups via Jira API.

It expected the following environment variables to be set:

- `ARTIFACTORY_USERNAME` - Admin user name for Artifactory
- `ARTIFACTORY_PASSWORD` - Corresponding admin password (or API key) for Artifactory admin user
- `GITHUB_USERNAME` - GitHub user name for a user with admin access to any CD enabled repos
- `GITHUB_TOKEN` - Corresponding token for the user with admin access to any CD enabled repos, [requires `repo` scope to create/update secrets](https://docs.github.com/en/free-pro-team@latest/rest/reference/actions#create-or-update-a-repository-secret)
- `JIRA_USERNAME` - User name (does not need admin permissions) for Jira
- `JIRA_PASSWORD` - Corresponding password for Jira user

### How It Works

The tool runs three steps in sequence:

1. Generate JSON payloads from YAML permission definition files.
2. Submit generated JSON payloads to Artifactory.
3. Remove all generated permission targets in Artifactory that have no corresponding generated JSON payload file.
"
177,hadynz/repository.arabic.xbmc-addons,Python,"Arabic XBMC Addons Repository
======================

* Author: Hady Osman (hadyos@gmail.com)
* Twitter: [@hadynz](https://twitter.com/hadynz)
* Version: 1.0.0
* Github: https://github.com/hadynz/repository.arabic.xbmc-addons

## Introduction
This repository contains a number of XBMC addons that are focused around providing Arabic streaming content.

Some plugins provide access to free and paid Arabic TV. Other plugins provide access to Arabic TV On-Demand which might include movies, clips and TV series.

## Arabic XBMC Addons
The following is a listing of the Arabic XBMC addons available from this repository:

\# | Plugin | Source | Subscription | Media Type
---|---|---|---|---
1 | Shahid.Net | http://shahid.mbc.net | No | TV On-Demand
2 | Al Qahera Al Youm | http://www.alqaheraalyoum.net‎ | No | TV Show
3 | ATN Network | http://www.atnnetwork.com | Yes | Live TV
4 | GL Arab | http://www.glarab.com | No | Live TV
5 | Teledunet | http://www.teledunet.com | No | Live TV
6 | Sotwesoora.Tv | http://www.sotwesoora.tv | No | Movies, TV Series
7 | Bokra | http://bokra.net/ | No | Movies, TV Series
8 | Panet | http://www.panet.co.il/ | No | Movies, TV Series
9 | Alarab | http://www.alarab.net/ | No | Movies, TV Series
10 | DailyTube4U.com | http://www.dailytube4u.com | No | On-Demand TV, News, Sports
11 | Syrian-drama | http://syrian-drama.com/ | No | Movies, TV Series
12 | Alquds Eyes | http://www.alqudseyes.com/ | No | Movies, TV Series
13 | Arabichannels | http://arabichannels.com/ | No | Live TV
14 | DramaCafe |http://online.dramacafe.tv/ | No | Movies, TV Series
15 | Dubai TV |http://vod.dmi.ae/ | No | Movies, TV Series
16 | CARTOONARABI|http://www.cartoonarabi.com| No | Movies, TV Series
17 | Arabic Live Stream Super Collection|Several providers | No | Live TV
18 | Al Noor TV |http://alnoortv.co | Yes | Movies, TV Series

## Installation
There are two main ways to install this XBMC repository:
### 1. Using XBMC HUB's Fusion Server (recommended)
1. Add http://fusion.tvaddons.co as a media source (or follow steps **1-13** from the following URL: https://www.tvaddons.co/fusion-kodi-krypton/).
2. Browse to the following direction structure: **/video/world/arabic**
3. Choose: **repository.arabic.xbmc-addons-1.0.0.zip**

### 2. Manual Download and Installation
1. Simply download the [repository zip](https://www.dropbox.com/s/aisgyozoxquyfc1/repository.arabic.xbmc-addons1.0.1%20%281%29.zip?dl=0) 
and install it in XBMC.
2. Read the following [guide](http://wiki.xbmc.org/index.php?title=Add-ons#How_to_install_from_zip) 
on the XBMC wiki to learn how to install addons from a repository.

## F.A.Q.
### When I install the repository from ZIP file, XBMC complains that it ""does not have the correct structure"".
This has typically meant that you have downloaded this github repository as a ZIP file (by clicking on the
'Download as zip' button) in the downloads page. Instead, you need to download the repository from the link
in the 'Download Packages' section of the downloads page.

## Contributors
* [hadynz](https://github.com/hadynz)
* [bisha77](https://github.com/bisha77)
* [Qwin](https://github.com/Qwin)
* [bbk79](https://github.com/bbk79)


## Contribution
If you want to contribute to this repository either reach me on [twitter](http://twitter.com/hadynz) or the XBMC forums via a [private message](http://forum.xbmc.org/member.php?action=profile&uid=137319).

## Questions, Comments, Feature Requests or Issues
If you have any suggestions or feedback to make this repository even more awesome sauce then go ahead and submit an 
[issue](https://github.com/hadynz/repository.arabic.xbmc-addons/issues), or [contact me directly](mailto:hadyos@gmail.com).

[![Bitdeli Badge](https://d2weczhvl823v0.cloudfront.net/hadynz/repository.arabic.xbmc-addons/trend.png)](https://bitdeli.com/free ""Bitdeli Badge"")

"
178,Gexos/Hacking-Tools-Repository,HTML,"Hacking Tools Repository
========================
**http://gexos.github.io/Hacking-Tools-Repository/**

***
**If you have suggestions or links to tools/scripts to submit, please do, you can fork, edit, send a pull request or you can leave a comment on the [wiki page here][1].**

**The main categories are:**

 - Passwords
 - Scanning
 - Sniffer
 - Enumeration
 - Networking Tools
 - Wireless
 - Bluetooth
 - Web Scanners
 - Database
 - Vuln Scanners
 - Vuln Apps
 - Live CD
 - String Manipulation

----------
This is a list of security tools that have been collected from the internet. These tools are specifically aimed toward security professionals and enthusiasts/hobbyists for testing and demonstrating security weaknesses.

These tools are created for the sole purpose of security awareness and education, they should not be used against systems that you do not have permission to test/attack. You could end up in jail.

Most of the tools are open source/free with a couple of exceptions, before using any tools, I recommend that you read the instructions/documentation available on each of the individual tool's websites. Although some of the tools could be listed in more than one category, they only appear in the list only once, under its primary category. 
"
179,archlinuxcn/repo,Shell,"Arch Linux Chinese Community Repository
====

Packaging consistency check: [![Build Status](https://travis-ci.org/archlinuxcn/repo.svg?branch=master)](https://travis-ci.org/archlinuxcn/repo)

For detailed information in Chinese, [visit here](https://www.archlinuxcn.org/archlinux-cn-repo-and-mirror/).
中文介绍[请看这里](https://www.archlinuxcn.org/archlinux-cn-repo-and-mirror/)。

### Usage

Add repo:

```
[archlinuxcn]
Server = https://repo.archlinuxcn.org/$arch
```
to your /etc/pacman.conf .

For mirrors (mainly in China), see https://github.com/archlinuxcn/mirrorlist-repo.

Import PGP Keys:

```bash
sudo pacman -Syy && sudo pacman -S archlinuxcn-keyring
```

### Issues

* Flag package OUT-OF-DATE by submiting new issues (please follow the template).
  * If the new release is within less than a day, please be patient and wait for up to one day; our bot is likely going to build a new one soon.
* If there is something wrong with provided packages, please submit issues of desired type.
* Please contact us via issues or email.
"
180,bosnadev/repository,PHP,"# Laravel Repositories

[![Build Status](https://travis-ci.org/bosnadev/repository.svg?branch=master)](https://travis-ci.org/bosnadev/repository) 
[![SensioLabsInsight](https://img.shields.io/sensiolabs/i/f39e6dc7-1364-481d-b722-8413bdc3200f.svg?style=flat)](https://insight.sensiolabs.com/projects/f39e6dc7-1364-481d-b722-8413bdc3200f)
[![Latest Stable Version](https://poser.pugx.org/bosnadev/repositories/v/stable)](https://packagist.org/packages/bosnadev/repositories)
[![Total Downloads](https://poser.pugx.org/bosnadev/repositories/downloads)](https://packagist.org/packages/bosnadev/repositories)
[![Monthly Downloads](https://poser.pugx.org/bosnadev/repositories/d/monthly)](https://packagist.org/packages/bosnadev/repositories)
[![License](https://poser.pugx.org/bosnadev/repositories/license)](https://packagist.org/packages/bosnadev/repositories)

Laravel Repositories is a package for Laravel 5 which is used to abstract the database layer. This makes applications much easier to maintain.

## Installation

Run the following command from you terminal:


 ```bash
 composer require ""bosnadev/repositories: 0.*""
 ```

or add this to require section in your composer.json file:

 ```
 ""bosnadev/repositories"": ""0.*""
 ```

then run ```composer update```


## Usage

First, create your repository class. Note that your repository class MUST extend ```Bosnadev\Repositories\Eloquent\Repository``` and implement model() method

```php
<?php namespace App\Repositories;

use Bosnadev\Repositories\Contracts\RepositoryInterface;
use Bosnadev\Repositories\Eloquent\Repository;

class FilmsRepository extends Repository {

    public function model() {
        return 'App\Film';
    }
}
```

By implementing ```model()``` method you telling repository what model class you want to use. Now, create ```App\Film``` model:

```php
<?php namespace App;

use Illuminate\Database\Eloquent\Model;

class Film extends Model {

    protected $primaryKey = 'film_id';

    protected $table = 'film';

    protected $casts = [
        ""rental_rate""       => 'float'
    ];
}
```

And finally, use the repository in the controller:

```php
<?php namespace App\Http\Controllers;

use App\Repositories\FilmsRepository as Film;

class FilmsController extends Controller {

    private $film;

    public function __construct(Film $film) {

        $this->film = $film;
    }

    public function index() {
        return \Response::json($this->film->all());
    }
}
```

## Available Methods

The following methods are available:

##### Bosnadev\Repositories\Contracts\RepositoryInterface

```php
public function all($columns = array('*'))
public function lists($value, $key = null)
public function paginate($perPage = 1, $columns = array('*'));
public function create(array $data)
// if you use mongodb then you'll need to specify primary key $attribute
public function update(array $data, $id, $attribute = ""id"")
public function delete($id)
public function find($id, $columns = array('*'))
public function findBy($field, $value, $columns = array('*'))
public function findAllBy($field, $value, $columns = array('*'))
public function findWhere($where, $columns = array('*'))
```

##### Bosnadev\Repositories\Contracts\CriteriaInterface

```php
public function apply($model, Repository $repository)
```

### Example usage


Create a new film in repository:

```php
$this->film->create(Input::all());
```

Update existing film:

```php
$this->film->update(Input::all(), $film_id);
```

Delete film:

```php
$this->film->delete($id);
```

Find film by film_id;

```php
$this->film->find($id);
```

you can also chose what columns to fetch:

```php
$this->film->find($id, ['title', 'description', 'release_date']);
```

Get a single row by a single column criteria.

```php
$this->film->findBy('title', $title);
```

Or you can get all rows by a single column criteria.
```php
$this->film->findAllBy('author_id', $author_id);
```

Get all results by multiple fields

```php
$this->film->findWhere([
    'author_id' => $author_id,
    ['year','>',$year]
]);
```

## Criteria

Criteria is a simple way to apply specific condition, or set of conditions to the repository query. Your criteria class MUST extend the abstract ```Bosnadev\Repositories\Criteria\Criteria``` class.

Here is a simple criteria:

```php
<?php namespace App\Repositories\Criteria\Films;

use Bosnadev\Repositories\Criteria\Criteria;
use Bosnadev\Repositories\Contracts\RepositoryInterface as Repository;

class LengthOverTwoHours extends Criteria {

    /**
     * @param $model
     * @param RepositoryInterface $repository
     * @return mixed
     */
    public function apply($model, Repository $repository)
    {
        $model = $model->where('length', '>', 120);
        return $model;
    }
}
```

Now, inside you controller class you call pushCriteria method:

```php
<?php namespace App\Http\Controllers;

use App\Repositories\Criteria\Films\LengthOverTwoHours;
use App\Repositories\FilmsRepository as Film;

class FilmsController extends Controller {

    /**
     * @var Film
     */
    private $film;

    public function __construct(Film $film) {

        $this->film = $film;
    }

    public function index() {
        $this->film->pushCriteria(new LengthOverTwoHours());
        return \Response::json($this->film->all());
    }
}
```


## Credits

This package is largely inspired by [this](https://github.com/prettus/l5-repository) great package by @andersao. [Here](https://github.com/anlutro/laravel-repository/) is another package I used as reference.
"
181,hassio-addons/repository,Jinja,"# Home Assistant Community Add-ons

![Project Stage][project-stage-shield]
![Maintenance][maintenance-shield]
[![License][license-shield]](LICENSE.md)

[![Discord][discord-shield]][discord]
[![Community Forum][forum-shield]][forum]

## About

Home Assistant allows anyone to create add-on repositories to share their
add-ons for Home Assistant easily. This repository is one of those repositories,
providing extra Home Assistant add-ons for your installation.

The primary goal of this project is to provide you (as a Home Assistant user)
with additional, high quality, add-ons that allow you to take your automated
home to the next level.

## Installation

In general, there is no need to install this repository on your
Home Assistant instance. It is activated and added by Home Assistant
by default.

However, if the repository is missing on your setup, adding this add-ons
repository to your Home Assistant instance is pretty easy. In the
Home Assistant add-on store, a possibility to add a repository is provided.

Use the following URL to add this repository:

```txt
https://github.com/hassio-addons/repository
```

## Add-ons provided by this repository

### &#10003; [AdGuard Home][addon-adguard]

![Latest Version][adguard-version-shield]
![Supports armhf Architecture][adguard-armhf-shield]
![Supports armv7 Architecture][adguard-armv7-shield]
![Supports aarch64 Architecture][adguard-aarch64-shield]
![Supports amd64 Architecture][adguard-amd64-shield]
![Supports i386 Architecture][adguard-i386-shield]

Network-wide ads & trackers blocking DNS server

[:books: AdGuard Home add-on documentation][addon-doc-adguard]

### &#10003; [AirCast][addon-aircast]

![Latest Version][aircast-version-shield]
![Supports armhf Architecture][aircast-armhf-shield]
![Supports armv7 Architecture][aircast-armv7-shield]
![Supports aarch64 Architecture][aircast-aarch64-shield]
![Supports amd64 Architecture][aircast-amd64-shield]
![Supports i386 Architecture][aircast-i386-shield]

AirPlay capabilities for your Chromecast devices.

[:books: AirCast add-on documentation][addon-doc-aircast]

### &#10003; [AirSonos][addon-airsonos]

![Latest Version][airsonos-version-shield]
![Supports armhf Architecture][airsonos-armhf-shield]
![Supports armv7 Architecture][airsonos-armv7-shield]
![Supports aarch64 Architecture][airsonos-aarch64-shield]
![Supports amd64 Architecture][airsonos-amd64-shield]
![Supports i386 Architecture][airsonos-i386-shield]

AirPlay capabilities for your Sonos (and UPnP) devices.

[:books: AirSonos add-on documentation][addon-doc-airsonos]

### &#10003; [AppDaemon 4][addon-appdaemon]

![Latest Version][appdaemon-version-shield]
![Supports armhf Architecture][appdaemon-armhf-shield]
![Supports armv7 Architecture][appdaemon-armv7-shield]
![Supports aarch64 Architecture][appdaemon-aarch64-shield]
![Supports amd64 Architecture][appdaemon-amd64-shield]
![Supports i386 Architecture][appdaemon-i386-shield]

Python Apps and Dashboard using AppDaemon 4.x for Home Assistant

[:books: AppDaemon 4 add-on documentation][addon-doc-appdaemon]

### &#10003; [Bitwarden (Vaultwarden)][addon-bitwarden]

![Latest Version][bitwarden-version-shield]
![Supports armhf Architecture][bitwarden-armhf-shield]
![Supports armv7 Architecture][bitwarden-armv7-shield]
![Supports aarch64 Architecture][bitwarden-aarch64-shield]
![Supports amd64 Architecture][bitwarden-amd64-shield]
![Supports i386 Architecture][bitwarden-i386-shield]

Open source password management solution

[:books: Bitwarden (Vaultwarden) add-on documentation][addon-doc-bitwarden]

### &#10003; [Bookstack][addon-bookstack]

![Latest Version][bookstack-version-shield]
![Supports armhf Architecture][bookstack-armhf-shield]
![Supports armv7 Architecture][bookstack-armv7-shield]
![Supports aarch64 Architecture][bookstack-aarch64-shield]
![Supports amd64 Architecture][bookstack-amd64-shield]
![Supports i386 Architecture][bookstack-i386-shield]

Simple & Free Wiki Software

[:books: Bookstack add-on documentation][addon-doc-bookstack]

### &#10003; [ESPHome][addon-esphome]

![Latest Version][esphome-version-shield]
![Supports armhf Architecture][esphome-armhf-shield]
![Supports armv7 Architecture][esphome-armv7-shield]
![Supports aarch64 Architecture][esphome-aarch64-shield]
![Supports amd64 Architecture][esphome-amd64-shield]
![Supports i386 Architecture][esphome-i386-shield]

ESPHome Hass.io add-on for intelligently managing all your ESP8266/ESP32 devices.

[:books: ESPHome add-on documentation][addon-doc-esphome]

### &#10003; [Example][addon-example]

![Latest Version][example-version-shield]
![Supports armhf Architecture][example-armhf-shield]
![Supports armv7 Architecture][example-armv7-shield]
![Supports aarch64 Architecture][example-aarch64-shield]
![Supports amd64 Architecture][example-amd64-shield]
![Supports i386 Architecture][example-i386-shield]

Example add-on by Community Home Assistant Add-ons

[:books: Example add-on documentation][addon-doc-example]

### &#10003; [FTP][addon-ftp]

![Latest Version][ftp-version-shield]
![Supports armhf Architecture][ftp-armhf-shield]
![Supports armv7 Architecture][ftp-armv7-shield]
![Supports aarch64 Architecture][ftp-aarch64-shield]
![Supports amd64 Architecture][ftp-amd64-shield]
![Supports i386 Architecture][ftp-i386-shield]

A secure and fast FTP server for Home Assistant

[:books: FTP add-on documentation][addon-doc-ftp]

### &#10003; [Folding@home][addon-foldingathome]

![Latest Version][foldingathome-version-shield]
![Supports armhf Architecture][foldingathome-armhf-shield]
![Supports armv7 Architecture][foldingathome-armv7-shield]
![Supports aarch64 Architecture][foldingathome-aarch64-shield]
![Supports amd64 Architecture][foldingathome-amd64-shield]
![Supports i386 Architecture][foldingathome-i386-shield]

Fighting disease with a world wide distributed super computer

[:books: Folding@home add-on documentation][addon-doc-foldingathome]

### &#10003; [Glances][addon-glances]

![Latest Version][glances-version-shield]
![Supports armhf Architecture][glances-armhf-shield]
![Supports armv7 Architecture][glances-armv7-shield]
![Supports aarch64 Architecture][glances-aarch64-shield]
![Supports amd64 Architecture][glances-amd64-shield]
![Supports i386 Architecture][glances-i386-shield]

A cross-platform system monitoring tool

[:books: Glances add-on documentation][addon-doc-glances]

### &#10003; [Grafana][addon-grafana]

![Latest Version][grafana-version-shield]
![Supports armhf Architecture][grafana-armhf-shield]
![Supports armv7 Architecture][grafana-armv7-shield]
![Supports aarch64 Architecture][grafana-aarch64-shield]
![Supports amd64 Architecture][grafana-amd64-shield]
![Supports i386 Architecture][grafana-i386-shield]

The open platform for beautiful analytics and monitoring

[:books: Grafana add-on documentation][addon-doc-grafana]

### &#10003; [Grocy][addon-grocy]

![Latest Version][grocy-version-shield]
![Supports armhf Architecture][grocy-armhf-shield]
![Supports armv7 Architecture][grocy-armv7-shield]
![Supports aarch64 Architecture][grocy-aarch64-shield]
![Supports amd64 Architecture][grocy-amd64-shield]
![Supports i386 Architecture][grocy-i386-shield]

ERP beyond your fridge! A groceries & household management solution for your home

[:books: Grocy add-on documentation][addon-doc-grocy]

### &#10003; [Home Panel][addon-home-panel]

![Latest Version][home-panel-version-shield]
![Supports armhf Architecture][home-panel-armhf-shield]
![Supports armv7 Architecture][home-panel-armv7-shield]
![Supports aarch64 Architecture][home-panel-aarch64-shield]
![Supports amd64 Architecture][home-panel-amd64-shield]
![Supports i386 Architecture][home-panel-i386-shield]

A web frontend for controlling the home

[:books: Home Panel add-on documentation][addon-doc-home-panel]

### &#10003; [InfluxDB][addon-influxdb]

![Latest Version][influxdb-version-shield]
![Supports armhf Architecture][influxdb-armhf-shield]
![Supports armv7 Architecture][influxdb-armv7-shield]
![Supports aarch64 Architecture][influxdb-aarch64-shield]
![Supports amd64 Architecture][influxdb-amd64-shield]
![Supports i386 Architecture][influxdb-i386-shield]

Scalable datastore for metrics, events, and real-time analytics

[:books: InfluxDB add-on documentation][addon-doc-influxdb]

### &#10003; [JupyterLab][addon-jupyterlab]

![Latest Version][jupyterlab-version-shield]
![Supports armhf Architecture][jupyterlab-armhf-shield]
![Supports armv7 Architecture][jupyterlab-armv7-shield]
![Supports aarch64 Architecture][jupyterlab-aarch64-shield]
![Supports amd64 Architecture][jupyterlab-amd64-shield]
![Supports i386 Architecture][jupyterlab-i386-shield]

Create documents containing live code, equations, visualizations, and explanatory text

[:books: JupyterLab add-on documentation][addon-doc-jupyterlab]

### &#10003; [Log Viewer][addon-log-viewer]

![Latest Version][log-viewer-version-shield]
![Supports armhf Architecture][log-viewer-armhf-shield]
![Supports armv7 Architecture][log-viewer-armv7-shield]
![Supports aarch64 Architecture][log-viewer-aarch64-shield]
![Supports amd64 Architecture][log-viewer-amd64-shield]
![Supports i386 Architecture][log-viewer-i386-shield]

Browser-based log utility for Home Assistant

[:books: Log Viewer add-on documentation][addon-doc-log-viewer]

### &#10003; [MQTT Server & Web client][addon-mqtt]

![Latest Version][mqtt-version-shield]
![Supports armhf Architecture][mqtt-armhf-shield]
![Supports armv7 Architecture][mqtt-armv7-shield]
![Supports aarch64 Architecture][mqtt-aarch64-shield]
![Supports amd64 Architecture][mqtt-amd64-shield]
![Supports i386 Architecture][mqtt-i386-shield]

Mosquitto MQTT Server bundled with Hivemq's web client

[:books: MQTT Server & Web client add-on documentation][addon-doc-mqtt]

### &#10003; [Matrix][addon-matrix]

![Latest Version][matrix-version-shield]
![Supports armhf Architecture][matrix-armhf-shield]
![Supports armv7 Architecture][matrix-armv7-shield]
![Supports aarch64 Architecture][matrix-aarch64-shield]
![Supports amd64 Architecture][matrix-amd64-shield]
![Supports i386 Architecture][matrix-i386-shield]

A secure and decentralized communication platform.

[:books: Matrix add-on documentation][addon-doc-matrix]

### &#10003; [Network UPS Tools][addon-nut]

![Latest Version][nut-version-shield]
![Supports armhf Architecture][nut-armhf-shield]
![Supports armv7 Architecture][nut-armv7-shield]
![Supports aarch64 Architecture][nut-aarch64-shield]
![Supports amd64 Architecture][nut-amd64-shield]
![Supports i386 Architecture][nut-i386-shield]

Manage battery backup (UPS) devices

[:books: Network UPS Tools add-on documentation][addon-doc-nut]

### &#10003; [Nginx Proxy Manager][addon-nginxproxymanager]

![Latest Version][nginxproxymanager-version-shield]
![Supports armhf Architecture][nginxproxymanager-armhf-shield]
![Supports armv7 Architecture][nginxproxymanager-armv7-shield]
![Supports aarch64 Architecture][nginxproxymanager-aarch64-shield]
![Supports amd64 Architecture][nginxproxymanager-amd64-shield]
![Supports i386 Architecture][nginxproxymanager-i386-shield]

Manage Nginx proxy hosts with a simple, powerful interface

[:books: Nginx Proxy Manager add-on documentation][addon-doc-nginxproxymanager]

### &#10003; [Node-RED][addon-node-red]

![Latest Version][node-red-version-shield]
![Supports armhf Architecture][node-red-armhf-shield]
![Supports armv7 Architecture][node-red-armv7-shield]
![Supports aarch64 Architecture][node-red-aarch64-shield]
![Supports amd64 Architecture][node-red-amd64-shield]
![Supports i386 Architecture][node-red-i386-shield]

Flow-based programming for the Internet of Things

[:books: Node-RED add-on documentation][addon-doc-node-red]

### &#10003; [Plex Media Server][addon-plex]

![Latest Version][plex-version-shield]
![Supports armhf Architecture][plex-armhf-shield]
![Supports armv7 Architecture][plex-armv7-shield]
![Supports aarch64 Architecture][plex-aarch64-shield]
![Supports amd64 Architecture][plex-amd64-shield]
![Supports i386 Architecture][plex-i386-shield]

Recorded media, live TV, online news, and podcasts ready to stream.

[:books: Plex Media Server add-on documentation][addon-doc-plex]

### &#10003; [Portainer][addon-portainer]

![Latest Version][portainer-version-shield]
![Supports armhf Architecture][portainer-armhf-shield]
![Supports armv7 Architecture][portainer-armv7-shield]
![Supports aarch64 Architecture][portainer-aarch64-shield]
![Supports amd64 Architecture][portainer-amd64-shield]
![Supports i386 Architecture][portainer-i386-shield]

Manage your Docker environment with ease

[:books: Portainer add-on documentation][addon-doc-portainer]

### &#10003; [SQLite Web][addon-sqlite-web]

![Latest Version][sqlite-web-version-shield]
![Supports armhf Architecture][sqlite-web-armhf-shield]
![Supports armv7 Architecture][sqlite-web-armv7-shield]
![Supports aarch64 Architecture][sqlite-web-aarch64-shield]
![Supports amd64 Architecture][sqlite-web-amd64-shield]
![Supports i386 Architecture][sqlite-web-i386-shield]

Explore your SQLite database

[:books: SQLite Web add-on documentation][addon-doc-sqlite-web]

### &#10003; [SSH & Web Terminal][addon-ssh]

![Latest Version][ssh-version-shield]
![Supports armhf Architecture][ssh-armhf-shield]
![Supports armv7 Architecture][ssh-armv7-shield]
![Supports aarch64 Architecture][ssh-aarch64-shield]
![Supports amd64 Architecture][ssh-amd64-shield]
![Supports i386 Architecture][ssh-i386-shield]

SSH & Web Terminal access to your Home Assistant instance

[:books: SSH & Web Terminal add-on documentation][addon-doc-ssh]

### &#10003; [Spotify Connect][addon-spotify]

![Latest Version][spotify-version-shield]
![Supports armhf Architecture][spotify-armhf-shield]
![Supports armv7 Architecture][spotify-armv7-shield]
![Supports aarch64 Architecture][spotify-aarch64-shield]
![Supports amd64 Architecture][spotify-amd64-shield]
![Supports i386 Architecture][spotify-i386-shield]

Play Spotify music on your Home Assistant device

[:books: Spotify Connect add-on documentation][addon-doc-spotify]

### &#10003; [TasmoAdmin][addon-tasmoadmin]

![Latest Version][tasmoadmin-version-shield]
![Supports armhf Architecture][tasmoadmin-armhf-shield]
![Supports armv7 Architecture][tasmoadmin-armv7-shield]
![Supports aarch64 Architecture][tasmoadmin-aarch64-shield]
![Supports amd64 Architecture][tasmoadmin-amd64-shield]
![Supports i386 Architecture][tasmoadmin-i386-shield]

Centrally manage all your Sonoff-Tasmota devices

[:books: TasmoAdmin add-on documentation][addon-doc-tasmoadmin]

### &#10003; [Tautulli][addon-tautulli]

![Latest Version][tautulli-version-shield]
![Supports armhf Architecture][tautulli-armhf-shield]
![Supports armv7 Architecture][tautulli-armv7-shield]
![Supports aarch64 Architecture][tautulli-aarch64-shield]
![Supports amd64 Architecture][tautulli-amd64-shield]
![Supports i386 Architecture][tautulli-i386-shield]

Monitoring and tracking tool for Plex Media Server

[:books: Tautulli add-on documentation][addon-doc-tautulli]

### &#10003; [The Lounge][addon-thelounge]

![Latest Version][thelounge-version-shield]
![Supports armhf Architecture][thelounge-armhf-shield]
![Supports armv7 Architecture][thelounge-armv7-shield]
![Supports aarch64 Architecture][thelounge-aarch64-shield]
![Supports amd64 Architecture][thelounge-amd64-shield]
![Supports i386 Architecture][thelounge-i386-shield]

A self-hosted web IRC client

[:books: The Lounge add-on documentation][addon-doc-thelounge]

### &#10003; [Tor][addon-tor]

![Latest Version][tor-version-shield]
![Supports armhf Architecture][tor-armhf-shield]
![Supports armv7 Architecture][tor-armv7-shield]
![Supports aarch64 Architecture][tor-aarch64-shield]
![Supports amd64 Architecture][tor-amd64-shield]
![Supports i386 Architecture][tor-i386-shield]

Protect your privacy and access Home Assistant via Tor.

[:books: Tor add-on documentation][addon-doc-tor]

### &#10003; [Traccar][addon-traccar]

![Latest Version][traccar-version-shield]
![Supports armhf Architecture][traccar-armhf-shield]
![Supports armv7 Architecture][traccar-armv7-shield]
![Supports aarch64 Architecture][traccar-aarch64-shield]
![Supports amd64 Architecture][traccar-amd64-shield]
![Supports i386 Architecture][traccar-i386-shield]

Modern GPS Tracking Platform

[:books: Traccar add-on documentation][addon-doc-traccar]

### &#10003; [UniFi Controller][addon-unifi]

![Latest Version][unifi-version-shield]
![Supports armhf Architecture][unifi-armhf-shield]
![Supports armv7 Architecture][unifi-armv7-shield]
![Supports aarch64 Architecture][unifi-aarch64-shield]
![Supports amd64 Architecture][unifi-amd64-shield]
![Supports i386 Architecture][unifi-i386-shield]

Manage your UniFi network using a web browser

[:books: UniFi Controller add-on documentation][addon-doc-unifi]

### &#10003; [Visual Studio Code][addon-vscode]

![Latest Version][vscode-version-shield]
![Supports armhf Architecture][vscode-armhf-shield]
![Supports armv7 Architecture][vscode-armv7-shield]
![Supports aarch64 Architecture][vscode-aarch64-shield]
![Supports amd64 Architecture][vscode-amd64-shield]
![Supports i386 Architecture][vscode-i386-shield]

Fully featured VSCode experience, to edit your HA config in the browser, including auto-completion!

[:books: Visual Studio Code add-on documentation][addon-doc-vscode]

### &#10003; [WireGuard][addon-wireguard]

![Latest Version][wireguard-version-shield]
![Supports armhf Architecture][wireguard-armhf-shield]
![Supports armv7 Architecture][wireguard-armv7-shield]
![Supports aarch64 Architecture][wireguard-aarch64-shield]
![Supports amd64 Architecture][wireguard-amd64-shield]
![Supports i386 Architecture][wireguard-i386-shield]

Fast, modern, secure VPN tunnel

[:books: WireGuard add-on documentation][addon-doc-wireguard]

### &#10003; [Z-Wave JS to MQTT][addon-zwavejs2mqtt]

![Latest Version][zwavejs2mqtt-version-shield]
![Supports armhf Architecture][zwavejs2mqtt-armhf-shield]
![Supports armv7 Architecture][zwavejs2mqtt-armv7-shield]
![Supports aarch64 Architecture][zwavejs2mqtt-aarch64-shield]
![Supports amd64 Architecture][zwavejs2mqtt-amd64-shield]
![Supports i386 Architecture][zwavejs2mqtt-i386-shield]

Fully configurable Z-Wave JS to MQTT gateway and control panel

[:books: Z-Wave JS to MQTT add-on documentation][addon-doc-zwavejs2mqtt]

### &#10003; [ZeroTier One][addon-zerotier]

![Latest Version][zerotier-version-shield]
![Supports armhf Architecture][zerotier-armhf-shield]
![Supports armv7 Architecture][zerotier-armv7-shield]
![Supports aarch64 Architecture][zerotier-aarch64-shield]
![Supports amd64 Architecture][zerotier-amd64-shield]
![Supports i386 Architecture][zerotier-i386-shield]

Radically simplify your network with a virtual networking layer that works the same everywhere

[:books: ZeroTier One add-on documentation][addon-doc-zerotier]

### &#10003; [chrony][addon-chrony]

![Latest Version][chrony-version-shield]
![Supports armhf Architecture][chrony-armhf-shield]
![Supports armv7 Architecture][chrony-armv7-shield]
![Supports aarch64 Architecture][chrony-aarch64-shield]
![Supports amd64 Architecture][chrony-amd64-shield]
![Supports i386 Architecture][chrony-i386-shield]

A local NTP (Network Time Protocol) server for cameras etc.

[:books: chrony add-on documentation][addon-doc-chrony]

### &#10003; [motionEye][addon-motioneye]

![Latest Version][motioneye-version-shield]
![Supports armhf Architecture][motioneye-armhf-shield]
![Supports armv7 Architecture][motioneye-armv7-shield]
![Supports aarch64 Architecture][motioneye-aarch64-shield]
![Supports amd64 Architecture][motioneye-amd64-shield]
![Supports i386 Architecture][motioneye-i386-shield]

Simple, elegant and feature-rich CCTV/NVR for your cameras

[:books: motionEye add-on documentation][addon-doc-motioneye]

### &#10003; [phpMyAdmin][addon-phpmyadmin]

![Latest Version][phpmyadmin-version-shield]
![Supports armhf Architecture][phpmyadmin-armhf-shield]
![Supports armv7 Architecture][phpmyadmin-armv7-shield]
![Supports aarch64 Architecture][phpmyadmin-aarch64-shield]
![Supports amd64 Architecture][phpmyadmin-amd64-shield]
![Supports i386 Architecture][phpmyadmin-i386-shield]

A web interface for the official MariaDB add-on

[:books: phpMyAdmin add-on documentation][addon-doc-phpmyadmin]

## Releases

Releases are based on [Semantic Versioning][semver], and use the format
of ``MAJOR.MINOR.PATCH``. In a nutshell, the version will be incremented
based on the following:

- ``MAJOR``: Incompatible or major changes.
- ``MINOR``: Backwards-compatible new features and enhancements.
- ``PATCH``: Backwards-compatible bugfixes and package updates.

## Support

Got questions?

You have several options to get them answered:

- The Home Assistant Community Add-ons [Discord Chat Server][discord]
- The Home Assistant [Community Forum][forum].
- The Home Assistant [Discord Chat Server][discord-ha].
- Join the [Reddit subreddit][reddit] in [/r/homeassistant][reddit]

You could also open an issue here on GitHub. Note, we use a separate
GitHub repository for each add-on. Please ensure you are creating the issue
on the correct GitHub repository matching the add-on.

- [Open an issue for the add-on: AdGuard Home][adguard-issue]
- [Open an issue for the add-on: AirCast][aircast-issue]
- [Open an issue for the add-on: AirSonos][airsonos-issue]
- [Open an issue for the add-on: AppDaemon 4][appdaemon-issue]
- [Open an issue for the add-on: Bitwarden (Vaultwarden)][bitwarden-issue]
- [Open an issue for the add-on: Bookstack][bookstack-issue]
- [Open an issue for the add-on: ESPHome][esphome-issue]
- [Open an issue for the add-on: Example][example-issue]
- [Open an issue for the add-on: FTP][ftp-issue]
- [Open an issue for the add-on: Folding@home][foldingathome-issue]
- [Open an issue for the add-on: Glances][glances-issue]
- [Open an issue for the add-on: Grafana][grafana-issue]
- [Open an issue for the add-on: Grocy][grocy-issue]
- [Open an issue for the add-on: Home Panel][home-panel-issue]
- [Open an issue for the add-on: InfluxDB][influxdb-issue]
- [Open an issue for the add-on: JupyterLab][jupyterlab-issue]
- [Open an issue for the add-on: Log Viewer][log-viewer-issue]
- [Open an issue for the add-on: MQTT Server & Web client][mqtt-issue]
- [Open an issue for the add-on: Matrix][matrix-issue]
- [Open an issue for the add-on: Network UPS Tools][nut-issue]
- [Open an issue for the add-on: Nginx Proxy Manager][nginxproxymanager-issue]
- [Open an issue for the add-on: Node-RED][node-red-issue]
- [Open an issue for the add-on: Plex Media Server][plex-issue]
- [Open an issue for the add-on: Portainer][portainer-issue]
- [Open an issue for the add-on: SQLite Web][sqlite-web-issue]
- [Open an issue for the add-on: SSH & Web Terminal][ssh-issue]
- [Open an issue for the add-on: Spotify Connect][spotify-issue]
- [Open an issue for the add-on: TasmoAdmin][tasmoadmin-issue]
- [Open an issue for the add-on: Tautulli][tautulli-issue]
- [Open an issue for the add-on: The Lounge][thelounge-issue]
- [Open an issue for the add-on: Tor][tor-issue]
- [Open an issue for the add-on: Traccar][traccar-issue]
- [Open an issue for the add-on: UniFi Controller][unifi-issue]
- [Open an issue for the add-on: Visual Studio Code][vscode-issue]
- [Open an issue for the add-on: WireGuard][wireguard-issue]
- [Open an issue for the add-on: Z-Wave JS to MQTT][zwavejs2mqtt-issue]
- [Open an issue for the add-on: ZeroTier One][zerotier-issue]
- [Open an issue for the add-on: chrony][chrony-issue]
- [Open an issue for the add-on: motionEye][motioneye-issue]
- [Open an issue for the add-on: phpMyAdmin][phpmyadmin-issue]

For a general repository issue or add-on ideas [open an issue here][issue]

## Contributing

This is an active open-source project. We are always open to people who want to
use the code or contribute to it.

We have set up a separate document containing our
[contribution guidelines](CONTRIBUTING.md).

Thank you for being involved! :heart_eyes:

## Adding a new add-on

We are currently not accepting third party add-ons to this repository.

For questions, please contact [Franck Nijhof][frenck]:

- Drop him an email: frenck@addons.community
- Chat with him on [Discord Chat][discord]
- Message him via the forums: [frenck][forum-frenck]

## License

MIT License

Copyright (c) 2017-2021 Franck Nijhof

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

[addon-adguard]: https://github.com/hassio-addons/addon-adguard-home/tree/v4.0.0
[addon-doc-adguard]: https://github.com/hassio-addons/addon-adguard-home/blob/v4.0.0/README.md
[adguard-issue]: https://github.com/hassio-addons/addon-adguard-home/issues
[adguard-version-shield]: https://img.shields.io/badge/version-v4.0.0-blue.svg
[adguard-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[adguard-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[adguard-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[adguard-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[adguard-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-aircast]: https://github.com/hassio-addons/addon-aircast/tree/v3.1.0
[addon-doc-aircast]: https://github.com/hassio-addons/addon-aircast/blob/v3.1.0/README.md
[aircast-issue]: https://github.com/hassio-addons/addon-aircast/issues
[aircast-version-shield]: https://img.shields.io/badge/version-v3.1.0-blue.svg
[aircast-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[aircast-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[aircast-armhf-shield]: https://img.shields.io/badge/armhf-no-red.svg
[aircast-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[aircast-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-airsonos]: https://github.com/hassio-addons/addon-airsonos/tree/v3.1.0
[addon-doc-airsonos]: https://github.com/hassio-addons/addon-airsonos/blob/v3.1.0/README.md
[airsonos-issue]: https://github.com/hassio-addons/addon-airsonos/issues
[airsonos-version-shield]: https://img.shields.io/badge/version-v3.1.0-blue.svg
[airsonos-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[airsonos-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[airsonos-armhf-shield]: https://img.shields.io/badge/armhf-no-red.svg
[airsonos-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[airsonos-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-appdaemon]: https://github.com/hassio-addons/addon-appdaemon/tree/v0.6.0
[addon-doc-appdaemon]: https://github.com/hassio-addons/addon-appdaemon/blob/v0.6.0/README.md
[appdaemon-issue]: https://github.com/hassio-addons/addon-appdaemon/issues
[appdaemon-version-shield]: https://img.shields.io/badge/version-v0.6.0-blue.svg
[appdaemon-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[appdaemon-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[appdaemon-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[appdaemon-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[appdaemon-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-bitwarden]: https://github.com/hassio-addons/addon-bitwarden/tree/v0.11.0
[addon-doc-bitwarden]: https://github.com/hassio-addons/addon-bitwarden/blob/v0.11.0/README.md
[bitwarden-issue]: https://github.com/hassio-addons/addon-bitwarden/issues
[bitwarden-version-shield]: https://img.shields.io/badge/version-v0.11.0-blue.svg
[bitwarden-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[bitwarden-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[bitwarden-armhf-shield]: https://img.shields.io/badge/armhf-no-red.svg
[bitwarden-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[bitwarden-i386-shield]: https://img.shields.io/badge/i386-no-red.svg
[addon-bookstack]: https://github.com/hassio-addons/addon-bookstack/tree/v0.10.0
[addon-doc-bookstack]: https://github.com/hassio-addons/addon-bookstack/blob/v0.10.0/README.md
[bookstack-issue]: https://github.com/hassio-addons/addon-bookstack/issues
[bookstack-version-shield]: https://img.shields.io/badge/version-v0.10.0-blue.svg
[bookstack-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[bookstack-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[bookstack-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[bookstack-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[bookstack-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-esphome]: https://github.com/esphome/hassio/tree/v1.17.2
[addon-doc-esphome]: https://github.com/esphome/hassio/blob/v1.17.2/README.md
[esphome-issue]: https://github.com/esphome/hassio/issues
[esphome-version-shield]: https://img.shields.io/badge/version-v1.17.2-blue.svg
[esphome-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[esphome-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[esphome-armhf-shield]: https://img.shields.io/badge/armhf-no-red.svg
[esphome-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[esphome-i386-shield]: https://img.shields.io/badge/i386-no-red.svg
[addon-example]: https://github.com/hassio-addons/addon-example/tree/v4.1.0
[addon-doc-example]: https://github.com/hassio-addons/addon-example/blob/v4.1.0/README.md
[example-issue]: https://github.com/hassio-addons/addon-example/issues
[example-version-shield]: https://img.shields.io/badge/version-v4.1.0-blue.svg
[example-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[example-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[example-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[example-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[example-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-ftp]: https://github.com/hassio-addons/addon-ftp/tree/v4.1.0
[addon-doc-ftp]: https://github.com/hassio-addons/addon-ftp/blob/v4.1.0/README.md
[ftp-issue]: https://github.com/hassio-addons/addon-ftp/issues
[ftp-version-shield]: https://img.shields.io/badge/version-v4.1.0-blue.svg
[ftp-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[ftp-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[ftp-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[ftp-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[ftp-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-foldingathome]: https://github.com/hassio-addons/addon-foldingathome/tree/v0.2.0
[addon-doc-foldingathome]: https://github.com/hassio-addons/addon-foldingathome/blob/v0.2.0/README.md
[foldingathome-issue]: https://github.com/hassio-addons/addon-foldingathome/issues
[foldingathome-version-shield]: https://img.shields.io/badge/version-v0.2.0-blue.svg
[foldingathome-aarch64-shield]: https://img.shields.io/badge/aarch64-no-red.svg
[foldingathome-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[foldingathome-armhf-shield]: https://img.shields.io/badge/armhf-no-red.svg
[foldingathome-armv7-shield]: https://img.shields.io/badge/armv7-no-red.svg
[foldingathome-i386-shield]: https://img.shields.io/badge/i386-no-red.svg
[addon-glances]: https://github.com/hassio-addons/addon-glances/tree/v0.12.1
[addon-doc-glances]: https://github.com/hassio-addons/addon-glances/blob/v0.12.1/README.md
[glances-issue]: https://github.com/hassio-addons/addon-glances/issues
[glances-version-shield]: https://img.shields.io/badge/version-v0.12.1-blue.svg
[glances-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[glances-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[glances-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[glances-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[glances-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-grafana]: https://github.com/hassio-addons/addon-grafana/tree/v6.3.3
[addon-doc-grafana]: https://github.com/hassio-addons/addon-grafana/blob/v6.3.3/README.md
[grafana-issue]: https://github.com/hassio-addons/addon-grafana/issues
[grafana-version-shield]: https://img.shields.io/badge/version-v6.3.3-blue.svg
[grafana-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[grafana-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[grafana-armhf-shield]: https://img.shields.io/badge/armhf-no-red.svg
[grafana-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[grafana-i386-shield]: https://img.shields.io/badge/i386-no-red.svg
[addon-grocy]: https://github.com/hassio-addons/addon-grocy/tree/v0.13.0
[addon-doc-grocy]: https://github.com/hassio-addons/addon-grocy/blob/v0.13.0/README.md
[grocy-issue]: https://github.com/hassio-addons/addon-grocy/issues
[grocy-version-shield]: https://img.shields.io/badge/version-v0.13.0-blue.svg
[grocy-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[grocy-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[grocy-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[grocy-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[grocy-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-home-panel]: https://github.com/hassio-addons/addon-home-panel/tree/v2.2.0
[addon-doc-home-panel]: https://github.com/hassio-addons/addon-home-panel/blob/v2.2.0/README.md
[home-panel-issue]: https://github.com/hassio-addons/addon-home-panel/issues
[home-panel-version-shield]: https://img.shields.io/badge/version-v2.2.0-blue.svg
[home-panel-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[home-panel-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[home-panel-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[home-panel-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[home-panel-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-influxdb]: https://github.com/hassio-addons/addon-influxdb/tree/v4.0.6
[addon-doc-influxdb]: https://github.com/hassio-addons/addon-influxdb/blob/v4.0.6/README.md
[influxdb-issue]: https://github.com/hassio-addons/addon-influxdb/issues
[influxdb-version-shield]: https://img.shields.io/badge/version-v4.0.6-blue.svg
[influxdb-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[influxdb-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[influxdb-armhf-shield]: https://img.shields.io/badge/armhf-no-red.svg
[influxdb-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[influxdb-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-jupyterlab]: https://github.com/hassio-addons/addon-jupyterlab/tree/v0.5.0
[addon-doc-jupyterlab]: https://github.com/hassio-addons/addon-jupyterlab/blob/v0.5.0/README.md
[jupyterlab-issue]: https://github.com/hassio-addons/addon-jupyterlab/issues
[jupyterlab-version-shield]: https://img.shields.io/badge/version-v0.5.0-blue.svg
[jupyterlab-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[jupyterlab-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[jupyterlab-armhf-shield]: https://img.shields.io/badge/armhf-no-red.svg
[jupyterlab-armv7-shield]: https://img.shields.io/badge/armv7-no-red.svg
[jupyterlab-i386-shield]: https://img.shields.io/badge/i386-no-red.svg
[addon-log-viewer]: https://github.com/hassio-addons/addon-log-viewer/tree/v0.11.0
[addon-doc-log-viewer]: https://github.com/hassio-addons/addon-log-viewer/blob/v0.11.0/README.md
[log-viewer-issue]: https://github.com/hassio-addons/addon-log-viewer/issues
[log-viewer-version-shield]: https://img.shields.io/badge/version-v0.11.0-blue.svg
[log-viewer-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[log-viewer-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[log-viewer-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[log-viewer-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[log-viewer-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-mqtt]: https://github.com/hassio-addons/addon-mqtt/tree/v1.2.0
[addon-doc-mqtt]: https://github.com/hassio-addons/addon-mqtt/blob/v1.2.0/README.md
[mqtt-issue]: https://github.com/hassio-addons/addon-mqtt/issues
[mqtt-version-shield]: https://img.shields.io/badge/version-v1.2.0-blue.svg
[mqtt-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[mqtt-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[mqtt-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[mqtt-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[mqtt-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-matrix]: https://github.com/hassio-addons/addon-matrix/tree/v0.10.0
[addon-doc-matrix]: https://github.com/hassio-addons/addon-matrix/blob/v0.10.0/README.md
[matrix-issue]: https://github.com/hassio-addons/addon-matrix/issues
[matrix-version-shield]: https://img.shields.io/badge/version-v0.10.0-blue.svg
[matrix-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[matrix-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[matrix-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[matrix-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[matrix-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-nut]: https://github.com/hassio-addons/addon-nut/tree/v0.7.0
[addon-doc-nut]: https://github.com/hassio-addons/addon-nut/blob/v0.7.0/README.md
[nut-issue]: https://github.com/hassio-addons/addon-nut/issues
[nut-version-shield]: https://img.shields.io/badge/version-v0.7.0-blue.svg
[nut-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[nut-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[nut-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[nut-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[nut-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-nginxproxymanager]: https://github.com/hassio-addons/addon-nginx-proxy-manager/tree/v0.11.0
[addon-doc-nginxproxymanager]: https://github.com/hassio-addons/addon-nginx-proxy-manager/blob/v0.11.0/README.md
[nginxproxymanager-issue]: https://github.com/hassio-addons/addon-nginx-proxy-manager/issues
[nginxproxymanager-version-shield]: https://img.shields.io/badge/version-v0.11.0-blue.svg
[nginxproxymanager-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[nginxproxymanager-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[nginxproxymanager-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[nginxproxymanager-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[nginxproxymanager-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-node-red]: https://github.com/hassio-addons/addon-node-red/tree/v9.0.1
[addon-doc-node-red]: https://github.com/hassio-addons/addon-node-red/blob/v9.0.1/README.md
[node-red-issue]: https://github.com/hassio-addons/addon-node-red/issues
[node-red-version-shield]: https://img.shields.io/badge/version-v9.0.1-blue.svg
[node-red-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[node-red-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[node-red-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[node-red-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[node-red-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-plex]: https://github.com/hassio-addons/addon-plex/tree/v2.6.0
[addon-doc-plex]: https://github.com/hassio-addons/addon-plex/blob/v2.6.0/README.md
[plex-issue]: https://github.com/hassio-addons/addon-plex/issues
[plex-version-shield]: https://img.shields.io/badge/version-v2.6.0-blue.svg
[plex-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[plex-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[plex-armhf-shield]: https://img.shields.io/badge/armhf-no-red.svg
[plex-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[plex-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-portainer]: https://github.com/hassio-addons/addon-portainer/tree/v1.5.0
[addon-doc-portainer]: https://github.com/hassio-addons/addon-portainer/blob/v1.5.0/README.md
[portainer-issue]: https://github.com/hassio-addons/addon-portainer/issues
[portainer-version-shield]: https://img.shields.io/badge/version-v1.5.0-blue.svg
[portainer-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[portainer-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[portainer-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[portainer-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[portainer-i386-shield]: https://img.shields.io/badge/i386-no-red.svg
[addon-sqlite-web]: https://github.com/hassio-addons/addon-sqlite-web/tree/v3.1.0
[addon-doc-sqlite-web]: https://github.com/hassio-addons/addon-sqlite-web/blob/v3.1.0/README.md
[sqlite-web-issue]: https://github.com/hassio-addons/addon-sqlite-web/issues
[sqlite-web-version-shield]: https://img.shields.io/badge/version-v3.1.0-blue.svg
[sqlite-web-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[sqlite-web-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[sqlite-web-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[sqlite-web-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[sqlite-web-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-ssh]: https://github.com/hassio-addons/addon-ssh/tree/v8.2.1
[addon-doc-ssh]: https://github.com/hassio-addons/addon-ssh/blob/v8.2.1/README.md
[ssh-issue]: https://github.com/hassio-addons/addon-ssh/issues
[ssh-version-shield]: https://img.shields.io/badge/version-v8.2.1-blue.svg
[ssh-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[ssh-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[ssh-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[ssh-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[ssh-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-spotify]: https://github.com/hassio-addons/addon-spotify-connect/tree/v0.8.2
[addon-doc-spotify]: https://github.com/hassio-addons/addon-spotify-connect/blob/v0.8.2/README.md
[spotify-issue]: https://github.com/hassio-addons/addon-spotify-connect/issues
[spotify-version-shield]: https://img.shields.io/badge/version-v0.8.2-blue.svg
[spotify-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[spotify-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[spotify-armhf-shield]: https://img.shields.io/badge/armhf-no-red.svg
[spotify-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[spotify-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-tasmoadmin]: https://github.com/hassio-addons/addon-tasmoadmin/tree/v0.15.0
[addon-doc-tasmoadmin]: https://github.com/hassio-addons/addon-tasmoadmin/blob/v0.15.0/README.md
[tasmoadmin-issue]: https://github.com/hassio-addons/addon-tasmoadmin/issues
[tasmoadmin-version-shield]: https://img.shields.io/badge/version-v0.15.0-blue.svg
[tasmoadmin-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[tasmoadmin-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[tasmoadmin-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[tasmoadmin-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[tasmoadmin-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-tautulli]: https://github.com/hassio-addons/addon-tautulli/tree/v2.2.2
[addon-doc-tautulli]: https://github.com/hassio-addons/addon-tautulli/blob/v2.2.2/README.md
[tautulli-issue]: https://github.com/hassio-addons/addon-tautulli/issues
[tautulli-version-shield]: https://img.shields.io/badge/version-v2.2.2-blue.svg
[tautulli-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[tautulli-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[tautulli-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[tautulli-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[tautulli-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-thelounge]: https://github.com/hassio-addons/addon-thelounge/tree/v0.12.0
[addon-doc-thelounge]: https://github.com/hassio-addons/addon-thelounge/blob/v0.12.0/README.md
[thelounge-issue]: https://github.com/hassio-addons/addon-thelounge/issues
[thelounge-version-shield]: https://img.shields.io/badge/version-v0.12.0-blue.svg
[thelounge-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[thelounge-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[thelounge-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[thelounge-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[thelounge-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-tor]: https://github.com/hassio-addons/addon-tor/tree/v3.0.4
[addon-doc-tor]: https://github.com/hassio-addons/addon-tor/blob/v3.0.4/README.md
[tor-issue]: https://github.com/hassio-addons/addon-tor/issues
[tor-version-shield]: https://img.shields.io/badge/version-v3.0.4-blue.svg
[tor-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[tor-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[tor-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[tor-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[tor-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-traccar]: https://github.com/hassio-addons/addon-traccar/tree/v0.12.0
[addon-doc-traccar]: https://github.com/hassio-addons/addon-traccar/blob/v0.12.0/README.md
[traccar-issue]: https://github.com/hassio-addons/addon-traccar/issues
[traccar-version-shield]: https://img.shields.io/badge/version-v0.12.0-blue.svg
[traccar-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[traccar-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[traccar-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[traccar-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[traccar-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-unifi]: https://github.com/hassio-addons/addon-unifi/tree/v0.22.0
[addon-doc-unifi]: https://github.com/hassio-addons/addon-unifi/blob/v0.22.0/README.md
[unifi-issue]: https://github.com/hassio-addons/addon-unifi/issues
[unifi-version-shield]: https://img.shields.io/badge/version-v0.22.0-blue.svg
[unifi-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[unifi-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[unifi-armhf-shield]: https://img.shields.io/badge/armhf-no-red.svg
[unifi-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[unifi-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-vscode]: https://github.com/hassio-addons/addon-vscode/tree/v3.3.1
[addon-doc-vscode]: https://github.com/hassio-addons/addon-vscode/blob/v3.3.1/README.md
[vscode-issue]: https://github.com/hassio-addons/addon-vscode/issues
[vscode-version-shield]: https://img.shields.io/badge/version-v3.3.1-blue.svg
[vscode-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[vscode-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[vscode-armhf-shield]: https://img.shields.io/badge/armhf-no-red.svg
[vscode-armv7-shield]: https://img.shields.io/badge/armv7-no-red.svg
[vscode-i386-shield]: https://img.shields.io/badge/i386-no-red.svg
[addon-wireguard]: https://github.com/hassio-addons/addon-wireguard/tree/v0.5.1
[addon-doc-wireguard]: https://github.com/hassio-addons/addon-wireguard/blob/v0.5.1/README.md
[wireguard-issue]: https://github.com/hassio-addons/addon-wireguard/issues
[wireguard-version-shield]: https://img.shields.io/badge/version-v0.5.1-blue.svg
[wireguard-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[wireguard-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[wireguard-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[wireguard-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[wireguard-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-zwavejs2mqtt]: https://github.com/hassio-addons/addon-zwavejs2mqtt/tree/v0.14.1
[addon-doc-zwavejs2mqtt]: https://github.com/hassio-addons/addon-zwavejs2mqtt/blob/v0.14.1/README.md
[zwavejs2mqtt-issue]: https://github.com/hassio-addons/addon-zwavejs2mqtt/issues
[zwavejs2mqtt-version-shield]: https://img.shields.io/badge/version-v0.14.1-blue.svg
[zwavejs2mqtt-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[zwavejs2mqtt-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[zwavejs2mqtt-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[zwavejs2mqtt-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[zwavejs2mqtt-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-zerotier]: https://github.com/hassio-addons/addon-zerotier/tree/v0.11.0
[addon-doc-zerotier]: https://github.com/hassio-addons/addon-zerotier/blob/v0.11.0/README.md
[zerotier-issue]: https://github.com/hassio-addons/addon-zerotier/issues
[zerotier-version-shield]: https://img.shields.io/badge/version-v0.11.0-blue.svg
[zerotier-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[zerotier-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[zerotier-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[zerotier-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[zerotier-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-chrony]: https://github.com/hassio-addons/addon-chrony/tree/v2.1.0
[addon-doc-chrony]: https://github.com/hassio-addons/addon-chrony/blob/v2.1.0/README.md
[chrony-issue]: https://github.com/hassio-addons/addon-chrony/issues
[chrony-version-shield]: https://img.shields.io/badge/version-v2.1.0-blue.svg
[chrony-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[chrony-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[chrony-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[chrony-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[chrony-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-motioneye]: https://github.com/hassio-addons/addon-motioneye/tree/v0.12.0
[addon-doc-motioneye]: https://github.com/hassio-addons/addon-motioneye/blob/v0.12.0/README.md
[motioneye-issue]: https://github.com/hassio-addons/addon-motioneye/issues
[motioneye-version-shield]: https://img.shields.io/badge/version-v0.12.0-blue.svg
[motioneye-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[motioneye-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[motioneye-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[motioneye-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[motioneye-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[addon-phpmyadmin]: https://github.com/hassio-addons/addon-phpmyadmin/tree/v0.4.0
[addon-doc-phpmyadmin]: https://github.com/hassio-addons/addon-phpmyadmin/blob/v0.4.0/README.md
[phpmyadmin-issue]: https://github.com/hassio-addons/addon-phpmyadmin/issues
[phpmyadmin-version-shield]: https://img.shields.io/badge/version-v0.4.0-blue.svg
[phpmyadmin-aarch64-shield]: https://img.shields.io/badge/aarch64-yes-green.svg
[phpmyadmin-amd64-shield]: https://img.shields.io/badge/amd64-yes-green.svg
[phpmyadmin-armhf-shield]: https://img.shields.io/badge/armhf-yes-green.svg
[phpmyadmin-armv7-shield]: https://img.shields.io/badge/armv7-yes-green.svg
[phpmyadmin-i386-shield]: https://img.shields.io/badge/i386-yes-green.svg
[awesome-shield]: https://img.shields.io/badge/awesome%3F-yes-brightgreen.svg
[awesome]: https://awesome-ha.com
[discord-ha]: https://discord.gg/c5DvZ4e
[discord-shield]: https://img.shields.io/discord/478094546522079232.svg
[discord]: https://discord.me/hassioaddons
[forum-frenck]: https://community.home-assistant.io/u/frenck/?u=frenck
[forum-shield]: https://img.shields.io/badge/community-forum-brightgreen.svg
[forum]: https://community.home-assistant.io?u=frenck
[frenck]: https://github.com/frenck
[gitlabci-shield]: https://gitlab.com/hassio-addons/repository/badges/master/pipeline.svg
[gitlabci]: https://gitlab.com/hassio-addons/repository/pipelines
[issue]: https://github.com/hassio-addons/repository/issues
[license-shield]: https://img.shields.io/github/license/hassio-addons/repository.svg
[maintenance-shield]: https://img.shields.io/maintenance/yes/2021.svg
[project-stage-shield]: https://img.shields.io/badge/project%20stage-production%20ready-brightgreen.svg
[reddit]: https://reddit.com/r/homeassistant
[semver]: http://semver.org/spec/v2.0.0.html"
182,sonatype/nexus-public,Java,"<!--

    Sonatype Nexus (TM) Open Source Version
    Copyright (c) 2008-present Sonatype, Inc.
    All rights reserved. Includes the third-party code listed at http://links.sonatype.com/products/nexus/oss/attributions.

    This program and the accompanying materials are made available under the terms of the Eclipse Public License Version 1.0,
    which accompanies this distribution and is available at http://www.eclipse.org/legal/epl-v10.html.

    Sonatype Nexus (TM) Professional Version is available from Sonatype, Inc. ""Sonatype"" and ""Sonatype Nexus"" are trademarks
    of Sonatype, Inc. Apache Maven is a trademark of the Apache Software Foundation. M2eclipse is a trademark of the
    Eclipse Foundation. All other trademarks are the property of their respective owners.

-->
# Sonatype Nexus Repository Open Source Codebase 

[![CircleCI Build Status](https://circleci.com/gh/sonatype/nexus-public.svg?style=shield ""CircleCI Build Status"")](https://circleci.com/gh/sonatype/nexus-public) [![Join the chat at https://gitter.im/sonatype/nexus-developers](https://badges.gitter.im/sonatype/nexus-developers.svg)](https://gitter.im/sonatype/nexus-developers?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

## Downloadable Bundles

 See: https://www.sonatype.com/download-oss-sonatype
 
## Build Requirements

Builds use Apache Maven and require Java 8. Apache Maven wrapper scripts are included in the source tree.

All release versioned dependencies should be available from the [Central](https://repo1.maven.org/maven2/) repository.

For SNAPSHOT sources, SNAPSHOT versioned dependencies may only be available from https://repository.sonatype.org/content/groups/sonatype-public-grid repository.

### Configuring Maven for SNAPSHOT Dependencies

Following best practices, the nexus-public POM does not include any root `<repositories>` elements.
    
Instead you are advised to [configure Apache Maven to point at single repository mirror URL](https://maven.apache.org/guides/mini/guide-mirror-settings.html#using-a-single-repository) that is a group repository containing both Central proxy repository with Release version policy and sonatype-public-grid with a SNAPSHOT version policy. You can use a [repository manager](https://www.sonatype.org/nexus/go/) to set up a group repository that contains both of these remotes.

Alternately, [add a custom profile to a settings.xml](https://maven.apache.org/guides/mini/guide-multiple-repositories.html) for repository manager development that includes both repositories.

## Building From Source

Released versions are tagged and branched using a name of the form `release-{version}`. For example: `release-3.29.2-02`

To build a tagged release, first fetch all tags:

```shell
git fetch --tags
```

Then checkout the remote branch you want. For example:

```shell
git checkout -b release-3.29.2-02 origin/release-3.29.2-02 --
```

Then build using the included Maven wrapper script. For example:

```shell
./mvnw clean install
```

For building SNAPSHOT versions, follow the same process, except your build may require access to [Sonatype Public Grid](https://repository.sonatype.org/content/groups/sonatype-public-grid) to successfully resolve dependencies.

## Running

To run Nexus Repository, after building, unzip the assembly and start the server:

    unzip -d target assemblies/nexus-base-template/target/nexus-base-template-*.zip
    ./target/nexus-base-template-*/bin/nexus console

The `nexus-base-template` assembly is used as the basis for the official Sonatype Nexus distributions.

## License

This project is licensed under the Eclipse Public License - v 1.0, you can read the full text [here](LICENSE.txt)

## Getting help

Looking to contribute to our code but need some help? There's a few ways to get information or our attention:

* File an issue in [our public JIRA](https://issues.sonatype.org/browse/NEXUS)
* Check out the [Nexus3](http://stackoverflow.com/questions/tagged/nexus3) tag on Stack Overflow
* Check out the [Nexus Repository User List](https://groups.google.com/a/glists.sonatype.com/forum/?hl=en#!forum/nexus-users)
* Connect with [@sonatypeDev](https://twitter.com/sonatypeDev) on Twitter
"
183,andersao/l5-repository,PHP,"# Laravel 5 Repositories

Laravel 5 Repositories is used to abstract the data layer, making our application more flexible to maintain.

[![Latest Stable Version](https://poser.pugx.org/prettus/l5-repository/v/stable)](https://packagist.org/packages/prettus/l5-repository) [![Total Downloads](https://poser.pugx.org/prettus/l5-repository/downloads)](https://packagist.org/packages/prettus/l5-repository) [![Latest Unstable Version](https://poser.pugx.org/prettus/l5-repository/v/unstable)](https://packagist.org/packages/prettus/l5-repository) [![License](https://poser.pugx.org/prettus/l5-repository/license)](https://packagist.org/packages/prettus/l5-repository)
[![Analytics](https://ga-beacon.appspot.com/UA-61050740-1/l5-repository/readme)](https://packagist.org/packages/prettus/l5-repository)
[![Code Climate](https://codeclimate.com/github/andersao/l5-repository/badges/gpa.svg)](https://codeclimate.com/github/andersao/l5-repository)

#### See versions: [1.0.*](https://github.com/andersao/l5-repository/tree/1.0.4) / [2.0.*](https://github.com/andersao/l5-repository/tree/2.0.14)
#### Migrate to: [2.0](migration-to-2.0.md) / [2.1](migration-to-2.1.md)

You want to know a little more about the Repository pattern? [Read this great article](http://bit.ly/1IdmRNS).

## Table of Contents

- <a href=""#installation"">Installation</a>
    - <a href=""#composer"">Composer</a>
    - <a href=""#laravel"">Laravel</a>
- <a href=""#methods"">Methods</a>
    - <a href=""#prettusrepositorycontractsrepositoryinterface"">RepositoryInterface</a>
    - <a href=""#prettusrepositorycontractsrepositorycriteriainterface"">RepositoryCriteriaInterface</a>
    - <a href=""#prettusrepositorycontractscacheableinterface"">CacheableInterface</a>
    - <a href=""#prettusrepositorycontractspresenterinterface"">PresenterInterface</a>
    - <a href=""#prettusrepositorycontractscriteriainterface"">CriteriaInterface</a>
- <a href=""#usage"">Usage</a>
	- <a href=""#create-a-model"">Create a Model</a>
	- <a href=""#create-a-repository"">Create a Repository</a>
	- <a href=""#generators"">Generators</a>
	- <a href=""#use-methods"">Use methods</a>
	- <a href=""#create-a-criteria"">Create a Criteria</a>
	- <a href=""#using-the-criteria-in-a-controller"">Using the Criteria in a Controller</a>
	- <a href=""#using-the-requestcriteria"">Using the RequestCriteria</a>
- <a href=""#cache"">Cache</a>
    - <a href=""#cache-usage"">Usage</a>
    - <a href=""#cache-config"">Config</a>
- <a href=""#validators"">Validators</a>
    - <a href=""#using-a-validator-class"">Using a Validator Class</a>
        - <a href=""#create-a-validator"">Create a Validator</a>
        - <a href=""#enabling-validator-in-your-repository-1"">Enabling Validator in your Repository</a>
    - <a href=""#defining-rules-in-the-repository"">Defining rules in the repository</a>
- <a href=""#presenters"">Presenters</a>
    - <a href=""#fractal-presenter"">Fractal Presenter</a>
        - <a href=""#create-a-presenter"">Create a Fractal Presenter</a>
        - <a href=""#implement-interface"">Model Transformable</a>
    - <a href=""#enabling-in-your-repository-1"">Enabling in your Repository</a>

## Installation

### Composer

Execute the following command to get the latest version of the package:

```terminal
composer require prettus/l5-repository
```

### Laravel

#### >= laravel5.5

ServiceProvider will be attached automatically

#### Other

In your `config/app.php` add `Prettus\Repository\Providers\RepositoryServiceProvider::class` to the end of the `providers` array:

```php
'providers' => [
    ...
    Prettus\Repository\Providers\RepositoryServiceProvider::class,
],
```

If Lumen

```php
$app->register(Prettus\Repository\Providers\LumenRepositoryServiceProvider::class);
```

Publish Configuration

```shell
php artisan vendor:publish --provider ""Prettus\Repository\Providers\RepositoryServiceProvider""
```

## Methods

### Prettus\Repository\Contracts\RepositoryInterface

- all($columns = array('*'))
- first($columns = array('*'))
- paginate($limit = null, $columns = ['*'])
- find($id, $columns = ['*'])
- findByField($field, $value, $columns = ['*'])
- findWhere(array $where, $columns = ['*'])
- findWhereIn($field, array $where, $columns = [*])
- findWhereNotIn($field, array $where, $columns = [*])
- findWhereBetween($field, array $where, $columns = [*])
- create(array $attributes)
- update(array $attributes, $id)
- updateOrCreate(array $attributes, array $values = [])
- delete($id)
- deleteWhere(array $where)
- orderBy($column, $direction = 'asc');
- with(array $relations);
- has(string $relation);
- whereHas(string $relation, closure $closure);
- hidden(array $fields);
- visible(array $fields);
- scopeQuery(Closure $scope);
- getFieldsSearchable();
- setPresenter($presenter);
- skipPresenter($status = true);


### Prettus\Repository\Contracts\RepositoryCriteriaInterface

- pushCriteria($criteria)
- popCriteria($criteria)
- getCriteria()
- getByCriteria(CriteriaInterface $criteria)
- skipCriteria($status = true)
- getFieldsSearchable()

### Prettus\Repository\Contracts\CacheableInterface

- setCacheRepository(CacheRepository $repository)
- getCacheRepository()
- getCacheKey($method, $args = null)
- getCacheMinutes()
- skipCache($status = true)

### Prettus\Repository\Contracts\PresenterInterface

- present($data);

### Prettus\Repository\Contracts\Presentable

- setPresenter(PresenterInterface $presenter);
- presenter();

### Prettus\Repository\Contracts\CriteriaInterface

- apply($model, RepositoryInterface $repository);

### Prettus\Repository\Contracts\Transformable

- transform();


## Usage

### Create a Model

Create your model normally, but it is important to define the attributes that can be filled from the input form data.

```php
namespace App;

class Post extends Eloquent { // or Ardent, Or any other Model Class

    protected $fillable = [
        'title',
        'author',
        ...
     ];

     ...
}
```

### Create a Repository

```php
namespace App;

use Prettus\Repository\Eloquent\BaseRepository;

class PostRepository extends BaseRepository {

    /**
     * Specify Model class name
     *
     * @return string
     */
    function model()
    {
        return ""App\\Post"";
    }
}
```

### Generators

Create your repositories easily through the generator.

#### Config

You must first configure the storage location of the repository files. By default is the ""app"" folder and the namespace ""App"". Please note that, values in the `paths` array are acutally used as both *namespace* and file paths. Relax though, both foreward and backward slashes are taken care of during generation.

```php
    ...
    'generator'=>[
        'basePath'=>app()->path(),
        'rootNamespace'=>'App\\',
        'paths'=>[
            'models'       => 'Entities',
            'repositories' => 'Repositories',
            'interfaces'   => 'Repositories',
            'transformers' => 'Transformers',
            'presenters'   => 'Presenters',
            'validators'   => 'Validators',
            'controllers'  => 'Http/Controllers',
            'provider'     => 'RepositoryServiceProvider',
            'criteria'     => 'Criteria',
        ]
    ]
```

You may want to save the root of your project folder out of the app and add another namespace, for example

```php
    ...
     'generator'=>[
        'basePath'      => base_path('src/Lorem'),
        'rootNamespace' => 'Lorem\\'
    ]
```

Additionally, you may wish to customize where your generated classes end up being saved.  That can be accomplished by editing the `paths` node to your liking.  For example:

```php
    'generator'=>[
        'basePath'=>app()->path(),
        'rootNamespace'=>'App\\',
        'paths'=>[
            'models'=>'Models',
            'repositories'=>'Repositories\\Eloquent',
            'interfaces'=>'Contracts\\Repositories',
            'transformers'=>'Transformers',
            'presenters'=>'Presenters'
            'validators'   => 'Validators',
            'controllers'  => 'Http/Controllers',
            'provider'     => 'RepositoryServiceProvider',
            'criteria'     => 'Criteria',
        ]
    ]
```

#### Commands

To generate everything you need for your Model, run this command:

```terminal
php artisan make:entity Post
```

This will create the Controller, the Validator, the Model, the Repository, the Presenter and the Transformer classes.
It will also create a new service provider that will be used to bind the Eloquent Repository with its corresponding Repository Interface.
To load it, just add this to your AppServiceProvider@register method:

```php
    $this->app->register(RepositoryServiceProvider::class);
```

You can also pass the options from the ```repository``` command, since this command is just a wrapper.

To generate a repository for your Post model, use the following command

```terminal
php artisan make:repository Post
```

To generate a repository for your Post model with Blog namespace, use the following command

```terminal
php artisan make:repository ""Blog\Post""
```

Added fields that are fillable

```terminal
php artisan make:repository ""Blog\Post"" --fillable=""title,content""
```

To add validations rules directly with your command you need to pass the `--rules` option and create migrations as well:

```terminal
php artisan make:entity Cat --fillable=""title:string,content:text"" --rules=""title=>required|min:2, content=>sometimes|min:10""
```

The command will also create your basic RESTfull controller so just add this line into your `routes.php` file and you will have a basic CRUD:

 ```php
 Route::resource('cats', CatsController::class);
 ```

When running the command, you will be creating the ""Entities"" folder and ""Repositories"" inside the folder that you set as the default.

Now that is done, you still need to bind its interface for your real repository, for example in your own Repositories Service Provider.

```php
App::bind('{YOUR_NAMESPACE}Repositories\PostRepository', '{YOUR_NAMESPACE}Repositories\PostRepositoryEloquent');
```

And use

```php
public function __construct({YOUR_NAMESPACE}Repositories\PostRepository $repository){
    $this->repository = $repository;
}
```

Alternatively, you could use the artisan command to do the binding for you.

```php
php artisan make:bindings Cats
```

### Use methods

```php
namespace App\Http\Controllers;

use App\PostRepository;

class PostsController extends BaseController {

    /**
     * @var PostRepository
     */
    protected $repository;

    public function __construct(PostRepository $repository){
        $this->repository = $repository;
    }

    ....
}
```

Find all results in Repository

```php
$posts = $this->repository->all();
```

Find all results in Repository with pagination

```php
$posts = $this->repository->paginate($limit = null, $columns = ['*']);
```

Find by result by id

```php
$post = $this->repository->find($id);
```

Hiding attributes of the model

```php
$post = $this->repository->hidden(['country_id'])->find($id);
```

Showing only specific attributes of the model

```php
$post = $this->repository->visible(['id', 'state_id'])->find($id);
```

Loading the Model relationships

```php
$post = $this->repository->with(['state'])->find($id);
```

Find by result by field name

```php
$posts = $this->repository->findByField('country_id','15');
```

Find by result by multiple fields

```php
$posts = $this->repository->findWhere([
    //Default Condition =
    'state_id'=>'10',
    'country_id'=>'15',
    //Custom Condition
    ['columnName','>','10']
]);
```

Find by result by multiple values in one field

```php
$posts = $this->repository->findWhereIn('id', [1,2,3,4,5]);
```

Find by result by excluding multiple values in one field

```php
$posts = $this->repository->findWhereNotIn('id', [6,7,8,9,10]);
```

Find all using custom scope

```php
$posts = $this->repository->scopeQuery(function($query){
    return $query->orderBy('sort_order','asc');
})->all();
```

Create new entry in Repository

```php
$post = $this->repository->create( Input::all() );
```

Update entry in Repository

```php
$post = $this->repository->update( Input::all(), $id );
```

Delete entry in Repository

```php
$this->repository->delete($id)
```

Delete entry in Repository by multiple fields

```php
$this->repository->deleteWhere([
    //Default Condition =
    'state_id'=>'10',
    'country_id'=>'15',
])
```

### Create a Criteria

#### Using the command

```terminal
php artisan make:criteria MyCriteria
```

Criteria are a way to change the repository of the query by applying specific conditions according to your needs. You can add multiple Criteria in your repository.

```php

use Prettus\Repository\Contracts\RepositoryInterface;
use Prettus\Repository\Contracts\CriteriaInterface;

class MyCriteria implements CriteriaInterface {

    public function apply($model, RepositoryInterface $repository)
    {
        $model = $model->where('user_id','=', Auth::user()->id );
        return $model;
    }
}
```

### Using the Criteria in a Controller

```php

namespace App\Http\Controllers;

use App\PostRepository;

class PostsController extends BaseController {

    /**
     * @var PostRepository
     */
    protected $repository;

    public function __construct(PostRepository $repository){
        $this->repository = $repository;
    }


    public function index()
    {
        $this->repository->pushCriteria(new MyCriteria1());
        $this->repository->pushCriteria(MyCriteria2::class);
        $posts = $this->repository->all();
		...
    }

}
```

Getting results from Criteria

```php
$posts = $this->repository->getByCriteria(new MyCriteria());
```

Setting the default Criteria in Repository

```php
use Prettus\Repository\Eloquent\BaseRepository;

class PostRepository extends BaseRepository {

    public function boot(){
        $this->pushCriteria(new MyCriteria());
        // or
        $this->pushCriteria(AnotherCriteria::class);
        ...
    }

    function model(){
       return ""App\\Post"";
    }
}
```

### Skip criteria defined in the repository

Use `skipCriteria` before any other chaining method

```php
$posts = $this->repository->skipCriteria()->all();
```

### Popping criteria

Use `popCriteria` to remove a criteria

```php
$this->repository->popCriteria(new Criteria1());
// or
$this->repository->popCriteria(Criteria1::class);
```


### Using the RequestCriteria

RequestCriteria is a standard Criteria implementation. It enables filters to perform in the repository from parameters sent in the request.

You can perform a dynamic search, filter the data and customize the queries.

To use the Criteria in your repository, you can add a new criteria in the boot method of your repository, or directly use in your controller, in order to filter out only a few requests.

#### Enabling in your Repository

```php
use Prettus\Repository\Eloquent\BaseRepository;
use Prettus\Repository\Criteria\RequestCriteria;


class PostRepository extends BaseRepository {

	/**
     * @var array
     */
    protected $fieldSearchable = [
        'name',
        'email'
    ];

    public function boot(){
        $this->pushCriteria(app('Prettus\Repository\Criteria\RequestCriteria'));
        ...
    }

    function model(){
       return ""App\\Post"";
    }
}
```

Remember, you need to define which fields from the model can be searchable.

In your repository set **$fieldSearchable** with the name of the fields to be searchable or a relation to fields.

```php
protected $fieldSearchable = [
	'name',
	'email',
	'product.name'
];
```

You can set the type of condition which will be used to perform the query, the default condition is ""**=**""

```php
protected $fieldSearchable = [
	'name'=>'like',
	'email', // Default Condition ""=""
	'your_field'=>'condition'
];
```


#### Enabling in your Controller

```php
	public function index()
    {
        $this->repository->pushCriteria(app('Prettus\Repository\Criteria\RequestCriteria'));
        $posts = $this->repository->all();
		...
    }
```

#### Example the Criteria

Request all data without filter by request

`http://prettus.local/users`

```json
[
    {
        ""id"": 1,
        ""name"": ""John Doe"",
        ""email"": ""john@gmail.com"",
        ""created_at"": ""-0001-11-30 00:00:00"",
        ""updated_at"": ""-0001-11-30 00:00:00""
    },
    {
        ""id"": 2,
        ""name"": ""Lorem Ipsum"",
        ""email"": ""lorem@ipsum.com"",
        ""created_at"": ""-0001-11-30 00:00:00"",
        ""updated_at"": ""-0001-11-30 00:00:00""
    },
    {
        ""id"": 3,
        ""name"": ""Laravel"",
        ""email"": ""laravel@gmail.com"",
        ""created_at"": ""-0001-11-30 00:00:00"",
        ""updated_at"": ""-0001-11-30 00:00:00""
    }
]
```

Conducting research in the repository

`http://prettus.local/users?search=John%20Doe`

or

`http://prettus.local/users?search=John&searchFields=name:like`

or

`http://prettus.local/users?search=john@gmail.com&searchFields=email:=`

or

`http://prettus.local/users?search=name:John Doe;email:john@gmail.com`

or

`http://prettus.local/users?search=name:John;email:john@gmail.com&searchFields=name:like;email:=`

```json
[
    {
        ""id"": 1,
        ""name"": ""John Doe"",
        ""email"": ""john@gmail.com"",
        ""created_at"": ""-0001-11-30 00:00:00"",
        ""updated_at"": ""-0001-11-30 00:00:00""
    }
]
```

By default RequestCriteria makes its queries using the **OR** comparison operator for each query parameter.
`http://prettus.local/users?search=age:17;email:john@gmail.com`

The above example will execute the following query:
``` sql
SELECT * FROM users WHERE age = 17 OR email = 'john@gmail.com';
```

In order for it to query using the **AND**, pass the *searchJoin* parameter as shown below:

`http://prettus.local/users?search=age:17;email:john@gmail.com&searchJoin=and`





Filtering fields

`http://prettus.local/users?filter=id;name`

```json
[
    {
        ""id"": 1,
        ""name"": ""John Doe""
    },
    {
        ""id"": 2,
        ""name"": ""Lorem Ipsum""
    },
    {
        ""id"": 3,
        ""name"": ""Laravel""
    }
]
```

Sorting the results

`http://prettus.local/users?filter=id;name&orderBy=id&sortedBy=desc`

```json
[
    {
        ""id"": 3,
        ""name"": ""Laravel""
    },
    {
        ""id"": 2,
        ""name"": ""Lorem Ipsum""
    },
    {
        ""id"": 1,
        ""name"": ""John Doe""
    }
]
```

Sorting through related tables

`http://prettus.local/users?orderBy=posts|title&sortedBy=desc`

Query will have something like this

```sql
...
INNER JOIN posts ON users.post_id = posts.id
...
ORDER BY title
...
```

`http://prettus.local/users?orderBy=posts:custom_id|posts.title&sortedBy=desc`

Query will have something like this

```sql
...
INNER JOIN posts ON users.custom_id = posts.id
...
ORDER BY posts.title
...
```

`http://prettus.local/users?orderBy=posts:custom_id,other_id|posts.title&sortedBy=desc`

Query will have something like this

```sql
...
INNER JOIN posts ON users.custom_id = posts.other_id
...
ORDER BY posts.title
...
```

Sorting multiple columns same sortedBy

`http://prettus.local/users?orderBy=name;created_at&sortedBy=desc`

Result will have something like this

```json
   [
       {
           ""id"": 1,
           ""name"": ""Laravel"",
           ""created_at"": ""-0001-11-29 00:00:00""
       },
       {
           ""id"": 3,
           ""name"": ""Laravel"",
           ""created_at"": ""-0001-11-28 00:00:00""
       },
       {
           ""id"": 2,
           ""name"": ""John Doe"",
           ""created_at"": ""-0001-11-30 00:00:00""
       }
   ]
```


Sorting multiple columns difference sortedBy

`http://prettus.local/users?orderBy=name;created_at&sortedBy=desc;asc`

Result will have something like this

```json
   [
       {
           ""id"": 3,
           ""name"": ""Laravel"",
           ""created_at"": ""-0001-11-28 00:00:00""
       },
       {
           ""id"": 1,
           ""name"": ""Laravel"",
           ""created_at"": ""-0001-11-29 00:00:00""
       },
       {
           ""id"": 2,
           ""name"": ""John Doe"",
           ""created_at"": ""-0001-11-30 00:00:00""
       }
   ]
```

Add relationship

`http://prettus.local/users?with=groups`

Between filter

`http://prettus.local/product?search=price:100,500&searchFields=price:between`

Result will have something like this

```json
   [
       {
           ""id"": 3,
           ""price"": ""150"",
           ""created_at"": ""-0001-11-28 00:00:00""
       },
       {
           ""id"": 1,
           ""price"": ""300"",
           ""created_at"": ""-0001-11-29 00:00:00""
       },
       {
           ""id"": 2,
           ""price"": ""450"",
           ""created_at"": ""-0001-11-30 00:00:00""
       }
   ]
```

WhereIn filter 

`http://prettus.local/product?search=price:300,500&searchFields=price:in`

Result will have something like this

```json
   [
       {
           ""id"": 1,
           ""price"": ""300"",
           ""created_at"": ""-0001-11-29 00:00:00""
       }
   ]
```

#### Overwrite params name

You can change the name of the parameters in the configuration file **config/repository.php**

### Cache

Add a layer of cache easily to your repository

#### Cache Usage

Implements the interface CacheableInterface and use CacheableRepository Trait.

```php
use Prettus\Repository\Eloquent\BaseRepository;
use Prettus\Repository\Contracts\CacheableInterface;
use Prettus\Repository\Traits\CacheableRepository;

class PostRepository extends BaseRepository implements CacheableInterface {

    use CacheableRepository;

    ...
}
```

Done , done that your repository will be cached , and the repository cache is cleared whenever an item is created, modified or deleted.

#### Cache Config

You can change the cache settings in the file *config/repository.php* and also directly on your repository.

*config/repository.php*

```php
'cache'=>[
    //Enable or disable cache repositories
    'enabled'   => true,

    //Lifetime of cache
    'minutes'   => 30,

    //Repository Cache, implementation Illuminate\Contracts\Cache\Repository
    'repository'=> 'cache',

    //Sets clearing the cache
    'clean'     => [
        //Enable, disable clearing the cache on changes
        'enabled' => true,

        'on' => [
            //Enable, disable clearing the cache when you create an item
            'create'=>true,

            //Enable, disable clearing the cache when upgrading an item
            'update'=>true,

            //Enable, disable clearing the cache when you delete an item
            'delete'=>true,
        ]
    ],
    'params' => [
        //Request parameter that will be used to bypass the cache repository
        'skipCache'=>'skipCache'
    ],
    'allowed'=>[
        //Allow caching only for some methods
        'only'  =>null,

        //Allow caching for all available methods, except
        'except'=>null
    ],
],
```

It is possible to override these settings directly in the repository.

```php
use Prettus\Repository\Eloquent\BaseRepository;
use Prettus\Repository\Contracts\CacheableInterface;
use Prettus\Repository\Traits\CacheableRepository;

class PostRepository extends BaseRepository implements CacheableInterface {

    // Setting the lifetime of the cache to a repository specifically
    protected $cacheMinutes = 90;

    protected $cacheOnly = ['all', ...];
    //or
    protected $cacheExcept = ['find', ...];

    use CacheableRepository;

    ...
}
```

The cacheable methods are : all, paginate, find, findByField, findWhere, getByCriteria

### Validators

Requires [prettus/laravel-validator](https://github.com/prettus/laravel-validator). `composer require prettus/laravel-validator`

Easy validation with `prettus/laravel-validator`

[For more details click here](https://github.com/prettus/laravel-validator)

#### Using a Validator Class

##### Create a Validator

In the example below, we define some rules for both creation and edition

```php
use \Prettus\Validator\LaravelValidator;

class PostValidator extends LaravelValidator {

    protected $rules = [
        'title' => 'required',
        'text'  => 'min:3',
        'author'=> 'required'
    ];

}
```

To define specific rules, proceed as shown below:

```php
use \Prettus\Validator\Contracts\ValidatorInterface;
use \Prettus\Validator\LaravelValidator;

class PostValidator extends LaravelValidator {

    protected $rules = [
        ValidatorInterface::RULE_CREATE => [
            'title' => 'required',
            'text'  => 'min:3',
            'author'=> 'required'
        ],
        ValidatorInterface::RULE_UPDATE => [
            'title' => 'required'
        ]
   ];

}
```

##### Enabling Validator in your Repository

```php
use Prettus\Repository\Eloquent\BaseRepository;
use Prettus\Repository\Criteria\RequestCriteria;

class PostRepository extends BaseRepository {

    /**
     * Specify Model class name
     *
     * @return mixed
     */
    function model(){
       return ""App\\Post"";
    }

    /**
     * Specify Validator class name
     *
     * @return mixed
     */
    public function validator()
    {
        return ""App\\PostValidator"";
    }
}
```

#### Defining rules in the repository

Alternatively, instead of using a class to define its validation rules, you can set your rules directly into the rules repository property, it will have the same effect as a Validation class.

```php
use Prettus\Repository\Eloquent\BaseRepository;
use Prettus\Repository\Criteria\RequestCriteria;
use Prettus\Validator\Contracts\ValidatorInterface;

class PostRepository extends BaseRepository {

    /**
     * Specify Validator Rules
     * @var array
     */
     protected $rules = [
        ValidatorInterface::RULE_CREATE => [
            'title' => 'required',
            'text'  => 'min:3',
            'author'=> 'required'
        ],
        ValidatorInterface::RULE_UPDATE => [
            'title' => 'required'
        ]
   ];

    /**
     * Specify Model class name
     *
     * @return mixed
     */
    function model(){
       return ""App\\Post"";
    }

}
```

Validation is now ready. In case of a failure an exception will be given of the type: *Prettus\Validator\Exceptions\ValidatorException*

### Presenters

Presenters function as a wrapper and renderer for objects.

#### Fractal Presenter

Requires [Fractal](http://fractal.thephpleague.com/). `composer require league/fractal`

There are two ways to implement the Presenter, the first is creating a TransformerAbstract and set it using your Presenter class as described in the Create a Transformer Class.

The second way is to make your model implement the Transformable interface, and use the default Presenter ModelFractarPresenter, this will have the same effect.

##### Transformer Class

###### Create a Transformer using the command

```terminal
php artisan make:transformer Post
```

This will generate the class beneath.

###### Create a Transformer Class

```php
use League\Fractal\TransformerAbstract;

class PostTransformer extends TransformerAbstract
{
    public function transform(\Post $post)
    {
        return [
            'id'      => (int) $post->id,
            'title'   => $post->title,
            'content' => $post->content
        ];
    }
}
```

###### Create a Presenter using the command

```terminal
php artisan make:presenter Post
```

The command will prompt you for creating a Transformer too if you haven't already.
###### Create a Presenter

```php
use Prettus\Repository\Presenter\FractalPresenter;

class PostPresenter extends FractalPresenter {

    /**
     * Prepare data to present
     *
     * @return \League\Fractal\TransformerAbstract
     */
    public function getTransformer()
    {
        return new PostTransformer();
    }
}
```

###### Enabling in your Repository

```php
use Prettus\Repository\Eloquent\BaseRepository;

class PostRepository extends BaseRepository {

    ...

    public function presenter()
    {
        return ""App\\Presenter\\PostPresenter"";
    }
}
```

Or enable it in your controller with

```php
$this->repository->setPresenter(""App\\Presenter\\PostPresenter"");
```

###### Using the presenter after from the Model

If you recorded a presenter and sometime used the `skipPresenter()` method or simply you do not want your result is not changed automatically by the presenter.
You can implement Presentable interface on your model so you will be able to present your model at any time. See below:

In your model, implement the interface `Prettus\Repository\Contracts\Presentable` and `Prettus\Repository\Traits\PresentableTrait`

```php
namespace App;

use Prettus\Repository\Contracts\Presentable;
use Prettus\Repository\Traits\PresentableTrait;

class Post extends Eloquent implements Presentable {

    use PresentableTrait;

    protected $fillable = [
        'title',
        'author',
        ...
     ];

     ...
}
```

There, now you can submit your Model individually, See an example:

```php
$repository = app('App\PostRepository');
$repository->setPresenter(""Prettus\\Repository\\Presenter\\ModelFractalPresenter"");

//Getting the result transformed by the presenter directly in the search
$post = $repository->find(1);

print_r( $post ); //It produces an output as array

...

//Skip presenter and bringing the original result of the Model
$post = $repository->skipPresenter()->find(1);

print_r( $post ); //It produces an output as a Model object
print_r( $post->presenter() ); //It produces an output as array

```

You can skip the presenter at every visit and use it on demand directly into the model, for it set the `$skipPresenter` attribute to true in your repository:

```php
use Prettus\Repository\Eloquent\BaseRepository;

class PostRepository extends BaseRepository {

    /**
    * @var bool
    */
    protected $skipPresenter = true;

    public function presenter()
    {
        return ""App\\Presenter\\PostPresenter"";
    }
}
```

##### Model Class

###### Implement Interface

```php
namespace App;

use Prettus\Repository\Contracts\Transformable;

class Post extends Eloquent implements Transformable {
     ...
     /**
      * @return array
      */
     public function transform()
     {
         return [
             'id'      => (int) $this->id,
             'title'   => $this->title,
             'content' => $this->content
         ];
     }
}
```

###### Enabling in your Repository

`Prettus\Repository\Presenter\ModelFractalPresenter` is a Presenter default for Models implementing Transformable

```php
use Prettus\Repository\Eloquent\BaseRepository;

class PostRepository extends BaseRepository {

    ...

    public function presenter()
    {
        return ""Prettus\\Repository\\Presenter\\ModelFractalPresenter"";
    }
}
```

Or enable it in your controller with

```php
$this->repository->setPresenter(""Prettus\\Repository\\Presenter\\ModelFractalPresenter"");
```

### Skip Presenter defined in the repository

Use *skipPresenter* before any other chaining method

```php
$posts = $this->repository->skipPresenter()->all();
```

or

```php
$this->repository->skipPresenter();

$posts = $this->repository->all();
```
"
184,yiyang74262580/Repository,,"# Repository
一些常用的软件的存储，欢迎使用
"
185,matthewschrager/Repository,C#,"Repository
=============

Repository is a generic implementation of the Repository pattern in C#. It provides a repository base class ```Repository<T>``` that exposes functions to store/retrieve data,
and an object context base class ```ObjectContext<T>``` that enables manipulation of data once it's retrieved. It also exposes an ```Items``` property that returns 
an ```IQueryable<T>``` which can be used to perform LINQ queries on the repository.

Implementations
================

This (code) repository comes with a few Repository implementations. The first and simplest is ```InMemoryRepository```, which acts as a temporary in-memory store useful mainly for testing. 
The second is an Entity Framework repository named ```EFRepository```, which uses [Entity Framework](http://msdn.microsoft.com/en-us/data/ef.aspx) as its 
storage interface. The thid is ```AzureRepository```, an implementation of ```Repository``` for [Azure Blob Storage](http://www.windowsazure.com/en-us/manage/services/storage/).
And the fourth is ```FileSystemRepository```, an implementation that serializes objects directly to the filesystem.


Pull requests for additional implementations (and improvements to existing ones) are welcome and encouraged.

Change Tracking
=================

As you'll see in the examples below, changes to objects accessed via a ```Repository``` instance are tracked automatically and committed back to the underlying data store whenever
a call to ```SaveChanges``` is made. This functionality is enabled automatically for any class which derives from ```Repository```; if you're writing a new implementation, you'll get
change tracking for free. See the 'Modify an object' example below to get an idea for how this works.

Examples
===========

Store an object:
-----------------

```C#
var repository = new MyConcreteRepository<int>();
repository.Insert(1); // Store a single value
repository.SaveChanges();


repository.Insert(new[] { 2, 3, 4, 5 }); // Store a bunch of values
repository.SaveChanges();
```

Retrieve an object:
--------------------

```C#
class MyClass 
{
	public String Key { get; set; }
}

/* ... */

using (var repository = new MyConcreteRepository<MyClass>())
{
	var objectContext = repository.Find(""myKey"");
	var obj = objectContext.Object;

	// Do cool things with this value
}
```

Query stored objects:
----------------------

```C#
class MyClass
{
	public String Key { get; set; }
	public int Value { get; set; }
}

/* ... */

using (var repository = new MyConcreteRepository<MyClass>())
{
	// Query objects based on their Value field
	var filteredItems = repository.Items.Where(x => x.Value > 5);

	// Do stuff with query result
}
```


Remove an object:
-----------------

```C#
class MyClass 
{
	public String Key { get; set; }
}

/* ... */

using (var repository = new MyConcreteRepository<MyClass>())
{
	repository.RemoveByKey(""myKey"");
	repository.SaveChanges();

	// Or...

	var obj = new MyClass { Key = ""myKey"" };
	repository.Remove(obj);
	repository.SaveChanges();

	// Or...

	repository.RemoveByKey(""myKey"");
	repository.SaveChanges();

	// Or...

	repository.RemoveAllByKey(""myKey"");
	repository.SaveChanges();

	// Or...
	repository.RemoveAll();
	repository.SaveChanges();
}
```

Modify a stored object:
-----------------------

```C#
class MyClass 
{
	public String Key { get; set; }
	public int Value { get; set; }
}

/* ... */

// Pull the object from the repository
using (var repository = new MyConcreteRepository<MyClass>())
{
	// Pull the object from the repository
	var valueContext = repository.Find(""myKey"");

	// Modify it
	valueContext.Object.Value = 5;

	// Save the changes
	repository.SaveChanges();
}
```


Typed Key Repositories
==========================
Stored objects are accessed by their keys, as in the examples above. Normally, keys are passed into ```Repository``` methods as untyped ```params``` arguments; as a result,
the compiler will not perform any checks for either the number or types of keys passed in. This allows for flexibility in the types of keys you can use,
but also negates some of the benefits of having a strongly-typed generic repository in the first place. 

In order to fix this potential problem, this library contains a series of ```Repository``` base classes that take type parameters for keys. These classes
simply wrap the ""untyped"" repositories with strongly-typed versions of certain methods, like ```Find``` and ```RemoveByKey```. In order to make typed versions of
your repositories available, simply derive from the strongly-typed versions of ```Repository``` and pass in instances of your untyped repositories to the base
constructor.

For example, ```EFRepository``` exposes strongly-typed versions (for classes with up to two key values) like so:


```C#
public class EFRepository<TContext, TValue, TKey> : Repository<TValue, TKey> where TValue : class where TContext : DbContext
{
    //===============================================================
    public EFRepository(Func<TContext, DbSet<TValue>> setSelector, TContext context = null)
        : base(new EFRepository<TContext, TValue>(setSelector, context))
    {}
    //===============================================================
}

public class EFRepository<TContext, TValue, TKey1, TKey2> : Repository<TValue, TKey1, TKey2>
    where TValue : class
    where TContext : DbContext
{
    //===============================================================
    public EFRepository(Func<TContext, DbSet<TValue>> setSelector, TContext context = null)
        : base(new EFRepository<TContext, TValue>(setSelector, context))
    { }
    //===============================================================
}
```

All these classes do is instantiate untyped instances of ```EFRepository``` and pass them into the base ```Repository``` constructor. Once instantiated,
these strongly-typed versions of ```EFRepository``` will enforce type safety on all key lookups. You can see this by looking at methods that make use of keys. For
example, the signature of ```Find``` in the strongly-typed version of ```Repository``` looks like this:

```C#
public ObjectContext<TValue> Find(TKey key)
{
    return InnerRepository.Find(key);
}
```

Notice how, instead of ```params Object[]```, the sole argument here is of type ```TKey```. This will allow the compiler to check for invalid lookups at compile-time,
which can be very handy.

EFRepository 
==============

In order to create an instance of ```EFRepository```, you need to first declare a derived class of 
[DbContext](http://msdn.microsoft.com/en-us/library/gg679505.aspx)
with ```DbSet``` instances for all of the types for which you want to have repositories. An example ```DbContext``` might look like this:

```C#
class TestClass
{
	[Key]
	public String Key { get; set; }
}

/* ... */

class MyContext : DbContext
{
	public DbSet<TestClass> TestClasses { get; set; }
}
```

Once a subclass of ```DbContext``` is defined, you can instantiate an ```EFRepository``` for type ```TestClass``` like so:

```C#
var repository = new EFRepository<MyContext, TestClass>(x => x.TestClasses);
```

The sole argument to the constructor is a ```Func<MyContext, TestClass>``` that tells the repository how to find the appropriate ```DbSet``` in the ```DbContext```.

Note that the procedures normally followed when using Entity Framework must still be followed here; that is, you should still specify connection strings in
your ```Web.config``` and decorate your key properties with ```[Key]``` attributes if necessary.

AzureRepository
===============

All you need for ```AzureRepository``` is a proper connection string for [Azure blob storage](http://www.windowsazure.com/en-us/develop/net/how-to-guides/blob-storage/). 
You can either pass this connection string directly to an instance of ```AzureRepository```, or you can store it in a App.config/Web.config file and reference it by name.
There's also a static ```ForStorageEmulator``` method that will create an instance of ```AzureRepository``` pointed at the local 
[Azure storage emulator](http://msdn.microsoft.com/en-us/library/windowsazure/ff683674.aspx).

Re-using the ```TestClass``` class from above, instantiating an ```AzureRepository``` looks like this:

```C#
// From explicit connection string
var repository = AzureRepository<TestClass>.FromExplicitConnectionString(x => x.Key, myConnectionString);

// From named connection string
var repository = AzureRepository<TestClass>.FromNamedConnectionString(x => x.Key, ""myConnectionStringName"");

// For storage emulator
var repository = AzureRepository<TestClass>.ForStorageEmulator(x => x.Key);
```

There is also an ```AzureOptions``` class that allows you to specify how e.g. objects are serialized to blob storage, access rules for stored objects, content type, etc.

FileSystemRepository
=====================

```FileSystemRepository``` is an implementation that (surprise) saves objects to the local filesystem. Instantiating one is very simple:

```C#
var repository = new FileSystemRepository<TestClass>(x => x.Key);
```

 You can also optionally specify a ```FileSystemOptions``` parameter that configures how objects are serialized to disk, where they're stored, the file extension, etc.

Implementing Your Own Repositories
================================

To understand how to go about doing implementing your own repositories, we need to 
briefly go over how repositories work under the hood.

All operations on ```Repository``` implementations are deferred; that is, they are not committed until a call to
```SaveChanges``` is made. This is achieved by storing a list of pending operations and applying them 
sequentially when ```SaveChanges``` is called. These pending operations are objects which implement the
interface ```Operation```, which has a single method called ```Apply```:

```C#
public interface Operation
{
    //===============================================================
    void Apply();
    //===============================================================
} 
```

There are three main types of operations: ```Insert```, ```Remove```, and ```Modify```. 
These are abstract base classes. Each ```Repository``` implementation will provide its own concrete versions
of these classes which perform the necessary operations. As an example, the abstract ```Insert``` class looks
like this:

```C#
public abstract class Insert<TValue> : Operation
{
    //===============================================================
    public Insert(IEnumerable<Object> keys, TValue value)
    {
        Keys = keys;
        Value = value;
    }
    //===============================================================
    public IEnumerable<Object> Keys { get; private set; }
    //===============================================================
    public TValue Value { get; private set; }
    //===============================================================
    public abstract void Apply();
    //===============================================================
}
```

Notice that this base class stores all of the information necessary to perform an insert into some underlying data
store, namely the object to be inserted and the keys under which it should be stored.

Now take a look at an implementation of ```Insert```, this one for ```FileSystemRepository```:

```C#
internal class FileSystemInsert<T> : Insert<T>
{
    //===============================================================
    public FileSystemInsert(IEnumerable<object> keys, T value, FileSystemInterface<T> fsInterface)
        : base(keys, value)
    {
        FileSystemInterface = fsInterface;
    }
    //===============================================================
    private FileSystemInterface<T> FileSystemInterface { get; set; }
    //===============================================================
    public override void Apply()
    {
        FileSystemInterface.StoreObject(Value, Keys);
    }
    //===============================================================
}
```

All this class does in its ```Apply``` method is call a supplied ```FileSystemInterface``` object to perform
the actual insert. The ```FileSystemInterface``` class is a utility class that handles filesystem manipulation,
but you could just as easily implement the filesystem serialization logic directly in ```Apply```. The point is 
that the implementation of ```Insert``` is responsible for initiating the actual insertion operation into the 
underlying data store. 

In order to implement your own repositories, you have to implement three methods from the base ```Repository```
class which correspond to the three types of operations mentioned above: 

```C#
//===============================================================
protected abstract Insert<T> CreateInsert(IEnumerable<object> keys, T value);
//===============================================================
protected abstract Remove CreateRemove(IEnumerable<object> keys);
//===============================================================
protected abstract Modify<T> CreateModify(IEnumerable<object> keys, T value, Action<T> modifier);
//===============================================================
```

These methods are responsible for returning implementations of the three types of operations, as shown
above. 

In addition to these methods, you must also implement three others:

```C#
//===============================================================
protected abstract ObjectContext<T> FindImpl(object[] keys);
//===============================================================
public abstract bool ExistsByKey(params Object[] keys);
//===============================================================
public abstract EnumerableObjectContext<T> Items { get; } 
//===============================================================
```

```FindImpl``` is reponsible for looking up an object from the underlying data store,
```ExistsByKey``` is responsible for checking for an object's existence by its key values,
and ```Items``` is responsible for enumerating the objects in the repository. These will likely
call methods directly on the underlying data store or make use of some sort of interface such as 
```FileSystemInterface``` (mentioned above). Again, the point is that these methods are responsible for
interfacing with the underlying data store. As an example, if you were implementing a repository for Amazon S3 
storage, the FindImpl method would likely call some sort of lookup method in the S3 API.


License
===========

Repository is licensed under the New BSD License:

```
Copyright (c) 2013, Matthew Schrager
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation and/or other materials provided with the distribution.
    * Neither the name of Matthew Schrager nor the
      names of its contributors may be used to endorse or promote products
      derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL MATTHEW SCHRAGER BE LIABLE FOR ANY
DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
```









"
186,ocaml/opam-repository,Shell,"This repository contains OCaml package and compiler metadata and is
used by the default installation of [opam](https://opam.ocaml.org/).

The state of the package ecosystem can be explored using opam-health-check<sup>[[1]]</sup>: http://check.ocamllabs.io/

## How to Contribute

Contributions are welcome !

The [CONTRIBUTING.md](CONTRIBUTING.md) document has general guidelines
on how to contribute.

If you would like to add a new package consult
[these instructions](https://opam.ocaml.org/doc/Packaging.html#Publishing).

## License

All the metadata contained in this repository are licensed under the
[CC0 1.0 Universal](http://creativecommons.org/publicdomain/zero/1.0/)
license.

Moreover, as the collection of the metadata in this repository is
technically a ""Database"" -- which is subject to a ""sui generis"" right
in Europe -- we would like to stress that even the *collection* of
the metadata contained in opam-repository is licensed under CC0 and
thus the simple act of cloning opam-repository is perfectly legal.

[1]: https://github.com/ocurrent/opam-health-check
"
187,sonatype/docker-nexus3,Groovy,"<!--

  Copyright (c) 2016-present Sonatype, Inc.

  Licensed under the Apache License, Version 2.0 (the ""License"");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an ""AS IS"" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.

-->

# Sonatype Nexus3 Docker: sonatype/nexus3

[![Join the chat at https://gitter.im/sonatype/nexus-developers](https://badges.gitter.im/sonatype/nexus-developers.svg)](https://gitter.im/sonatype/nexus-developers?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

A Dockerfile for Sonatype Nexus Repository Manager 3, starting with 3.18 the image is based on the [Red Hat Universal Base Image](https://www.redhat.com/en/blog/introducing-red-hat-universal-base-image) while earlier versions used CentOS.

* [Contribution Guidlines](#contribution-guidelines)
* [Running](#running)
* [Building the Nexus Repository Manager image](#building-the-nexus-repository-manager-image)
* [Chef Solo for Runtime and Application](#chef-solo-for-runtime-and-application)
* [Testing the Dockerfile](#testing-the-dockerfile)
* [Red Hat Certified Image](#red-hat-certified-image)
* [Notes](#notes)
  * [Persistent Data](#persistent-data)
* [Getting Help](#getting-help)

## Contribution Guidelines

Go read [our contribution guidelines](https://github.com/sonatype/docker-nexus3/blob/master/.github/CONTRIBUTING.md) to get a bit more familiar with how
we would like things to flow.

## Running

To run, binding the exposed port 8081 to the host, use:

```
$ docker run -d -p 8081:8081 --name nexus sonatype/nexus3
```

When stopping, be sure to allow sufficient time for the databases to fully shut down.  

```
docker stop --time=120 <CONTAINER_NAME>
```


To test:

```
$ curl http://localhost:8081/
```

## Building the Nexus Repository Manager image

To build a docker image from the Docker file you can use this command:

```
$ docker build --rm=true --tag=sonatype/nexus3 .
```

The following optional variables can be used when building the image:

- NEXUS_VERSION: Version of the Nexus Repository Manager
- NEXUS_DOWNLOAD_URL: Download URL for Nexus Repository, alternative to using `NEXUS_VERSION` to download from Sonatype
- NEXUS_DOWNLOAD_SHA256_HASH: Sha256 checksum for the downloaded Nexus Repository Manager archive. Required if `NEXUS_VERSION`
 or `NEXUS_DOWNLOAD_URL` is provided

## Chef Solo for Runtime and Application

Chef Solo is used to build out the runtime and application layers of the Docker image. The Chef cookbook being used is available
on GitHub at [sonatype/chef-nexus-repository-manager](https://github.com/sonatype/chef-nexus-repository-manager).

## Testing the Dockerfile

We are using `rspec` as the test framework. `serverspec` provides a docker backend (see the method `set` in the test code)
 to run the tests inside the docker container, and abstracts away the difference between distributions in the tests
 (e.g. yum, apt,...).

    rspec [--backtrace] spec/Dockerfile_spec.rb

## Red Hat Certified Image

A Red Hat certified container image can be created using `Dockerfile.rh.ubi` which is built to be compliant with Red Hat certification.
The image includes additional meta data to comform with Kubernetes and OpenShift standards, a directory with the
licenses applicable to the software and a man file for help on how to use the software. It also uses an ENTRYPOINT
script the ensure the running user has access to the appropriate permissions for OpenShift 'restricted' SCC. 

The Red Hat certified container image is available from the 
[Red Hat Container Catalog](https://access.redhat.com/containers/#/registry.connect.redhat.com/sonatype/nexus-repository-manager)
and qualified accounts can pull it from registry.connect.redhat.com.

## Other Red Hat Images

In addition to the Universal Base Image, we can build images based on:
* Red Hat Enterprise Linux: `Dockerfile.rh.el`
* CentOS: `Dockerfile.rh.centos`

## Notes

* Our [system requirements](https://help.sonatype.com/display/NXRM3/System+Requirements) should be taken into account when provisioning the Docker container.
* Default user is `admin` and the uniquely generated password can be found in the `admin.password` file inside the volume. See [Persistent Data](#user-content-persistent-data) for information about the volume.

* It can take some time (2-3 minutes) for the service to launch in a
new container.  You can tail the log to determine once Nexus is ready:

```
$ docker logs -f nexus
```

* Installation of Nexus is to `/opt/sonatype/nexus`.  

* A persistent directory, `/nexus-data`, is used for configuration,
logs, and storage. This directory needs to be writable by the Nexus
process, which runs as UID 200.

* There is an environment variable that is being used to pass JVM arguments to the startup script

  * `INSTALL4J_ADD_VM_PARAMS`, passed to the Install4J startup script. Defaults to `-Xms2703m -Xmx2703m -XX:MaxDirectMemorySize=2703m -Djava.util.prefs.userRoot=${NEXUS_DATA}/javaprefs`.

  This can be adjusted at runtime:

  ```
  $ docker run -d -p 8081:8081 --name nexus -e INSTALL4J_ADD_VM_PARAMS=""-Xms2703m -Xmx2703m -XX:MaxDirectMemorySize=2703m -Djava.util.prefs.userRoot=/some-other-dir"" sonatype/nexus3
  ```

  Of particular note, `-Djava.util.prefs.userRoot=/some-other-dir` can be set to a persistent path, which will maintain
  the installed Nexus Repository License if the container is restarted.
  
  Be sure to check the [memory requirements](https://help.sonatype.com/display/NXRM3/System+Requirements#SystemRequirements-MemoryRequirements) when deciding how much heap and direct memory to allocate.

* Another environment variable can be used to control the Nexus Context Path

  * `NEXUS_CONTEXT`, defaults to /

  This can be supplied at runtime:

  ```
  $ docker run -d -p 8081:8081 --name nexus -e NEXUS_CONTEXT=nexus sonatype/nexus3
  ```

### Persistent Data

There are two general approaches to handling persistent storage requirements
with Docker. See [Managing Data in Containers](https://docs.docker.com/engine/tutorials/dockervolumes/)
for additional information.

  1. *Use a docker volume*.  Since docker volumes are persistent, a volume can be created specifically for
  this purpose.  This is the recommended approach.  

  ```
  $ docker volume create --name nexus-data
  $ docker run -d -p 8081:8081 --name nexus -v nexus-data:/nexus-data sonatype/nexus3
  ```

  2. *Mount a host directory as the volume*.  This is not portable, as it
  relies on the directory existing with correct permissions on the host.
  However it can be useful in certain situations where this volume needs
  to be assigned to certain specific underlying storage.  

  ```
  $ mkdir /some/dir/nexus-data && chown -R 200 /some/dir/nexus-data
  $ docker run -d -p 8081:8081 --name nexus -v /some/dir/nexus-data:/nexus-data sonatype/nexus3
  ```

## Getting Help

Looking to contribute to our Docker image but need some help? There's a few ways to get information or our attention:

* Chat with us on [Gitter](https://gitter.im/sonatype/nexus-developers)
* File an issue [on our public JIRA](https://issues.sonatype.org/projects/NEXUS/)
* Check out the [Nexus3](http://stackoverflow.com/questions/tagged/nexus3) tag on Stack Overflow
* Check out the [Nexus Repository User List](https://groups.google.com/a/glists.sonatype.com/forum/?hl=en#!forum/nexus-users)

## License Disclaimer

_Nexus Repository OSS is distributed with Sencha Ext JS pursuant to a FLOSS Exception agreed upon between Sonatype, Inc. and Sencha Inc. Sencha Ext JS is licensed under GPL v3 and cannot be redistributed as part of a closed source work._
"
188,puli/repository,PHP,"The Puli Repository Component
=============================

[![Build Status](https://travis-ci.org/puli/repository.svg?branch=1.0)](https://travis-ci.org/puli/repository)
[![Build status](https://ci.appveyor.com/api/projects/status/a0g5jdtj78wv53c0/branch/1.0?svg=true)](https://ci.appveyor.com/project/webmozart/repository/branch/1.0)
[![Scrutinizer Code Quality](https://scrutinizer-ci.com/g/puli/repository/badges/quality-score.png?b=1.0)](https://scrutinizer-ci.com/g/puli/repository/?branch=1.0)
[![Latest Stable Version](https://poser.pugx.org/puli/repository/v/stable.svg)](https://packagist.org/packages/puli/repository)
[![Total Downloads](https://poser.pugx.org/puli/repository/downloads.svg)](https://packagist.org/packages/puli/repository)
[![Dependency Status](https://www.versioneye.com/php/puli:repository/1.0.0/badge.svg)](https://www.versioneye.com/php/puli:repository/1.0.0)

Latest release: [1.0.0-beta10](https://packagist.org/packages/puli/repository#1.0.0-beta10)

PHP >= 5.3.9

The [Puli] Repository Component provides an API for storing arbitrary resources
in a filesystem-like repository:

```php
use Puli\Repository\InMemoryRepository;
use Puli\Repository\Resource\DirectoryResource;

$repo = new InMemoryRepository();
$repo->add('/config', new DirectoryResource('/path/to/resources/config'));

// /path/to/resources/config/routing.yml
echo $repo->get('/config/routing.yml')->getBody();
```

The following [`ResourceRepository`] implementations are currently supported:

* [`InMemoryRepository`]
* [`FilesystemRepository`]
* [`NullRepository`]
* [`JsonRepository`]
* [`OptimizedJsonRepository`]

The following [`Resource`] implementations are currently supported:

* [`GenericResource`]
* [`FileResource`]
* [`DirectoryResource`]
* [`LinkResource`]

Authors
-------

* [Bernhard Schussek] a.k.a. [@webmozart]
* [The Community Contributors]

Installation
------------

Follow the [Getting Started] guide to install Puli in your project.

Documentation
-------------

Read the [Puli Documentation] to learn more about Puli.

Contribute
----------

Contributions to Puli are always welcome!

* Report any bugs or issues you find on the [issue tracker].
* You can grab the source code at Puli’s [Git repository].

Support
-------

If you are having problems, send a mail to bschussek@gmail.com or shout out to
[@webmozart] on Twitter.

License
-------

All contents of this package are licensed under the [MIT license].

[Puli]: http://puli.io
[Bernhard Schussek]: http://webmozarts.com
[The Community Contributors]: https://github.com/puli/repository/graphs/contributors
[Installation guide]: http://docs.puli.io/en/latest/installation.html
[Puli Documentation]: http://docs.puli.io/en/latest/index.html
[issue tracker]: https://github.com/puli/issues/issues
[Git repository]: https://github.com/puli/repository
[@webmozart]: https://twitter.com/webmozart
[MIT license]: LICENSE
[`ResourceRepository`]: http://api.puli.io/latest/class-Puli.Repository.Api.ResourceRepository.html
[`InMemoryRepository`]: http://api.puli.io/latest/class-Puli.Repository.InMemoryRepository.html
[`FilesystemRepository`]: http://api.puli.io/latest/class-Puli.Repository.FilesystemRepository.html
[`NullRepository`]: http://api.puli.io/latest/class-Puli.Repository.NullRepository.html
[`JsonRepository`]: http://api.puli.io/latest/class-Puli.Repository.JsonRepository.html
[`OptimizedJsonRepository`]: http://api.puli.io/latest/class-Puli.Repository.OptimizedJsonRepository.html
[`Resource`]: http://api.puli.io/latest/class-Puli.Repository.Api.Resource.Resource.html
[`GenericResource`]: http://api.puli.io/latest/class-Puli.Repository.Resource.GenericResource.html
[`FileResource`]: http://api.puli.io/latest/class-Puli.Repository.Resource.FileResource.html
[`DirectoryResource`]: http://api.puli.io/latest/class-Puli.Repository.Resource.DirectoryResource.html
[`LinkResource`]: http://api.puli.io/latest/class-Puli.Repository.Resource.LinkResource.html
"
189,LuaDist/Repository,CMake,"Primary LuaDist Repository
==========================

[![Build Status](https://secure.travis-ci.org/LuaDist/Repository.png?branch=master)](http://travis-ci.org/LuaDist/Repository)

This repository aggregates all the supported modules of the LuaDist project. Its primary purpose is to provide a manifest for modules. Modules are referenced using [git submodules][sub] and should always point to individual module repositories in the LuaDist project. Its secondary purpose is to act as an install and bootstrap process for LuaDist based installations.

This repository contains an installation script that allows automated building of LuaDist modules. There are two modes of operation available. First mode is for bootstrapping the _luadist_ deployment utility that offers complete package management functionality and automated dependency resolving. However this requires compilation of _openssl_ and other utilities you may not want.

The second mode of operation directly checks out repositories using git or available submodules and installs the modules without dependency handling. Using this approach you can tailor your distribution from ground up without unneeded dependencies.

Bootstraping LuaDist deployment tool
---

Please make sure your system has git, CMake 2.8  and a compiler tool-chain available. On Ubuntu this requires git, cmake, build-essential. This build will take quite a while to compile, please be patient.

```bash
git clone git://github.com/LuaDist/Repository.git
cd Repository
git submodule update --init --recursive bootstrap
./install bootstrap
```

Once the installation finishes the LuaDist folder should contain a fully versioned LuaDist distribution.

```bash
cd _install/bin
./luadist list # lists installed modules
./luadist search # lists online repository
./luadist install luaexpat # installs luaexpat
```

Using the install script to generate distribution without versioning.
---

To make a distribution containing luajit, luasocket and luafilesystem you can use the install utility directly:

```bash
./install luajit luasocket luafilesystem
```

Note that this mode of installation installs most recent versions of modules and does not handle dependencies automatically. If you checked out any of the modules using submodules the utility will use the local files, otherwise it will access remote git repositories. However, the installation script is able to install specific tags of modules. It is up to you to install correct dependencies, otherwise the distribution may be unusable.

```bash
./install lua-5.1.4 md5-1.1.2 
./_install/bin/lua
> require ""md5""
```

Cloning
-------

To clone the full repository:

```bash	
git clone git://github.com/LuaDist/Repository.git
cd Repository
git submodule update --init --recursive
```

To clone individual modules you can specify the module name as follows:

```bash
git submodule update --init --recursive lua
```

Note that submodules do not point to latest versions of modules but rather to _stable_ versions. To update to latest version do:

```bash
cd module
git checkout master
git pull
```

By default all submodules are accessed using the `git://` protocol. Developers can update all remotes to support push through ssl as reqired by GitHub using the following command:

```bash
git submodule foreach 'git remote set-url --push origin git@github.com:LuaDist/$path.git'
```

We also recommend switching all submodules to the master branch using the following command:

```bash
git submodule foreach 'git checkout master && git pull'
```

Contributing
------------

1. Submit a [issue][issue] with a link to your git repository of the module.
2. A maintainer will fork the module into LuaDist grant you the rights to push changes into it.
3. The maintainer will add a submodule referencing the forked module into this LuaDist/Repository.

Call for Maintainers
--------------------

If you would like to help us maintain the repository and update modules without maintainers you are more than welcome. Please contact us at our [development list][mail]

[sub]: http://github.com/guides/developing-with-submodules
[issue]: http://github.com/LuaDist/Repository/issues
[mail]: http://groups.google.com/group/luadist
"
190,OnsenUI/onsen.io,HTML,"# onsen.io

[![Circle CI](https://circleci.com/gh/OnsenUI/OnsenUI.svg?style=svg)](https://circleci.com/gh/OnsenUI/onsen.io)

This repository contains the assets in Onsen UI Website, available on http://onsen.io.
Please Visit [Onsen UI](https://github.com/OnsenUI/OnsenUI) if you need access to the framework itself.

## Installation

```bash
git clone --recurse-submodules git@github.com:OnsenUI/onsen.io.git
cd onsen.io
yarn global add gulp
yarn install

# Update necessary submodules
git submodule update --remote dist/v2/OnsenUI dist/playground

# Build Onsen UI
(cd dist/v2/OnsenUI/css-components && yarn install)
(cd dist/v2/OnsenUI && yarn install && yarn build)
(cd dist/v2/OnsenUI/bindings/react && yarn install && yarn gen-docs)
```

## How to Build

```bash
gulp generate --lang en
gulp generate --lang ja
```

## Edit & Serve

```bash
gulp serve --lang en
```

## Translate

```bash
1. $ gulp i18n-extract # This will generate POT files into src/i18n/gettext
2. And use PO editor to generate po files
3. $ gulp i18n-translate # This will translate and overwrite files into src/documents_ja/
```

## Releasing to Production
This repository is set up on [CircleCI](https://circleci.com/gh/OnsenUI/onsen.io) to automatically build with every commit. Commits to `master` are automatically deployed to the staging website at `s.onsen.io`. To deploy to production, merge `master` into the `production` branch. Typically this merge is done through a pull request on GitHub, even if you immediately merge it yourself. Once merged, CircleCI will automatically deploy everything in the `production` branch.

## How to contribute

We will happily accept contributions to the onsen.io website. However, it is much more likely that it is another repository that you want to make your pull request to. This repository contains the Onsen UI marketing materials, and brings in most of the rest of the content from other repositories.

### Which repository to choose?
Here is where you should make changes:

- Onsen UI Framework: https://github.com/OnsenUI/OnsenUI
- Documentation
  - API Definitions: https://github.com/OnsenUI/OnsenUI (see its [documentation guide](https://github.com/OnsenUI/OnsenUI/blob/master/CONTRIBUTING.md#documentation) for how this works)
  - Tutorials: https://github.com/OnsenUI/playground (taken from the examples in the Playground)
  - Guides: This repository
  - Design: This repository
- Theme Roller: https://github.com/OnsenUI/theme-roller
- Playground: https://github.com/OnsenUI/playground
- News Dropdown: https://github.com/OnsenUI/recent-activities
- Homepage Kitchensink Example: https://github.com/OnsenUI/vue-onsenui-kitchensink
- Onsen.io Landing Pages: This repository
- Community Forum: Runs on NodeBB and is not part of a repository on GitHub

### Contributing to this repository
If the repository you need to make changes to is this one, then the workflow is simple, and the same as you will see in other open source projects.

1. Fork the repository
2. Commit your changes. Please squash commits where possible.
3. Make a pull request to `master`.
4. After you've made a pull request we will review it. If everything is fine and we like the change the contribution will be pulled into the repository. In the case where there are some issues with the code or we disagree with how it's been implemented we will describe the issues in the comments so they can be corrected.
"
191,AlexanderSharykin/RepositoryUtilities,C#,"﻿Generates Github-style chart for Svn repository activity.

![](https://github.com/AlexanderSharykin/RepositoryUtilities/blob/master/ActivityChart.png)

SharpSvn is used to get Svn repository info
"
192,mzmine/mzmine2,Java,"# About MZmine 2
[![Build Status](https://travis-ci.org/mzmine/mzmine2.svg?branch=master)](https://travis-ci.org/mzmine/mzmine2)

MZmine 2 is an open-source software for mass-spectrometry data processing. The goals of the project is to provide a user-friendly, flexible and easily extendable software with a complete set of modules covering the entire MS data analysis workflow.

More information about the software can be found on the [MZmine](http://mzmine.github.io) website.


## License
MZmine 2 is a free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either [version 2](http://www.gnu.org/licenses/gpl-2.0.html) of the License, or (at your option) any [later version](http://www.gnu.org/licenses/gpl.html).

MZmine 2 is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.


## Development

### Tutorial

Please read our brief [tutorial](http://mzmine.github.io/development.html) on how to contribute new code to MZmine.

### Java version

MZmine development requires Java Development Kit (JDK) version 12 or newer (http://jdk.java.net).

### Building

To build the MZmine package from the sources, run the following command:

    ./gradlew

or

    gradlew.bat

The final MZmine distribution will be placed in build/MZmine-version-platform.zip

If you encounter any problems, please contact the developers:
https://github.com/mzmine/mzmine2/issues

### Code style

Since this is a collaborative project, please adhere to the following code formatting conventions:
* We use the Google Java Style Guide (https://github.com/google/styleguide)
* Please write JavaDoc comments as full sentences, starting with a capital letter and ending with a period. Brevity is preferred (e.g., ""Calculates standard deviation"" is preferred over ""This method calculates and returns a standard deviation of given set of numbers"").

"
193,jackiekazil/data-wrangling,HTML,"## Data Wrangling with Python

Welcome to the code repository for [Data Wrangling with Python](http://shop.oreilly.com/product/0636920032861.do)! We hope you find the code and data here useful. If you have any questions reach out to @kjam or @JackieKazil on Twitter or GitHub.

### Code Structure

We've kept all of the code samples in folders separated by chapters and the data in a similar fashion. You'll likely want to 'undo' this work. It will be far more useful for you to have the data all in one folder so you can easily import and use. Remember, storing data in a repository is usually not a good idea; we've done it here so you can replicate the work you see in the book.

### Code Examples

We have not included every code sample you've found in the book, but we have included a majority of the finished scripts. Although these are included, we encourage you to write out each code sample on your own and use these only as a reference.

We've also included some of the data investigation and IPython exploration used to first determine what to explore with the book. If you have any questions about the code you see in the book or the exploration conclusions, please reach out. Most of the exploration was performed using an older version of some of the libraries, so it might not work without modification.

#### Scraping Data Folders

In case the web pages the book uses change significantly, we've included copies of the web pages as they are now. You can use them and tell the scraping library to access them via a File URI (normally `file://file_name.html`).

 * Note: this has already occured with the Fairphone page. Please see `data/chp11/fairphone.html` to see the old page as it was in the book


### Firefox Issues

Depending on your version of Firefox and Selenium, you may run into JavaScript errors. Here are some fixes:
 * Use an older version of Firefox
 * Upgrade Selenium to >=3.0.2 and download the [geckodriver](https://github.com/mozilla/geckodriver/releases). Make sure the geckodriver is findable by your PATH variable. You can do this by adding this line to your `.bashrc` or `.bash_profile`. (Wondering what these are? Please read the Appendix C on learning the command line).
 * Use [PhantomJS](http://phantomjs.org/) with Selenium (change your browser line to `webdriver.PhantomJS('path/to/your/phantomjs/installation')`)
 * Use Chrome, InternetExplorer or any other [supported browser](http://www.seleniumhq.org/about/platforms.jsp)

Feel free to reach out if you have any questions!

### Corrections?

If you find any issues in these code examples, feel free to submit an Issue or Pull Request. We appreciate your input!

### Questions?

Reach out to @kjam and @JackieKazil on Twitter or GitHub. @kjam is also often on freenode. :)
"
194,haskell/hackage-server,Haskell,"# hackage-server

[![Build Status](https://travis-ci.org/haskell/hackage-server.png?branch=master)](https://travis-ci.org/haskell/hackage-server)

This is the `hackage-server` code. This is what powers <http://hackage.haskell.org>, and many other private hackage instances. The `master` branch is suitable for general usage. Specific policy and documentation for the central hackage instance exists in the `central-server` branch.

## Installing ICU

ICU stands for ""International Components for Unicode"". The `icu4c` is a set
of libraries that provide Unicode and Globalization support.
The [text-icu](https://hackage.haskell.org/package/text-icu) Haskell package
uses the [icu4c](http://icu-project.org/apiref/icu4c/) library to build.

You'll need to do the following to get hackage-server's dependency `text-icu` to build:

### Mac OS X

    brew install icu4c
    brew link icu4c --force

Besides that, you might also need to include these in the `cabal.project.local` you created:

```
package text-icu
  extra-include-dirs: /usr/local/opt/icu4c/include
  extra-lib-dirs: /usr/local/opt/icu4c/lib
```

### Ubuntu/Debian

    sudo apt-get update
    sudo apt-get install unzip libicu-dev

### Fedora/CentOS
    sudo dnf install unzip libicu-devel

## Setting up security infrastructure

Out of the box the server comes with some example keys and TUF metadata. The
example keys are in `example-keys/`; these keys were used to create

    datafiles/TUF/root.json
    datafiles/TUF/mirrors.json
    datafiles/TUF/timestamp.private
    datafiles/TUF/snapshot.private

While these files will enable you to start the server without doing anything
else, you should replace all these files before deploying your server. In the
remainder of this section we will explain how to do that.

The first step is to create your own keys using the
[hackage-repo-tool](http://hackage.haskell.org/package/hackage-repo-tool):

    hackage-repo-tool create-keys --keys /path/to/keys

Then copy over the timestamp and snapshot keys to the TUF directory:

    cp /path/to/keys/timestamp/<id>.private datafiles/TUF/timestamp.private
    cp /path/to/keys/snapshot/<id>.private  datafiles/TUF/snapshot.private

Create root information:

    hackage-repo-tool create-root --keys /path/to/keys -o datafiles/TUF/root.json

And finally create a list of mirrors (this is necessary even if you don't have
any mirrors):

    hackage-repo-tool create-mirrors --keys /path/to/keys -o datafiles/TUF/mirrors.json

The `create-mirrors` command takes a list of mirrors as additional arguments if
you do want to list mirrors.

In order for secure clients to bootstrap the root security metadata from your
server, you will need to provide them with the public key IDs of your root keys;
you can find these as the file names of the files created in
`/path/to/keys/root` (as well as in the generated root.json under the
`signed.roles.root.keyids`). An example `cabal` client configuration might look
something like

    remote-repo my-private-hackage
      url: http://example.com:8080/
      secure: True
      root-keys: 865cc6ce84231ccc990885b1addc92646b7377dd8bb920bdfe3be4d20c707796
                 dd86074061a8a6570348e489aae306b997ed3ccdf87d567260c4568f8ac2cbee
                 e4182227adac4f3d0f60c9e9392d720e07a8586e6f271ddcc1697e1eeab73390
      key-threshold: 2

## Running

    cabal install -j --enable-tests

    hackage-server init
    hackage-server run

If you want to run the server directly from the build tree, run

    cabal v2-run -- hackage-server init

once to initialise the state. After that you can run the server with

    cabal v2-run -- hackage-server run --static-dir=datafiles/ --base-uri=http://127.0.0.1:8080

By default the server runs on port `8080` with the following settings:

    URL:      http://localhost:8080/
    username: admin
    password: admin

To specify something different, see `hackage-server init --help` for details.

The http://127.0.0.1:8080/packages/uploaders/edit is usel to add users
(e.g. `admin`) to *Uploaders* group.

The server can be stopped by using `Control-C`.

This will save the current state and shutdown cleanly. Running again
will resume with the same state.

### Resetting

To reset everything, kill the server and delete the server state:

```bash
rm -rf state/
```

Note that the `datafiles/` and `state/` directories differ:
`datafiles` is for static html, templates and other files.
The `state` directory holds the database (using `acid-state`
and a separate blob store).

### Creating users & uploading packages

* Admin front-end: <http://localhost:8080/admin>
* List of users: <http://localhost:8080/users/>
* Register new users: <http://localhost:8080/users/register>

Currently there is no restriction on registering, but only an admin
user can grant privileges to registered users e.g. by adding them to
other groups. In particular there are groups:

 * admins `http://localhost:8080/users/admins/` -- administrators can
   do things with user accounts like disabling, deleting, changing
   other groups etc.
 * trustees `http://localhost:8080/packages/trustees/` -- trustees can
   do janitorial work on all packages
 * mirrors `http://localhost:8080/packages/mirrorers/` -- for special
   mirroring clients that are trusted to upload packages
 * per-package maintainer groups
   `http://localhost:8080/package/foo/maintainers` -- users allowed to
   upload packages
 * uploaders `http://localhost:8080/packages/uploaders/` -- for
   uploading new packages

### Mirroring

There is a client program included in the hackage-server package called
hackage-mirror. It's intended to run against two servers, syncing all the
packages from one to the other, e.g. getting all the packages from the old
hackage and uploading them to a local instance of a hackage-server.

To try it out:

1. On the target server, add a user to the mirrorers group via
   http://localhost:8080/packages/mirrorers/
1. Create a config file that contains the source and target
   servers. Assuming you are cloning the packages on
   <http://hackage.haskell.org> locally, create the file servers.cfg:
```
source ""hackage""
  uri: http://hackage.haskell.org
  type: secure

target ""mirror""
  uri: http://admin:admin@localhost:8080
  type: hackage2

  post-mirror-hook: ""shell command to execute""
```
Recognized types are hackage2, secure and local. The target server name was displayed when you ran.

Note, the target must _not_ have a trailing slash, or confusion will tend to occur. Additionally, if you have ipv6 setup on the machine, you may need to replace `localhost` with `127.0.0.1`.

Also note that you should mirror _from_ hackage2 or secure typically and mirror _to_ hackage2. Only mirroring from secure will include dependency revision information.

```bash
   hackage-server run.
```

1. Run the client, pointing to the config file:

```bash
hackage-mirror servers.cfg
```

This will do a one-time sync, and will bail out at the first sign of
trouble. You can also do more robust and continuous mirroring. Use the
flag `--continuous`. It will sync every 30 minutes (configurable with
`--interval`). In this mode it carries on even when some packages
cannot be mirrored for some reason and remembers them so it doesn't
try them again and again. You can force it to try again by deleting
the state files it mentions.
"
195,haskell/hackage-server,Haskell,"# hackage-server

[![Build Status](https://travis-ci.org/haskell/hackage-server.png?branch=master)](https://travis-ci.org/haskell/hackage-server)

This is the `hackage-server` code. This is what powers <http://hackage.haskell.org>, and many other private hackage instances. The `master` branch is suitable for general usage. Specific policy and documentation for the central hackage instance exists in the `central-server` branch.

## Installing ICU

ICU stands for ""International Components for Unicode"". The `icu4c` is a set
of libraries that provide Unicode and Globalization support.
The [text-icu](https://hackage.haskell.org/package/text-icu) Haskell package
uses the [icu4c](http://icu-project.org/apiref/icu4c/) library to build.

You'll need to do the following to get hackage-server's dependency `text-icu` to build:

### Mac OS X

    brew install icu4c
    brew link icu4c --force

Besides that, you might also need to include these in the `cabal.project.local` you created:

```
package text-icu
  extra-include-dirs: /usr/local/opt/icu4c/include
  extra-lib-dirs: /usr/local/opt/icu4c/lib
```

### Ubuntu/Debian

    sudo apt-get update
    sudo apt-get install unzip libicu-dev

### Fedora/CentOS
    sudo dnf install unzip libicu-devel

## Setting up security infrastructure

Out of the box the server comes with some example keys and TUF metadata. The
example keys are in `example-keys/`; these keys were used to create

    datafiles/TUF/root.json
    datafiles/TUF/mirrors.json
    datafiles/TUF/timestamp.private
    datafiles/TUF/snapshot.private

While these files will enable you to start the server without doing anything
else, you should replace all these files before deploying your server. In the
remainder of this section we will explain how to do that.

The first step is to create your own keys using the
[hackage-repo-tool](http://hackage.haskell.org/package/hackage-repo-tool):

    hackage-repo-tool create-keys --keys /path/to/keys

Then copy over the timestamp and snapshot keys to the TUF directory:

    cp /path/to/keys/timestamp/<id>.private datafiles/TUF/timestamp.private
    cp /path/to/keys/snapshot/<id>.private  datafiles/TUF/snapshot.private

Create root information:

    hackage-repo-tool create-root --keys /path/to/keys -o datafiles/TUF/root.json

And finally create a list of mirrors (this is necessary even if you don't have
any mirrors):

    hackage-repo-tool create-mirrors --keys /path/to/keys -o datafiles/TUF/mirrors.json

The `create-mirrors` command takes a list of mirrors as additional arguments if
you do want to list mirrors.

In order for secure clients to bootstrap the root security metadata from your
server, you will need to provide them with the public key IDs of your root keys;
you can find these as the file names of the files created in
`/path/to/keys/root` (as well as in the generated root.json under the
`signed.roles.root.keyids`). An example `cabal` client configuration might look
something like

    remote-repo my-private-hackage
      url: http://example.com:8080/
      secure: True
      root-keys: 865cc6ce84231ccc990885b1addc92646b7377dd8bb920bdfe3be4d20c707796
                 dd86074061a8a6570348e489aae306b997ed3ccdf87d567260c4568f8ac2cbee
                 e4182227adac4f3d0f60c9e9392d720e07a8586e6f271ddcc1697e1eeab73390
      key-threshold: 2

## Running

    cabal install -j --enable-tests

    hackage-server init
    hackage-server run

If you want to run the server directly from the build tree, run

    cabal v2-run -- hackage-server init

once to initialise the state. After that you can run the server with

    cabal v2-run -- hackage-server run --static-dir=datafiles/ --base-uri=http://127.0.0.1:8080

By default the server runs on port `8080` with the following settings:

    URL:      http://localhost:8080/
    username: admin
    password: admin

To specify something different, see `hackage-server init --help` for details.

The http://127.0.0.1:8080/packages/uploaders/edit is usel to add users
(e.g. `admin`) to *Uploaders* group.

The server can be stopped by using `Control-C`.

This will save the current state and shutdown cleanly. Running again
will resume with the same state.

### Resetting

To reset everything, kill the server and delete the server state:

```bash
rm -rf state/
```

Note that the `datafiles/` and `state/` directories differ:
`datafiles` is for static html, templates and other files.
The `state` directory holds the database (using `acid-state`
and a separate blob store).

### Creating users & uploading packages

* Admin front-end: <http://localhost:8080/admin>
* List of users: <http://localhost:8080/users/>
* Register new users: <http://localhost:8080/users/register>

Currently there is no restriction on registering, but only an admin
user can grant privileges to registered users e.g. by adding them to
other groups. In particular there are groups:

 * admins `http://localhost:8080/users/admins/` -- administrators can
   do things with user accounts like disabling, deleting, changing
   other groups etc.
 * trustees `http://localhost:8080/packages/trustees/` -- trustees can
   do janitorial work on all packages
 * mirrors `http://localhost:8080/packages/mirrorers/` -- for special
   mirroring clients that are trusted to upload packages
 * per-package maintainer groups
   `http://localhost:8080/package/foo/maintainers` -- users allowed to
   upload packages
 * uploaders `http://localhost:8080/packages/uploaders/` -- for
   uploading new packages

### Mirroring

There is a client program included in the hackage-server package called
hackage-mirror. It's intended to run against two servers, syncing all the
packages from one to the other, e.g. getting all the packages from the old
hackage and uploading them to a local instance of a hackage-server.

To try it out:

1. On the target server, add a user to the mirrorers group via
   http://localhost:8080/packages/mirrorers/
1. Create a config file that contains the source and target
   servers. Assuming you are cloning the packages on
   <http://hackage.haskell.org> locally, create the file servers.cfg:
```
source ""hackage""
  uri: http://hackage.haskell.org
  type: secure

target ""mirror""
  uri: http://admin:admin@localhost:8080
  type: hackage2

  post-mirror-hook: ""shell command to execute""
```
Recognized types are hackage2, secure and local. The target server name was displayed when you ran.

Note, the target must _not_ have a trailing slash, or confusion will tend to occur. Additionally, if you have ipv6 setup on the machine, you may need to replace `localhost` with `127.0.0.1`.

Also note that you should mirror _from_ hackage2 or secure typically and mirror _to_ hackage2. Only mirroring from secure will include dependency revision information.

```bash
   hackage-server run.
```

1. Run the client, pointing to the config file:

```bash
hackage-mirror servers.cfg
```

This will do a one-time sync, and will bail out at the first sign of
trouble. You can also do more robust and continuous mirroring. Use the
flag `--continuous`. It will sync every 30 minutes (configurable with
`--interval`). In this mode it carries on even when some packages
cannot be mirrored for some reason and remembers them so it doesn't
try them again and again. You can force it to try again by deleting
the state files it mentions.
"
196,mp2893/doctorai,Python,"Doctor AI
=========================================

Doctor AI is a automatic diagnosis machine that predicts medical codes that occur in the next visit, while also predicting the time duration until the next visit.

#### Relevant Publications

Doctor AI implements an algorithm introduced in the following:

	Doctor AI: Predicting Clinical Events via Recurrent Neural Networks  
	Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, Jimeng Sun  
	arXiv preprint arXiv:1511.05942
	
	Medical Concept Representation Learning from Electronic Health Records and its Application on Heart Failure Prediction  
	Edward Choi, Andy Schuetz, Walter F. Stewart, Jimeng Sun  
	arXiv preprint arXiv:1602.03686

#### Running Doctor AI

**STEP 1: Installation**  

1. Install [python](https://www.python.org/), [Theano](http://deeplearning.net/software/theano/index.html). We use Python 2.7, Theano 0.7. Theano can be easily installed in Ubuntu as suggested [here](http://deeplearning.net/software/theano/install_ubuntu.html#install-ubuntu)

2. If you plan to use GPU computation, install [CUDA](https://developer.nvidia.com/cuda-downloads)

3. Download/clone the Doctor AI code  

**STEP 2: Preparing training data**  

0. You can use ""process_mimic.py"" to process MIMIC-III dataset and generate a suitable training dataset for Doctor AI. Place the script to the same location where the MIMIC-III CSV files are located, and run the script. Instructions are described inside the script. However, I recommend the readers to read the following steps to understand the structure of the training data and learn how to prepare their own dataset.

1. Doctor AI's training dataset needs to be a Python Pickled list of list of list. Each list corresponds to patients, visits, and medical codes (e.g. diagnosis codes, medication codes, procedure codes, etc.)
First, medical codes need to be converted to an integer. Then a single visit can be seen as a list of integers. Then a patient can be seen as a list of visits.
For example, [5,8,15] means the patient was assigned with code 5, 8, and 15 at a certain visit.
If a patient made two visits [1,2,3] and [4,5,6,7], it can be converted to a list of list [[1,2,3], [4,5,6,7]].
Multiple patients can be represented as [[[1,2,3], [4,5,6,7]], [[2,4], [8,3,1], [3]]], which means there are two patients where the first patient made two visits and the second patient made three visits.
This list of list of list needs to be pickled using cPickle. We will refer to this file as the ""visit file"".

2. The total number of unique medical codes is required to run Doctor AI.
For example, if the dataset is using 14,000 diagnosis codes and 11,000 procedure codes, the total number is 25,000. 

3. The label dataset (let us call this ""label file"") needs to have the same format as the ""visit file"".
The important thing is, time steps of both ""label file"" and ""visit file"" need to match. DO NOT train Doctor AI with labels that is one time step ahead of the visits. It is tempting since Doctor AI predicts the labels of the next visit. But it is internally taken care of.
You can use the ""visit file"" as the ""label file"" if you want Doctor AI to predict the exact codes. 
Or you can use a grouped codes as the ""label file"" if you are okay with reasonable predictions and want to save time. 
For example, ICD9 diagnosis codes can be grouped into 283 categories by using [CCS](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp) groupers. 
We STRONGLY recommend you do this, because the number of medical codes can be as high as tens of thousands, 
which can cause not only low predictive performance but also memory issues. (The high-end GPUs typically have only 12GB of VRAM)

4. Same as step 2, you will need to remember the total number of unique codes in the ""label file"".
If you are using ""visit file"" as the ""label file"", than the number of unique codes will be the same, of course.

5. The ""visit file"" and ""label file"" need to have 3 sets respectively: training set, validation set, and test set.
The file extension must be "".train"", "".valid"", and "".test"" respectivley.  
For example, if you want to use a file named ""my_visit_sequences"" as the ""visit file"", then Doctor AI will try to load ""my_visit_sequences.train"", ""my_visit_sequences.valid"", and ""my_visit_sequences.test"".  
This is also true for the ""label file""

5. You can use the time duration between visits as an additional source of information. Let us call this ""time file"".
""time file"" needs to be prepared as a Python Pickled List of List. Each list corresponds to patients and the duration between each visit.
For example, given a ""visit file"" [[[1,2,3], [4,5,6,7]], [[2,4], [8,3,1], [3]]], its corresponding ""time file"" should look like [[0, 15], [0, 45, 23]].
Of course, the numbers are fake, but the important thing is that the duration for the first visit needs to be zero. 
Use ""--time\_file"" option to use ""time file""
Remember that the "".train"", "".valid"", "".test"" rule also applies to the ""time file"" as well.

**Additional: Predicting time duration until next visit**  
In addtion to predicting the codes of the next visit, you can make Doctor AI predict the time duration until next visit. 
Use ""--predict\_time"" option to do this. And obviously, predicting time requires the ""time file"".  
Time prediction also comes with many hyperparameters such as ""--tradeoff"", ""--L2\_time"", ""--use\_log\_time"". 
Refer to ""--help"" for more detailed information

**Additional: Using your own medical code representations**  
Doctor AI internally learns vector representation of medical codes while training. These vectors are initialized with random values of course.  
You can, however, also provide medical code representations, if you have one. (They can be easily trained by using Skip-gram like algorithms.)
If you want to provide the medical code representations, it has to be a list of list (basically a matrix) of N rows and M columns where N is the number of unique codes in your ""visit file"" and M is the size of the code representations.
Specify the path to your code representation file using ""--embed\_file"".  
For more details regarding the training of medical code representations and using them for predictive tasks, please refer to the second paper of the ""Related Publication"" section.  
Additionally even if you provided your own medical code representations, you can re-train (a.k.a fine-tune) them as you train Doctor AI. 
Use ""--embed\_finetune"" option to do this. If you are not providing your own medical code representations, Doctor AI will use randomly initialized one, which obviously requires this fine-tuning process. Since the default is to use the fine-tuning, you do not need to worry about this.

**STEP 3: Running Doctor AI**  

1. The minimum input you need to run Doctor AI is the ""visit file"", the number of unique medical codes in the ""visit file"", 
the ""label file"", the number of unique medical codes in the ""label file"", and the output path. The output path is where the learned weights will be saved.  
`python doctorAI.py <visit file> <# codes in the visit file> <label file> <# codes in the label file> <output path>`  

2. Specifying `--verbose` option will print training process after each 10 mini-batches.

3. You can specify how many GRU layers you want to use by using ""--hidden\_dim\_size"" option.
For example ""--hidden\_dim\_size \[400,200\]"" will give you a two layer GRU where the lower layer uses a 400-dimensional hidden layer 
and the upper layer uses a 200-dimensional hidden layer.

4. Additional options can be specified such as the size of the embedding layer, batch size, the number of epochs, dropout rate, etc. Detailed information can be accessed by `python doctorAI.py --help`

**STEP 4: Getting your results**  

Doctor AI checks the validation cross entropy after each epoch, and if it is lower than all previous value, it will save the current model. The model file is generated by [numpy.savez_compressed](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.savez_compressed.html).

**Step 5: Testing your model**

1. Using the file ""testDoctorAI.py"", you can calculate the recall@10,20,30 for the code prediction and R^2 for the time prediction. First you need to have a trained model that was saved by numpy.savez\_compressed. Note that you need to know the configuration with which you trained Doctor AI (use of ""time file"", use of ""--use\_log\_time"", value of ""--hidden\_dim\_size"", etc.)

2. Again, you need the ""visit file"" and ""label file"" prepared in the same way. This time, however, you do not need to follow the "".train"", "".valid"", "".test"" rule. The testing script will try to load the file name as given.

3. Using additional options such as ""--hidden\_dim\_size"" and ""--use\_log\_time"", you should use exactly the same configuration with which you trained the model. For more detailed information, use ""--help"" option.

4. To evaluate the time prediction performance, we provide R^2 error. In order to calculate this, you need to provide the mean value of all durations in the ""time file"" you used to train Doctor AI. (You must ignore the 0 duration of the first visits or course) Use ""--mean\_duration"" option to do this.

5. The minimum input to run the testing script is the ""model file"", ""visit file"", ""label file"", and ""hidden dim size"".  
`python testDoctorAI.py <model file> <visit file> <label file> <hidden_dim_size>`
"
197,SwiftPackageIndex/PackageList,Swift,"![Valid JSON](https://github.com/SwiftPackageIndex/PackageList/workflows/Valid%20JSON/badge.svg)

# The Swift Package Index

Anyone can add a package to the [Swift Package Index](https://swiftpackageindex.com). Every package indexed by the site comes from a list of package repository URLs, stored in a [publicly available JSON file](https://github.com/SwiftPackageIndex/PackageList/blob/main/packages.json). To add a package to the index, add a URL to a package repository to that file.

Please feel free to submit your own, or other people's repositories to this list. There are a few requirements, but they aren't onerous.

The easiest way to validate that packages meet the requirements is to run the validation tool included in this repository. Fork [this repository](https://github.com/SwiftPackageIndex/PackageList/) and clone your fork locally. Then edit `packages.json` and add the package URL(s) to the JSON. Finally, in the directory where you have the clone of your fork of this repository, run the following command:

```shell
swift ./validate.swift
```

When validation succeeds, commit your changes and submit your pull request! Your package(s) will appear in the index within a few minutes.

---

If you would prefer to validate the requirements manually, please verify that:

* The package repositories are all publicly accessible.
* The packages all contain a `Package.swift` file in the root folder.
* The packages are written in Swift 4.0 or later.
* The packages all contain at least one product (either library or executable), and at least one product must be usable in other Swift apps.
* The packages all have at least one release tagged as a [semantic version](https://semver.org/).
* The packages all output valid JSON from `swift package dump-package` with the latest Swift toolchain.
* The package URLs are all fully specified including the protocol (usually `https`) and the `.git` extension.
* The packages all compile without errors.
* The packages JSON file is sorted alphabetically.

**Note:** There's no gatekeeping or quality threshold to be included in the [Swift Package Index](https://swiftpackageindex.com). As long as packages are valid, and meet the requirements above, we will accept them.
"
198,openstreetmap/josm,Java,"Supplemental information for JOSM -- the Java OpenStreetMap Editor

=============================================================================
            I. Install & Launch
=============================================================================

Installation notes
------------------
To run JOSM, you need:

* The JOSM .jar file, e.g., josm-tested.jar or josm-latest.jar
* Java Runtime Environment (JRE) 8, or later.


How to get Java Runtime Environment
-----------------------------------
You need JRE Version 8, or later.

Microsoft Windows and Apple Mac OS X users should visit https://www.java.com
and download the latest Java executable for their system.

Linux users should visit http://www.oracle.com/technetwork/java/index.html
There is a Linux binary installer, which you must execute from a console, or
use the mechanism of your distribution's packaging system.


How to launch
-------------
Microsoft Windows users launch by double-clicking on the .jar file.
If this does not work, open a command shell and type
""java -jar josm-latest.jar""  in the directory that holds the file. (Please
replace josm-latest.jar with the name of your .jar file, if you aren't using
the latest version.)

Under Linux, open a shell, go to the file directory and type
""java -jar josm-latest.jar"" to launch. If this does not work, try to set
your JAVA_HOME variable to the java executable location (the root location,
not the bin).

macOS users just click on the .jar file icon.

=============================================================================
            II. Development
=============================================================================

How to get the source code
--------------------------
Download it directly from the subversion at
https://josm.openstreetmap.de/svn/trunk. To use the command line subversion
client, type

svn co https://josm.openstreetmap.de/svn/trunk josm


Files & directories
-------------------
This is an overview of the files and directories in the JOSM code repository:
- build.xml                 ant build file (standard way to create a JOSM binary)
- CONTRIBUTION              list of major code contributors
- gpl-2.0.txt, gpl-3.0.txt  full text of the GNU General Public License
- ide                       IDE-specific files
  - eclipse/                preconfigured Eclipse configuration files
  - netbeans/               preconfigured Netbeans project
- josm.jnlp                 Java Web Start launcher file (used on the website for the tested version)
- josm-latest.jnlp          Java Web Start launcher file (used on the website for the latest version)
- LICENSE                   the JOSM license terms
- native/                   OS-specific files
  - linux/                  files useful for Linux distributions, including Appdata files, .desktop
                            files, Debian/Ubuntu scripts, man pages, icons, etc.
  - macosx/                 files needed to create the MacOS X package
  - windows/                files needed to create the Windows installer
- nodist/                   files not included in JOSM binary
  - data/                   data files that are useful for development, but not distributed
    - exif-direction-example.jpg
                            sample image, that contains direction information in the EXIF header
                            (keys: Exif.GPSInfo.GPSImgDirectionRef, Exif.GPSInfo.GPSImgDirection)
    - filterTests.osm       used for unit testing of the filter feature
                            (see test/unit/org/openstreetmap/josm/data/osm/FilterTest.java)
    - Join_Areas_Tests.osm  some examples to test the 'join areas' feature
    - *.*                   various other data files used for unit testing and as reference file
    - projection/           projection files
      - *.gsb               NTv2 grid files for projection support, downloaded by the
                            client on demand (see CONTRIBUTION)
      - CHENyx06-Distribution.pdf
                            archive of terms of use for the CHENyx06.gsb file
      - epsg                EPSG data file, taken from the proj.4 project
                            (see https://github.com/OSGeo/proj.4/blob/master/nad/epsg)
      - esri                ESRI data file, taken from the proj.4 project
                            (see https://github.com/OSGeo/proj.4/blob/master/nad/esri)
      - josm-epsg           customizations to the epsg file, used together with the epsg file
                            to generate data/projection/custom-epsg
                            
      - projection-reference-data.csv
                            reference data for projection tests
                            (see test/unit/org/openstreetmap/josm/data/projection/ProjectionRefTest.java)
      - projection-regression-test-data.csv
                            regression data for projection tests
                            (see test/unit/org/openstreetmap/josm/data/projection/ProjectionRegressionTest.java)
    - trans/*.lang          translation data for files that are not distributed, but used
                            by the server for localization of the services;
                            currently contains plugin descriptions in order to include translations
                            in the downloaded plugin list
  - images/                 images, which are not for distribution, but may be useful later (e.g. high
                            resolution and vector versions)
  - styles/                 files needed for map style maintenance
    - potlatch2/README      infos on how to update the Potlatch 2 style from upstream sources
- README                    this file
- resources/                resource files that will be included in the JOSM jar file
  - data/                   data files that will be included in the JOSM jar file
    - fonts/                font files used for map rendering
    - gpx/                  different color gradients for gpx drawing
    - projection/           projection files
      - custom-epsg         list of projection definitions, auto-generated file created by ant task 'epsg'
    - security/*.pem        certificates that we like to accept for TLS connections, but are missing in the
                            default Java certificate store
    - validator/            data files used by the JOSM validator
      - *.cfg               files designed for the old tagchecker, still in use
      - *.mapcss            default validation rules for the MapCSS-based tagchecker
    - boundaries.osm        OSM file containing boundary data for the states of the earth, including
                            data for right and left-hand traffic
    - defaultpresets.xml    data file for the core tagging presets
    - help-browser.css      CSS file for the help sites (HTML content is downloaded from the website
                            on demand, but displayed inside the programm in a Java web browser component.)
    - *.lang                translation data
    - *.xsd                 xml schema files for validation of configuration files
  - images/                 images distributed with the JOSM binary
    - icons                 images for the Potlatch 2 style
    - presets               images for the main mappaint style and the internal presets
  - styles/                 map styles included in JOSM
- scripts/                  various scripts used by JOSM developers
  - BuildProjectionDefinitions.java
                            called from the ant build file to combine the files epsg and josm-epsg
                            to create the custom-epsg file for distribution
  - geticons.pl             tool to find all used icons and allows deleting unused icons
                            searches also for images with incompatible svg code
  - optimize-images         short script to decrease size of PNG images
  - since_xxx.py            developer tool to replace ""@since xxx"" in Javadoc by the upcoming revision number
  - SyncEditorLayerIndex.java
                            script to compare and analyse the differences of the editor layer index and the
                            JOSM imagery list (see https://josm.openstreetmap.de/wiki/ImageryCompare)
  - TagInfoExtract.java     extracts tag information for the taginfo project
- src/                      the source code of the application
- start.html                HTML page to run the applet version of JOSM
- test/                     automated software tests
    - data/                 resources used for some tests
    - functional/           functional tests (source code)
    - lib/                  libraries needed for (some of) the tests, including JUnit
    - performance/          performance tests (source code)
    - unit/                 unit tests (source code)
- tools/                    libraries and tools that help in the development process
    - animal-sniffer-ant-tasks.jar
                            used to build and check code signatures to ensure plugins binary compatibility 
    - appbundler-1.0ea.jar  used to build Mac OS X package
    - checkstyle/           libs and config files for checkstyle (automatically detects code style
                            problems in source code); can be launched as an ant target in build.xml
    - ivy/                  Apache Ivy binary, configuration file, and downloaded dependencies 
    - jacocoant.jar         used to include coverage data into JUnit test reports
    - japicc/               used to generate a compatibility report between optimized jar and normal one
    - spotbugs/             libs and config files for spotbugs (automatically detects common bugs and potential
                            problems in source code); can be launched as an ant target in build.xml
    - xmltask.jar           used to edit XML files from Ant for the OSX package

Third party libraries
---------------------
There are some third party libraries which are directly included in the source code tree, in particular:

* jmapviewer: Java component to browse a TMS map
    src/org/openstreetmap/gui (svn external)
    -> https://josm.openstreetmap.de/osmsvn/applications/viewer/jmapviewer/
* Apache commons compress: Support for bzip2 compression when opening files
    -> https://github.com/apache/commons-compress
* Apache commons validator: Improved validator routines
    src/org/openstreetmap/josm/data/validation/routines
    -> http://commons.apache.org/proper/commons-validator
* SVG Salamander: Support for SVG image format
    src/com/kitfox/svg
    -> https://github.com/blackears/svgSalamander
* Metadata Extractor: Read EXIF Metadata of photos
    -> https://github.com/drewnoakes/metadata-extractor
* Signpost: OAuth library
    -> https://github.com/mttkay/signpost
* MultiSplitPane: Small lib for GUI layout management
    src/org/openstreetmap/josm/gui/MultiSplitLayout.java, MultiSplitPane.java
    -> https://github.com/floscher/multi-split
    -> https://community.oracle.com/docs/DOC-983539
* swinghelper: Class CheckThreadViolationRepaintManager to find EDT violations
    src/org/openstreetmap/josm/gui/util/CheckThreadViolationRepaintManager.java
    -> https://github.com/floscher/swinghelper
* xz extractor
    -> https://tukaani.org/xz/java.html
* OpeningHoursParser (MIT license)
    -> https://github.com/simonpoole/OpeningHoursParser
"
199,shihenw/convolutional-pose-machines-release,Jupyter Notebook,"# Convolutional Pose Machines
Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh, ""[Convolutional Pose Machines](http://arxiv.org/abs/1602.00134)"", CVPR 2016.

This project is licensed under the terms of the GPL v2 license. By using the software, you are agreeing to the terms of the [license agreement](https://github.com/shihenw/convolutional-pose-machines-release/blob/master/LICENSE).

Contact: Shih-En Wei (weisteady@gmail.com)

![Teaser?](https://github.com/shihenw/convolutional-pose-machines-release/blob/master/testing/python/figures/teaser-github.png)

## Recent Updates
- Synced our fork of caffe with most recent version (Dec. 2016) so that Pascal GPUs can work (tested with CUDA 8.0 and CUDNN 5).
- Including a VGG-pretrained model in matlab (and also python) code. This model was used in CVPR'16 demo. It scores 90.1% on MPI test set, and can be trained in much shorter time than previous models.
- We are working on [releasing code](https://github.com/ZheC/Multi-Person-Pose-Estimation/) of our [new work in multi-person pose estimation](https://arxiv.org/abs/1611.08050) demonstrated in ECCV'16 (best demo award!).

## Before Everything
- Watch some [videos](https://www.youtube.com/playlist?list=PLNh5A7HtLRcpsMfvyG0DED-Dr4zW5Lpcg).
- Install [Caffe](http://caffe.berkeleyvision.org/). If you are interested in training this model on your own machines, or realtime systems, please use [our version](https://github.com/shihenw/caffe) (a submodule in this repo) with customized layers. Make sure you have compiled python and matlab interface. This repository at least runs on Ubuntu 14.04, OpenCV 2.4.10, CUDA 8.0, and CUDNN 5. The following assumes you use `cmake` to compile caffe in `<repo path>/caffe/build`.
[//]: # (- Copy `caffePath.cfg.example` to `caffePath.cfg` and set your own path in it.)
- Include `<repo path>/caffe/build/install/lib` in environment variable `$LD_LIBRARY_PATH`.
- Include `<repo path>/caffe/build/install/python` in environment variable `$PYTHONPATH`.

## Testing
First, run `testing/get_model.sh` to retreive trained models from our web server.

### Python
- This [demo file](https://github.com/shihenw/convolutional-pose-machines-release/blob/master/testing/python/demo.ipynb) shows how to detect multiple people's poses as we demonstrated in CVPR'16. For real-time performance, please read it for further explanation.

### Matlab
- 1. `CPM_demo.m`: Put the testing image into `sample_image` then run it! You can select models (we provided 4) or other parameters in `config.m`. If you just want to try our best-scoring model, leave them default.
- 2. `CPM_benchmark.m`: Run the model on test benchmark and see the scores. Prediction files will be saved in `testing/predicts`.


## Training
- Run `get_data.sh` to get datasets including [FLIC Dataset](http://vision.grasp.upenn.edu/cgi-bin/index.php?n=VideoLearning.FLIC), [LEEDS Sport Dataset](http://www.comp.leeds.ac.uk/mat4saj/lsp.html) and its [extended training set](http://www.comp.leeds.ac.uk/mat4saj/lspet.html), and [MPII Dataset](http://human-pose.mpi-inf.mpg.de/).
- Run `genJSON(<dataset_name>)` to generate a json file in `training/json/` folder (you'll have to create it). Dataset name can be `MPI`, `LEEDS`, or `FLIC`. The json files contain raw informations needed for training from each individual dataset.
- Run `python genLMDB.py` to generate LMDBs for CPM data layer in [our caffe](https://github.com/shihenw/caffe). Change the main function to select dataset, and note that you can generate a LMDB with multiple datasets.
- Run `python genProto.py` to get prototxt for caffe. Read [further explanation](https://github.com/shihenw/caffe) for layer parameters.
- Train with generated prototxts and collect caffemodels.

## Related Repository
- [Convolutional Pose Machines in Tensorflow](https://github.com/psycharo/cpm)

## Citation
Please cite CPM in your publications if it helps your research:

    @inproceedings{wei2016cpm,
        author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},
        booktitle = {CVPR},
        title = {Convolutional pose machines},
        year = {2016}
    }
"
200,ace-lectures/pattern-repository,Java,"# Catalogue des patrons de conception du GoF

  * Auteur : Sébastien Mosser (UQAM)
  * Contributeurs: Mireille Blay-Fornarino (UCA), Philippe Collet (UCA)
  * Version : 2019.03
  * Intégration continue : [![Build Status](https://travis-ci.com/ace-lectures/pattern-repository.svg?branch=master)](https://travis-ci.com/ace-lectures/pattern-repository)

Ce dépôt est un support au cours INF-5153 de l'Université du Québec à Montréal. Il recense des implémentation en Java des patrons de conceptions vu dans ce cours. Il est créé en collaboration avec l'Université Côte d'Azur (IUT & Polytech).

## Patrons disponibles

### Création

Patron de conception | Description | Code d'exemple | Diapositives |
---------------------| :---------: | :------------: | :----------: |
[Abstract Factory](./creation/abstract_factory) | :white_check_mark: | :x: | :x: |
[Builder](./creation/builder) | :white_check_mark: | :white_check_mark: | :x: |
[Factory](./creation/factory) | :white_check_mark: | :x: | :x: |
[Prototype](./creation/prototype) | :white_check_mark: | :white_check_mark: | :x: |
[Singleton](./creation/singleton) | :white_check_mark: | :white_check_mark: | :x: |


### Structure

Patron de conception | Description | Code d'exemple | Diapositives |
---------------------| :---------: | :------------: | :----------: |
[Adapter](./structure/adapter) | :white_check_mark: | :x: | :x: |
[Composite](./structure/composite) | :white_check_mark: | :white_check_mark: | :x: |
[Decorator](./structure/decorator) | :white_check_mark: | :white_check_mark: | :x: |
[Facade](./structure/facade) | :white_check_mark: | :x: | :x: |
[Proxy](./structure/proxy) | :white_check_mark: | :white_check_mark: | :x: |

### Comportement

Patron de conception | Description | Code d'exemple | Diapositives |
---------------------| :---------: | :------------: | :----------: |
[Command](./comportement/command) | :white_check_mark: | :x: | :x: |
[Observer](./comportement/observer) | :white_check_mark: | :white_check_mark: | :x: |
[State](./comportement/state) | :white_check_mark: | :white_check_mark: | :x: |
[Strategy](./comportement/strategy) | :white_check_mark: | :white_check_mark: | :x: |
[Template Method](./comportement/template_method) | :white_check_mark: | :white_check_mark: | :x: |
[Visitor](./comportement/visitor) | :white_check_mark: | :white_check_mark: | :x: |

## Bonus

  - [ ] `Observer` en utilisant le patron présent dans Java
  - [ ] `Prototype` en utilisant le mécanisme de `Cloneable` de Java


## Contribuer au catalogue

Pour contribuer au catalogue, n'hésitez pas :

  - A _forker_ le dépôt pour proposer de nouvelles implémentations (_pull requests_ bienvenues);
  - A utiliser le système d'_issues_ de Github pour rapporter des problèmes ou proposer des évolutions.

### Patrons non couverts dans le cours

  - Structure
    - [ ] Bridge
    - [ ] Flyweight
  - Comportement
    - [ ] Interpreter
    - [ ] Chain of Responsbility
    - [ ] Iterator
    - [ ] Mediator
    - [ ] Memento     

## Environement Logiciel

  - Le patrons sont implémenté en Java, version 1.8;
    - L'intégration continue utilise `openJDK` en version 11 pour compiler. 
  - Le dépôt est un projet Maven (3.6);
    - Pour compiler l'intégralité des codes, `mvn clean package` à la racine du dépôt.
    - Pour lancer un projet donné, après l'avoir compilé: `mvn -q exec:java`
  - Les diagrammes UMLs sont donnés en utilisant le format de PlantUML;
    - Pour produire une image `.png` ou `.pdf` à partir de la description `.puml`, vous devez utiliser le logiciel `plantuml` (avec le `charset` UTF-8 pour les caractères accentués).
    - Chaque sous-repertoire `uml` contient une `Makefile` qui construit les images à partir des descriptions (`make png` et `make pdf`).
"
201,alanmcgovern/monotorrent,C#,"MonoTorrent
========

[![NuGet version](https://badge.fury.io/nu/monotorrent.svg)](https://www.nuget.org/packages/MonoTorrent/)

[![Build status - master](https://img.shields.io/azure-devops/build/alanmcgovern0144/MonoTorrent/2/master?label=Build%20%28master%29)](https://dev.azure.com/alanmcgovern0144/MonoTorrent/_build?view=runs&branchFilter=6) [![Test status - master](https://img.shields.io/azure-devops/tests/alanmcgovern0144/monotorrent/2/master?label=Tests%20%28master%29)](https://dev.azure.com/alanmcgovern0144/MonoTorrent/_build?view=runs&branchFilter=6) [![Coverage - master](https://img.shields.io/azure-devops/coverage/alanmcgovern0144/monotorrent/2/master?label=Coverage%20%28master%29)](https://dev.azure.com/alanmcgovern0144/MonoTorrent/_build?view=runs&branchFilter=6)
 
[![Build status - release](https://img.shields.io/azure-devops/build/alanmcgovern0144/MonoTorrent/4/monotorrent-1.0?label=Build%20%28release%29)](https://dev.azure.com/alanmcgovern0144/MonoTorrent/_build?view=runs&branchFilter=28) [![Tests - release](https://img.shields.io/azure-devops/tests/alanmcgovern0144/monotorrent/4/monotorrent-1.0?label=Tests%20%28release%29)](https://dev.azure.com/alanmcgovern0144/MonoTorrent/_build?view=runs&branchFilter=28) [![Coverage -release](https://img.shields.io/azure-devops/coverage/alanmcgovern0144/monotorrent/4/monotorrent-1.0?label=Coverage%20%28release%29)](https://dev.azure.com/alanmcgovern0144/MonoTorrent/_build?view=runs&branchFilter=28)

[![Backers on Open Collective](https://opencollective.com/monotorrent/all/badge.svg?label=Backers)](https://opencollective.com/monotorrent)


# Supported Specifications

This is a list of all the BEPs which have been implemented in MonoTorrent. A full list of all available BEPs can be seen [here](http://www.bittorrent.org/beps/bep_0000.html)

## Final/Active BEPs
* BEP 3  - [The BitTorrent Protocol Specification](https://www.bittorrent.org/beps/bep_0003.html). ([Alternative specification](https://wiki.theory.org/index.php/BitTorrentSpecification))
* BEP 20 - [Peer ID Conventions](http://www.bittorrent.org/beps/bep_0020.html)

## Accepted BEPs

* BEP 5  - [DHT Protocol](http://www.bittorrent.org/beps/bep_0005.html)
* BEP 6  - [Fast Extension](http://www.bittorrent.org/beps/bep_0006.html)
* BEP 9  - [Extension for Peers to Send Metadata Files](http://www.bittorrent.org/beps/bep_0009.html)
* BEP 10 - [Extension Protocol](http://www.bittorrent.org/beps/bep_0010.html)
* BEP 11 - [Peer Exchange (PEX)](http://www.bittorrent.org/beps/bep_0011.html)
* BEP 12 - [Multitracker Metadata Extension](http://www.bittorrent.org/beps/bep_0012.html)
* BEP 14 - [Local Service/Peer Discovery](http://www.bittorrent.org/beps/bep_0014.html)
* BEP 15 - [UDP Tracker Protocol](http://www.bittorrent.org/beps/bep_0015.html)
* BEP 19 - [HTTP/FTP/Web Seeding (GetRight-style)
](http://www.bittorrent.org/beps/bep_0019.html)
* BEP 23 - [Tracker Returns Compact Peer Lists](http://www.bittorrent.org/beps/bep_0023.html)
* BEP 27 - [Private Torrents](http://www.bittorrent.org/beps/bep_0027.html)

## Draft BEPs

* BEP 16 - [Superseeding](http://www.bittorrent.org/beps/bep_0016.html)
* BEP 48 - [Tracker Protocol Extension: Scrape](http://www.bittorrent.org/beps/bep_0048.html)

## Others
* [Message Stream Encryption (Vuze)](http://wiki.vuze.com/w/Message_Stream_Encryption)


# Supported Client Features

The client downloads torrents and has a wide range of functionality.

* Prioritise specific files.
* Selective file downloading (including the ability to not download specific files).
* Rarest first piece picking (takes priorisation into account).
* End-game mode to boost the last 1-2% of the download.
* Sequential downloading (for media files).
* Per-torrent download/upload rate limiting.
* Overall download/upload rate limiting.
* In memory cache to reduce disk reads.
* Auto-throttling if the download rate exceeds the piece verification/disk write rate.
* IPV4 connections.
* IPV6 connections.
* IP address ban lists.
* Creating torrents from a single file, a folder, or arbitrary files in arbitrary folders.
* Fast resume data can be saved/restored to avoid hashing the data every time a torrent is started.
* Incremental piece hashing (reduces disk reads by incrementally hashing each block in a piece as it is received).
* Partial Hash Checking. If a `TorrentFile` has its `Priority` set to `DoNotDownload` then these files will be skipped when the hash check runs. If the priority is raised then the files will be automatically hash checked (if needed) before any piece is downloaded.
* Sparse files (NTFS filesystem).

* [UPnP port forwarding](https://github.com/mono/Mono.Nat).
* [NAT-PMP port forwarding](https://github.com/mono/Mono.Nat).
* Creating and using [Magnet URI](https://en.wikipedia.org/wiki/Magnet_URI).


# Supported Tracker Features

This is a standard bittorrent tracker server.

* HTTP announce and scrape requests.
* UDP announce and scrape requests.
* Compact peer responses (reduces bandwidth)
* Optionally allows unregistered torrents. In this mode the tracker will begin maintaining peer lists for a torrent as soon as the first announce request is received. 


## JetBrains

A special thank you to [JetBrains](http://www.jetbrains.com/?from=monotorrent) for supplying a free license to their tooling so I can continue to deliver great features on this opensource project.

* [dotTrace](http://www.jetbrains.com/dottrace/?from=monotorrent) - Performance profiling
* [dotMemory](http://www.jetbrains.com/dotmemory/?from=monotorrent) - Memory allocation/retention profiling

## Contributors

### Code Contributors

This project exists thanks to all the people who contribute. [[Contribute](CONTRIBUTING.md)].
<a href=""https://github.com/alanmcgovern/monotorrent/graphs/contributors""><img src=""https://opencollective.com/monotorrent/contributors.svg?width=890&button=false"" /></a>

### Financial Contributors

Become a financial contributor and help us sustain our community. [[Contribute](https://opencollective.com/monotorrent/contribute)]

#### Individuals

<a href=""https://opencollective.com/monotorrent""><img src=""https://opencollective.com/monotorrent/individuals.svg?width=890""></a>

#### Organizations

Support this project with your organization. Your logo will show up here with a link to your website. [[Contribute](https://opencollective.com/monotorrent/contribute)]

<a href=""https://opencollective.com/monotorrent/organization/0/website""><img src=""https://opencollective.com/monotorrent/organization/0/avatar.svg""></a>
<a href=""https://opencollective.com/monotorrent/organization/1/website""><img src=""https://opencollective.com/monotorrent/organization/1/avatar.svg""></a>
<a href=""https://opencollective.com/monotorrent/organization/2/website""><img src=""https://opencollective.com/monotorrent/organization/2/avatar.svg""></a>
<a href=""https://opencollective.com/monotorrent/organization/3/website""><img src=""https://opencollective.com/monotorrent/organization/3/avatar.svg""></a>
<a href=""https://opencollective.com/monotorrent/organization/4/website""><img src=""https://opencollective.com/monotorrent/organization/4/avatar.svg""></a>
<a href=""https://opencollective.com/monotorrent/organization/5/website""><img src=""https://opencollective.com/monotorrent/organization/5/avatar.svg""></a>
<a href=""https://opencollective.com/monotorrent/organization/6/website""><img src=""https://opencollective.com/monotorrent/organization/6/avatar.svg""></a>
<a href=""https://opencollective.com/monotorrent/organization/7/website""><img src=""https://opencollective.com/monotorrent/organization/7/avatar.svg""></a>
<a href=""https://opencollective.com/monotorrent/organization/8/website""><img src=""https://opencollective.com/monotorrent/organization/8/avatar.svg""></a>
<a href=""https://opencollective.com/monotorrent/organization/9/website""><img src=""https://opencollective.com/monotorrent/organization/9/avatar.svg""></a>
"
202,PacktPublishing/Learning-PySpark,Jupyter Notebook,"# Learning PySpark
This is the code repository for [Learning PySpark](https://www.packtpub.com/big-data-and-business-intelligence/learning-pyspark?utm_source=github&utm_medium=repository&utm_content=9781786463708), published by Packt. It contains all the supporting project files necessary to work through the book from start to finish.

## About the book
Apache Spark is an open source framework for efficient cluster computing with a strong interface for data parallelism and fault tolerance. This book will show you how to leverage the power of Python and put it to use in the Spark ecosystem. You will start by getting a firm understanding of the Spark 2.0 architecture and how to set up a Python environment for Spark.

You will get familiar with the modules available in PySpark. You will learn how to abstract data with RDDs and DataFrames and understand the streaming capabilities of PySpark. Also, you will get a thorough overview of machine learning capabilities of PySpark using ML and MLlib, graph processing using GraphFrames, and polyglot persistence using Blaze. Finally, you will learn how to deploy your applications to the cloud using the spark-submit command.

By the end of this book, you will have established a firm understanding of the Spark Python API and how it can be used to build data-intensive applications.

## Instructions and Navigation
All of the code is organized into folders. Each folder starts with a number followed by the application name. For example, Chapter 03.

The code will look like the following:
          
        data_key = sc.parallelize( 
             [('a', 4),('b', 3),('c', 2),('a', 8),('d', 2),('b', 1), 
             ('d', 3)],4) 
        data_key.reduceByKey(lambda x, y: x + y).collect() 

### Software requirements:
For this book you need a personal computer (can be either Windows machine, Mac, or Linux). To run Apache Spark, you will need Java 7+ and an installed and conﬁgured Python 2.6+ or 3.4+ environment; we use the Anaconda distribution of Python in version 3.5, which can be downloaded from https://www.continuum.io/downloads. 

The Python modules we randomly use throughout the book come preinstalled with Anaconda. We also use GraphFrames and TensorFrames that can be loaded dynamically while starting a Spark instance: to load these you just need an Internet connection. It is fine if some of those modules are not currently installed on your machine – we will guide you through the installation process. 

### Note:
Chapter 11 and Bonus Chapter 02 does not contain code files.

## Related Products:
* [Python Machine Learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning?utm_source=github&utm_medium=repository&utm_content=9781783555130)

* [Spark for Python Developers](https://www.packtpub.com/big-data-and-business-intelligence/spark-python-developers?utm_source=github&utm_medium=repository&utm_content=9781784399696)

* [Python Data Analysis Cookbook](https://www.packtpub.com/big-data-and-business-intelligence/python-data-analysis-cookbook?utm_source=github&utm_medium=repository&utm_content=9781785282287)

### Suggestions and Feedback
[Click here](https://docs.google.com/forms/d/e/1FAIpQLSe5qwunkGf6PUvzPirPDtuy1Du5Rlzew23UBp2S-P3wB-GcwQ/viewform) if you have any feedback or suggestions.


"
203,traviscross/mtr,C,"WHAT IS MTR?
===

mtr combines the functionality of the 'traceroute' and 'ping' programs
in a single network diagnostic tool.

As mtr starts, it investigates the network connection between the host
mtr runs on and a user-specified destination host.  After it
determines the address of each network hop between the machines,
it sends a sequence of ICMP ECHO requests to each one to determine the
quality of the link to each machine.  As it does this, it prints
running statistics about each machine.

mtr is distributed under the GNU General Public License version 2.
See the COPYING file for details.

INSTALLING
===

If you're building this from a tarball, compiling mtr is as
simple as:

	./configure && make

Please note that this refers to the tarballs from
 https://www.bitwizard.nl/mtr/files/
and not the tarballs that github can produce. 

(in the past, there was a Makefile in the distribution that did
the `./configure` for you and then ran make again with the generated
Makefile, but this has suffered some bitrot. It didn't work well
with git.) 

If you're building from the git repository, you'll need to run:

	./bootstrap.sh && ./configure && make

When it looks as if the compilation was successful, you can
test mtr with

	sudo ./mtr <host>

(fill in a hostname or IP address where it says <host>) or
immediately continue on to installing:

	make install

Note that mtr-packet must be suid-root because it requires access to
raw IP sockets.  See SECURITY for security information.

Older versions used to require a non-existent path to GTK for a
correct build of a non-gtk version while GTK was installed. This is
no longer necessary. `./configure --without-gtk` should now work.
If it doesn't, try `make WITHOUT_X11=YES` as the make step.

On Solaris, you'll need to use GNU make to build.
(Use `gmake` rather than `make`.)

On Solaris (and possibly other systems) the ""gtk"" library may be
installed in a directory where the dynamic linker refuses to look when
a binary is setuid. Roman Shterenzon reports that adding
        -Wl,-rpath=/usr/lib
to the commandline will work if you are using gnu LD. He tells me that
you're out of luck when you use the sun LD. That's not quite true, as
you can move the gtk libraries to `/usr/lib` instead of leaving them in
`/usr/local/lib`.  (when the ld tells you that `/usr/local/lib` is untrusted
and `/usr/lib` is trusted, and you trust the gtk libs enough to want them
in a setuid program, then there is something to say for moving them
to the ""trusted"" directory.)

Building on MacOS should not require any special steps.

BUILDING FOR WINDOWS
===

Building for Windows requires Cygwin.  To obtain Cygwin, see
https://cygwin.com/install.html.
Next, re-run cygwin's `setup-x86.exe` (or `setup-x86_64.exe` if you're using 64bit cygwin) with the following arguments,  
which will install the packages required for building:

        setup-x86.exe --package-manager --wait --packages automake,pkg-config,make,gcc-core,libncurses-devel,libjansson-devel

Build as under Unix:

        ./bootstrap.sh && ./configure && make

Finally, install the built binaries:

        make install


WHERE CAN I GET THE LATEST VERSION OR MORE INFORMATION?
===

mtr is now hosted on github.
https://github.com/traviscross/mtr

See the mtr web page at http://www.BitWizard.nl/mtr/

Bug reports and feature requests should be submitted to the Github bug tracking system.

Patches can be submitted by cloning the Github repository and issuing
a pull request, or by email to me. Please use unified diffs. Usually
the diff is sort of messy, so please check that the diff is clean and
doesn't contain too much of your local stuff (for example, I don't
want/need the ""configure"" script that /your/ automake made for you).

(There used to be a mailinglist, but all it got was spam. So
when the server was upgraded, the mailing list died.)


REW

"
204,deepmind/deepmind-research,Jupyter Notebook,"# DeepMind Research

This repository contains implementations and illustrative code to accompany
DeepMind publications. Along with publishing papers to accompany research
conducted at DeepMind, we release open-source
[environments](https://deepmind.com/research/open-source/open-source-environments/),
[data sets](https://deepmind.com/research/open-source/open-source-datasets/),
and [code](https://deepmind.com/research/open-source/open-source-code/) to
enable the broader research community to engage with our work and build upon it,
with the ultimate goal of accelerating scientific progress to benefit society.
For example, you can build on our implementations of the
[Deep Q-Network](https://github.com/deepmind/dqn) or
[Differential Neural Computer](https://github.com/deepmind/dnc), or experiment
in the same environments we use for our research, such as
[DeepMind Lab](https://github.com/deepmind/lab) or
[StarCraft II](https://github.com/deepmind/pysc2).

If you enjoy building tools, environments, software libraries, and other
infrastructure of the kind listed below, you can view open positions to work in
related areas on our [careers page](https://deepmind.com/careers/).

For a full list of our publications, please see
https://deepmind.com/research/publications/

## Projects

*   [A Deep Learning Approach for Characterizing Major Galaxy Mergers](galaxy_mergers)
*   [Better, Faster Fermionic Neural Networks](kfac_ferminet_alpha) (KFAC implementation)
*   [Object-based attention for spatio-temporal reasoning](object_attention_for_reasoning)
*   [Effective gene expression prediction from sequence by integrating long-range interactions](enformer)
*   [Satore: First-order logic saturation with atom rewriting](satore)
*   [Characterizing signal propagation to close the performance gap in unnormalized ResNets](nfnets), ICLR 2021
*   [Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples](adversarial_robustness)
*   [Functional Regularisation for Continual Learning](functional_regularisation_for_continual_learning), ICLR 2020
*   [Self-Supervised MultiModal Versatile Networks](mmv), NeurIPS 2020
*   [ODE-GAN: Training GANs by Solving Ordinary Differential Equations](ode_gan), NeurIPS 2020
*   [Algorithms for Causal Reasoning in Probability Trees](causal_reasoning)
*   [Gated Linear Networks](gated_linear_networks), NeurIPS 2020
*   [Value-driven Hindsight Modelling](himo), NeurIPS 2020
*   [Targeted free energy estimation via learned mappings](learned_free_energy_estimation), Journal of Chemical Physics 2020
*   [Learning to Simulate Complex Physics with Graph Networks](learning_to_simulate), ICML 2020
*   [Physically Embedded Planning Problems](physics_planning_games)
*   [PolyGen: PolyGen: An Autoregressive Generative Model of 3D Meshes](polygen), ICML 2020
*   [Bootstrap Your Own Latent](byol)
*   [Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks](catch_carry), SIGGRAPH 2020
*   [MEMO: A Deep Network For Flexible Combination Of Episodic Memories](memo), ICLR 2020
*   [RL Unplugged: Benchmarks for Offline Reinforcement Learning](rl_unplugged)
*   [Disentangling by Subspace Diffusion (GEOMANCER)](geomancer), NeurIPS 2020
*   [What can I do here? A theory of affordances in reinforcement learning](affordances_theory), ICML 2020
*   [Scaling data-driven robotics with reward sketching and batch reinforcement learning](sketchy), RSS 2020
*   [The Option Keyboard: Combining Skills in Reinforcement Learning](option_keyboard), NeurIPS 2019
*   [VISR - Fast Task Inference with Variational Intrinsic Successor Features](visr), ICLR 2020
*   [Unveiling the predictive power of static structure in glassy systems](glassy_dynamics), Nature Physics 2020
*   [Multi-Object Representation Learning with Iterative Variational Inference (IODINE)](iodine)
*   [AlphaFold CASP13](alphafold_casp13), Nature 2020
*   [Unrestricted Adversarial Challenge](unrestricted_advx)
*   [Hierarchical Probabilistic U-Net (HPU-Net)](hierarchical_probabilistic_unet)
*   [Training Language GANs from Scratch](scratchgan), NeurIPS 2019
*   [Temporal Value Transport](tvt), Nature Communications 2019
*   [Continual Unsupervised Representation Learning (CURL)](curl), NeurIPS 2019
*   [Unsupervised Learning of Object Keypoints (Transporter)](transporter), NeurIPS 2019
*   [BigBiGAN](bigbigan), NeurIPS 2019
*   [Deep Compressed Sensing](cs_gan), ICML 2019
*   [Side Effects Penalties](side_effects_penalties)
*   [PrediNet Architecture and Relations Game Datasets](PrediNet)
*   [Unsupervised Adversarial Training](unsupervised_adversarial_training), NeurIPS 2019
*   [Graph Matching Networks for Learning the Similarity of Graph Structured
    Objects](graph_matching_networks), ICML 2019
*   [REGAL: Transfer Learning for Fast Optimization of Computation Graphs](regal)
*   [Deep Ensembles: A Loss Landscape Perspective](ensemble_loss_landscape)




## Disclaimer

*This is not an official Google product.*
"
205,Sroy20/machine-learning-interview-questions,,"In different files, I list various questions that might be asked in a ML interview. Here is the table of contents:

1. [Deep Learning Questions](https://github.com/Sroy20/machine-learning-interview-questions/blob/master/list_of_questions_deep_learning.md)
1. [General Machine Learning Questions](https://github.com/Sroy20/machine-learning-interview-questions/blob/master/list_of_questions_machine_learning.md)
1. [Mathematics for Machine Learning Questions](https://github.com/Sroy20/machine-learning-interview-questions/blob/master/list_of_questions_mathematics.md)
"
206,x64dbg/docs,Python,"# docs

Documentation repository for [x64dbg](http://x64dbg.com) at [Read the Docs](https://readthedocs.org/projects/x64dbg).

## Building

1. `pip install sphinx recommonmark==0.4.0 sphinx_rtd_theme==0.2.5b2`
2. Add `relpath = relpath.replace(os.path.sep, '/')` to `C:\Python27\Lib\site-packages\recommonmark\transform.py` line `63`
3. run `makechm.bat`. It will build the .CHM help file.
"
207,thuehlinger/daemons,Ruby,"Ruby Daemons
============
[![Build Status](https://travis-ci.org/thuehlinger/daemons.svg?branch=master)](https://travis-ci.org/thuehlinger/daemons)[![Code Climate](https://codeclimate.com/github/acuppy/daemons/badges/gpa.svg)](https://codeclimate.com/github/acuppy/daemons)[![Test Coverage](https://circleci.com/gh/acuppy/daemons.svg?style=shield&circle-token=a4f96fd41f7682661d6543e30207427ac8870c0d)](https://circleci.com/gh/acuppy/daemons)

Daemons provides an easy way to wrap existing ruby scripts (for example a self-written server)
to be _run as a daemon_ and to be _controlled by simple start/stop/restart commands_.

If you want, you can also use daemons to _run blocks of ruby code in a daemon process_ and to control
these processes from the main application.

Besides this basic functionality, daemons offers many advanced features like _exception backtracing_
and logging (in case your ruby script crashes) and _monitoring_ and automatic restarting of your processes
if they crash.

Basic Usage
-----------

You can use Daemons in four different ways:

### 1. Create wrapper scripts for your server scripts or applications

Layout: suppose you have your self-written server `myserver.rb`:

``` ruby
# this is myserver.rb
# it does nothing really useful at the moment

loop do
  sleep(5)
end
```

To use `myserver.rb` in a production environment, you need to be able to
run `myserver.rb` in the _background_ (this means detach it from the console, fork it
in the background, release all directories and file descriptors).

Just create `myserver_control.rb` like this:

``` ruby
# this is myserver_control.rb
require 'daemons'

Daemons.run('myserver.rb')
```

And use it like this from the console:

``` ruby
$ ruby myserver_control.rb start
    (myserver.rb is now running in the background)
$ ruby myserver_control.rb restart
    (...)
$ ruby myserver_control.rb stop
```

For testing purposes you can even run `myserver.rb` _without forking_ in the background:

``` ruby
$ ruby myserver_control.rb run
```

An additional nice feature of Daemons is that you can pass _additional arguments_ to the script that
should be daemonized by seperating them by two _hyphens_:

``` ruby
$ ruby myserver_control.rb start -- --file=anyfile --a_switch another_argument
```


### 2. Create wrapper scripts that include your server procs

Layout: suppose you have some code you want to run in the background and control that background process
from a script:

``` ruby
# this is your code
# it does nothing really useful at the moment

loop do
  sleep(5)
end
```

To run this code as a daemon create `myproc_control.rb` like this and include your code:

``` ruby
# this is myproc_control.rb
require 'daemons'

Daemons.run_proc('myproc.rb') do
  loop do
    sleep(5)
  end
end
```

And use it like this from the console:

``` ruby
$ ruby myproc_control.rb start
    (myproc.rb is now running in the background)
$ ruby myproc_control.rb restart
    (...)
$ ruby myproc_control.rb stop
```

For testing purposes you can even run `myproc.rb` _without forking_ in the background:

``` ruby
$ ruby myproc_control.rb run
```

### 3. Control a bunch of daemons from another application

Layout: you have an application `my_app.rb` that wants to run a bunch of
server tasks as daemon processes.

``` ruby
# this is my_app.rb
require 'daemons'

task1 = Daemons.call(:multiple => true) do
  # first server task

  loop do
    conn = accept_conn()
    serve(conn)
  end
end

task2 = Daemons.call do
  # second server task

  loop do
    something_different()
  end
end

# the parent process continues to run

# we can even control our tasks, for example stop them
task1.stop
task2.stop

exit
```

### 4. Daemonize the currently running process

Layout: you have an application `my_daemon.rb` that wants to run as a daemon
(but without the ability to be controlled by daemons via start/stop commands)

``` ruby
# this is my_daemons.rb
require 'daemons'

# Initialize the app while we're not a daemon
init()

# Become a daemon
Daemons.daemonize

# The server loop
loop do
  conn = accept_conn()
  serve(conn)
end
```

For further documentation, refer to the module documentation of Daemons.

Displaying daemon status
------------------------

When daemonizing a process using a wrapper script, as examples 1 and 2 above,
the status can be shown using

``` ruby
$ ruby myproc_control.rb status
```

By default this will display whether or not the daemon is running and, if it
is, its PID.

A custom message can be shown with

``` ruby
def custom_show_status(app)
  # Display the default status information
  app.default_show_status

  puts
  puts ""PS information""
  system(""ps -p #{app.pid.pid.to_s}"")

  puts
  puts ""Size of log files""
  system(""du -hs /path/to/logs"")
end

Daemons.run('myserver.rb', { show_status_callback: :custom_show_status })
```

Documentation
-------------------

Documentation can be found at http://www.rubydoc.info/gems/daemons.

Author
------

Written 2005-2021 by Thomas Uehlinger, 2014-2016 by Aaron Stone.
"
208,fvwmorg/fvwm,C,"FVWM
----

[![Build Status](https://travis-ci.org/fvwmorg/fvwm.svg?branch=master)](https://travis-ci.org/fvwmorg/fvwm)

------------------------------------------------------------------------------

**Please note that FVWM2 is in maintenance mode.  This means it won't be
receiving any new features.  Bug fixes will be to the core of FVWM2 only.  Any
problems found in modules will be to fix segfaults only.**

**All users are hereforth encouraged to use [fvwm3](https://github.com/fvwmorg/fvwm3), and to report any bugs.**

**The rest of this README and associated documentation for FVWM is for
historical purposes only.**

------------------------------------------------------------------------------

Welcome to fvwm.  Fvwm is a multiple large virtual desktop window manager,
originally (a looooong time ago!) derived from twm.

Fvwm is intended to have a small memory footprint but a rich feature set, be
extremely customizable and extendible, and have a high degree of Motif mwm
compatibility.

**ALL VERSIONS OF FVWM PRIOR TO 2.6.X ARE NOT SUPPORTED UPSTREAM.**

Users of fvwm prior to 2.6.X can make use of the
[fvwm-convert-2.6 utility](./bin/fvwm-convert-2.6.in) to try and have their
configuration file converted to a newer syntax.  Please see its manpage for
more details on how to use it.

Releases
--------

* Latest stable release: [2.6.7](https://github.com/fvwmorg/fvwm/releases/tag/2.6.7)
* Long-term stable release: [fvwm2-stable](https://github.com/fvwmorg/fvwm/archive/fvwm2-stable.zip)

The **Long-term stable release** represents the fvwm version prior to 2.6.7
which contains ported bug-fixes from 2.6.7, but retains the modules and other
features removed in 2.6.7.  This version **won't** receive any new features but
may occsaionally receive bug-fixes.

Installation
------------

See [the installation instructions](./INSTALL.md)

Development
-----------

Those interested in contributing to FVWM should have [a read of the developer
documentation](./docs/DEVELOPERS.md).

A [TODO file](./TODO.md) exists, and sometimes even things from it are worked
on.

Comments, Questions?
--------------------

There is the `#fvwm` IRC channel on `freenode.net` which you can use to chat
to people about FVWM.

If you have any questions, concerns, bug reports, enhancement requests,
etc., please feel free to send an email to the fvwm mailing list, or open a
Github issue.

Have fun!

-- The fvwm-workers
"
209,AlexanderSharykin/RepositoryUtilities,C#,"﻿Generates Github-style chart for Svn repository activity.

![](https://github.com/AlexanderSharykin/RepositoryUtilities/blob/master/ActivityChart.png)

SharpSvn is used to get Svn repository info
"
210,OnsenUI/onsen.io,HTML,"# onsen.io

[![Circle CI](https://circleci.com/gh/OnsenUI/OnsenUI.svg?style=svg)](https://circleci.com/gh/OnsenUI/onsen.io)

This repository contains the assets in Onsen UI Website, available on http://onsen.io.
Please Visit [Onsen UI](https://github.com/OnsenUI/OnsenUI) if you need access to the framework itself.

## Installation

```bash
git clone --recurse-submodules git@github.com:OnsenUI/onsen.io.git
cd onsen.io
yarn global add gulp
yarn install

# Update necessary submodules
git submodule update --remote dist/v2/OnsenUI dist/playground

# Build Onsen UI
(cd dist/v2/OnsenUI/css-components && yarn install)
(cd dist/v2/OnsenUI && yarn install && yarn build)
(cd dist/v2/OnsenUI/bindings/react && yarn install && yarn gen-docs)
```

## How to Build

```bash
gulp generate --lang en
gulp generate --lang ja
```

## Edit & Serve

```bash
gulp serve --lang en
```

## Translate

```bash
1. $ gulp i18n-extract # This will generate POT files into src/i18n/gettext
2. And use PO editor to generate po files
3. $ gulp i18n-translate # This will translate and overwrite files into src/documents_ja/
```

## Releasing to Production
This repository is set up on [CircleCI](https://circleci.com/gh/OnsenUI/onsen.io) to automatically build with every commit. Commits to `master` are automatically deployed to the staging website at `s.onsen.io`. To deploy to production, merge `master` into the `production` branch. Typically this merge is done through a pull request on GitHub, even if you immediately merge it yourself. Once merged, CircleCI will automatically deploy everything in the `production` branch.

## How to contribute

We will happily accept contributions to the onsen.io website. However, it is much more likely that it is another repository that you want to make your pull request to. This repository contains the Onsen UI marketing materials, and brings in most of the rest of the content from other repositories.

### Which repository to choose?
Here is where you should make changes:

- Onsen UI Framework: https://github.com/OnsenUI/OnsenUI
- Documentation
  - API Definitions: https://github.com/OnsenUI/OnsenUI (see its [documentation guide](https://github.com/OnsenUI/OnsenUI/blob/master/CONTRIBUTING.md#documentation) for how this works)
  - Tutorials: https://github.com/OnsenUI/playground (taken from the examples in the Playground)
  - Guides: This repository
  - Design: This repository
- Theme Roller: https://github.com/OnsenUI/theme-roller
- Playground: https://github.com/OnsenUI/playground
- News Dropdown: https://github.com/OnsenUI/recent-activities
- Homepage Kitchensink Example: https://github.com/OnsenUI/vue-onsenui-kitchensink
- Onsen.io Landing Pages: This repository
- Community Forum: Runs on NodeBB and is not part of a repository on GitHub

### Contributing to this repository
If the repository you need to make changes to is this one, then the workflow is simple, and the same as you will see in other open source projects.

1. Fork the repository
2. Commit your changes. Please squash commits where possible.
3. Make a pull request to `master`.
4. After you've made a pull request we will review it. If everything is fine and we like the change the contribution will be pulled into the repository. In the case where there are some issues with the code or we disagree with how it's been implemented we will describe the issues in the comments so they can be corrected.
"
211,grpc/grpc.io,HTML,"# gRPC website

The [grpc.io][] site, built using [Hugo][] and hosted on [Netlify][].

## Build prerequisites

### 1. Install the following tools

- **[Hugo, extended edition][hugo-install]**; match the version specified in
  [netlify.toml](netlify.toml)
- **Node**, the latest [LTS release][]. Like Netlify, we use **[nvm][]**, the
  Node Version Manager, to install and manage Node versions:
  ```console
  $ nvm install --lts
  $ nvm use --lts
  ```

### 2. Clone this repo _and_ its submodules

> IMPORTANT: This repo has **recursive git _submodules_**, which affects how you
> clone it.

The _simplest_ way to get a full copy of this repo is to clone the repo _and_
its submodules _at the same time_ by running this command from a command shell /
window:

```console
$ git clone --recurse-submodules https://github.com/grpc/grpc.io.git
```
### 3. Change directories

From this point on you'll be working from the `grpc.io` directory:

```console
$ cd grpc.io
```
### 4. Did you get the submodules?

Forgetting to clone the submodules is a common error.

If you forgot the `--recurse-submodules` option when you ran the `clone` command
given in step 2 above or, if you cloned the repo using another method, as
described in [Cloning a repository][], then you'll need to fetch the repo's
submodules.

To (recursively) fetch the submodules, run the following command from the
`grpc.io` directory:

```console
$ git submodule update --init --recursive --depth 1
```

> NOTE: Unsure if you've fetched the submodules? The Git command above is
> idempotent, so you can safely (re-)run it -- if you already have the
> submodules, it will have no effect.

### 5. Run installation scripts

Install NPM packages:

```console
$ npm install
```

## Build the site

Run the following command to have Hugo generate the site files:

```console
$ hugo
```

The `public` folder contains the generated site.

## Serve the site locally

To locally serve this site, use **one** of the following commands.

> **Note**: Hugo serves pages from memory by default, so if you want to
> (re-)generate the website files to disk, use the build command above.

### a) Serve using [Hugo][] via `make`

```console
$ make serve
hugo server
Start building sites …
...
Environment: ""development""
Serving pages from memory
...
Web Server is available at http://localhost:1313/ (bind address 127.0.0.1)
Press Ctrl+C to stop
```

This generates _unminified_ site pages. Other [Makefile](Makefile) targets
include the following:

- `serve-drafts` to also serve draft and future pages
- `serve-production` to server files exactly as they'll appear in production

### b) Serve using Netlify dev

[Netlify dev][] uses Hugo under the hood to serve the site, but it also creates
a proxy (at port 8888 by default), to handle [site redirects][]:

```console
$ npx netlify dev
```

If you also want to serve draft and future pages use this command:

```console
$ npx netlify dev -c ""hugo serve -DFw""
```

## Site deploys and PR previews

Commits to the `main` branch are _automatically_ published by [Netlify][]. You
can see deploy logs and more from the [Netlify gRPC Team dashboard][], provided
you have the necessary permissions.

_PR previews_ are automatically built by [Netlify][] as well. By default, a PR
preview is identical to a production build.

If you want draft and future pages to also appear in a PR preview, then make
sure that the word ""draft"" appears in the branch name used to create the PR.

## Checking links

You can check the site's internal links by running this command:

```console
$ make check-internal-links
```

This deletes the generated `public` directory, builds the ""production"" version
of the site, and verifies that internal links are valid. Please note that
internal links prefixed with `/grpc` do not work in your local environment
(there are redirects applied by [Netlify](https://netlify.com)). Any errors
returned from `/grpc` links are false negatives that you can ignore.

[Cloning a repository]: https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository
[grpc.io]: https://grpc.io
[Hugo]: https://gohugo.io
[hugo-install]: https://gohugo.io/getting-started/installing
[LTS release]: https://nodejs.org/en/about/releases/
[Netlify]: https://netlify.com
[Netlify dev]: https://www.netlify.com/products/dev
[Netlify gRPC Team dashboard]: https://app.netlify.com/teams/grpc/overview
[nvm]: https://github.com/nvm-sh/nvm/blob/master/README.md#installing-and-updating
[site redirects]: layouts/index.redirects
"
212,colcon/colcon-mixin-repository,Python,"Repository of CLI mixins
========================

Various command line options are tedious to write and/or difficult to remember.
To make common command line options easier to invoke this repository makes
these ""shortcuts"" available, e.g. for the command line tool `colcon`.

How to fetch the information
----------------------------

To register this repository with `colcon` (using the identifier ""default"")
invoke the following command:

```
colcon mixin add default https://raw.githubusercontent.com/colcon/colcon-mixin-repository/master/index.yaml
```

Afterwards as well as on a regular base fetch the latest content from the
repository:

```
colcon mixin update default
```

### Use a local mixin repository

The `index` as well as the mixins can also be local files.
That is e.g. useful when iterating on the mixin files before publishing them:

```
git clone https://github.com/colcon/colcon-mixin-repository.git
colcon mixin add default file://`pwd`/colcon-mixin-repository/index.yaml
```

After editing either the `index.yaml` file or any of the `.mixin` files `mixin update default` needs to be run again.

How to use the information
--------------------------

To show the mixins available and their mapping invoke the following command:

```
colcon mixin show
```

To apply CLI mixins pass the option `--mixin` to the colcon verb followed by
the names of the mixins.

How to contribute additional information
----------------------------------------

Initially fork this repository.
For each contribution perform the following steps:

* Create or modify one or multiple files ending with `.mixin`.
* Add any new files in alphabetical order to the `index.yaml` file.
* Run the `lint.py` script to ensure that the changes follow the recommended
  yaml style.
* Create a pull request with the changes.
"
213,RIPE-NCC/whois,Java,"RIPE NCC RDAP Implementation
-----------------------------
Read more about the RDAP specification in the RFC documents on the IETF site: https://datatracker.ietf.org/wg/weirds/documents/

Multiple country attributes are not returned
--------------------------------------------
inetnum and inet6num objects can contain multiple country attributes, but RDAP only allows a single value.

This implementation returns the first country attribute value, and includes an explanatory notice.

Multiple language attributes are not returned
---------------------------------------------
inetnum, inet6num, and organisation objects can have multiple language attributes, but only the first language is returned.

Multiple organisation e-mail and phone attributes are returned, but not with preferences
----------------------------------------------------------------------------------------
Preferences are not assigned to multiple e-mail or phone elements.

AS block returned if AS number not found
----------------------------------------
If an AS number is allocated to the RIPE region, that is returned.

If an AS number is allocated to a different region, a redirect is returned.

If an AS number is not allocated to any region, the parent AS block is returned. This includes reserved AS numbers.

Custom ""ZONE"" role for domain objects
-------------------------------------
For zone-c attributes in domain objects, a custom ""ZONE"" role is used, which is not in the RDAP spec.

Ref. https://wiki.tools.ietf.org/html/rfc7483 section 10.2.4.

Organisation role ""registrant"" is ambiguous
-------------------------------------------
The role ""registrant"" is used to identify organisation entities, however this is ambiguous as it's also used for mntner entities.

Entity Primary Key can match multiple objects
---------------------------------------------
If an entity primary key matches more than one object, a 500 Internal Server Error is returned.

For example: https://rdap.db.ripe.net/entity/KR4422-RIPE

Related Contact information is Filtered
---------------------------------------
Any related contact entities (""technical"",""administrative"",""abuse"" etc.) have filtered contact information, i.e. ""e-mail"" and ""notify"" values are not included. This was done to avoid blocking clients for inadvertently querying excessively for personal data.

A workaround is to query for each entity separately using the contact's nic-hdl, and the unfiltered information is returned (although a limit for personal data does apply).

Entity Search
--------------------------
Entity search on a handle is limited to returning 100 results.

Domain Search
--------------------------
Domain search is restricted to only search for reverse delegations, and results are limited to 100.

Netname may not match Whois
----------------------------
The netname value returned by RDAP may not match what is returned by Whois.

Entity does not include networks
---------------------------------
An entity (i.e. for an organisation) should include any related networks. 

This list of networks should have a maximum size to prevent the response from growing too large and taking too long.

Ref. RFC 7483, Section 5.1 The Entity Object Class. (https://tools.ietf.org/html/rfc7483#section-5.1).

Example:
* Request: http://rdap.db.ripe.net/entity/ORG-RIEN1-RIPE
 * Response: Should include ""networks"" element with referenced networks, including 193.0.0.0 - 193.0.23.255

Nameserver queries always return Not Implemented
-------------------------------------------------
The RIPE database doesn't contain any forward domain objects, consequently a nameserver query will always return Not Implemented.

Only ""mnt-by:"" Maintainers are Listed as Registrants
-----------------------------------------------------
Only maintainers referenced in ""mnt-by:"" attributes will be listed as Registrants in responses.
"
214,glouppe/phd-thesis,TeX,"Understanding Random Forests
============================

PhD dissertation, Gilles Louppe, July 2014. Defended on October 9, 2014. 

_arXiv:_ http://arxiv.org/abs/1407.7502

_Mirrors:_ 
- http://hdl.handle.net/2268/170309
- http://www.montefiore.ulg.ac.be/~glouppe/pdf/phd-thesis.pdf

_License:_ BSD 3 clause

_Contact:_ Gilles Louppe ([@glouppe](https://twitter.com/glouppe/), <g.louppe@gmail.com>)

Please cite using the following BibTex entry:

```
@phdthesis{louppe2014understanding,
  title={Understanding Random Forests: From Theory to Practice},
  author={Louppe, Gilles},
  school={University of Liege, Belgium},
  year=2014,
  month=10,
  note={arXiv:1407.7502}
}
```

---

Data analysis and machine learning have become an integrative part of the
modern scientific methodology, offering automated procedures for the prediction
of a phenomenon based on past observations, unraveling underlying patterns in
data and providing insights about the problem. Yet, caution should
avoid using machine learning as a black-box tool, but rather consider it as a
methodology, with a rational thought process that is entirely dependent on the
problem under study. In particular, the use of algorithms
should ideally require a reasonable understanding of their
mechanisms, properties and limitations, in order to better apprehend and
interpret their results.

Accordingly, the goal of this thesis is to provide an in-depth
analysis of random forests, consistently calling into
question each and every part of the algorithm, in order to shed new light on
its learning capabilities, inner workings and interpretability. The first
part of this work studies the induction of decision trees and the construction of
ensembles of randomized trees, motivating their design and purpose whenever
possible. Our contributions follow with an original complexity
analysis of random forests, showing their good computational performance
and scalability, along with an in-depth discussion of their
implementation details, as contributed within Scikit-Learn.

In the second part of this work, we analyze and discuss the interpretability of
random forests in the eyes of variable importance measures. The core of our
contributions rests in the theoretical characterization of the Mean Decrease of
Impurity variable importance measure, from which we prove and derive some of
its properties in the case of multiway totally randomized trees and in
asymptotic conditions. In consequence of this work, our analysis  demonstrates
that variable importances as computed from non-totally randomized trees (e.g.,
standard Random Forest) suffer from a combination of defects, due to masking
effects, misestimations of node impurity or due to the binary structure of
decision trees.

Finally, the last part of this dissertation addresses limitations of random
forests in the context of large datasets. Through extensive experiments, we
show that subsampling both samples and features simultaneously provides on par
performance while lowering at the same time the memory requirements. Overall
this paradigm highlights an intriguing practical fact: there is often no need
to build single models over immensely large datasets. Good performance can
often be achieved by building models on (very) small random parts of the data
and then combining them all in an ensemble, thereby avoiding all practical
burdens of making large data fit into memory.
"
215,creative-computing-society/Hacktoberfest2020_CCS,HTML,"# Hacktoberfest2020_CCS
This is the repository for Hacktoberfest 2020

Cards in this repo should show up on https://ccstiet.com/hacktoberfest in a while."
216,sous-chefs/nodejs,Ruby,"# [nodejs-cookbook](https://github.com/redguide/nodejs)

[![CK Version](http://img.shields.io/cookbook/v/nodejs.svg?branch=master)](https://supermarket.chef.io/cookbooks/nodejs) [![Build Status](https://img.shields.io/travis/redguide/nodejs.svg)](https://travis-ci.org/redguide/nodejs)

Installs node.js/npm and includes a resource for managing npm packages

## Requirements

### Platforms

- Debian/Ubuntu
- RHEL/CentOS/Scientific/Amazon/Oracle
- openSUSE
- Windows

Note: Source installs require GCC 4.8+, which is not included on older distro releases

### Chef

- Chef Infra Client 14+

### Cookbooks

- ark

## Usage

Include the nodejs recipe to install node on your system based on the default installation method:

```ruby
include_recipe ""nodejs""
```

### Install methods

#### Package

Install node from packages:

```ruby
node['nodejs']['install_method'] = 'package' # Not necessary because it's the default
include_recipe ""nodejs""
# Or
include_recipe ""nodejs::nodejs_from_package""
```

By default this will setup deb/rpm repositories from nodesource.com, which include up to date NodeJS packages. If you prefer to use distro provided package you can disable this behavior by setting `node['nodejs']['install_repo']` to `false`.

#### Binary

Install node from official prebuilt binaries:

```ruby
node['nodejs']['install_method'] = 'binary'
include_recipe ""nodejs""

# Or
include_recipe ""nodejs::nodejs_from_binary""

# Or set a specific version of nodejs to be installed
node.default['nodejs']['install_method'] = 'binary'
node.default['nodejs']['version'] = '5.9.0'
node.default['nodejs']['binary']['checksum'] = '99c4136cf61761fac5ac57f80544140a3793b63e00a65d4a0e528c9db328bf40'

# Or fetch the binary from your own location
node.default['nodejs']['install_method'] = 'binary'
node.default['nodejs']['binary']['url'] = 'https://s3.amazonaws.com/my-bucket/node-v7.8.0-linux-x64.tar.gz'
node.default['nodejs']['binary']['checksum'] = '0bd86f2a39221b532172c7d1acb57f0b0cba88c7b82ea74ba9d1208b9f6f9697'
```

#### Source

Install node from sources:

```ruby
node['nodejs']['install_method'] = 'source'
include_recipe ""nodejs""
# Or
include_recipe ""nodejs::nodejs_from_source""
```

#### Chocolatey

Install node from chocolatey:

```chef
node['nodejs']['install_method'] = 'chocolatey'
include_recipe ""nodejs""
# Or
include_recipe ""nodejs::nodejs_from_chocolatey""
```

## NPM

Npm is included in nodejs installs by default. By default, we are using it and call it `embedded`. Adding recipe `nodejs::npm` assure you to have npm installed and let you choose install method with `node['nodejs']['npm']['install_method']`

```ruby
include_recipe ""nodejs::npm""
```

_Warning:_ This recipe will include the `nodejs` recipe, which by default includes `nodejs::nodejs_from_package` if you did not set `node['nodejs']['install_method']`.

## Resources

### npm_package

note: This resource was previously named nodejs_npm. Calls to that resource name will still function, but cookbooks should be updated for the new npm_package resource name.

`npm_package` lets you install npm packages from various sources:

- npm registry:

  - name: `property :package`
  - version: `property :version` (optional)

- url: `property :url`

  - for git use `git://{your_repo}`

- from a json (package.json by default): `property :json`

  - use `true` for default
  - use a `String` to specify json file

Packages can be installed globally (by default) or in a directory (by using `attribute :path`)

You can specify an `NPM_TOKEN` environment variable for accessing [NPM private modules](https://docs.npmjs.com/private-modules/intro) by using `attribute :npm_token`

You can specify a `NODE_ENV` environment variable, in the case that some element of your installation depends on this by using `attribute :node_env`. E.g., using [`node-config`](https://www.npmjs.com/package/config) as part of your postinstall script. Please note that adding the `--production` option will override this to `NODE_ENV=production`.

You can append more specific options to npm command with `attribute :options` array :

You can specify auto_update as false to stop the npm install command from running and updating an installed package. Running the command will update packages within restrictions imposed by a package.json file. The default behavior is to update automatically.

- use an array of options (w/ dash), they will be added to npm call.
- ex: `['--production','--force']` or `['--force-latest']`

You can specify live_stream true for the resource to have the package install information included in the chef-client log outout for better npm package diagnostics and trouble shooting.

This LWRP attempts to use vanilla npm as much as possible (no custom wrapper).

### Packages

```ruby
npm_package 'express'

npm_package 'async' do
  version '0.6.2'
end

npm_package 'request' do
  url 'github mikeal/request'
end

npm_package 'grunt' do
  path '/home/random/grunt'
  json true
  user 'random'
  node_env 'staging'
end

npm_package 'my_private_module' do
  path '/home/random/myproject' # The root path to your project, containing a package.json file
  json true
  npm_token '12345-abcde-e5d4c3b2a1'
  user 'random'
  options ['--production'] # Only install dependencies. Skip devDependencies
end
```

[Working Examples](test/cookbooks/nodejs_test/recipes/npm.rb)

Or add packages via attributes (which accept the same attributes as the LWRP above):

```json
""nodejs"": {
  ""npm_packages"": [
    {
      ""name"": ""express""
    },
    {
      ""name"": ""async"",
      ""version"": ""0.6.2""
    },
    {
      ""name"": ""request"",
      ""url"": ""github mikeal/request""
    }
    {
      ""name"": ""grunt"",
      ""path"": ""/home/random/grunt"",
      ""json"": true,
      ""user"": ""random""
    }
  ]
}
```

## License & Authors

**Author:** Marius Ducea (marius@promethost.com)
**Author:** Nathan L Smith (nlloyds@gmail.com)
**Author:** Guilhem Lettron (guilhem@lettron.fr)
**Author:** Barthelemy Vessemont (bvessemont@gmail.com)

**Copyright:** Chef Software, Inc.

```
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```
"
217,Huawei/dockyard,Go,"# Dockyard - Container And Artifact Repository 

![Dockyard](docs/images/dockyard.jpg ""Dockyard - Container And Artifact Repository"")

## What is Dockyard ?
Dockyard is a container and artifact repository storing and distributing container image, software artifact and virtual images of KVM or XEN. It's key features and goals include:
- Multi supported distribute protocols include Docker Registry V1 & V2 and App Container Image Discovery.
- Multi supported software artifact format include jar, war, tar and so on.
- Multi supported virtual images of KVM, XEN, VirtualBox and so on.
- Container image, software artifact and virtual images encryption, verification and vulnerability analysis.
- Custome distribute protocol by framework base HTTPS and peer to peer. 
- Authentication in distributing process and authorization for public and private repository.
- Supporting mainstream object storage service like Amazon S3, Google Cloud Storage. 
- Built-in object storage service for deployment convenience.
- Web UI portal for all functions above.

## Why it matters ?

## The Dockyard's Story :)

## Runtime configuration

```
runmode = ""dev""

listenmode = ""https""
httpscertfile = ""cert/containerops/containerops.crt""
httpskeyfile = ""cert/containerops/containerops.key""

[site]
domain = ""containerops.me""

[log]
filepath = ""log/backend.log""
level = ""info""

[database]
driver = ""mysql""
uri = ""containerops:containerops@/containerops?charset=utf8&parseTime=True&loc=Asia%2FShanghai""

[deployment]
domains = ""containerops.me""

[dockerv1]
standalone = ""true""
version = ""0.9""
storage = ""/tmp/data/dockerv1""

[dockerv2]
distribution = ""registry/2.0""
storage = ""/tmp/data/dockerv2""

[appc]
storage = ""/tmp/data/appc""

[updateserver]
keymanager = ""/tmp/containerops_keymanager_cache""
storage = ""/tmp/containerops_storage_cache""

```

#### Nginx configuration
It's a Nginx config example. You can change **client_max_body_size** what limited upload file size. You should copy `containerops.me` keys from `cert/containerops.me` to `/etc/nginx`, then run **Dockyard** with `http` mode and listen on `127.0.0.1:9911`.

```nginx
upstream dockyard_upstream {
  server 127.0.0.1:9911;
}

server {
  listen 80;
  server_name containerops.me;
  rewrite  ^/(.*)$  https://containerops.me/$1  permanent;
}

server {
  listen 443;

  server_name containerops.me;

  access_log /var/log/nginx/containerops-me.log;
  error_log /var/log/nginx/containerops-me-errror.log;

  ssl on;
  ssl_certificate /etc/nginx/containerops.me.crt;
  ssl_certificate_key /etc/nginx/containerops.me.key;

  client_max_body_size 1024m;
  chunked_transfer_encoding on;

  proxy_redirect     off;
  proxy_set_header   X-Real-IP $remote_addr;
  proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;
  proxy_set_header   X-Forwarded-Proto $scheme;
  proxy_set_header   Host $http_host;
  proxy_set_header   X-NginX-Proxy true;
  proxy_set_header   Connection """";
  proxy_http_version 1.1;

  location / {
    proxy_pass         http://dockyard_upstream;
  }
}
```

### Database Configuration

#### Database SQL

```
INSERT INTO mysql.user(Host,User,Password) VALUES ('localhost', 'containerops', password('containerops'));
CREATE DATABASE `containerops` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;
GRANT ALL PRIVILEGES ON containerops.* TO containerops@localhost IDENTIFIED BY 'containerops';
FLUSH PRIVILEGES;
```

#### Initlization Tables

```
./dockyard database migrate
```

### Start dockyard service
- Run directly:

```bash
./dockyard daemon run --address 0.0.0.0 --port 443
```

- Run with Nginx:

```bash
./dockyard daemon run --address 127.0.0.1 --port 9911 &
```

## How to build

We are using [glide](https://glide.sh/) as package manager/

* retrieve dependencies
```
glide install
```

* build (with go 1.6+)
```
go build
```

## How to involve
If any issues are encountered while using the dockyard project, several avenues are available for support:
<table>
<tr>
	<th align=""left"">
	Issue Tracker
	</th>
	<td>
	https://github.com/Huawei/dockyard/issues
	</td>
</tr>
<tr>
	<th align=""left"">
	Google Groups
	</th>
	<td>
	https://groups.google.com/forum/#!forum/dockyard-dev
	</td>
</tr>
</table>

### Pull Requests

If you want to contribute to the template, you can create pull requests. All pull requests must be done to the `develop` branch. We are working on build an automated tests with ourself means *containerops*, now we just add *Travis CI* instead.

## Who should join
- Ones who want to choose a container image hub instead of docker hub.
- Ones who want to ease the burden of container image management.

## Certificate of Origin
By contributing to this project you agree to the Developer Certificate of
Origin (DCO). This document was created by the Linux Kernel community and is a
simple statement that you, as a contributor, have the legal right to make the
contribution. 

```
Developer Certificate of Origin
Version 1.1

Copyright (C) 2004, 2006 The Linux Foundation and its contributors.
660 York Street, Suite 102,
San Francisco, CA 94110 USA

Everyone is permitted to copy and distribute verbatim copies of this
license document, but changing it is not allowed.

Developer's Certificate of Origin 1.1

By making a contribution to this project, I certify that:

(a) The contribution was created in whole or in part by me and I
    have the right to submit it under the open source license
    indicated in the file; or

(b) The contribution is based upon previous work that, to the best
    of my knowledge, is covered under an appropriate open source
    license and I have the right under that license to submit that
    work with modifications, whether created in whole or in part
    by me, under the same open source license (unless I am
    permitted to submit under a different license), as indicated
    in the file; or

(c) The contribution was provided directly to me by some other
    person who certified (a), (b) or (c) and I have not modified
    it.

(d) I understand and agree that this project and the contribution
    are public and that a record of the contribution (including all
    personal information I submit with it, including my sign-off) is
    maintained indefinitely and may be redistributed consistent with
    this project or the open source license(s) involved.
```

## Format of the Commit Message

You just add a line to every git commit message, like this:

    Signed-off-by: Meaglith Ma <genedna@gmail.com>

Use your real name (sorry, no pseudonyms or anonymous contributions.)

If you set your `user.name` and `user.email` git configs, you can sign your
commit automatically with `git commit -s`.
"
218,freeorion/freeorion,C++,"FreeOrion
=========

FreeOrion is a free, Open Source, turn-based space empire and galactic conquest
computer game.

FreeOrion is inspired by the tradition of the Master of Orion games, but does
not try to be a clone or remake of that series or any other game.  It builds
on the classic *4X* (eXplore, eXpand, eXploit and eXterminate) model.

By adding scripting capabilities to the game engine the FreeOrion project aims
to give the community an easy way to customize the game mechanics and
presentation to create a living, breathing universe in a *grand campaign* model.


Requirements
------------

FreeOrion requires an *OpenGL 2.1* capable graphic card and a display with a
minimum resolution of at least *800x600*.

FreeOrion requires at least *Windows XP* with *Service Pack 2* or later,
*Mac OSX 10.9* or later or any reasonably recent Linux distribution on x86
compatible processors.  Other operating systems and architectures have reported
to be working by users, but are not actively supported by the FreeOrion project.


Download
--------

[FreeOrion Stable Releases] are the recommended way to play FreeOrion.  Stable
Releases can be obtained as native installer binaries for Windows and Mac OSX
or as source releases for Linux and other UNIX-ish platforms from GitHub.

Some Linux distributions like Fedora, Debian and Arch provide packages of
FreeOrion. Alternatively, FreeOrion is also available on [Flathub].

[FreeOrion Weekly Releases] are in-development releases intended for enthusiasts
and testers, who want to track or contribute to the development.  Weekly
Releases can be obtained as native installer binaries for Windows and Mac OSX
from Sourceforge.


Install
-------

For Windows execute the native installer binary and follow the on-screen
instructions of the installer to install FreeOrion.

For Mac OSX, open the downloaded DMG file and copy the contained FreeOrion
application to your system Applications folder by Drag and Drop.

For Linux or other from-source installations in general please refer to the
[Build Instructions](BUILD.md).

Various Linux distributions provide the stable release of FreeOrion in
a prebuilt way.  Usually you can install those packages by either using
a graphical package manager and searching for *FreeOrion* or by installing the
packages via the command line.

  * [Debian package] stable release: `# apt-get install freeorion`
  * [Ubuntu package] stable release: `# apt-get install freeorion`
  * [Fedora package] stable release: `# dnf install freeorion`
  * [Gentoo package] stable release: `# emerge games-strategy/freeorion`
  * [Void package] stable release: `# xbps-install freeorion`
  * [ArchLinux package] stable release
  * [openSUSE package] stable release: `# zypper in freeorion`

To install FreeOrion from Flathub, follow the instructions to [install Flatpak
and Flathub] and then install [FreeOrion][Flathub].


### Directories

* install directory  
The location of this README.md file, called `<install_dir>` below.
* config - game settings  
Called `<config_dir>` below.  
    * linux  
`$XDG_CONFIG_HOME/freeorion` which defaults to `~/.config/freeorion`  
    * OSX  
`$HOME/Library/Application Support/FreeOrion/` which defaults to `~/Library/Application Support/FreeOrion/`  
    * Windows  
`$APPDATA\FreeOrion`  
* data - local user data, saved games, log files  
Called `<data_dir>` below.  
    * linux  
`$XDG_DATA_HOME/freeorion` which defaults to `~/.config/freeorion`  
    * OSX  
`$HOME/Library/Application Support/FreeOrion/` which defaults to `~/Library/Application Support/FreeOrion/`  
    * Windows  
`$APPDATA\FreeOrion`  
* resource directory - audio, visual and textual UI content, python scripts  
Called `<resource_dir>` below.  
`<install_dir>/default/`  
`<resource_dir>` can be redirected in `<config_dir>/config.xml` or `<config_dir>/persistent_config.xml`  
* stringtables - translation indices for various languages  
    `<resource_dir>/stringtables/`  
* scripting - FreeOrion Content Scripts (FOCS) describing game content (tech, species etc.)  
    `<resource_dir>/scripting/`  
* AI - AI for computer controlled empires  
    `<resource_dir>/python/AI`  

Contact and Getting Help
------------------------

Visit the [FreeOrion Homepage] to learn more about the project.  Also you can
get in touch with the FreeOrion developers and join the community in the
[FreeOrion Forum].


Contribute
----------

The FreeOrion project encourages anybody to contribute to FreeOrion. For more
details please see the [Contribution Guidelines](CONTRIBUTING.md).


License
-------

The FreeOrion *source code* is licensed under the terms of [GPL v2],
*game assets* are licensed under the terms of [CC-BY-SA-3.0] and *game content
scripts* are licensed under the terms of both [GPL v2] and [CC-BY-SA-3.0].
For more details please see the [License File](default/COPYING).

Additional to the immediate project sources, the FreeOrion source tree bundles
some third party projects or assets which may be also licensed under different
terms than the FreeOrion project.  For more details please consult the
accompanying license file.

  * *GiGi* library located within the `GG/` directory.
  * *Roboto* font located within the `default/data/fonts/` directory.
  * *DejaVuSans* located within the `default/data/fonts/` directory.


[FreeOrion Homepage]: http://www.freeorion.org/
[FreeOrion Forum]: http://www.freeorion.org/forum/
[FreeOrion Stable Releases]: https://github.com/freeorion/freeorion/releases
[FreeOrion Weekly Releases]: https://sourceforge.net/projects/freeorion/files/FreeOrion/Test/
[Flathub]: https://flathub.org/apps/details/org.freeorion.FreeOrion
[install Flatpak and Flathub]: https://flatpak.org/setup/
[FreeOrion Development]: https://github.com/freeorion/freeorion
[Debian Package]: https://packages.debian.org/source/sid/freeorion
[Ubuntu Package]: https://launchpad.net/ubuntu/+source/freeorion
[Fedora Package]: https://apps.fedoraproject.org/packages/freeorion
[Gentoo package]: https://packages.gentoo.org/packages/games-strategy/freeorion
[openSUSE Package]: https://build.opensuse.org/package/show/games/freeorion
[Void package]: https://github.com/voidlinux/void-packages/tree/master/srcpkgs/freeorion
[ArchLinux Package]: https://aur.archlinux.org/packages/freeorion/
[GPL v2]: https://www.gnu.org/licenses/gpl-2.0.txt
[CC-BY-SA-3.0]: https://creativecommons.org/licenses/by-sa/3.0/legalcode
"
219,sous-chefs/haproxy,Ruby,"# haproxy Cookbook

[![CI State](https://github.com/sous-chefs/haproxy/workflows/ci/badge.svg)](https://github.com/sous-chefs/haproxy/actions?query=workflow%3Aci)
[![Cookbook Version](https://img.shields.io/cookbook/v/haproxy.svg)](https://supermarket.chef.io/cookbooks/haproxy)
[![OpenCollective](https://opencollective.com/sous-chefs/backers/badge.svg)](#backers)
[![OpenCollective](https://opencollective.com/sous-chefs/sponsors/badge.svg)](#sponsors)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)

Installs and configures HAProxy.

## Maintainers

This cookbook is maintained by the Sous Chefs. The Sous Chefs are a community of Chef cookbook maintainers working together to maintain important cookbooks. If you’d like to know more please visit [sous-chefs.org](https://sous-chefs.org/) or come chat with us on the Chef Community Slack in [#sous-chefs](https://chefcommunity.slack.com/messages/C2V7B88SF).

## Requirements

* HAProxy `stable` or `LTS`
* Chef 13.9+

### Platforms

This cookbook officially supports and is tested against the following platforms:

* debian: 8 & 9
* ubuntu: 16.04 & 18.04
* centos: 7
* amazonlinux: 2

PRs are welcome to add support for additional platforms.

### Examples

Please check for working examples in [TEST](./test/fixtures/cookbooks/test/)

## Common Resource Features

HAProxy has many configurable options available, this cookbook makes the most popular options available as resource properties.

If you wish to use a HAProxy property that is not listed the `extra_options` hash is available to take in any number of additional values.

For example, the ability to disable listeners is not provided out of the box. Further examples can be found in either `test/fixtures/recipes` or `spec/test/recipes`. If you have questions on how this works or would like to add more examples so it is easier to understand, please come talk to us on the [Chef Community Slack](http://community-slack.chef.io/) on the #sous-chefs channel.

```ruby
haproxy_listen 'disabled' do
  bind '0.0.0.0:1337'
  mode 'http'
  extra_options('disabled': '')
end
```

The `extra_options` hash is of `String => String` or `String => Array`. When an `Array` value is provided. The values are looped over mapping the key to each value in the config.

For example:

```ruby
haproxy_listen 'default' do
  extra_options(
    'http-request' => [ 'set-header X-Public-User yes', 'del-header X-Bad-Header' ]
    )
end
```

Becomes:

```haproxy
listen default
  ...
  http-request set-header X-Public-User yes
  http-request del-header X-Bad-Header
```

## Resources

* [haproxy_acl](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_acl.md)
* [haproxy_backend](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_backend.md)
* [haproxy_cache](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_cache.md)
* [haproxy_config_defaults](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_config_defaults.md)
* [haproxy_config_global](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_config_global.md)
* [haproxy_fastcgi](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_fastcgi.md)
* [haproxy_frontend](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_frontend.md)
* [haproxy_install](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_install.md)
* [haproxy_listen](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_listen.md)
* [haproxy_mailer](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_mailer.md)
* [haproxy_peer](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_peer.md)
* [haproxy_resolver](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_resolver.md)
* [haproxy_service](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_service.md)
* [haproxy_use_backend](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_use_backend.md)
* [haproxy_userlist](https://github.com/sous-chefs/haproxy/tree/master/documentation/haproxy_userlist.md)

## Configuration Validation

The `haproxy.cfg` file has a few specific rule orderings that will generate validation errors if not loaded properly. If using any combination of the below rules, avoid the errors by loading the rules via `extra_options` to specify the top down order as noted below in config file.

### frontend & listen

```haproxy
  tcp-request connection
  tcp-request session
  tcp-request content
  monitor fail
  block (deprecated)
  http-request
  reqxxx (any req excluding reqadd, e.g. reqdeny, reqallow)
  reqadd
  redirect
  use_backend
```

```ruby
  extra_options(
    'tcp-request' => 'connection set-src src,ipmask(24)',
    'reqdeny' => '^Host:\ .*\.local',
    'reqallow' => '^Host:\ www\.',
    'use_backend' => 'dynamic'
  )
```

### backend

```haproxy
  http-request
  reqxxx (any req excluding reqadd, e.g. reqdeny, reqallow)
  reqadd
  redirect
```

```ruby
  extra_options(
    'http-request' => 'set-path /%[hdr(host)]%[path]',
    'reqdeny' => '^Host:\ .*\.local',
    'reqallow' => '^Host:\ www\.',
    'redirect' => 'dynamic'
  )
```

## Contributors

This project exists thanks to all the people who [contribute.](https://opencollective.com/sous-chefs/contributors.svg?width=890&button=false)

### Backers

Thank you to all our backers!

![https://opencollective.com/sous-chefs#backers](https://opencollective.com/sous-chefs/backers.svg?width=600&avatarHeight=40)

### Sponsors

Support this project by becoming a sponsor. Your logo will show up here with a link to your website.

![https://opencollective.com/sous-chefs/sponsor/0/website](https://opencollective.com/sous-chefs/sponsor/0/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/1/website](https://opencollective.com/sous-chefs/sponsor/1/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/2/website](https://opencollective.com/sous-chefs/sponsor/2/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/3/website](https://opencollective.com/sous-chefs/sponsor/3/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/4/website](https://opencollective.com/sous-chefs/sponsor/4/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/5/website](https://opencollective.com/sous-chefs/sponsor/5/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/6/website](https://opencollective.com/sous-chefs/sponsor/6/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/7/website](https://opencollective.com/sous-chefs/sponsor/7/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/8/website](https://opencollective.com/sous-chefs/sponsor/8/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/9/website](https://opencollective.com/sous-chefs/sponsor/9/avatar.svg?avatarHeight=100)
"
220,Rhombik/rhombik-object-repository,HTML,"rhombik-object-repository
=============

This is all development information. The site isn't up to par yet, but you can see an example at [rhombik.com](http://alpha.rhombik.com)

rhombik-object-repository is an AGPL licensed object repository, competing in the same sphere as [thingiverse](http://thingiverse.com), [cubehero](http://cubehero.com), [youMagine](http://www.youmagine.com)
, and [bld3r](http://bld3r.com).

###What makes rhombik different?###

Open source.

If you're one of the independent 3D printer manufacturers like lulzbot, printrbot, or makers tool works; you need to send your users to your competitors website. It was somewhat tolerable before they threw ""makerbot"" branding all over the site. Now it's not.

So where should they send your users? Most of the alternatives have the potential to go the same way as thingiverse. If you want a platform that isn't going to get baught out by your competitors, you need open source.

We're commited to federation. If rhombik ever goes down or gets baught, you can just take our source and launch a new repo. It will be easy for users to migrate their projects. Don't bet on another horse you don't have control of.

###Developer information###

Rhombik uses:

 * Django web framework
 * pymarkdown
 * Haystack search
 * Celery queueing
 * selenium for generating thumbnails of javascript previewers
 * ...A bunch more that aren't really important enough to list.

Right now the code base is a bit of a mess. We're big believers in ""release early, release often, hopefully get around to writing ~~better~~ some test cases"". If you're interested in devloping for this, shoot me an [email](mailto://traverse.da@gmail.com) or visit our irc channel #rhombik on freenode. I can help get you up to speed on the code base. Take a look at the bug list for an idea of what needs doing.

---
To set up a development enviroment simply

    ##Let's make a new directory so the rhombik dependencies don't interfere with the rest of the system
    mkdir rhombik-env
    ##Creates a virtualenv using python 2
    virtualenv -p $(which python2) rhombik-env
    cd rhombik-env
    ##Tell our shell to use our rhombik enviroment version of python
    source bin/activate
    ##Download the latest rhombik
    git clone https://github.com/Rhombik/rhombik-object-repository.git
    cd rhombik-object-repository
    ##Tell the rhombik-env python to download all of our python dependencies. This will install them to rhombik-env, not our normal python.
    pip install -r requirements.txt
    #follow the prompts to add a new devleoper superuser account to the test DB.
    python manage.py syncdb

Unfortunately django-pipeline requires the nodejs coffee command, which means we can't just rely on pip for all of our dependencies. Install nodejs and run.

    sudo npm install coffee-script less rapydscript -g

Note that npm can have errors installing coffee on some distributions. If you notice javascript breaking you can try reinstalling coffee script using your distro's package manager.

We also need sass. Run 

    gem install sass


Run the server.

    python manage.py runserver

Then navigate to http://localhost:8000

You might also want to actually run the task queue services, to get a better idea of what performance is actually like.

    celery worker -A Settings --loglevel DEBUG

And add ""CELERY_ALWAYS_EAGER=False"" to your ""Settings/settings.py"".

---

If you want to know how we did something, please drop us a line. The code is probably pretty gnarly, and not the good kind of gnarly. The bad kind, where things are gnarled., but we're here to explain anything that doesn't make sense.

We would love to restructure this at some point.

**project**

This app has all our default views. It also has the basic ""project"" model.

**filemanager**

This app contains the basic structure of our file systems. 

It has a ""fileobject"" model. That model contains the actual uploaded file. Each fileobject gets attached to a project. It has fields for subfolders, and ""rendertype"" which tells the front end what to use to display a preview and what to use to create a thumbnail. The rendertype field is filled by ""thumbnailer.thumbnailer2"".

It also has a ""thumbobject"" model. Each thumbobject is a png preview of a fileobject. It attaches to a fileobject. It gets the actual image from ""thumbnailer.thumbnailer2"". It's unique for [project, sizex, sizey]. This allows you to generate thumbnails of different sizes for each project.

**thumbnailer**

At the core of our service is a very robust thumbnailer. It takes screenshots of javascript renderers, so that what the use sees in a thumbnail and what the user see in a live preview are identical. Thumbnailing like that can take a while for more complicated file types, so we have a queueing system and an ajax thumbnail loader. It's overly complicated and poorly documents. But the short story is that in order to get a thumbnail you need to call somefileobject.get\_thumbnail(thumbX, thumbY), and then pass that data in a list[] to the gallery.html template. So something like

```python
#in your view
#Set up a list to store your images in
myimages = []
#get the first fileobject in the database.
filedata = fileobject.objects.get(pk=1))
#generate a 64x64 thumbnail (or ajax loader if it takes too long) for our file
myimages.append(filedata.get_thumb(64,64))

render\_to\_string('mytemplate.html', dict(myimages=myimages, testgallery=""testgallery"")
```
```
#In ""mytemplate.html""
{% include ""gallery.html"" with images=myimages galleryname=testgallery %}
```

**multiuploader**

Multiuploader contains the uploader code.



"
221,vmware-tanzu/tgik,Shell,"<p align=""center""><img src=""tgik-repo.png"" width=""750""></p>


Official repository for TGIK!

TGIK is a weekly live video stream that we broadcast live at 1pm pacific time from the VMware Cloud Native headquarters (usually) in Bellevue, Washington all about Kubernetes.

The [index](playlist.md) contains a list of all episodes.

The [official YouTube channel can be found here](https://tgik.io).

## Suggest an episode

If you have an idea for TGIK please [open an issue in the TGIK issue tracker](https://github.com/vmware-tanzu/tgik/issues/new).
We pick the episodes as we see fit with what's happening in the community.
If we select your suggestion we will link back to your original work here!
"
222,ztrhgf/Powershell_CICD_repository,PowerShell,"# Fully automated CI/CD solution for (not just) PowerShell content management in your Active Directory environment
Repository contains necessary files + installer that will create your own fully automated company CD/CD like repository, which can be used to manage the whole lifecycle of the (primarily) PowerShell content. So the only thing you will have to worry about now on, is code writing :-)
Everything else, like code backups, validations, auditing, signing, modules generation, content distribution etc will be automated.

- To get some quick insight, watch this [short introduction video](https://youtu.be/-xSJXbmOgyk) or super short examples of [new function creation](https://youtu.be/XvTe6ppsHgI), [new 'global' variable creation](https://youtu.be/Cb981bQ5SV4), [script validations](https://youtu.be/myxzPZZ8gEk). For more examples and explanation of how this works watch [quite long but detailed video](https://youtu.be/R3wjRT0zuOk) (examples starts at 10:12). Případně [českou verzi videa](https://youtu.be/Jylfq7lYzG4).

[<img src=""https://media.giphy.com/media/hAfuEpFUrP2Nn79v7c/giphy.gif"" width=""30%"">](https://youtu.be/037Ki_Hx0kY4)

- **!!! To test this solution in safe manner in under 5 minutes, check this out !!!** [<img src=""https://media.giphy.com/media/27URm9VNtXQyaKqmvf/giphy.gif"" width=""30%"">](https://youtu.be/o_QlF5YCMGU)

- To set this up in your environment please follow these [installation instructions](https://github.com/ztrhgf/Powershell_CICD_repository/blob/master/1.%20HOW%20TO%20INSTALL.md).

- In case you like this solution, found any bug or have improvement suggestion, please contact me at **ondrejsebela'at'gmail.com**.


# Main features:
- **unifies PowerShell environment across whole Active Directory**
  - same PowerShell modules, functions and variables everywhere (but can be customized by editing modulesConfig.ps1)
  - (optional) global Powershell profile to unify repository administrators experience
    - shows how many commits is this console behind, simplifies prompt, omits commands with plaintext password from history etc
- literally **all scripting content from whole Active Directory environment can be stored and managed from one place**
  - thanks to possibility to distribute any content to any location
- **based on GIT**
  - version control system
  - auditing (who changed what and when)
  - ...
- **extremely simplifies PowerShell content management by automating**
  - **code validation**
  - **code formatting**
  - **content distribution**
      - using: GIT hooks, PowerShell scripts, GPO and VSC editor Workspace settings
- adheres to the principles of **configuration as a code**
- written by Windows administrator for Windows administrators i.e. 
  - **easy to use**
    - fully managed from Visual Studio Code editor
    - GIT knowledge not needed
    - Refresh-Console function for forcing synchronization of repository data on any client and importing such data to running Powershell console
  - **boost PowerShell adoption between admins**, because of easy know-how/functions sharing
  - **customizable**
    - everything is written in Powershell so you can easily add/remove features
  - **idiot-proof :)**
    - won't let you commit change, that would break your environment 
      - script contain syntax errors
    - warns against commiting change, that could break your environment (if changed object is used elsewhere)
      - modification of parameters of the function (applies just for functions in scripts2module folder)
      - modification of variable value (applies just for variables in Variables PS module)
      - deletion of function or variable
     - etc
- **no paid tools needed**
- last but not least
  - automatic **scheduled task creation** (from XML definition), so ps1 script (modules that it depend on) and sched. task, that should run it, can be distributed together
  - automatic **script signing** (if enabled)

- check [examples](https://github.com/ztrhgf/Powershell_CICD_repository/blob/master/2.%20HOW%20TO%20USE%20-%20EXAMPLES.md) or [watch short introduction video](https://youtu.be/-xSJXbmOgyk) for getting better insight
  
# [Installation](https://github.com/ztrhgf/Powershell_CICD_repository/blob/master/1.%20HOW%20TO%20INSTALL.md)
# [Examples](https://github.com/ztrhgf/Powershell_CICD_repository/blob/master/2.%20HOW%20TO%20USE%20-%20EXAMPLES.md)
# [Repository logic & content explanation](https://github.com/ztrhgf/Powershell_CICD_repository/blob/master/3.%20SIMPLIFIED%20EXPLANATION%20OF%20HOW%20IT%20WORKS.md)
# [FAQ](https://github.com/ztrhgf/Powershell_CICD_repository/blob/master/FAQ.md)
# [Changelog](https://github.com/ztrhgf/Powershell_CICD_repository/releases)
"
223,jonatasbaldin/awesome-awesome-awesome,,"# awesome-awesome-awesome ![Awesome Badge](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)

Awesome list of repositories of awesome lists 🤷

* [jonatasbaldin/awesome-awesome-awesome](https://github.com/jonatasbaldin/awesome-awesome-awesome)
* [sindresorhus/awesome](https://github.com/sindresorhus/awesome)
* [bayandin/awesome-awesomeness](https://github.com/bayandin/awesome-awesomeness)
* [orsinium/generated-awesomeness](https://github.com/orsinium/generated-awesomeness)
* [t3chnoboy/awesome-awesome-awesome](https://github.com/t3chnoboy/awesome-awesome-awesome)
* [lockys/AwesomeSearch](https://github.com/lockys/AwesomeSearch)
* [emijrp/awesome-awesome](https://github.com/emijrp/awesome-awesome)
"
224,nartc/nest-mean,TypeScript,"# NestJS Tutorial Repository

Repository will be separated into `server` and `client` directory for **NestJS** backend and **Angular** frontend resepctively. 

- [x] Server repository
- [x] Client repository
- [x] Docker support
- [x] Hook up Server and Client

## Server-side (NestJS)

This repository houses the Project's backend written using **NestJS**

- **NSwag**: Nswag allows us to generate API Calls to our Backend on our Frontend in forms of Functions. The abstraction of **HttpClientModule** takes place in the generated file.
- **Steps**:
    1. `cd ./server` & `npm i` to install all dependencies for Server side
    2. Have an instance of **MongoDB** running (`mongod`). If you use an IDE like WebStorm, I have a script called: `mongo:local` that is going to run `mongod` subsequently so you can setup a `Compound Run Configuration` with `start:dev` and `mongo:local` to start the Backend with ease.
    3. `npm run start:dev` to start the server
- **Note**: If there's issue connecting to local MongoDB and you make sure that you already have `mongod` running, go to `config/default.ts` and check if the `MONGO_URI` is correct. 


## Client-side (Angular)

This repository houses the Project's frontend written using **Angular 6**

- **Ant Design**: The components design is by AntDesign (https://ng.ant.design/docs/introduce/en). I really like the subtle looks of AntDesign.
- **Steps**: `npm i` to install all the dependencies then just start the application with `ng serve`
- **Note**: Might be worth it to take a look at `proxy.conf.json` and how I setup the CLI to use the `proxy` file when serving so that we can call our backend on `localhost:3000`. This is so-called **Cross Domains Request** and our backend does not have CORS setup. Proxy will help us making the requests from `4200` to `3000`.

## Docker

Docker is supported.

- **Branch**: `docker`
- **Steps**: Just clone the repository, check out `docker` branch then from `root` directory, run `docker-compose up` and Docker will take over.
- **Note**: Angular application will be served by NGINX on `localhost`; Nest application will be running on `localhost:3000`; **cAdvisor** which monitors our containers will be running on `localhost:8080`. Again, it's worthwhile to explore the Dockerfile in both `client` and `server` directory; also `nginx.conf` and `docker-compose.yml` to get the gist of how Docker and Docker Compose work.

P.S: Pull Requests, Contributions are most definitely welcomed :)
"
225,mateodelnorte/meta-npm,JavaScript,"[![Build Status](https://travis-ci.com/mateodelnorte/meta-npm.svg?branch=master)](https://travis-ci.com/mateodelnorte/meta-npm)

# meta-npm

npm plugin for [meta](https://github.com/mateodelnorte/meta)

Using `meta npm link && meta npm link --all` enables a lerna-like experience for local development by creating symlinks so each project uses the development version of any other project in the meta repo:

For example, meta itself is developed in this way:

```sh
# install meta
npm i -g meta

# clone and enter the meta repo
meta git clone git@github.com:mateodelnorte/meta.git
cd ./meta

# install plugins
npm install

# run install for all child repos
meta npm install

# create symlinks to/from all child repos
meta npm link --all

# link meta itself globally
npm link
```

## Usage

```
➜  meta npm

  Usage: meta-npm [options] [command]

  Options:

    -h, --help    output usage information

  Commands:

    clean         delete the node_modules folder in meta and child repositories
    install       npm install meta and child repositories
    link [--all]  npm link child repositories where used within child and meta repositories
    outdated      check outdated dependencies in meta and child repositories
    publish       npm publish meta and child repositories
    run           npm run commands against meta and child repositories
    symlink       directly symlink meta and child repositories without using global npm link
    help [cmd]    display help for [cmd]
```

"
226,reportportal/reportportal,Shell,"# [ReportPortal.io](http://ReportPortal.io)

[![Join Slack chat!](https://reportportal-slack-auto.herokuapp.com/badge.svg)](https://reportportal-slack-auto.herokuapp.com)
[![stackoverflow](https://img.shields.io/badge/reportportal-stackoverflow-orange.svg?style=flat)](http://stackoverflow.com/questions/tagged/reportportal)
[![GitHub contributors](https://img.shields.io/badge/contributors-102-blue.svg)](https://reportportal.io/community)
[![Docker Pulls](https://img.shields.io/docker/pulls/reportportal/service-api.svg?maxAge=25920)](https://hub.docker.com/u/reportportal/)
[![License](https://img.shields.io/badge/license-Apache-brightgreen.svg)](https://www.apache.org/licenses/LICENSE-2.0)
[![Build with Love](https://img.shields.io/badge/build%20with-❤%EF%B8%8F%E2%80%8D-lightgrey.svg)](http://reportportal.io?style=flat)


Report Portal organized into multiple repositories.

Application Core based on micro-services architecture and includes next mandatory services:
![structure](https://github.com/reportportal/reportportal/blob/master/public/rp_repo_structure.png)

## Repositories structure

ReportPortal **server side** consists of the following services:
- [`service-authorization`](https://github.com/reportportal/service-authorization) Authorization Service. In charge of access tokens distribution
- [`service-api`](https://github.com/reportportal/service-api) API Service. Application Backend
- [`service-ui`](https://github.com/reportportal/service-ui) UI Service. Application Frontend
- [`service-index`](https://github.com/reportportal/service-index) Index Service. Info and health checks per service.
- [`service-analyzer`](https://github.com/reportportal/service-auto-analyzer) Analyzer Service. Finds most relevant test fail problem.
- [`gateway`](https://github.com/containous/traefik) Traefik Gateway Service. Main entry point to application. Port used by gateway should be opened and accessible from outside network.
- [`rabbitmq`](https://github.com/rabbitmq) Load balancer for client requests. Bus for messages between servers.
- [`minio`](https://github.com/minio/minio) Attachments storage.

Available plugins developed by ReportPortal team:

- [`plugin-bts-jira`](https://github.com/reportportal/plugin-bts-jira) JIRA Plugin. Interaction with JIRA. [Link to download](https://search.maven.org/search?q=g:%22com.epam.reportportal%22%20AND%20a:%22plugin-bts-jira%22)
- [`plugin-bts-rally`](https://github.com/reportportal/plugin-bts-rally) Rally Plugin. Interaction with Rally. [Link to download](https://search.maven.org/search?q=g:%22com.epam.reportportal%22%20AND%20a:%22plugin-bts-rally%22) 
- [`plugin-saucelabs`](https://github.com/reportportal/plugin-saucelabs) Sauce Labs Plugin. Interaction with Sauce Labs. [Link to download](https://search.maven.org/search?q=g:%22com.epam.reportportal%22%20AND%20a:%22plugin-saucelabs%22)

**Client side** adapters related repositories:

- [`client-*`](https://github.com/reportportal?utf8=%E2%9C%93&q=client-) - API integrations. Http clients, which process HTTP request sending.
- [`agent-*`](https://github.com/reportportal?utf8=%E2%9C%93&q=agent-) - Frameworks integration. Custom reporters/listeners, which monitor test events and trigger event sending via [`client-*`](https://github.com/reportportal?utf8=%E2%9C%93&q=client-)
- [`logger-*`](https://github.com/reportportal?utf8=%E2%9C%93&q=logger-) - Logging integration. Logger appenders, which help to collect logs, bind it with test-case item via `agent-*` and send to server via `client-*`

**Other repositories** stored according to next rules
- [`service-*`](https://github.com/reportportal?utf8=%E2%9C%93&q=service-) - micro-services which are a part of Application
- [`commons-*`](https://github.com/reportportal?utf8=%E2%9C%93&q=commons-) - common libraries, models, etc., used by micro-services


## Installation steps

#### Simple setup with Docker
Best for demo purposes and small teams. MongoDB database included into the compose.

1. Install [Docker](https://docs.docker.com/engine/installation/) ([Engine](https://docs.docker.com/engine/installation/), [Compose](https://docs.docker.com/compose/install/))
2. Download [Example of compose descriptor](https://github.com/reportportal/reportportal/blob/master/docker-compose.yml) to any folder

  ```Shell
  $ curl https://raw.githubusercontent.com/reportportal/reportportal/master/docker-compose.yml -o docker-compose.yml
  ```
3. Deploy ReportPortal using `docker-compose` within the same folder

  ```Shell
  $ docker-compose -p reportportal up
  ```
To start ReportPortal in daemon mode, add '-d' argument:
  ```Shell
  $ docker-compose -p reportportal up -d
  ```  
4. Open in your browser IP address of deployed environment at port `8080`

  ```
  $ http://IP_ADDRESS:8080
  ```
5. Use next login\pass for access: `default\1q2w3e` and  `superadmin\erebus`. 

>Please change admin password for security.

>Mentioned compose file deploy all available Bug Tracking System integrations, which not always needed, but use resources

#### Production-ready set and Custom deployment with Docker

For production usage we recommend to:
- deploy MongoDB database at separate environment, and connect App to this server. MongoDB is mandatory part.
- choose only required Bug Tracking System integration service. Exclude the rest

To customize deployment and make it production-ready please follow [customization steps and details](https://github.com/reportportal/reportportal/wiki/Production-Ready-set-and-Deployment-Customization)


## Integration. How to get log data in

You should add **Client Side** code inside your test automation. It consists of:

- [`client-*`](https://github.com/reportportal?utf8=%E2%9C%93&q=client-) - API integrations. Http clients, which process HTTP request sending. E.g. for Java ([`client-java-*`](https://github.com/reportportal?utf8=%E2%9C%93&q=client-java-))
- [`agent-*`](https://github.com/reportportal?utf8=%E2%9C%93&q=agent-) - Frameworks integration. Custom reporters/listeners, which monitor test events and trigger event sending via [`client-*`](https://github.com/reportportal?utf8=%E2%9C%93&q=client-)
- [`logger-*`](https://github.com/reportportal?utf8=%E2%9C%93&q=logger-) - Logging integration. Logger appenders, which helps to collect logs, bind it with test-case via `agent-*` and send to server via `client-*`

[Integration steps and documentation](http://reportportal.io/#documentation/%EF%BB%BFTest-framework-integration)

## Contribution

There are many different ways to contribute to Report Portal's development, just find the one that best fits with your skills. Examples of contributions we would love to receive include:

- **Code patches**
- **Documentation improvements**
- **Translations**
- **Bug reports**
- **Patch reviews**
- **UI enhancements**

Big features are also welcome but if you want to see your contributions included in Report Portal codebase we strongly recommend you start by initiating a [chat through our Team in Slack](https://reportportal-slack-auto.herokuapp.com).

[Contribution details](https://github.com/reportportal/reportportal/wiki/Contribution)

## Documentation

* [User Manual](http://reportportal.io/#documentation)
* [Wiki and Guides](https://github.com/reportportal/reportportal/wiki)


## Community / Support

* [**Slack chat**](https://reportportal-slack-auto.herokuapp.com)
* [**Security Advisories**](https://github.com/reportportal/reportportal/blob/master/SECURITY_ADVISORIES.md)
* [GitHub Issues](https://github.com/reportportal/reportportal/issues)
* [Stackoverflow Questions](http://stackoverflow.com/questions/tagged/reportportal)
* [Twitter](http://twitter.com/ReportPortal_io)
* [Facebook](https://www.facebook.com/ReportPortal.io)
* [VK](https://vk.com/reportportal_io)
* [YouTube Channel](https://www.youtube.com/channel/UCsZxrHqLHPJcrkcgIGRG-cQ)

## License

Report Portal is [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).

"
227,OpenScienceMOOC/Module-6-Open-Access-to-Research-Papers,HTML,"# Module 6: Open Access to Research Papers

## Rationale <a name=""Rationale""></a>

Making scholarly research outputs openly available to everyone is simple, legal, and has demonstrable benefits to authors, making it a good beginning step for a researcher just beginning to explore the open world. There is a set of knowledge required to navigate the Open Access landscape, involving copyright, article status, repositories, and economics. This module will introduce key concepts and tools that can help a researcher make their work openly available and maximize the benefits to themselves and others.


## Learning outcomes <a name=""Learning outcomes""></a>

1. Researchers will become familiar with the history of scholarly publishing, and development of the present Open Access landscape.
1. Researchers will gain a multi-stakeholder insight into Open Access, and be able to convey a balanced overview of the perceived advantages and disadvantages associated with Open Access publishing.
1. Researchers will be able to describe some of the complexities of the current the Open Access landscape, including allowances for self-archiving and embargoes, copyright transfer, and publishing contracts.
1. Based on community-specific practices, the researcher will be able to use the different types of outlets (repositories) available for self-archiving, as well as the range of Open Access journal types available to them.
1. Each researcher will able to make all of their own research papers Open Access through a combination of journals and development of a personal self-archiving protocol.
1. Researchers will be able to describe the current ebb and flow in the debates around preprints, and be able to locate and use relevant disciplinary preprint platforms.
1. Researchers will be able to use services like ImpactStory to track the proportion of their research that is Open Access.

## Development team

* Charlotte Weber - Team Lead
* Jon Tennant - Dinosaur whisperer
* Tobias Steiner - Open Ed Quizzard
* Encarni Martínez - Wonderful Brains
* Ritwik Agarwal - Open Science Activist
* Erzsébet Tóth-Czifra - Digital Human
* Paola Masuzzo - Batman of Open Science
* Britta Nölte - Aktivseniorin
* Andy  Nobes - Socratic Goldfish
* Josmel Pacheco-Mendoza - Regulus
* George Macgregor - Repository Revolutionary

## Key documents <a name=""Key documents""></a>

- [Contributing](CONTRIBUTING.md)
- [Module design protocol](https://github.com/OpenScienceMOOC/Module-6-Open-Access-to-Research-Papers/tree/master/production_toolkit/MODULE_DESIGN_PROTOCOL.md)
- [Code of conduct](CODE_OF_CONDUCT.md)
- [Key elements](key_elements.md)

## Code of conduct

All modules of the Open Science MOOC are released with a [Contributor Code of Conduct](CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.

## Licenses <a name=""Licenses""></a>

### Content 
MOOC content license: [![CC0 Public Domain Dedication](https://img.shields.io/badge/License-CC0%201.0-lightgrey.svg)](https://creativecommons.org/publicdomain/zero/1.0/)
"
228,planetfederal/suite,Java,"Boundless Server
================

These instructions describe how to build platform independent components of Boundless Server.

## Prerequisites

The following base software packages are required.

* Java 8 - [Java Development Kit (JDK)](http://www.oracle.com/technetwork/java/javase/downloads/index.html) - 1.8+ or [Open JDK](http://openjdk.java.net/install/)
* [Apache Ant](http://ant.apache.org/bindownload.cgi) - 1.8+
* [Git](http://git-scm.com/) - 1.7.10+*

Some modules require additional packages:

* [Apache Maven](http://maven.apache.org/download.html) - 3.0+
* [Python 2](https://www.python.org/downloads/)
* [JSTools](https://github.com/whitmo/jstools) - Latest
* [Sphinx](http://sphinx.pocoo.org/) - 1.0+ (the full build requires LaTeX support)
* [NodeJS](http://nodejs.org/) - Latest version
  * [Bower](http://bower.io/) - `npm install -g bower`
  * [Grunt](http://gruntjs.com/) - `npm install -g grunt-cli`
  * [Gulp](http://gulpjs.com/) - `npm install -g gulp`
* [GDAL](https://trac.osgeo.org/gdal/wiki/DownloadingGdalBinaries) - 1.9.1 or newer, must include ogr2ogr

Ensure that all the above are installed so that the associated executables are on the 
`PATH` of the user building the server. 

## Developer Guide

If you just want to build server locally, and do not intend to contribute changes, you can skip this section and proceed to the [Quickstart](#quickstart).

### Getting Started

1. [Fork](https://guides.github.com/activities/forking/) the server projuct to your github account. This fork will be used to stage your changes.

1. Clone your fork of server in the desktop:

        % git clone git://github.com/<yourusername>/server.git server
        % cd server

1. Add an upstream remote pointing to the boundless server project:

        % git remote add upstream https://www.github.com/boundlessgeo/server.git server

1. This remote will be used to update your fork of server to the latest from the boundless suie repository. Any time you want to get the latest changes, simply pull from the upstream remote:

        % git pull upstream master

### Submodules

Server includes code from several other projects by way of [Git Submodules](https://git-scm.com/book/en/v2/Git-Tools-Submodules). Most server submodules are in [geoserver/externals](https://github.com/boundlessgeo/server/tree/master/geoserver/externals). Submodules function like a link to a specific revision of a project. When using git on the desktop, submodules behave like single files as long as you are outside of them. However, they can be traversed like directories and once inside a submodule, git behaves as if you are in a checkout of the submodule project itself. 

1. When inside a submodule, you can update the revision it links to by pulling from a remote. Many submodules link to a specific branch, so make sure you get the right one (ask a developer if you are unsure)!

        % cd geoserver/externals/geoserver
        % git pull origin 2.7.x

2. In order for this update to be reflected in the server project, it must be commited like any other change:

        % cd ../
        % git add geoserver
        % git commit -m ""update geoserver submodule""

3. You can then push this change to your fork and create a pull request, like any other commit.

### Release Branches

During regular development, server changes are commited to the master branch. Prior to a release, a release branch (of the form `r4.7`) is created. Any changes should be made against that branch instead, and backported to master if necessary. 

1. When commiting a change to the release branch, note the commit id:

        [r4.7 0c66de5] update geoserver submodule

2. To backport this commit to master, switch to the master branch and use `cherry-pick` to copy the commit. Remember to push your change up to the server repository:

        % git checkout master
        % git cherry-pick 0c66de5
        % git push upstream master

Certain submodules (mainly geoserver) will also have release specific branches. If you are updating a submodule on the release branch, first check if it has its own branch for this release (usually of the form `server-4.7`). Ask a developer if you are unsure.

### What's next

To build Boundless Server, go to step 2 of the [Quickstart](#quickstart).

For more information about the build system, see the [Build System Overview](#build-system-overview).

For information on the individual components that comprise server, follow the links in the [Modules](#modules) section.

If you are preparing for a new release of Boundless Server, refer to the [Release Procedure](#release-procedure) section.

## Quickstart

1. Clone the repository:

        % git clone git://github.com/boundlessgeo/server.git server
        % cd server

1. Initialize submodule dependencies:

        % git submodule update --init --recursive

1. Do a full build:

        % ant

1. Or build the module of your choice:

        % cd docs
        % ant 

## Build System Overview

The server repository is made up a number of modules (ie projects). During development 
typically modules are built individually as opposed to all at once. The primary build 
tool for server is Ant. For some modules the ant script delegates to the modules native build tool such as Maven or Sphinx. 

All top level modules have a `build.xml` that defines the following targets:

1. `build` - Builds the project, the result of this is something deployable in the development environment. This target is the default.
1. `clean` - Cleans the project deleting all build artifacts. 
1. `assemble` - Assembles the project into a zip archive suitable for deployment in production. This is the artifact consumed by installer builders. 
1. `publish` - Publishes the zip archive to a final location.
1. `all` - Runs all the above targets.

Building for development purposes typically looks like:

    ant clean build

Building for deployment purposes typically looks like:

    ant clean build assemble

Or to build everything:

    ant all

The highest level build files simply delegate to the lower level build files. This means that performing a top level build and performing a build for a specific module will result in the same artifacts being produced for that module.

The [build](build) directory contains common build files used by modules.

 * [common.xml](build/common.xml) - Common ant targets used by module 
 build files. Every module build file imports this file as the first step.
 * [build.properties](build/build.properties) - Default build properties that can
 be overridden on a global or per module basis.
 * [build_properties.py](build/build_properties.py) - Used by the doc build to parse properties from [build.properties](build/build.properties) into the server build environment.

### Build Properties

Many aspects of the server build such as file locations, executable names, etc... are 
parameterized into build properties. The default [build.properties](build/build.properties)
contains a list of default values. Often these default properties must be overridden to 
cater to the environment (eg. Windows vs Unix) or to cater to specifics of a particular module. 

There are two ways to override build properties: 

1. The first is to specify them directly to the ant build command with the Java system property (-D) syntax. For example:

          % ant -Dserver.build_cat=release build

1. Creating a file named `local.properties` either at the global level or at the module level. The global `local.properties` is located in the [build](build) directory next to 
`build.properties`. Module specific `local.properties` files are located next to the module `build.xml` file. Naturally the module specific local properties file overrides properties from its global counterpart. 

Using any combination of the above method it should never be necessary to modify the `build.properties` file directly.

The build.properties file *should* be modified when development begins on a new server version, in order to update the appropriate version numbers and geoserver extensions.

### Release Procedure

When performing a new Server release, there are several changes that should be made in this repo to prepare for the release.

## Update Versions

In [build.properties](build/build.properties), update the following properties:

* `server.version` - This controls the version that is displayed on the Dashboard and in the Docs, and should be updated for every release.

If this release includes a change to the geoserver major version (For example, from 2.12 to 2.13), you also need to update the following properties:

* `gt.major_version`
* `gwc.major_version`
* `gs.major_version`

Additionally, you will need to update the GeoTools, GeoWebcache, and GeoServer versions in the following files:

* geoserver/webapp/pom.xml
* geowebcache/pom.xml


## Update Submodules

[Update any submodules](#submodules) that have changed since the last release. For a minor release, this is usually just [geotools](geoserver/geotools), [geowebcache](geoserver/geowebcache), and [geoserver](geoserver/geoserver). This should typically be updated to the latest commit in the stable branch.

For a major release, also be sure to update [geomesa](geoserver/externals/geomesa) and [geoserver-exts](geoserver/externals/geoserver-exts) to a version that supports the new geotools/geoserver major versions.

## Update Docs

Update the [What's new](docs/usermanual/source/whatsnew.rst) page with a short, user-friendly list of the highlights of the current release.

If there have been any changes in the packaging or install procedures, update the [installation instructions](docs/usermanual/source/install) accordingly. This is usually only necessary for major releases.

### Versioned build

To build server with a specific minor version assigned to geotools, geowebcache, and geoserver (instead of -SNAPSHOT), use the [build/versions.xml](build/versions.xml) ant script to set a custom version. For example, to build server 4.9-beta1:

        % ant -f build/versions.xml set-versions -Dserver.minor_version=-beta1
        % ant all -Dserver.minor_version=-beta1

To undo this action and reset te versions back to -SNAPSHOT:

       % git reset --hard HEAD
       % git submodule foreach --recursive git reset --hard

### Custom-building a GeoServer extension for Support

Occasionally, we will have to build (or re-build) an extension or jar after a server release has gone out.
I will be using Server 4.9.1 as the server version for the purposes of this example.

1. Checkout the correct server branch and update submodules:

        % git checkout r4.9.1
        % git submodule update --init --recursive

1. Set the server version for the geoserver artifacts:

        % ant -f build/versions.xml set-versions -Dserver.minor_version=-server-4.9.1

1. If you are building an extension that does not normally ship with server, modify `build/build.properties` and add that module to `gs.exts_core` or `gs.exts_comm` (depending upon whether or not it is a community module).

1. Change to the `geoserver` directory and run a build.

        % cd geoserver
        % ant clean build assemble -Dserver.minor_version=-server-4.9.1

1. The geoserver artifacts will be in `geoserver/geoserver/src/target/release`. The server war will be in `webapp/target`.

## Modules

The server repository is composed of the following modules:

* [composer](composer/README.md)
* [dashboard](dashboard/README.md)
* [docs](docs/README.md)
* [geoserver](geoserver/README.md)
* [geowebcache](geowebcache/README.md)
* [wpsbuilder](wpsbuilder/README.md)

Consult the module README files for module specific information.
"
229,bitcoin-core/gui,C++,"Bitcoin Core integration/staging tree
=====================================

https://bitcoincore.org

For an immediately usable, binary version of the Bitcoin Core software, see
https://bitcoincore.org/en/download/.

Further information about Bitcoin Core is available in the [doc folder](/doc).

What is Bitcoin?
----------------

Bitcoin is an experimental digital currency that enables instant payments to
anyone, anywhere in the world. Bitcoin uses peer-to-peer technology to operate
with no central authority: managing transactions and issuing money are carried
out collectively by the network. Bitcoin Core is the name of open source
software which enables the use of this currency.

For more information read the original Bitcoin whitepaper.

License
-------

Bitcoin Core is released under the terms of the MIT license. See [COPYING](COPYING) for more
information or see https://opensource.org/licenses/MIT.

Development Process
-------------------

The `master` branch is regularly built (see `doc/build-*.md` for instructions) and tested, but it is not guaranteed to be
completely stable. [Tags](https://github.com/bitcoin/bitcoin/tags) are created
regularly from release branches to indicate new official, stable release versions of Bitcoin Core.

The https://github.com/bitcoin-core/gui repository is used exclusively for the
development of the GUI. Its master branch is identical in all monotree
repositories. Release branches and tags do not exist, so please do not fork
that repository unless it is for development reasons.

The contribution workflow is described in [CONTRIBUTING.md](CONTRIBUTING.md)
and useful hints for developers can be found in [doc/developer-notes.md](doc/developer-notes.md).

Testing
-------

Testing and code review is the bottleneck for development; we get more pull
requests than we can review and test on short notice. Please be patient and help out by testing
other people's pull requests, and remember this is a security-critical project where any mistake might cost people
lots of money.

### Automated Testing

Developers are strongly encouraged to write [unit tests](src/test/README.md) for new code, and to
submit new unit tests for old code. Unit tests can be compiled and run
(assuming they weren't disabled in configure) with: `make check`. Further details on running
and extending unit tests can be found in [/src/test/README.md](/src/test/README.md).

There are also [regression and integration tests](/test), written
in Python.
These tests can be run (if the [test dependencies](/test) are installed) with: `test/functional/test_runner.py`

The CI (Continuous Integration) systems make sure that every pull request is built for Windows, Linux, and macOS,
and that unit/sanity tests are run automatically.

### Manual Quality Assurance (QA) Testing

Changes should be tested by somebody other than the developer who wrote the
code. This is especially important for large or high-risk changes. It is useful
to add a test plan to the pull request description if testing the changes is
not straightforward.

Translations
------------

Changes to translations as well as new translations can be submitted to
[Bitcoin Core's Transifex page](https://www.transifex.com/bitcoin/bitcoin/).

Translations are periodically pulled from Transifex and merged into the git repository. See the
[translation process](doc/translation_process.md) for details on how this works.

**Important**: We do not accept translation changes as GitHub pull requests because the next
pull from Transifex would automatically overwrite them again.
"
230,hibernate/hibernate-demos,Java,"# hibernate-demos

This repository contains demos used live during presentations or in blog posts; The following demos are currently available:

* Hibernate ORM
    - _core/Basic_: simple comparison between JDBC, ORM, and JPA
    - _core/Fetching Strategies_: ""when"" (laziness) and ""how"" (fetch style)
    - _core/Value Generation_: annotations allowing in-memory and DB generated properties, both for INSERT and INSERT/UPDATE actions
    - _core/Multi-Tenancy_: multiple, concurrent databases and clients from a single Hibernate instance
    - _core/Caching_: entity second level cache (2LC) and query cache
    - _core/Envers_: historical/audited data
    - _core/Spatial_: geographical data
    - _osgi_: tutorials demonstrating all possible ORM OSGi configurations
* Hibernate OGM
    - _hiking-demo_: Demo project used for the talk ""Hibernate OGM: Talking to NoSQL in Red Hat JBoss EAP"" presented at Red Hat Summit 2014. It shows how to use MongoDB as data store in a Java EE application through JPA / Hibernate OGM.
* Hibernate Search
    - _hsearch-elasticsearch-wikipedia_: Demonstrates a REST service using Hibernate Search + Elasticsearch to search a large dataset from Wikipedia.
    - _hsearch-quarkus_: Demonstrates a REST service with CRUD and full-text search features
      implemented using Quarkus, Hibernate Search and Elasticsearch.
      Accompanies the blog post https://in.relation.to/2019/11/12/hibernate-search-quarkus/
    - _hsearch-feature-examples_: Demonstrates various features of Hibernate Search 6 with Elasticsearch in Quarkus.
      Used in [Quarkus insights Episode 32](https://www.youtube.com/watch?v=hwxWx-ORVwM).
    - [OBSOLETE] _hsearch-with-elasticsearch_: Shows how to use the Elasticsearch backend in Hibernate Search 5.x.
      Used for the talk ""From Hibernate to Elasticsearch in no Time"" at JavaZone 2016.
* Hibernate Validator
    - _threeten-extra-validator-example_: Constraint validators for ThreeTen Extra date/time types.
      Accompanies the blog post http://in.relation.to/2017/03/02/adding-custom-constraint-definitions-via-the-java-service-loader/
    - _time-duration-validator-example_: Custom constraint and validator, retrieved via the service loader.
      Accompanies the blog post http://in.relation.to/2017/03/02/adding-custom-constraint-definitions-via-the-java-service-loader/
    - _updating-hv-in-wildfly_: How to upgrade WildFly 10 to the latest version of Hibernate Validator.
      Accompanies the blog post http://in.relation.to/2017/04/04/testing-bean-validation-2-0-on-wildfly-10/
    - _javafx-validation-example_: Shows usage of Bean Validation 2 with JavaFX
    - _custom-value-extractors_: Shows how to put constraints to custom containers such as Guava's `Multimap`.
      Accompanies the blog post http://in.relation.to/2018/02/26/putting-bean-validation-constraints-to-multimaps/

* Java 9
    - _multi-release-jar-demo_: Shows how to build multi-release JARs with Java 9.
      Accompanies the blog post http://in.relation.to/2017/02/13/building-multi-release-jars-with-maven/
    - _custom-jlink-plugin_: Shows how to customize Java 9 modular runtime images with jlink plug-ins. The example shows a plug-in for adding a Jandex annotation index for one or more modules to the runtime image.
* Other
    - _wildfly-patch-creation_: How to create WildFly patch files.
      Accompanies the blog post http://in.relation.to/2017/05/29/creating-patches-for-wildfly/
    - _cdi-jpa-testing_: How to run JUnit tests using CDI, JPA and JTA

## License

If not stated otherwise, the demos are licensed under the Apache License, Version 2.0 (see https://www.apache.org/licenses/LICENSE-2.0). Refer to the headers of individual files for specific license and copyright information, in particular of included library files.
"
231,vrpn/vrpn,C,"IMPORTANT LEGAL INFORMATION for offsite use is in README.Legal
IMPORTANT compiling at other sites information in README.Compile

NOTE:	See https://github.com/vrpn/vrpn/wiki for
information on VRPN.


Join the chat at https://gitter.im/vrpn/vrpn
"
232,chrisdee/Tools,PowerShell,
233,google/gitiles,Java,"# Gitiles - A simple JGit repository browser

Gitiles is a simple repository browser for Git repositories, built on JGit. Its
guiding principle is simplicity: it has no formal access controls, no write
access, no fancy Javascript, etc.

Gitiles automatically renders `*.md` Markdown files into HTML for simplified
documentation. Refer to the [Markdown documentation](/Documentation/markdown.md)
for details.

## Configuration

Gitiles is configurable in a git-style configuration file named
`gitiles.config`. Refer to the [configuration documentation](/Documentation/config.md)
for details.

## Bugs

Use the [issue tracker at github](https://github.com/google/gitiles/issues) to
file bugs.

## Contributing to Gitiles

Please refer to the [Developer Guide](/Documentation/developer-guide.md).
"
234,palominodb/PalominoDB-Public-Code-Repository,Perl,"This is the PalominoDB git repository.
It contains many wonderful and useful things.

For information on getting started with git please see: http://git-scm.com.

Sub directory usage:
 - nagios/:
   - Used for nagios plugins.
   - Should be one directory per-plugin.
   - Every plugin should come with a README in addition to any built-in documentation.
 - configs/:
   - Used for stored configs.
   - Should be one directory per service. (e.g., 'nagios', 'mysql').
"
235,djoos-cookbooks/newrelic,Ruby,"[![Build Status](https://travis-ci.org/djoos-cookbooks/newrelic.png)](https://travis-ci.org/djoos-cookbooks/newrelic)

# newrelic cookbook

## Description

This cookbook provides an easy way to install various New Relic application monitoring agents and the New Relic Infrastructure agent.

The agent installs are being converted into libraries, currently the following agents are now resources:

* server_monitor
* php_agent
* java_agent
* ruby_agent
* python_agent
* nodejs_agent
* dotnet_agent
* infrastructure_agent

More information?

* https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/getting-started/introduction-new-relic-infrastructure
* https://docs.newrelic.com/docs/php/new-relic-for-php
* https://docs.newrelic.com/docs/python/new-relic-for-python
* https://docs.newrelic.com/docs/dotnet/new-relic-for-net
* https://docs.newrelic.com/docs/java/new-relic-for-java
* https://docs.newrelic.com/docs/nodejs/installing-and-maintaining-nodejs
* https://docs.newrelic.com/docs/ruby/new-relic-for-ruby

## Requirements

### Chef version:

Make sure you run Chef >= 12.1

### Cookbooks:

* poise-python
* curl
* apt
* yum

### Platforms:

* Debian
* Ubuntu
* RHEL
* CentOS
* Fedora
* Scientific
* Amazon
* Windows
* SmartOS
* Oracle

## Attributes

### default.rb:

#### BASIC

* `node['newrelic']['license']` - Your New Relic license key. Default is `nil`
* `node['newrelic']['server_monitoring']['license']` - Your New Relic license key for server monitoring purposes (defaults to value of node['newrelic']['license'])
* `node['newrelic']['application_monitoring']['license']` - Your New Relic license key for application monitoring purposes (defaults to value of node['newrelic']['license'])

#### ADVANCED

* `node['newrelic']['server_monitoring']['logfile']`
* `node['newrelic']['server_monitoring']['loglevel']`
* `node['newrelic']['server_monitoring']['proxy']`
* `node['newrelic']['server_monitoring']['ssl']`
* `node['newrelic']['server_monitoring']['ssl_ca_bundle']`
* `node['newrelic']['server_monitoring']['ssl_ca_path']`
* `node['newrelic']['server_monitoring']['hostname']`
* `node['newrelic']['server_monitoring']['labels']`
* `node['newrelic']['server_monitoring']['pidfile']`
* `node['newrelic']['server_monitoring']['collector_host']`
* `node['newrelic']['server_monitoring']['timeout']`
* `node['newrelic']['server_monitoring']['other_options']`

* `node['newrelic']['application_monitoring']['enabled']`
* `node['newrelic']['application_monitoring']['logfile']`
* `node['newrelic']['application_monitoring']['logfile_path']`
* `node['newrelic']['application_monitoring']['loglevel']`
* `node['newrelic']['application_monitoring']['app_name']`
* `node['newrelic']['application_monitoring']['daemon']['logfile']`
* `node['newrelic']['application_monitoring']['daemon']['loglevel']`
* `node['newrelic']['application_monitoring']['daemon']['port']`
* `node['newrelic']['application_monitoring']['daemon']['max_threads']`
* `node['newrelic']['application_monitoring']['daemon']['ssl']`
* `node['newrelic']['application_monitoring']['daemon']['ssl_ca_path']`
* `node['newrelic']['application_monitoring']['daemon']['ssl_ca_bundle']`
* `node['newrelic']['application_monitoring']['daemon']['proxy']`
* `node['newrelic']['application_monitoring']['daemon']['pidfile']`
* `node['newrelic']['application_monitoring']['daemon']['location']`
* `node['newrelic']['application_monitoring']['daemon']['collector_host']`
* `node['newrelic']['application_monitoring']['daemon']['dont_launch']`
* `node['newrelic']['application_monitoring']['capture_params']`
* `node['newrelic']['application_monitoring']['ignored_params']`
* `node['newrelic']['application_monitoring']['error_collector']['enable']`
* `node['newrelic']['application_monitoring']['error_collector']['ignore_errors']`
* `node['newrelic']['application_monitoring']['error_collector']['ignore_status_codes']`
* `node['newrelic']['application_monitoring']['error_collector']['record_database_errors']`
* `node['newrelic']['application_monitoring']['error_collector']['prioritize_api_errors']`
* `node['newrelic']['application_monitoring']['browser_monitoring']['auto_instrument']`
* `node['newrelic']['application_monitoring']['transaction_tracer']['enable']`
* `node['newrelic']['application_monitoring']['transaction_tracer']['threshold']`
* `node['newrelic']['application_monitoring']['transaction_tracer']['detail']`
* `node['newrelic']['application_monitoring']['transaction_tracer']['slow_sql']`
* `node['newrelic']['application_monitoring']['transaction_tracer']['stack_trace_threshold']`
* `node['newrelic']['application_monitoring']['transaction_tracer']['explain_threshold']`
* `node['newrelic']['application_monitoring']['transaction_tracer']['record_sql']`
* `node['newrelic']['application_monitoring']['transaction_tracer']['custom']`
* `node['newrelic']['application_monitoring']['framework']`
* `node['newrelic']['application_monitoring']['webtransaction']['name']['remove_trailing_path']`
* `node['newrelic']['application_monitoring']['webtransaction']['name']['functions']`
* `node['newrelic']['application_monitoring']['webtransaction']['name']['files']`
* `node['newrelic']['application_monitoring']['cross_application_tracer']['enable']` - Implemented for Java, PHP, Python and Ruby
* `node['newrelic']['application_monitoring']['thread_profiler']['enable']` - Implemented for Java, Python and Ruby

### repository.rb:

* `node['newrelic']['repository']['key']` - URL to the New Relic repository key, defaults to ""http://download.newrelic.com/548C16BF.gpg""
* `node['newrelic']['repository']['proxy']` - Repository proxy host, defaults to nil
* `node['newrelic']['repository']['proxy_username']` - Repository proxy username, defaults to nil
* `node['newrelic']['repository']['proxy_password']` - Repository proxy password, defaults to nil

### python_agent.rb:

* `node['newrelic']['python_agent']['agent_action']` - Agent action, defaults to :install
* `node['newrelic']['python_agent']['python_version']` - Defaults to ""latest"". Version numbers can be found at http://download.newrelic.com/python_agent/release/
* `node['newrelic']['python_agent']['python_venv']` - Virtual environment, default to nil
* `node['newrelic']['python_agent']['config_file']` - The New Relic python agent config file, defaults to ""/etc/newrelic/newrelic.ini""
* `node['newrelic']['python_agent']['template']['cookbook']` - Sets cookbook for template, defaults to 'newrelic'
* `node['newrelic']['python_agent']['template']['source']` - Sets source for template, defaults to 'agent/python/newrelic.ini.erb'
* `node['newrelic']['python_agent']['feature_flag']` - Sets feature_flag, defaults to nil

### dotnet_agent.rb:

* `node['newrelic']['dotnet_agent']['https_download']` - The URL to download the MSI installer from New Relic. Defaults to nil, so default in resource (https://download.newrelic.com/dot_net_agent/latest_release/x64) is picked up.
* `node['newrelic']['dotnet_agent']['install_level']` - The install version of the .NET Agent. Defaults to nil, so default in resource ('1') is picked up.

### java_agent.rb:

* `node['newrelic']['java_agent']['version']` - New Relic Java Agent version to use. To find the current version, check [New Relic repo](https://download.newrelic.com/newrelic/java-agent/newrelic-agent/current/)
* `node['newrelic']['java_agent']['install_dir']` - The directory to install the newrelic jar and config file
* `node['newrelic']['java_agent']['app_user']` - The user that runs the Java application that will use the New Relic Java agent
* `node['newrelic']['java_agent']['app_group']` - The group for the app_user
* `node['newrelic']['java_agent']['audit_mode']` - Boolean, log all data to and from New Relic in plain text
* `node['newrelic']['java_agent']['log_file_count']` - The number of log files to use
* `node['newrelic']['java_agent']['log_limit_in_kbytes']` - The maximum number of bytes to write to any one log file
* `node['newrelic']['java_agent']['log_daily']` - Override other log rolling configuration and roll the logs daily
* `node['newrelic']['java_agent']['agent_action']` - Agent action, defaults to :install
* `node['newrelic']['java_agent']['execute_agent_action']` - Execute the agent action or not, defaults to true
* `node['newrelic']['java_agent']['enable_custom_tracing']` - Configure New Relic to detect custom traces
* `node['newrelic']['java_agent']['app_location']` - Application server's location, defaults to nil
* `node['newrelic']['java_agent']['template']['cookbook']` - Sets cookbook for template, defaults to 'newrelic'
* `node['newrelic']['java_agent']['template']['source']` - Sets source for template, defaults to 'agent/newrelic.yml.erb'

### nodejs_agent.rb

* `node['newrelic']['nodejs_agent']['agent_action']` - Agent action, defaults to :install
* `node['newrelic']['nodejs_agent']['apps']` - Array of Hash describing the apps to monitor

e.g.

```ruby
[
   { 'app_name' => 'My Application', 'app_path' => ""/path/to/app/root"" }
]
```

You then need to modify your application ""main"" file to add the following on the first line:

```ruby
javascript
require('newrelic');
```

* `node['newrelic']['nodejs_agent']['template']['cookbook']` - Sets cookbook for template, defaults to 'newrelic'
* `node['newrelic']['nodejs_agent']['template']['source']` - Sets source for template, defaults to 'agent/nodejs/newrelic.js.erb'

### ruby_agent.rb:

* `node['newrelic']['ruby_agent']['agent_action']` - Agent action, defaults to :install
* `node['newrelic']['ruby_agent']['install_dir']` - The directory to for the config file
* `node['newrelic']['ruby_agent']['app_user']` - The user that runs the Ruby application that will use the New Relic Ruby agent
* `node['newrelic']['ruby_agent']['app_group']` - The group for the app_user
* `node['newrelic']['ruby_agent']['audit_mode']` - Boolean, log all data to and from New Relic in plain text
* `node['newrelic']['ruby_agent']['log_file_count']` - The number of log files to use
* `node['newrelic']['ruby_agent']['log_limit_in_kbytes']` - The maximum number of bytes to write to any one log file
* `node['newrelic']['ruby_agent']['log_daily']` - Override other log rolling configuration and roll the logs daily
* `node['newrelic']['ruby_agent']['template']['cookbook']` - Sets cookbook for template, defaults to 'newrelic'
* `node['newrelic']['ruby_agent']['template']['source']` - Sets source for template, defaults to 'agent/newrelic.yml.erb'

## MeetMe plugin

To make sure the cookbook is focussed on getting New Relic server and application monitoring, no plugin logic is provided here.
The New Relic MeetMe plugin-logic is still available, in a separate cookbook: [newrelic_meetme_plugin](https://github.com/djoos-cookbooks/newrelic_meetme_plugin).

## Resources / Providers

### `newrelic_server_monitor`
This cookbook includes an LWRP for installing the server monitor agent

The `newrelic_server_monitor` resource will handle the requirements to configure server monitoring.

#### Actions

- :install -  will setup the New Relic repository, install and install package.
- :remove -  Uninstall the New Relic package

#### Attribute parameters

* `'service_name'` - The New Relic server monitoring service name, defaults to 'newrelic-sysmond'
* `'service_notify_action'` - The New Relic server monitoring notify action, defaults to "":restart""
* `'service_actions'` - The New Relic server monitoring service actions, defaults to ""`%w(enable start)`"" (#starts the service if it's not running and enables it to start at system boot time)
* `'config_path'` - The New Relic server monitoring config path, defaults to ""/etc/newrelic""
* `'config_file_group'` - The New Relic server monitoring config file group, defaults to ""newrelic""
* `'windows_version'` - the Windows version to install, defaults to ""2.0.0.198""
* `'windows64_checksum'` - checksum of the 64-bit Windows version, defaults to ""5a8f3f5e8f15997463430401756d377c321c8899c2790ca85e5587a5b643651e""
* `'windows32_checksum'` - checksum of the 32-bit Windows version, defaults to ""ac2b65eecaad461fdd2e4386e3e4c9f96ea940b35bdf7a8c532c21dbd1c99ff0""
* `'cookbook'` - Sets cookbook for template, defaults to 'newrelic'
* `'source'` - Sets source for template, defaults to 'agent/server_monitor/nrsysmond.cfg.erb'

#### Advanced parameters

* `'logfile'` - defaults to nil
* `'loglevel'` - defaults to nil
* `'proxy'` - defaults to nil
* `'ssl'` - defaults to nil
* `'ssl_ca_bundle'` - defaults to nil
* `'ssl_ca_path'` - defaults to nil
* `'hostname'` - defaults to nil
* `'labels'` - defaults to nil
* `'pidfile'` - defaults to nil
* `'collector_host'` - defaults to nil
* `'timeout'` - defaults to nil
* `'other_options'` - defaults to empty Hash
* `'alert_policy_id'` - defaults to nil

#### Example
```ruby
newrelic_server_monitor 'Install' do
  license '0000ffff0000ffff0000ffff0000ffff0000ffff'
end
```

### `newrelic_agent_php`
This cookbook includes an LWRP for installing the php agent

The `newrelic_agent_php` resource will handle the requirements to install php application monitoring.

#### Actions

- :install -  will setup the New Relic repository, install package and update php config with license key.
- :remove -  Uninstall the New Relic package

#### Attribute parameters

* `'version'` - PHP Agent version, defaults to latest
* `'license'` - New Relic license key
* `'install_silently'` - Determine whether to run the install in silent mode, defaults to false
* `'app_name'` - The application name, defaults to `PHP Application`.
* `'startup_mode'` - The newrelic-daemon startup mode (""agent""/""external""), defaults to ""agent""
* `'service_name'` - The web server service name
* `'service_action'` - The web server service action, defaults to ""restart""
* `'config_file'` - The New Relic php agent config file, depends on your php external configuration directory; e.g. /etc/php5/conf.d/newrelic.ini, /etc/php5/mods-available/newrelic.ini, ... Defaults to nil
* `'config_file_to_be_deleted'` - The New Relic php agent-generated config file, e.g. /etc/php5/cli/conf.d/newrelic.ini. If set, the file will get deleted during the Chef run as we want the Chef-generated config file to be used instead (`'config_file'`), defaults to nil
* `'enable_module'` - Enables 'newrelic' php module (ie. php5enmod newrelic / phpenmod newrelic) if true. Needed if you use the mods-available directory, defaults to false
* `'cookbook_ini'` - Sets cookbook for .ini template, defaults to 'newrelic'
* `'source_ini'` - Sets source for .ini template, defaults to 'agent/php/newrelic.ini.erb'
* `'cookbook'` - Sets cookbook for template, defaults to 'newrelic'
* `'source'` - Sets source for template, defaults to 'agent/php/newrelic.cfg.erb'

#### Advanced parameters

* `'enabled'` - Defaults to true
* `'logfile'` - Defaults to nil
* `'loglevel'` - Defaults to nil
* `'daemon_logfile'` - Defaults to '/var/log/newrelic/newrelic-daemon.log'
* `'daemon_loglevel'` - Defaults to nil
* `'daemon_port'` - Defaults to nil
* `'daemon_max_threads'` - Defaults to nil
* `'daemon_ssl'` - Defaults to true
* `'daemon_ssl_ca_path'` - Defaults to nil
* `'daemon_ssl_ca_bundle'` - Defaults to nil
* `'daemon_proxy'` - Defaults to nil
* `'daemon_pidfile'` - Defaults to nil
* `'daemon_location'` - Defaults to nil
* `'daemon_collector_host'` - Defaults to nil
* `'daemon_dont_launch'` - Defaults to nil
* `'capture_params'` - Defaults to false
* `'ignored_params'` - Defaults to nil
* `'error_collector_enable'` - Defaults to true
* `'error_collector_record_database_errors'` - Defaults to true
* `'error_collector_prioritize_api_errors'` - Defaults to false
* `'browser_monitoring_auto_instrument'` - Defaults to true
* `'transaction_tracer_enable'` - Defaults to true
* `'transaction_tracer_threshold'` - Defaults to nil
* `'transaction_tracer_detail'` - Defaults to nil
* `'transaction_tracer_slow_sql'` - Defaults to true
* `'transaction_tracer_stack_trace_threshold'` - Defaults to nil
* `'transaction_tracer_explain_threshold'` - Defaults to nil
* `'transaction_tracer_record_sql'` - Defaults to nil
* `'transaction_tracer_custom'` - Defaults to nil
* `'framework'` - Defaults to nil
* `'webtransaction_name_remove_trailing_path'` - Defaults to false
* `'webtransaction_name_functions'` - Defaults to nil
* `'webtransaction_name_files'` - Defaults to nil
* `'cross_application_tracer_enable'` - Defaults to true
* `'distributed_tracing_enabled'` - Defaults to false
* `'distributed_tracing_exclude_newrelic_header'` - Default to false

#### Example

```ruby
newrelic_agent_php 'Install' do
  license '0000ffff0000ffff0000ffff0000ffff0000ffff'
  app_name 'php_test_app'
  service_name 'httpd'
  config_file '/etc/php.d/newrelic.ini'
end
```

### `newrelic_agent_ruby`
This cookbook includes an LWRP for installing the ruby agent

The `newrelic_agent_ruby` resource will handle the requirements to install ruby application monitoring.

#### Actions

- :install -  will setup the New Relic repository, install package and update ruby config with license key.
- :remove -  Uninstall the New Relic package

#### Attribute parameters

* `'install_dir'` - The directory to for the config file
* `'app_user'` - The user that runs the Ruby application that will use the New Relic Ruby agent
* `'app_group'` - The group for the app_user
* `'audit_mode'` - Boolean, log all data to and from New Relic in plain text
* `'log_file_count'` - The number of log files to use
* `'log_limit_in_kbytes'` - The maximum number of bytes to write to any one log file
* `'log_daily'` - Override other log rolling configuration and roll the logs daily
* `'template_cookbook'` - Sets cookbook for template, defaults to 'newrelic'
* `'template_source'` - Sets source for template, defaults to 'agent/newrelic.yml.erb

#### Advanced parameters

* `'enabled'` - Defaults to true
* `'app_name'` - Defaults to nil
* `'high_security'` - Defaults to false
* `'owner'` - Defaults to 'newrelic'
* `'group'` - Defaults to 'newrelic'
* `'logfile'` - Defaults to 'newrelic-daemon.log'
* `'logfile_path'` - Defaults to '/var/log/newrelic/'
* `'loglevel'` - Defaults to nil
* `'audit_mode'` - Defaults to false
* `'log_file_count'` - Fixnum, defaults to 1
* `'log_limit_in_kbytes'` - Fixnum, defaults to 0
* `'log_daily'` - Defaults to true
* `'daemon_ssl'` - Defaults to true
* `'daemon_proxy'` - Defaults to nil
* `'daemon_proxy_host'` - Defaults to nil
* `'daemon_proxy_port'` - Defaults to nil
* `'daemon_proxy_user'` - Defaults to nil
* `'daemon_proxy_password'` - Defaults to nil
* `'capture_params'` - Defaults to nil
* `'ignored_params'` - Defaults to nil
* `'transaction_tracer_enable'` - Defaults to true
* `'transaction_tracer_threshold'` - Defaults to nil
* `'transaction_tracer_record_sql'` - Defaults to nil
* `'transaction_tracer_stack_trace_threshold'` - Defaults to nil
* `'transaction_tracer_slow_sql'` - Defaults to nil
* `'transaction_tracer_explain_threshold'` - Defaults to nil
* `'error_collector_enable'` - Defaults to true
* `'error_collector_ignore_errors'` - Defaults to nil
* `'error_collector_ignore_status_codes'` - Defaults to nil
* `'circuitbreaker_enabled'` - Defaults to true
* `'circuitbreaker_memory_threshold'` - Defaults to 20
* `'circuitbreaker_gc_cpu_threshold'` - Defaults to 10
* `'browser_monitoring_auto_instrument'` - Defaults to nil
* `'cross_application_tracer_enable'` - Defaults to true
* `'thread_profiler_enable'` - Defaults to true

#### Example
```ruby
newrelic_agent_ruby 'Install' do
  license '0000ffff0000ffff0000ffff0000ffff0000ffff'
  app_name 'ruby_test_app'
end
```

### `newrelic_agent_java`
This cookbook includes an LWRP for installing the java agent

The `newrelic_agent_java` resource will handle the requirements to install java application monitoring.

#### Actions

- :install -  will retrieve Java agent, install and update config with license key.
- :remove -  Uninstall the New Relic agent.

#### Attribute parameters

* `'license'` - New Relic license key
* `'version'` - New Relic Java Agent version to use. To find the current version, check New Relic repo
* `'https_download'` - The url to download the jar for the New Relic Java agent. If you override version parameter, you must also update this.
* `'jar_file'` - The name of the newrelic jar file that will be used locally, defaults to `newrelic-agent-version.jar`
* `'install_dir'` - The directory to install the newrelic jar and config file
* `'app_user'` - The user that runs the Java application that will use the New Relic Java agent
* `'app_group'` - The group for the app_user
* `'audit_mode'` - Boolean, log all data to and from New Relic in plain text
* `'log_file_count'` - The number of log files to use
* `'log_limit_in_kbytes'` - The maximum number of bytes to write to any one log file
* `'log_daily'` - Override other log rolling configuration and roll the logs daily
* `'agent_action'` - Agent action, defaults to `:install`
* `'execute_agent_action'` - Execute the agent action or not, defaults to true
* `'app_location'` - Application's location, defaults to `install_dir`
* `'template_cookbook'` - Sets cookbook for template, defaults to 'newrelic'
* `'template_source'` - Sets source for template, defaults to `agent/newrelic.yml.erb`

#### Advanced parameters

* `'enabled'` - Defaults to true
* `'high_security'` - Defaults to false
* `'owner'` - Defaults to 'newrelic'
* `'group'` - Defaults to 'newrelic'
* `'logfile'` - Defaults to nil
* `'logfile_path'` - Defaults to nil
* `'loglevel'` - Defaults to nil
* `'audit_mode'` - Defaults to false
* `'log_file_count'` - Defaults to nil
* `'log_limit_in_kbytes'` - Defaults to nil
* `'log_daily'` - Defaults to false
* `'daemon_ssl'` - Defaults to true
* `'daemon_proxy'` - Defaults to nil
* `'daemon_proxy_host'` - Defaults to nil
* `'daemon_proxy_port'` - Defaults to nil
* `'daemon_proxy_user'` - Defaults to nil
* `'daemon_proxy_password'` - Defaults to nil
* `'capture_params'` - Defaults to nil
* `'ignored_params'` - Defaults to nil
* `'transaction_tracer_enable'` - Defaults to true
* `'transaction_tracer_threshold'` - Defaults to nil
* `'transaction_tracer_record_sql'` - Defaults to nil
* `'transaction_tracer_stack_trace_threshold'` - Defaults to nil
* `'transaction_tracer_slow_sql'` - Defaults to nil
* `'transaction_tracer_explain_threshold'` - Defaults to nil
* `'error_collector_enable'` - Defaults to true
* `'error_collector_ignore_errors'` - Defaults to nil
* `'error_collector_ignore_classes'` - Defaults to nil
* `'error_collector_ignore_status_codes'` - Defaults to nil
* `'circuitbreaker_enabled'` - Defaults to true
* `'circuitbreaker_memory_threshold'` - Defaults to 20
* `'circuitbreaker_gc_cpu_threshold'` - Defaults to 10
* `'browser_monitoring_auto_instrument'` - Defaults to nil
* `'cross_application_tracer_enable'` - Defaults to true
* `'thread_profiler_enable'` - Defaults to true

#### Example
```ruby
newrelic_agent_java 'Install' do
  license '0000ffff0000ffff0000ffff0000ffff0000ffff'
  install_dir '/opt/newrelic/java'
  app_name 'java_test_app'
```

### `newrelic_agent_python`
This cookbook includes an LWRP for installing the newrelic python agent

The `newrelic_agent_python` resource will handle the requirements to install python application monitoring.

#### Actions

- :install -  will setup the New Relic repository, install package and update newrelic python config with license key.
- :remove -  Uninstall the New Relic package

#### Attribute parameters
See https://docs.newrelic.com/docs/agents/python-agent/installation-configuration/python-agent-configuration#general-settings
for an explanation on each attribute.

* `'license'` - NewRelic license key
* `'version'` - Python agent version. Will default to latest if nil.
* `'virtualenv'` - VirtualEnv to install puthon agent into. Default nil.
* `'config_file'` - Path to config file. Default '/etc/newrelic/newrelic.ini'
* `'cookbook'` - Cookbook holding config template. Default this cookbook.
* `'source'` - Config template source. Default 'agent/python/newrelic.ini.erb'
* `'app_name'` - Your newrelic python app name as it will show in the UI. Default => 'Python Application'

#### Advanced parameters

* `'enabled'` - Defaults to true
* `'logfile'` - Defaults to '/tmp/newrelic-python-agent.log'
* `'loglevel'` - Defaults to 'info'
* `'daemon_ssl'` - Defaults to true
* `'high_security'` - Defaults to false
* `'capture_params'` - Defaults to false
* `'ignored_params'` - Defaults to ' '
* `'transaction_tracer_enable'` - Defaults to true
* `'transaction_tracer_threshold'` - Defaults to 'apdex_f'
* `'transaction_tracer_record_sql'` - Defaults to 'obfuscated'
* `'transaction_tracer_stack_trace_threshold'` - Defaults to '0.5'
* `'transaction_tracer_slow_sql'` - Defaults to true
* `'transaction_tracer_explain_threshold'` - Defaults to '0.5'
* `'thread_profiler_enable'` - Defaults to true
* `'error_collector_enable'` - Defaults to true
* `'error_collector_ignore_errors'` - Defaults to ' '
* `'browser_monitoring_auto_instrument'` - Defaults to true
* `'cross_application_tracer_enable'` - Defaults to true
* `'feature_flag'` - Defaults to nil

#### Example

```ruby
include_recipe 'python'

newrelic_agent_python 'Install' do
  license '0000ffff0000ffff0000ffff0000ffff0000ffff'
  app_name 'my_python_app'
end
```

### `newrelic_agent_nodejs`
This cookbook includes an LWRP for installing the newrelic nodejs agent
The `newrelic_agent_nodejs` resource will handle the requirements to install nodejs application monitoring.

#### Actions

- :install -  will setup the New Relic repository, install npm package and update newrelic nodejs config with license key.
- :remove -  Uninstall the New Relic package

#### Attribute parameters
https://docs.newrelic.com/docs/agents/nodejs-agent/installation-configuration/nodejs-agent-configuration
for an explanation on each attribute.

* `'license'` - NewRelic license key
* `'version'` - NewRelic npm package version. Will default to latest if nil.
* `'app_name'` - Your newrelic nodejs app name as it will show in the UI. Default => 'My Node App'
* `'app_path'` - Required true. Default nil.  You must provide a valid path to your nodejs app root dir.
* `'cookbook'` - Cookbook holding config template. Default this cookbook.
* `'source'` - Config template source. Default 'agent/python/newrelic.ini.erb'

#### Advanced parameters

* `'enabled'` - Defaults to true
* `'logfile'` - Defaults to '/tmp/newrelic-python-agent.log'
* `'loglevel'` - Defaults to 'info'

#### Example

```ruby
newrelic_agent_nodejs '/var/my_node_approot' do
  license '0000ffff0000ffff0000ffff0000ffff0000ffff'
  app_name 'my_nodejs_app'
end
```

### `newrelic_agent_dotnet`
This cookbook includes an LWRP for installing the dotnet agent

The `newrelic_agent_dotnet` resource will handle the requirements to install .Net application monitoring.

#### Actions

- :install - installs the New Relic agent.
- :remove - uninstalls the New Relic agent.

#### Attribute parameters

* `'https_download'` - The URL to download the MSI installer from New Relic. Default is to pull ""latest""
* `'install_level'` - The install version of the .NET Agent. Default is '1' but can use '50' for a complete installation

#### Example
```ruby
newrelic_agent_dotnet 'Install' do
  license '0000ffff0000ffff0000ffff0000ffff0000ffff'
end
```

### `newrelic_agent_infrastructure`
This cookbook includes an LWRP for installing the infrastructure agent

The `newrelic_agent_infrastructure` resource will handle the requirements to set up the infrastructure agent.

#### Actions

- :install -  will setup the New Relic Infrastructure agent.

#### Attribute parameters

* `'license'` - New Relic license key
* `'version'` - New Relic Infrastructure Agent version to use. To find the current version, check New Relic repo
* `'display_name'` - Overrides the auto-generated hostname for reporting, defaults to nil
* `'logfile'` - To log to another location, provide a full path and file name, defaults to nil
* `'verbose'` - Enables verbose logging for the agent, defaults to 0
* `'proxy'` - Defaults to nil
* `'custom_attributes'` - Sets key-value pairs (similar to tags in other tools) used to annotate the data from the Infrastructure agent, defaults to {}
* `'on_host_integrations_enable'` - Installs New Relic Infrastructure on-host integration package, defaults to false
* `'template_cookbook'` - Sets cookbook for template, defaults to 'newrelic'
* `'template_source'` - Sets source for template, defaults to 'agent/infrastructure/newrelic.yml.erb'
* `'service_actions'` - The New Relic infrastructure agent service actions, defaults to ""`%w(enable start)`"" (#starts the service if it's not running and enables it to start at system boot time)
* `'windows_version'` - the Windows version to install, defaults to ""1.0.703""
* `'windows_checksum'` - checksum of the (64-bit) Windows version, defaults to ""3c9f98325dc484ee8735f01b913803eaef54f06641348b3dd9f3c0b3cd803ace""
* `'strip_command_line'` - Undocumented boolean flag for the newrelic-infra.yml file, if set to false includes the parameters in the command line reported by the infrastructure agent. Defaults to nil, which defaults to false for the infrastructure agent.

### `newrelic_deployment`
This cookbook includes an LWRP for notifying New Relic of a deployment

#### Actions

- :notify - notifies New Relic of a deployment

#### Attribute parameters

* `'key_type'` - Your New Relic API key type (api_key or license_key, defaults to api_key currently for backwards compatibility)
* `'key'` - Your New Relic key (see key_type for more information on what value to provide here exactly)
* `'app_name'` - The name of the application, found in the newrelic.yml file
* `'app_id'` - The ID # of the application
* `'description'` - Text annotation for the deployment (notes for you)
* `'revision'` - The revision number from your source control system (SVN, git, etc.)
* `'changelog'` - A list of changes for this deployment
* `'user'` - The name of the user/process that triggered this deployment

#### Example(s)

```ruby
newrelic_deployment ""my-application"" do
    key_type ""license_key""
    key ""yourlicensekey""
    app_id 1234567
    description ""some description""
    revision ""some revision""
    changelog ""some changelog""
    user ""chef-client""
    action :notify
end
```

This cookbook includes an LWRP for generating the newrelic.yml configuration file in a specific path, which can be used to generate multiple configurations when deploying multiple different applications

### `newrelic_yml`

#### Actions

- :generate - Generate the newrelic.yml config file (unique and default action)

#### Example usage - Java agent

1. Install the Java Agent: add the newrelic::java_agent recipe to your run list. A newrelic.yml will be generated but not linked to anything.
2. In your application cookbook, generate the newrelic.yml for this application:

```ruby
yml_path = ""#{my_app_path}/newrelic.yml""
newrelic_yml yml_path do
  app_name 'my-super-duper-application'
  agent_type 'java'
end
```

3. Configure your app for newrelic using your config file and newrelic.jar:

```bash
 java -Dnewrelic.config.file=#{newrelicyml}  -javaagent:#{node['newrelic']['install_dir']}/newrelic.jar [rest of your args]
```

## Usage

1. include `recipe[newrelic]` in a run list to implicly run `recipe[newrelic::server_monitor_agent]`
--- OR ---
include the bits and pieces explicitly in a run list:
```ruby
`recipe[newrelic::repository]`
`recipe[newrelic::server_monitor_agent]`
`recipe[newrelic::dotnet_agent]`
`recipe[newrelic::java_agent]`
`recipe[newrelic::nodejs_agent]`
`recipe[newrelic::php_agent]`
`recipe[newrelic::python_agent]`
`recipe[newrelic::ruby_agent]`
`recipe[newrelic::infrastructure_agent]`
```
2. change the `node['newrelic']['license']` attribute to your New Relic license keys
--- OR ---
[override the attributes on a higher level](http://wiki.opscode.com/display/chef/Attributes#Attributes-AttributesPrecedence)

## References

* [New Relic home page](http://newrelic.com/)
* [New Relic for Server Monitoring](https://docs.newrelic.com/docs/server/new-relic-for-server-monitoring)
* [New Relic for PHP](https://docs.newrelic.com/docs/php/new-relic-for-php)
* [newrelic-daemon startup modes](https://newrelic.com/docs/php/newrelic-daemon-startup-modes)
* [New Relic for Python](https://docs.newrelic.com/docs/python/new-relic-for-python)
* [New Relic for .NET](https://docs.newrelic.com/docs/dotnet/new-relic-for-net)
* [New Relic for Java](https://docs.newrelic.com/docs/java/new-relic-for-java)
* [""newrelic"" cookbook by heavywater on github](https://github.com/heavywater/chef-newrelic)
* [""newrelic_monitoring"" cookbook on community.opscode.com](http://community.opscode.com/cookbooks/newrelic_monitoring)
* [""newrelic_monitoring"" cookbook on github](https://github.com/8thBridge/chef-newrelic-monitoring)
* a very big thanks to heavywater <darrin@heavywater.ca> for the original version of this cookbook

## License and Authors

Author: David Joos <development@davidjoos.com>
Copyright: 2016, David Joos

Author: David Joos <david.joos@escapestudios.com>
Author: Escape Studios Development <dev@escapestudios.com>
Copyright: 2012-2015, Escape Studios

Unless otherwise noted, all files are released under the MIT license,
possible exceptions will contain licensing information in them.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
"
236,miztiik/DevOps-Demos,,"# Welcome To DevOps Repo
This repository holds all of my experiments in the area of DevOps. Below link is given which describes the experiments.

Follow the demos in [YouTube](https://www.youtube.com/watch?v=8D46Pgbz0gg&list=PLxzKY3wu0_FJdJd3IKdiM4Om1hGo2Hsdt)

--Thank You--
"
237,mp2893/med2vec,Python,"Med2Vec
=========================================

Med2Vec is a multi-layer representation learning tool for learning code representations and visit representations from EHR datasets.

[![Med2Vec Coordinate-wise Interpretation Demo](http://www.cc.gatech.edu/~echoi48/images/med2vec_interpret.png)](https://youtu.be/UR_f2rmMJkk?t=2m34s ""Med2Vec Coordinate-wise Interpretation Demo - Click to Watch!"")
Med2Vec embeddings not only help improve predictive performance of healthcare applications, but also enable the interpretation of the learned code representations in a coodinate-wise manner. You can see that these six coordinates (chosen by their strong correlation with patient severity level) of the code representation space demonstrate medically coherent groups of symptoms (diagnoses, medications, and procedures). 

#### Relevant Publications

Med2Vec implements an algorithm introduced in the following [paper](http://www.kdd.org/kdd2016/subtopic/view/multi-layer-representation-learning-for-medical-concepts):

    Multi-layer Representation Learning for Medical Concepts
	Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coffey, 
	Michael Thompson, James Bost, Javier Tejedor-Sojo, Jimeng Sun
	KDD 2016, pp.1495-1504

#### Running Med2Vec

**STEP 1: Installation**  

1. Install [python](https://www.python.org/), [Theano](http://deeplearning.net/software/theano/index.html). We use Python 2.7, Theano 0.7. Theano can be easily installed in Ubuntu as suggested [here](http://deeplearning.net/software/theano/install_ubuntu.html#install-ubuntu)

2. If you plan to use GPU computation, install [CUDA](https://developer.nvidia.com/cuda-downloads)

3. Download/clone the Med2Vec code  

**STEP 2: Fast way to test Med2Vec with MIMIC-III**

This step describes how to run, with minimum number of steps, Med2Vec using MIMIC-III. 

0. You will first need to request access for [MIMIC-III](https://mimic.physionet.org/gettingstarted/access/), a publicly avaiable electronic health records collected from ICU patients over 11 years. 

1. You can use ""process_mimic.py"" to process MIMIC-III dataset and generate a suitable training dataset for Med2Vec.
Place the script to the same location where the MIMIC-III CSV files are located, and run the script. 
The execution command is `python process_mimic.py ADMISSIONS.csv DIAGNOSES_ICD.csv <output file>`.
Instructions are described inside the script. 

2. Run Med2Vec using the "".seqs"" file generated by process_mimic.py, using the following command.
`python med2vec.py <seqs file> 4894 <output path>`
where 4894 is the number of unique ICD9 diagnosis codes in the dataset.
As described in the paper, however, it is a good idea to use the grouped codes for training the Softmax component of Med2Vec. Therefore we recommend using the following command instead.
`python med2vec.py <seqs file> 4894 <output path> --label_file <3digitICD9.seqs file> --n_output_codes 942`
where 942 is the number of unique 3-digit ICD9 diagnosis codes in the dataset.
You can also use "".3digitICD9.seqs"" to begin with, if you interested in learning the representation of 3-digit ICD9 codes only, using the following command.
`python med2vec.py <3digitICD9.seqs file> 942 <output path>`

3. As suggested in STEP 4, you might want to adjust the hyper-parameters. 
I recommend decreasing the `--batch_size` to 100 or so, since the default value 1,000 is too big considering the small number of patients in MIMIC-III datasets. 
There are only 7,500 patients who made more than a single visit, and most of them have only two visits.

**STEP 3: Preparing training data**  

1. Med2Vec training data need to be a Python Pickled list of list of medical codes (e.g. diagnosis codes, medication codes, or procedure codes). 
First, medical codes need to be converted to an integer. Then a single visit can be converted as a list of integers. 
For example, [5,8,15] means the patient was assigned with code 5, 8, and 15 at a certain visit. 
If a patient made two visits [1,2,3] and [4,5,6,7], it can be converted to a list of list [[1,2,3], [4,5,6,7]]. 
If there are multiple patients, each patient must be delimited by a list [-1]. 
For example, [[1,2,3], [4,5,6,7], [-1], [2,4], [8,3,1], [3]] means there are two patients where the first patient made two visits and the second patient made three visits. 
This list of list needs to be pickled using cPickle. We will refer to this file as the ""visit file"".

2. The total number of unique medical codes is required to run Med2Vec. 
For example, if the dataset is using 14,000 diagnosis codes and 11,000 procedure codes, the total number is 25,000. 
Note that using a huge number of codes could lead to memory problems, depending on your RAM/VRAM (thanks for the tip [tRosenflanz](https://github.com/tRosenflanz))

3. For a faster training, you can provide an additional dataset, which is simply the same dataset in step 1, but with grouped medical codes. 
For example, ICD9 diagnosis codes can be grouped into 283 categories by using [CCS](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp) groupers. 
You will still be able to learn the code representations for the original un-grouped codes. 
The grouped dataset is used only for speeding up the training speed. (Refer to section 4.4 of the paper) 
The grouped dataset should be prepared in the same way as the dataset in step 1. We will refer to this grouped dataset as the ""label file"".

4. Same as step 2, you will need to remember the total number of unique grouped codes if you plan to use this grouped dataset.

5. If you wish to use patient demographic information (e.g. age, weight, gender) you need to create a demographics vector for each visit the patient made. 
For example, if you are using age (real-valued) and ethnicity(categorical, assume 6 categories), you can create a vector such as [45.0, 0, 0, 0, 0, 1, 0]. 
Similar to the [-1] vector in step 1, each patient is delimited with an all-zero vector. 
Therefore the demographic information will be a pickled matrix where column size is the size of the demographics vector and row size is the number of total visits of all patients plus the delimiters. 
We will refer to this file as the ""demo file"".

6. Similar to step 2, you will need to remeber the size of the demographics vector if you plan to use the demo file. 
In the example of step 5, the size of the demographics vector is 7.

**STEP 4: Running Med2Vec**  

1. The minimum input you need to run Med2Vec is the visit file, the number of unique medical codes and the output path
`python med2vec <path/to/visit_file> <the number of unique medical codes> <path/to/output>`  

2. Specifying `--verbose` option will print training process after each 10 mini-batches.

3. Additional options can be specified such as the size of the code representation, the size of the visit representation and the number of epochs. Detailed information can be accessed by `python med2vec --help`

**STEP 5: Looking at your results**  

Med2Vec produces a model file after each epoch. The model file is generated by [numpy.savez_compressed](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.savez_compressed.html).

The 2D scatterplot of the learned code representations would look similar to [this](http://mp2893.com/scatterplot/nnsg_h200e49_category10.html).
(This is the scatterplot of the code representations trained with Non-negative Skip-gram, which is essentially Med2Vec minus the visit-level training)
"
238,components/jquery,JavaScript,"jQuery Component
================

Shim [repository](https://github.com/components/jquery) for the [jQuery](http://jquery.com).

If you're looking for jquery-migrate: It got it's [own repository](https://github.com/components/jquery-migrate) since jQuery v3.0.0.

Package Managers
----------------

* [Bower](http://bower.io/): `jquery`
* [Component](https://github.com/component/component): `components/jquery`
* [Composer](http://packagist.org/packages/components/jquery): `components/jquery`
* [spm](http://spmjs.io/package/jquery): `jquery`
"
239,TRI-ML/packnet-sfm,Python,"## PackNet-SfM: 3D Packing for Self-Supervised Monocular Depth Estimation

[Install](#install) // [Datasets](#datasets) // [Training](#training) // [Evaluation](#evaluation) // [Models](#models) // [License](#license) // [References](#references)

<a href=""https://www.tri.global/"" target=""_blank"">
 <img align=""right"" src=""/media/figs/tri-logo.png"" width=""20%""/>
</a>

<a href=""https://www.youtube.com/watch?v=b62iDkLgGSI"" target=""_blank"">
<img width=""60%"" src=""/media/figs/packnet-ddad.gif""/>
</a>

Official [PyTorch](https://pytorch.org/) implementation of _self-supervised_ monocular depth estimation methods invented by the ML Team at [Toyota Research Institute (TRI)](https://www.tri.global/), in particular for _PackNet_: [**3D Packing for Self-Supervised Monocular Depth Estimation (CVPR 2020 oral)**](https://arxiv.org/abs/1905.02693),
*Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos and Adrien Gaidon*.
Although self-supervised (i.e. trained only on monocular videos), PackNet outperforms other self, semi, and fully supervised methods. Furthermore, it gets better with input resolution and number of parameters, generalizes better, and can run in real-time (with TensorRT). See [References](#references) for more info on our models.

This is also the official implementation of [**Neural Ray Surfaces for Self-Supervised Learning of Depth and Ego-motion (3DV 2020 oral)**](https://arxiv.org/abs/2008.06630), *Igor Vasiljevic, Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Wolfram Burgard, Greg Shakhnarovich and Adrien Gaidon*.  Neural Ray Surfaces (NRS) generalize self-supervised depth and pose estimation beyond the pinhole model to all central cameras, allowing the learning of meaningful depth and pose on non-pinhole cameras such as fisheye and catadioptric.

## Install

You need a machine with recent Nvidia drivers and a GPU with at least 6GB of memory (more for the bigger models at higher resolution). We recommend using docker (see [nvidia-docker2](https://github.com/NVIDIA/nvidia-docker) instructions) to have a reproducible environment. To setup your environment, type in a terminal (only tested in Ubuntu 18.04):

```bash
git clone https://github.com/TRI-ML/packnet-sfm.git
cd packnet-sfm
# if you want to use docker (recommended)
make docker-build
```

We will list below all commands as if run directly inside our container. To run any of the commands in a container, you can either start the container in interactive mode with `make docker-start-interactive` to land in a shell where you can type those commands, or you can do it in one step:

```bash
# single GPU
make docker-run COMMAND=""some-command""
# multi-GPU
make docker-run-mpi COMMAND=""some-command""
```

For instance, to verify that the environment is setup correctly, you can run a simple overfitting test:

```bash
# download a tiny subset of KITTI
curl -s https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/KITTI_tiny.tar | tar xv -C /data/datasets/
# in docker
make docker-run COMMAND=""python3 scripts/train.py configs/overfit_kitti.yaml""
```

If you want to use features related to [AWS](https://aws.amazon.com/) (for dataset access)
and [Weights & Biases (WANDB)](https://www.wandb.com/) (for experiment management/visualization), then you should create associated accounts and configure your shell with the following environment variables:

```bash
export AWS_SECRET_ACCESS_KEY=""something""
export AWS_ACCESS_KEY_ID=""something""
export AWS_DEFAULT_REGION=""something""
export WANDB_ENTITY=""something""
export WANDB_API_KEY=""something""
```

To enable WANDB logging and AWS checkpoint syncing, you can then set the corresponding configuration parameters in `configs/<your config>.yaml` (cf. [configs/default_config.py](./configs/default_config.py) for defaults and docs):

```yaml
wandb:
    dry_run: True                                 # Wandb dry-run (not logging)
    name: ''                                      # Wandb run name
    project: os.environ.get(""WANDB_PROJECT"", """")  # Wandb project
    entity: os.environ.get(""WANDB_ENTITY"", """")    # Wandb entity
    tags: []                                      # Wandb tags
    dir: ''                                       # Wandb save folder
checkpoint:
    s3_path: ''       # s3 path for AWS model syncing
    s3_frequency: 1   # How often to s3 sync
```

If you encounter out of memory issues, try a lower `batch_size` parameter in the config file.

NB: if you would rather not use docker, you could create a [conda](https://docs.conda.io/en/latest/) environment via following the steps in the Dockerfile and mixing `conda` and `pip` at your own risks...

## Datasets

Datasets are assumed to be downloaded in `/data/datasets/<dataset-name>` (can be a symbolic link).

### Dense Depth for Autonomous Driving (DDAD)

Together with PackNet, we introduce **Dense Depth for Automated Driving** ([DDAD](https://github.com/TRI-ML/DDAD)): a new dataset that leverages diverse logs from TRI's fleet of well-calibrated self-driving cars equipped with cameras and high-accuracy long-range LiDARs.  Compared to existing benchmarks, DDAD enables much more accurate 360 degree depth evaluation at range, see the official [DDAD repository](https://github.com/TRI-ML/DDAD) for more info and instructions. You can also download DDAD directly via:

```bash
curl -s https://tri-ml-public.s3.amazonaws.com/github/DDAD/datasets/DDAD.tar | tar -xv -C /data/datasets/
```

### KITTI

The KITTI (raw) dataset used in our experiments can be downloaded from the [KITTI website](http://www.cvlibs.net/datasets/kitti/raw_data.php).
For convenience, we provide the standard splits used for training and evaluation: [eigen_zhou](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_zhou_files.txt), [eigen_train](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_train_files.txt), [eigen_val](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_val_files.txt) and [eigen_test](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_test_files.txt), as well as pre-computed ground-truth depth maps: [original](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw_velodyne.tar.gz) and [improved](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw_groundtruth.tar.gz).
The full KITTI_raw dataset, as used in our experiments, can be directly downloaded [here](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/KITTI_raw.tar.gz) or with the following command:

```bash
# KITTI_raw
curl -s https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/KITTI_raw.tar | tar -xv -C /data/datasets/
```

### Tiny DDAD/KITTI

For simple tests, we also provide a ""tiny"" version of [DDAD](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/DDAD_tiny.tar) and [KITTI](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/KITTI_tiny.tar):

```bash
# DDAD_tiny
curl -s https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/DDAD_tiny.tar | tar -xv -C /data/datasets/
# KITTI_tiny
curl -s https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/KITTI_tiny.tar | tar -xv -C /data/datasets/
```
### OmniCam

The raw data for the catadioptric OmniCam dataset can be downloaded from the [Omnicam website](http://www.cvlibs.net/projects/omnicam/).  For convenience, we provide the dataset for testing the Neural Ray Surfaces (NRS) model.  The dataset can be downloaded with the following command:

```bash
# omnicam
curl -s https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/OmniCam.tar | tar -xv -C /data/datasets/
```

The ray surface template we used for training on OmniCam can be found [here](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/templates/omnicam_ray_template.npy). 

## Training

PackNet can be trained from scratch in a fully self-supervised way (from video only, cf. [CVPR'20](#cvpr-packnet)), in a semi-supervised way (with sparse lidar using our reprojected 3D loss, cf. [CoRL'19](#corl-ssl)), and it can also use a fixed pre-trained semantic segmentation network to guide the representation learning further (cf. [ICLR'20](#iclr-semguided)).

Any training, including fine-tuning, can be done by passing either a `.yaml` config file or a `.ckpt` model checkpoint to [scripts/train.py](./scripts/train.py):

```bash
python3 scripts/train.py <config.yaml or checkpoint.ckpt>
```

If you pass a config file, training will start from scratch using the parameters in that config file. Example config files are in [configs](./configs).
If you pass instead a `.ckpt` file, training will continue from the current checkpoint state.

Note that it is also possible to define checkpoints within the config file itself. These can be done either individually for the depth and/or pose networks or by defining a checkpoint to the model itself, which includes all sub-networks (setting the model checkpoint will overwrite depth and pose checkpoints). In this case, a new training session will start and the networks will be initialized with the model state in the `.ckpt` file(s). Below we provide the locations in the config file where these checkpoints are defined:

```yaml
checkpoint:
    # Folder where .ckpt files will be saved during training
    filepath: /path/to/where/checkpoints/will/be/saved
model:
    # Checkpoint for the model (depth + pose)
    checkpoint_path: /path/to/model.ckpt
    depth_net:
        # Checkpoint for the depth network
        checkpoint_path: /path/to/depth_net.ckpt
    pose_net:
        # Checkpoint for the pose network
        checkpoint_path: /path/to/pose_net.ckpt
```

Every aspect of the training configuration can be controlled by modifying the yaml config file. This include the model configuration (self-supervised, semi-supervised, loss parameters, etc), depth and pose networks configuration (choice of architecture and different parameters), optimizers and schedulers (learning rates, weight decay, etc), datasets (name, splits, depth types, etc) and much more. For a comprehensive list please refer to [configs/default_config.py](./configs/default_config.py).

## Evaluation

Similar to the training case, to evaluate a trained model (cf. above or our [pre-trained models](#models)) you need to provide a `.ckpt` checkpoint, followed optionally by a `.yaml` config file that overrides the configuration stored in the checkpoint.

```bash
python3 scripts/eval.py --checkpoint <checkpoint.ckpt> [--config <config.yaml>]
```

You can also directly run inference on a single image or folder:

```bash
python3 scripts/infer.py --checkpoint <checkpoint.ckpt> --input <image or folder> --output <image or folder> [--image_shape <input shape (h,w)>]
```

## Models

### DDAD

| Model | Abs.Rel. | Sqr.Rel | RMSE | RMSElog | d < 1.25 |
| :--- | :---: | :---: | :---: |  :---: |  :---: |
| _ResNet18, Self-Supervised, 384x640, ImageNet &rightarrow; DDAD (D)_ | _0.213_ | _4.975_ | _18.051_ | _0.340_ | _0.761_ |
| _PackNet,  Self-Supervised, 384x640, DDAD (D)_ | _0.162_ | _3.917_ | _13.452_ | _0.269_ | _0.823_ |
| [ResNet18, Self-Supervised, 384x640, ImageNet &rightarrow; DDAD (D)](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/ResNet18_MR_selfsup_D.ckpt)* | 0.227 | 11.293 | 17.368 | 0.303 | 0.758 |
| [PackNet,  Self-Supervised, 384x640, DDAD (D)](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNet01_MR_selfsup_D.ckpt)* | 0.173 | 7.164 | 14.363 | 0.249 | 0.835 |

*: Note that this repository's results differ slightly from the ones reported in our [CVPR'20 paper](https://arxiv.org/abs/1905.02693) (first two rows), although conclusions are the same. Since CVPR'20, we have officially released an updated [DDAD dataset](https://github.com/TRI-ML/DDAD) to account for privacy constraints and improve scene distribution. Please use the latest numbers when comparing to the official DDAD release.

### KITTI

| Model | Abs.Rel. | Sqr.Rel | RMSE | RMSElog | d < 1.25 |
| :--- | :---: | :---: | :---: |  :---: |  :---: |
| [ResNet18, Self-Supervised, 192x640, ImageNet &rightarrow; KITTI (K)](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/ResNet18_MR_selfsup_K.ckpt) | 0.116 | 0.811 | 4.902 | 0.198 | 0.865 |
| [PackNet, Self-Supervised, 192x640, KITTI (K)](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNet01_MR_selfsup_K.ckpt) | 0.111 | 0.800 | 4.576 | 0.189 | 0.880 |
| [PackNet, Self-Supervised Scale-Aware, 192x640, CS &rightarrow; K](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNet01_MR_velsup_CStoK.ckpt) | 0.108 | 0.758 | 4.506 | 0.185 | 0.887 |
| [PackNet, Self-Supervised Scale-Aware, 384x1280, CS &rightarrow; K](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNet01_HR_velsup_CStoK.ckpt) | 0.106 | 0.838 | 4.545 | 0.186 | 0.895 |
| [PackNet, Semi-Supervised (densified GT), 192x640, CS &rightarrow; K](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNet01_MR_semisup_CStoK.ckpt) | 0.072 | 0.335 | 3.220 | 0.115 | 0.934 |

All experiments followed the [Eigen et al.](https://arxiv.org/abs/1406.2283) protocol for [training](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_zhou_files.txt) and [evaluation](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_test_files.txt), with [Zhou et al](https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/)'s preprocessing to remove static training frames. The PackNet model pre-trained on Cityscapes  used for fine-tuning on KITTI can be found [here](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/PackNet01_MR_selfsup_CS.ckpt).

### OmniCam

Our NRS model for OmniCam can be found [here](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/models/nrs/omnicam_pretrained.tar.gz).

### Precomputed Depth Maps

For convenience, we also provide pre-computed depth maps for supervised training and evaluation:

- PackNet, Self-Supervised Scale-Aware, 192x640, CS &rightarrow; K |
[eigen_train_files](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw/eigen_train_files/KITTI_raw-eigen_train_files-PackNet01_MR_velsup_CStoK.tar.gz) |
[eigen_zhou_files](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw/eigen_zhou_files/KITTI_raw-eigen_zhou_files-PackNet01_MR_velsup_CStoK.tar.gz) |
[eigen_val_files](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw/eigen_val_files/KITTI_raw-eigen_val_files-PackNet01_MR_velsup_CStoK.tar.gz) |
[eigen_test_files](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw/eigen_test_files/KITTI_raw-eigen_test_files-PackNet01_MR_velsup_CStoK.tar.gz) |

- PackNet, Semi-Supervised (densified GT), 192x640, CS &rightarrow; K |
[eigen_train_files](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw/eigen_train_files/KITTI_raw-eigen_train_files-PackNet01_MR_semisup_CStoK.tar.gz) |
[eigen_zhou_files](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw/eigen_zhou_files/KITTI_raw-eigen_zhou_files-PackNet01_MR_semisup_CStoK.tar.gz) |
[eigen_val_files](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw/eigen_val_files/KITTI_raw-eigen_val_files-PackNet01_MR_semisup_CStoK.tar.gz) |
[eigen_test_files](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw/eigen_test_files/KITTI_raw-eigen_test_files-PackNet01_MR_semisup_CStoK.tar.gz) |

## License

The source code is released under the [MIT license](LICENSE.md).

## References

[**PackNet**](#cvpr-packnet) relies on symmetric packing and unpacking blocks to jointly learn to compress and decompress detail-preserving representations using 3D convolutions. It also uses depth superresolution, which we introduce in [SuperDepth (ICRA 2019)](#icra-superdepth). Our network can also output metrically scaled depth thanks to our weak velocity supervision ([CVPR 2020](#cvpr-packnet)).

We also experimented with sparse supervision from as few as 4-beam LiDAR sensors, using a novel reprojection loss that minimizes distance errors in the image plane ([CoRL 2019](#corl-ssl)). By enforcing a sparsity-inducing data augmentation policy for ego-motion learning, we were also able to effectively regularize the pose network and enable stronger generalization performance ([CoRL 2019](#corl-pose)). In a follow-up work, we propose the injection of semantic information directly into the decoder layers of the depth networks, using pixel-adaptive convolutions to create semantic-aware features and further improve performance ([ICLR 2020](#iclr-semguided)).

Depending on the application, please use the following citations when referencing our work:

<a id=""cvpr-packnet""> </a>
**3D Packing for Self-Supervised Monocular Depth Estimation (CVPR 2020 oral)** \
*Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos and Adrien Gaidon*, [**[paper]**](https://arxiv.org/abs/1905.02693), [**[video]**](https://www.youtube.com/watch?v=b62iDkLgGSI)

```
@inproceedings{packnet,
  author = {Vitor Guizilini and Rares Ambrus and Sudeep Pillai and Allan Raventos and Adrien Gaidon},
  title = {3D Packing for Self-Supervised Monocular Depth Estimation},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  primaryClass = {cs.CV}
  year = {2020},
}
```

<a id=""3dv-nrs""> </a>
**Neural Ray Surfaces for Self-Supervised Learning of Depth and Ego-motion (3DV 2020 oral)** \
*Igor Vasiljevic, Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Wolfram Burgard, Greg Shakhnarovich, Adrien Gaidon*, [**[paper]**](https://arxiv.org/abs/2008.06630), [**[video]**](https://www.youtube.com/watch?v=4TLJG6WJ7MA&feature=youtu.be)

```
@inproceedings{vasiljevic2020neural,
  title={Neural Ray Surfaces for Self-Supervised Learning of Depth and Ego-motion},
  author={Vasiljevic, Igor and Guizilini, Vitor and Ambrus, Rares and Pillai, Sudeep and Burgard, Wolfram and Shakhnarovich, Greg and Gaidon, Adrien},
  booktitle = {International Conference on 3D Vision},
  primaryClass = {cs.CV},
  year={2020}
}

```

<a id=""iclr-semguided""> </a>
**Semantically-Guided Representation Learning for Self-Supervised Monocular Depth (ICLR 2020)** \
*Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus and Adrien Gaidon*, [**[paper]**](https://arxiv.org/abs/2002.12319)

```
@inproceedings{packnet-semguided,
  author = {Vitor Guizilini and Rui Hou and Jie Li and Rares Ambrus and Adrien Gaidon},
  title = {Semantically-Guided Representation Learning for Self-Supervised Monocular Depth},
  booktitle = {International Conference on Learning Representations (ICLR)}
  month = {April},
  year = {2020},
}
```

<a id=""corl-ssl""> </a>
**Robust Semi-Supervised Monocular Depth Estimation with Reprojected Distances (CoRL 2019 spotlight)** \
*Vitor Guizilini, Jie Li, Rares Ambrus, Sudeep Pillai and Adrien Gaidon*, [**[paper]**](https://arxiv.org/abs/1910.01765),[**[video]**](https://www.youtube.com/watch?v=cSwuF-XA4sg)

```
@inproceedings{packnet-semisup,
  author = {Vitor Guizilini and Jie Li and Rares Ambrus and Sudeep Pillai and Adrien Gaidon},
  title = {Robust Semi-Supervised Monocular Depth Estimation with Reprojected Distances},
  booktitle = {Conference on Robot Learning (CoRL)}
  month = {October},
  year = {2019},
}
```

<a id=""corl-pose""> </a>
**Two Stream Networks for Self-Supervised Ego-Motion Estimation (CoRL 2019 spotlight)** \
*Rares Ambrus, Vitor Guizilini, Jie Li, Sudeep Pillai and Adrien Gaidon*, [**[paper]**](https://arxiv.org/abs/1910.01764)

```
@inproceedings{packnet-twostream,
  author = {Rares Ambrus and Vitor Guizilini and Jie Li and Sudeep Pillai and Adrien Gaidon},
  title = {{Two Stream Networks for Self-Supervised Ego-Motion Estimation}},
  booktitle = {Conference on Robot Learning (CoRL)}
  month = {October},
  year = {2019},
}
```

<a id=""icra-superdepth""> </a>
**SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation (ICRA 2019)** \
*Sudeep Pillai, Rares Ambrus and Adrien Gaidon*, [**[paper]**](https://arxiv.org/abs/1810.01849), [**[video]**](https://www.youtube.com/watch?v=jKNgBeBMx0I&t=33s)

```
@inproceedings{superdepth,
  author = {Sudeep Pillai and Rares Ambrus and Adrien Gaidon},
  title = {SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)}
  month = {May},
  year = {2019},
}
```"
240,hiteshbpatel/Android_Blog_Projects,Java,
241,vmware/open-vm-tools,C,"# General
## What is the open-vm-tools project?
open-vm-tools is a set of services and modules that enable several features in VMware products for better management of, and seamless user interactions with, guests. It includes kernel modules for enhancing the performance of virtual machines running Linux or other VMware supported Unix like guest operating systems. 
 
open-vm-tools enables the following features in VMware products:

- The ability to perform virtual machine power operations gracefully.
- Execution of VMware provided or user configured scripts in guests during various power operations.
- The ability to run programs, commands and file system operation in guests to enhance guest automation.
- Authentication for guest operations. 
- Periodic collection of network, disk, and memory usage information from the guest.
- Generation of heartbeat from guests to hosts so VMware's HA solution can determine guests' availability.
- Clock synchronization between guests and hosts or client desktops.
- Quiescing guest file systems to allow hosts to capture file-system-consistent guest snapshots.
- Execution of pre-freeze and post-thaw scripts while quiescing guest file systems.
- The ability to customize guest operating systems immediately after powering on virtual machines.
- Enabling shared folders between host and guest file systems on VMware Workstation and VMware Fusion.
- Copying and pasting text, graphics, and files between guests and hosts or client desktops.

## Can you provide more details on the actual code being released?
The following components have been released as open source software:
- Linux, Solaris and FreeBSD drivers for various devices and file system access.
- The memory balloon driver for reclaiming memory from guests.
- The PowerOps plugin to perform graceful power operation and run power scripts.
- The VIX plugin to run programs and commands, and perform file system operations in guests.
- The GuestInfo plugin to periodically collect various statistics from guests.
- The TimeSync plugin to perform time synchronization.
- The dndcp plugin to support drag and drop, and text and file copy/paste operations.
- The ResolutionSet plugin to adjust guest screen resolutions automatically based on window sizes.
- The guest authentication service.
- The toolbox command to perform disk wiping and shrinking, manage power scripts, and time synchronization.
- The guest SDK libraries to provide information about virtual machines to guests.
- Clients and servers for shared folders support.
- Multiple monitor support.
- The GTK Toolbox UI.
 
## Is open-vm-tools available with Linux distributions?
Yes. open-vm-tools packages for user space components are available with new versions of major Linux distributions, and are installed as part of the OS installation in several cases. Please refer to VMware KB article http://kb.vmware.com/kb/2073803 for details. All leading Linux vendors support open-vm-tools and bundle it with their products. For information about OS compatibility for open-vm-tools, see the 
VMware Compatibility Guide at http://www.vmware.com/resources/compatibility
Automatic installation of open-vm-tools along with the OS installation eliminates the need to separately install open-vm-tools in guests. If open-vm-tools is not installed automatically, you may be able to manually install it from the guest OS vendor's public repository. Installing open-vm-tools from the Linux vendor's repository reduces virtual machine downtime because future updates to open-vm-tools are included with the OS maintenance patches and updates.
**NOTE**: Most of the Linux distributions ship two or more open-vm-tools packages. ""open-vm-tools"" is the core package without any dependencies on X libraries and ""open-vm-tools-desktop"" is an additional package with dependencies on ""open-vm-tools"" core package and X libraries. The ""open-vm-tools-sdmp"" package contains a plugin for Service Discovery. There may be additional packages, please refer to the documentation of the OS vendor. Note that the open-vm-tools packages available with Linux distributions do not include Linux drivers because Linux drivers are available as part of Linux kernel itself. Linux kernel versions 3.10 and later include all of the Linux drivers present in open-vm-tools except the vmhgfs driver. The vmhgfs driver was required for enabling shared folders feature, but is superseded by vmhgfs-fuse which does not require a kernel driver.

## Will there be continued support for VMware Tools and OSP? 
VMware Tools will continue to be available under a commercial license. It is recommended that open-vm-tools be used for the Linux distributions where open-vm-tools is available. VMware will not provide OSPs for operating systems where open-vm-tools is available.

## How does this benefit other open source projects?
Under the terms of the GPL, open source community members are able to use the open-vm-tools code to develop their own applications, extend it, and contribute to the community. They can also incorporate some or all of the code into their projects, provided they comply with the terms of the GPL.

# License Related
## What license is the code being released under?
The code is being released under GPL v2 and GPL v2 compatible licenses. To be more specific, the Linux kernel modules are being released under the GPL v2, while almost all of the user level components are being released under the LGPL v2.1. The SVGA and mouse drivers have been available under the X11 license for quite some time. There are certain third party components released under BSD style licenses, to which VMware has in some cases contributed, and will continue to distribute with open-vm-tools.
 
## Why did you choose these licenses?
We chose the GPL v2 for the kernel components to be consistent with the Linux kernel's license. We chose the LGPL v2.1 for the user level components because some of the code is implemented as shared libraries and we do not wish to restrict proprietary code from linking against those libraries. For consistency, we decided to license the rest of the userlevel code under the LGPL v2.1 as well.

## What are the obligations that the license(s) impose?
Each of these licenses have different obligations.
For questions about the GPL, LGPL licenses, the Free Software Foundation's GPL FAQ page provides lots of useful information. 
For questions about the other licenses like the X11, BSD licenses, the Open Source Initiative has numerous useful resources including mailing lists. 
The Software Freedom Law Center provides legal expertise and consulting for free and open source software (FOSS) developers.

## Can I use all or part of this code in my proprietary software? Do I have to release the source code if I do?
Different open source licenses have different requirements regarding the release of source code. Since the code is being released under various open source licenses, you will need to comply with the terms of the corresponding licenses.

## Am I required to contribute back any changes I make to the code?
No, you aren't required to contribute any changes that you make back to the open-vm-tools project. However, we encourage you to do so.

## Can I use all or part of this code in another open source package?
Yes, as long as you comply with the appropriate license(s).
 
## Can I package this for my favorite operating system?
Yes! Please do. 

## Will the commercial version (VMware Tools) differ from the open source version (open-vm-tools)? If so, how?
Our goal is to work towards making the open source version as close to the commercial version as possible. However, we do currently make use of certain components licensed from third parties as well as components from other VMware products which are only available in binary form.

## If I use the code from the open-vm-tools project in my project/product, can I call my project/product VMware Tools?
No, since your project/product is not a VMware project/product.

# Building open-vm-tools
## How do I build open-vm-tools?
open-vm-tools uses the GNU Automake tool for generating Makefiles to build all sources. More information about Automake can be found here: http://www.gnu.org/software/automake/
## Project build information:
The following steps will work on most recent Linux distributions:
```
autoreconf -i
./configure
make
sudo make install
sudo ldconfig
```

To build the optional sdmp (Service Discovery) plugin use the `--enable-servicediscovery` option to invoke the configure script:
```
./configure --enable-servicediscovery
```

## Getting configure options and help
If you are looking for help or additional settings for the building of this project, the following configure command will display a list of help options:
```
./configure --help
```
When using configure in the steps above it is only necessary to call ./configure once unless there was a problem after the first invocation.

# Getting Involved
## How can I get involved today?
You can get involved today in several different ways:
- Start using open-vm-tools today and give us feedback.
- Suggest feature enhancements.
- Identify and submit bugs under issues section: https://github.com/vmware/open-vm-tools/issues
- Start porting the code to other operating systems.   Here is the list of operating systems with open-vm-tools:

  * Red Hat Enterprise Linux 7.0 and later releases
  * SUSE Linux Enterprise 12 and later releases
  * Ubuntu 14.04 and later releases
  * CentOS 7 and later releases
  * Debian 7.x and later releases
  * Oracle Linux 7 and later 
  * Fedora 19 and later releases
  * openSUSE 11.x and later releases
 
## Will external developers be allowed to become committers to the project?
Yes. Initially, VMware engineers will be the only committers. As we roll out our development infrastructure, we will be looking to add external committers to the project as well.

## How can I submit code changes like bug fixes, patches, new features to the project?
Initially, you can submit bug fixes, patches and new features to the project development mailing list as attachments to emails or bug reports. To contribute source code, you will need to fill out a contribution agreement form as part of the submission process. We will have more details on this process shortly.

## What is the governance model for managing this as an open source project?
The feature roadmap and schedules for the open-vm-tools project will continue to be defined by VMware. Initially, VMware engineers will be the only approved committers. We will review incoming submissions for suitability for merging into the project. We will be looking to add community committers to the project based on their demonstrated contributions to the project. Finally, we also plan to set up a process for enhancement proposals, establishing sub-projects and so on.

## Will you ship code that I contribute with VMware products? If so, will I get credit for my contributions?
Contributions that are accepted into the open-vm-tools project's main source tree will likely be a part of VMware Tools. We also recognize the value of attribution and value your contributions. Consequently, we will acknowledge contributions from the community that are distributed with VMware's products.

## Do I need to sign something before making a contribution?
Yes. We have a standard contribution agreement that covers all contributions made to the project. It gives VMware and you joint copyright interests in the code you are contributing. The agreement also gives VMware flexibility with licensing and also helps avoid any copyright/licensing related issues that may arise in the future. In order for us to include your contribution in our source tree, we ask that you send us a signed copy of the agreement. You can do this in one of two ways:
Fax to +1.650.427.5003, Attn: Product & Technology Law Group
Scan and email it to oss-queries_at_vmware.com
Agreement: http://open-vm-tools.sourceforge.net/files/vca.pdf

# Compatibilty

## What Operating Systems are supported for customization?
The [Guest OS Customization Support Matrix](http://partnerweb.vmware.com/programs/guestOS/guest-os-customization-matrix.pdf) provides details about the guest operating systems supported for customization.

## Which versions of open-vm-tools are compatible with other VMware products?

The [VMware Product Interoperability Matrix](http://partnerweb.vmware.com/comp_guide2/sim/interop_matrix.php) provides details about the compatibility of different versions of VMware Tools (includes open-vm-tools) and other VMware Products.

# Internationalization
## Which languages are supported?

open-vm-tools supports the following languages:
- English
- French
- German
- Spanish
- Italian
- Japanese
- Korean
- Simplified Chinese
- Traditional Chinese

# Other
## Mailing Lists
Please send an email to one of these mailing lists based on the nature of your question.
- Development related questions : open-vm-tools-devel@lists.sourceforge.net
- Miscellaneous questions: open-vm-tools-discuss@lists.sourceforge.net
- General project announcements: open-vm-tools-announce@lists.sourceforge.net
"
242,Flowpack/Flowpack.ElasticSearch.ContentRepositoryQueueIndexer,PHP,"# Neos CMS Elasticsearch indexer based on a job queue

[![Latest Stable Version](https://poser.pugx.org/flowpack/elasticsearch-contentRepositoryQueueIndexer/v/stable)](https://packagist.org/packages/flowpack/elasticsearch-contentRepositoryQueueIndexer) [![Total Downloads](https://poser.pugx.org/flowpack/elasticsearch-contentRepositoryQueueIndexer/downloads)](https://packagist.org/packages/flowpack/elasticsearch-contentRepositoryQueueIndexer)

This package can be used to index a huge amount of nodes in Elasticsearch indexes. This
package use the Flowpack JobQueue packages to handle the indexing asynchronously.

**Topics**

* [Installation](#installation-and-configuration)
* [Indexing](#indexing)
* [SupervisorD configuration](#supervisord-configuration)
* [Update Instructions](#update-instructions)


# Installation and Configuration

You need to install the correct Queue package based on your needs.

Available packages:

  - [sqlite](https://packagist.org/packages/flownative/jobqueue-sqlite)
  - [beanstalkd](https://packagist.org/packages/flowpack/jobqueue-beanstalkd)
  - [doctrine](https://packagist.org/packages/flowpack/jobqueue-doctrine)
  - [redis](https://packagist.org/packages/flowpack/jobqueue-redis)

Please check the package documentation for specific configurations.

The default configuration uses the FakeQueue, which is provided by the JobQueue.Common package. Note that with that package jobs are executed synchronous with the `flow nodeindexqueue:build` command.

Check the ```Settings.yaml``` to adapt based on the Queue package, you need to adapt the ```className```:

    Flowpack:
      JobQueue:
        Common:
          presets:
            'Flowpack.ElasticSearch.ContentRepositoryQueueIndexer':
              className: 'Flowpack\JobQueue\Common\Queue\FakeQueue'

If you use the [doctrine](https://packagist.org/packages/flownative/jobqueue-doctrine) package you have to set the ```tableName``` manually:

    Flowpack:
      JobQueue:
        Common:
          presets:
            'Flowpack.ElasticSearch.ContentRepositoryQueueIndexer':
              className: 'Flowpack\JobQueue\Doctrine\Queue\DoctrineQueue'
          queues:
            'Flowpack.ElasticSearch.ContentRepositoryQueueIndexer':
              options:
                tableName: 'flowpack_jobqueue_QueueIndexer'
            'Flowpack.ElasticSearch.ContentRepositoryQueueIndexer.Live':
              options:
                tableName: 'flowpack_jobqueue_QueueIndexerLive'

# Indexing

## Batch Indexing

### How to build indexing jobs

    flow nodeindexqueue:build --workspace live

#### How to process indexing jobs

You can use this CLI command to process indexing job:

    flow nodeindexqueue:work --queue batch

## Live Indexing

You can disable async live indexing by editing ```Settings.yaml```:

    Flowpack:
      ElasticSearch:
        ContentRepositoryQueueIndexer:
          enableLiveAsyncIndexing: false

You can use this CLI command to process indexing job:

    flow nodeindexqueue:work --queue live

# Supervisord configuration

You can use tools like ```supervisord``` to manage long running processes. Bellow you can find a basic configuration:

    [supervisord]

    [supervisorctl]

    [program:elasticsearch_batch_indexing]
    command=php flow nodeindexqueue:work --queue batch
    stdout_logfile=AUTO
    stderr_logfile=AUTO
    numprocs=4
    process_name=elasticsearch_batch_indexing_%(process_num)02d
    environment=FLOW_CONTEXT=""Production""
    autostart=true
    autorestart=true
    stopsignal=QUIT

    [program:elasticsearch_live_indexing]
    command=php flow nodeindexqueue:work --queue live
    stdout_logfile=AUTO
    stderr_logfile=AUTO
    numprocs=4
    process_name=elasticsearch_live_indexing_%(process_num)02d
    environment=FLOW_CONTEXT=""Production""
    autostart=true
    autorestart=true
    stopsignal=QUIT

# Update Instructions

## Breaking change after an upgrade to 3.0

* Previously the Beanstalk queue package was installed by default, this is no longer
the case.

## Breaking change after an upgrade to 5.0

* The beanstalk queue configuration is removed. The FakeQueue is used if not configured to another queuing package.

License
-------

Licensed under MIT, see [LICENSE](LICENSE)
"
243,arXivTimes/arXivTimes,,"# arXivTimes

![arXivTimesLogo.PNG](./arXivTimesLogo.PNG)

機械学習関係の論文を調査し、共有するためのリポジトリです。

**Follow Me on the Twitter!**

[@arxivtimes](https://twitter.com/arxivtimes)

論文輪講も併せて実施しています。輪講の内容はこちらから。

[arXivTimes Medium](https://medium.com/@arxivtimes)

`#arXivTimes`タグをつけている、GitHubで論文をまとめているリポジトリはこちら。

[arXivTimes family](https://github.com/topics/arxivtimes)

# Contents

* [Article Summaries](https://github.com/arXivTimes/arXivTimes/issues)
  * 論文の一言まとめ、また概要をIssueで管理しています。
* [Datasets](https://github.com/arXivTimes/arXivTimes/tree/master/datasets)
  * 機械学習に利用可能なデータセットをまとめています。
* [Tools](https://github.com/arXivTimes/arXivTimes/tree/master/tools)
  * 機械学習モデルの実装に役立つツールをまとめています。
* Conference Related Papers
  * arXivTimesに投稿された論文のうち、学会に提出された論文を年代ごとにまとめています。
  * [NIPS](https://github.com/arXivTimes/arXivTimes/projects/1)
  * [ICLR](https://github.com/arXivTimes/arXivTimes/projects/2)
  * [ICML](https://github.com/arXivTimes/arXivTimes/projects/3)
  * [CVPR](https://github.com/arXivTimes/arXivTimes/projects/6)
  * [ACL](https://github.com/arXivTimes/arXivTimes/projects/4)
  * [AAAI](https://github.com/arXivTimes/arXivTimes/projects/5)

Conference Information

* 各学会の締め切り: [AI Conference Deadlines](https://aideadlin.es/)
* 各学会のBest Paper: [Best Paper Awards in Computer Science](https://jeffhuang.com/best_paper_awards.html)

# How to Contribute

論文のまとめを投稿する際は、以下の要領でお願いいたします。

* 登録したい論文は、Issueに登録を行ってください。
* Issueのタイトルを論文とし、内容はIssue Templateに従ってください。登録時必須なのは以下点のみです
  * 一言でいうと
  * 論文リンク
  * 著者/所属機関 ※論文からのコピペでOK
  * 投稿日付 Submission dateです。yyyy/MM/ddの形式でお願いします。学会に提出されている場合は、それについても記載いただけると助かります(NIPS 2017など)
* 「一言でいうと」の分量は、Twitterで呟ける程度が目安です。問題設定・アプローチ手法・結果が端的にまとまっているのがよい「一言」です。未読の人になるべくその内容が伝わるよう、工夫を凝らしてください。
* Issueのより詳細な内容について記載を行う場合は、担当者に(Assignees)自分を設定してください。これは読んでいる論文のバッティングを防ぐための措置です
* 論文の内容に応じて、Labelをつけてください(現時点ではContributorのみ付与可能なので、投稿頂いた場合こちらで付与を行わせていただきます)
* コメントには、論文を読んでの所感や評価、また理解が難しい点などがあったらその旨を記載してください
* 論文が学会に採択された場合、投稿した論文のコメントにてご連絡ください。Projectページに反映します。

なお、まとめについて内容に自信がない、という場合はコメントで@arxivtimesbotに対してメンションでレビューを依頼できます。

![mention](https://user-images.githubusercontent.com/544269/29446756-d88b8daa-8428-11e7-8ff1-e4f938d94e0c.png)

レビューの依頼が行われると、arXivtimesのメンバーがレビューします。

タグを増やしたい、テンプレートを変えたい、という要望は`proposal`のタグをつけてIssueにあげてください。

また、その他のコンテンツについては他のOSS同様、Pull Requestにてお願いいたします。
"
244,python-babel/babel,Python,"About Babel
===========

Babel is a Python library that provides an integrated collection of
utilities that assist with internationalizing and localizing Python
applications (in particular web-based applications.)

Details can be found in the HTML files in the ``docs`` folder.

For more information please visit the Babel web site:

http://babel.pocoo.org/

Join the chat at https://gitter.im/python-babel/babel

Contributing to Babel
=====================

If you want to contribute code to Babel, please take a look at our
`CONTRIBUTING.md <https://github.com/python-babel/babel/blob/master/CONTRIBUTING.md>`__.

If you know your way around Babels codebase a bit and like to help
further, we would appreciate any help in reviewing pull requests. Please
contact us at https://gitter.im/python-babel/babel if you're interested!
"
245,gin-gonic/examples,Go,"# Gin Examples

This repository contains a number of ready-to-run examples demonstrating various use cases of [Gin](https://github.com/gin-gonic/gin).

Refer to the [Gin documentation](https://gin-gonic.com/docs/) for how to execute the example tutorials.

## Contributing

Are you missing an example? Please feel free to open an issue or commit one pull request.

Please see [CONTRIBUTING.md](./CONTRIBUTING.md) for instructions on how to contribute.
"
246,krazydanny/laravel-repository,PHP,"Laravel Model Repository
========================

[![Latest Stable Version](https://img.shields.io/github/v/release/krazydanny/laravel-repository?include_prereleases)](https://packagist.org/packages/krazydanny/laravel-repository) [![Donate](https://img.shields.io/badge/donate-paypal-blue.svg)](https://paypal.me/danielspadafora) [![License](https://img.shields.io/github/license/krazydanny/laravel-repository)](https://github.com/krazydanny/laravel-repository/blob/master/LICENSE)



This package provides an abstraction layer for easily implementing industry-standard caching strategies with Eloquent models.


- [Laravel Model Repository](#laravel-model-repository)
	- [Main Advantages](#main-advantages)
		- [Simplify caching strategies and buy time](#simplify-caching-strategies-and-buy-time)	
		- [Save cache storage and money](#save-cache-storage-and-money)
	- [Installation](#installation)
		- [Laravel version Compatibility](#laravel-version-compatibility)
		- [Lumen version Compatibility](#lumen-version-compatibility)
		- [Install the package via Composer](#install-the-package-via-composer)
	- [Creating a Repository for a Model](#creating-a-repository-for-a-model)
	- [Use with Singleton Pattern](#use-with-singleton-pattern)
	- [Eloquent like methods](#eloquent-like-methods)
	- [Making Eloquent Queries](#making-eloquent-queries)
	- [Caching methods overview](#caching-methods-overview)
	- [Implementing Caching Strategies](#implementing-caching-strategies)
		- [Read-Aside](#read-aside-cache)
		- [Read-Through](#read-through-cache)
		- [Write-Through](#write-through-cache)
		- [Write-Back](#write-back-cache)
	- [Pretty Queries](#pretty-queries)
	- [Cache Invalidation Techniques](#cache-invalidation-techniques)
		- [Saving cache storage](#saving-cache-storage)
		- [Keeping cache consistency](#keeping-cache-consistency)
	- [Exception handling](#exception-handling)
		- [Database Exceptions](#database-exceptions)
		- [Cache Store Exceptions](#cache-store-exceptions)
	- [Repository Events](#repository-events)
	- [Some things I wish somebody told me before](#some-things-i-wish-somebody-told-me-before)
	- [Bibliography](#bibliography)


<br>

Main Advantages
---------------


### Simplify caching strategies and buy time

Implementing high availability and concurrency caching strategies could be a complex and time consuming task without the appropiate abstraction layer.

Laravel Model Repository simplifies caching strategies using human-readable chained methods for your existing Eloquent models :)


### Save cache storage and money

Current available methods for caching Laravel models store the entire PHP object in cache. That consumes a lot of extra storage and results in slower response times, therefore having a more expensive infrastructure.

Laravel Model Repository stores only the business specific data of your model in order to recreate exactly the same instance later (after data being loaded from cache). Saving more than 50% of cache storage and significantly reducing response times from the cache server.

<br>

Installation
------------
Make sure you have properly configured a cache connection and driver in your Laravel/Lumen project. You can find cache configuration instructions for Laravel at https://laravel.com/docs/7.x/cache and for Lumen at https://lumen.laravel.com/docs/6.x/cache


### Laravel version Compatibility

 Laravel  | Package
:---------|:----------
 5.6.x    | 1.2.0
 5.7.x    | 1.2.0
 5.8.x    | 1.2.0
 6.x      | 1.2.0
 7.x      | 1.2.0


### Lumen version Compatibility

 Lumen    | Package
:---------|:----------
 5.6.x    | 1.2.0
 5.7.x    | 1.2.0
 5.8.x    | 1.2.0
 6.x      | 1.2.0
 7.x      | 1.2.0



### Install the package via Composer

```bash
$ composer require krazydanny/laravel-repository
```

<br>

Creating a Repository for a Model
---------------------------------	

In order to simplify caching strategies we will encapsulate model access within a model repository.

Two parameters can be passed to the constructor. The first parameter (required) is the model's full class name. The second parameter (optional) is the prefix to be used in cache to store model data.


```php
namespace App\Repositories;

use App\User;
use KrazyDanny\Laravel\Repository\BaseRepository;

class UserRepository extends BaseRepository {

	public function __construct ( ) {

		parent::__construct(
			User::class, // Model's full class name
			'Users' // OPTIONAL the name of the cache prefix. The short class name will be used by default. In this case would be 'User'
		);
	}
}

```

<br>


Use with Singleton Pattern
--------------------------

As a good practice to improve performance and keep your code simple is strongly recommended to use repositories along with the singleton pattern, avoiding the need for creating separate instances for the same repository at different project levels.

First register the singleton call in a service provider:

```php
namespace App\Providers;

use App\Repositories\UserRepository;
use Illuminate\Support\ServiceProvider;

class AppServiceProvider extends ServiceProvider {

    public function register ( ) {

        $this->app->singleton( 
           UserRepository::class, 
            function () {
                return (new UserRepository);
            }
        );
    }

    # other service provider methods here
}

```

Add a line like this on every file you call the repository in order to keep code clean and pretty ;)

```php
use App\Repositories\UserRepository;

```

Then access the same repository instance anywhere in your project :)

```php
$userRepository = app( UserRepository::class );

```

You can also typehint it as a parameter in controllers, event listeners, middleware or any other service class and laravel will automatically inject the repository instance

```php
namespace App\Http\Controllers;

use Illuminate\Http\Request;
use App\Repositories\UserRepository;

class UserController extends Controller
{
	public function myMethod( UserRepository $userRepository, $id){
		// you can now use the repository to work with cached models
		$user = $userRepository->get( $id );
	}
}
```
<br>

Eloquent like methods
---------------------

Calling Eloquent-like methods directly from our repository gives us the advantage of combining them with caching strategies. First, let's see how we call them. It's pretty straightforward :)

### create()

Create a new model:
```php
$user = app( UserRepository::class )->create([
	'firstname' => 'Krazy',
	'lastname'  => 'Danny',
	'email'	    => 'somecrazy@email.com',
	'active'    => true,
]);

$user_id = $user->getKey();

```

### get()

Get a specific model by ID:
```php
$user = app( UserRepository::class )->get( $user_id );

```

### save()

Update a specific model:
```php
$user->active = false;

app( UserRepository::class )->save( $user );

```

### delete()

Delete a specific model:
```php
app( UserRepository::class )->delete( $user );

```

<br>


Making Eloquent Queries
-----------------------

Unlike get() or save(), query methods work a little different. They receive as parameter the desired query builder instance (Illuminate\Database\Eloquent\Builder) in order to execute the query.

This will allow us to combine queries with caching strategies, as we will cover forward on this document. For now let's focus on the query methods only. For example:

### find()

To find all models under a certain criteria:
```php
$q = User::where( 'active', true );

$userCollection = app( UserRepository::class )->find( $q );

```

### first()

To get the first model instance under a certain criteria:
```php
$q = User::where( 'active', true );

$user = app( UserRepository::class )->first( $q );

```

### count()

To count all model instances under a certain criteria:
```php
$q = User::where( 'active', true );

$userCount = app( UserRepository::class )->count( $q );

```

<br>


Caching methods overview
------------------------

### remember() & during()

Calling remember() before any query method like find(), first() or count() stores the query result in cache for a given time. Always followed by the during() method, which defines the duration of the results in cache (TTL/Time-To-Live in seconds)

**VERY IMPORTANT:** For Laravel/Lumen v5.7 and earlier versions TTL param passed to during() are minutes instead of seconds. This library follows Laravel standards so check what unit of time your version uses for the Cache facade.


```php
$q = User::where( 'active', true );

app( UserRepository::class )->remember()->during( 3600 )->find( $q );

```


Also a model instance could be passed as parameter in order to store that specific model in cache.


```php
app( UserRepository::class )->remember( $user )->during( 3600 );

```


### according()

The according() method does almost the same as during() but with a difference, it reads the time-to-live in seconds from a given model's attribute:

```php
app( ReservationsRepository::class )->remember( $reservation )->according( 'expiresIn' );

```

This is useful if different instances of the same class have/need different or dynamic time-to-live values.



### rememberForever()

Calling rememberForever() before any query method like find(), first() or count() stores the query result in cache without an expiration time.


```php
$q = User::where( 'active', true );

app( UserRepository::class )->rememberForever()->find( $q );

```


Also a model instance could be passed as parameter in order to store that specific model in cache without expiration.


```php
app( UserRepository::class )->rememberForever( $user );

```


### fromCache()

Calling fromCache() before any query method like find(), first() or count() will try to retrieve the results from cache ONLY.


```php
$q = User::where( 'active', true );

app( UserRepository::class )->fromCache()->find( $q );

```


Also a model instance could be passed as parameter in order to retrieve that specific model from cache ONLY.


```php
app( UserRepository::class )->fromCache( $user );

```


### forget()

This method removes one or many models (or queries) from cache. It's very useful when you have updated models in the database and need to invalidate cached model data or related query results (for example: to have real-time updated cache).

The first parameter must be an instance of the model, a specific model ID (primary key) or a query builder instance (Illuminate\Database\Eloquent\Builder).


Forget query results:
```php
$query = User::where( 'active', true );

app( UserRepository::class )->forget( $query );

```

Forget a specific model using the object:
```php
app( UserRepository::class )->forget( $userModelInstance );

```

Forget a specific model by id:
```php
app( UserRepository::class )->forget( $user_id );

```

The second parameter (optional) could be an array to queue forget() operations in order to be done in a single request to the cache server. 

When passed the forget() method appends to the array (by reference) the removal operations instead of sending them instantly to the cache server.

It's useful when you need to expire many cached queries or models of the same repository. You can do it in one request optimizing response times for your cache server, therefore your app :)

For example:
```php
$user->active = false;
$user->save();

$forgets = [];

#removes user model from cache
app( UserRepository::class )->forget( $user, $forgets );

#removes query that finds active users
$query = User::where( 'active', true );
app( UserRepository::class )->forget( $query, $forgets );

#requests all queued removals to the cache server
app( UserRepository::class )->forget( $forgets );

```

<br>


Implementing Caching Strategies
-------------------------------

<br>

### Read-Aside Cache

<p align=""center"">
  <img alt=""Read Aside Caching"" src=""https://github.com/krazydanny/laravel-repository/blob/master/read-aside-cache.png"">
</p>

**How it works?**

1. The app first looks the desired model or query in the cache. If the data was found in cache, we’ve cache hit. The model or query results are read and returned to the client without database workload at all.
2. If model or query results were not found in cache we have a cache miss, then data is retrieved from database.
3. Model or query results retrived from database are stored in cache in order to have a successful cache hit next time.

**Use cases**

Works best for heavy read workload scenarios and general purpose.

**Pros**

Provides balance between lowering database read workload and cache storage use.

**Cons**

In some cases, to keep cache up to date in real-time,  you may need to implement cache invalidation using the forget() method.

**Usage**


When detecting you want a model or query to be remembered in cache for a certain period of time, Laravel Model Repository will automatically first try to retrieve it from cache. Otherwise will automatically retrieve it from database and store it in cache for the next time :)


Read-Aside a specific model by ID:
```php
$user = app( UserRepository::class )->remember()->during( 3600 )->get( $user_id );

```

Read-Aside query results:
```php
$q = User::where( 'active', true );

$userCollection = app( UserRepository::class )->remember()->during( 3600 )->find( $q );

$userCount = app( UserRepository::class )->remember()->during( 3600 )->count( $q );

$firstUser = app( UserRepository::class )->remember()->during( 3600 )->first( $q );

```
<br>

### Read-Through Cache

<p align=""center"">
  <img alt=""Read Through Caching"" src=""https://github.com/krazydanny/laravel-repository/blob/master/read-through-cache.png"">
</p>

**How it works?**

1. The app first looks the desired model or query in the cache. If the data was found in cache, we’ve cache hit. The model or query results are read and returned to the client without database workload at all.
2. If model or query results were not found in cache we have a cache miss, then data is retrieved from database ONLY THIS TIME in order to be always available from cache.


**Use cases**

Works best for heavy read workload scenarios where the same model or query is requested constantly.

**Pros**

Keeps database read workload at minimum because always retrieves data from cache.

**Cons**

If you want cache to be updated you must combine with Write-Through strategy (incrementing writes latency and workload in some cases) or implementing cache invalidation using the forget() method.


**Usage**


When detecting you want a model or query to be remembered in cache forever, Laravel Model Repository will automatically first try to retrieve it from cache. Otherwise will automatically retrieve it from database and store it without expiration, so it will be always available form cache :)


Read-Through a specific model by ID:
```php
$user = app( UserRepository::class )->rememberForever()->get( $user_id );

```

Read-Through query results:
```php
$q = User::where( 'active', true );

$userCollection = app( UserRepository::class )->rememberForever()->find( $q );

$userCount = app( UserRepository::class )->rememberForever()->count( $q );

$firstUser = app( UserRepository::class )->rememberForever()->first( $q );

```
<br>

### Write-Through Cache

<p align=""center"">
  <img alt=""Write Through Caching"" src=""https://github.com/krazydanny/laravel-repository/blob/master/write-through-cache.png"">
</p>

**How it works?**

Models are always stored in cache and database.


**Use cases**

Used in scenarios where consistency is a priority or needs to be granted.


**Pros**

No cache invalidation techniques required. No need for using forget() method.

**Cons**

Could introduce write latency in some scenarios because data is always written in cache and database.


**Usage**

When detecting you want a model to be remembered in cache, Laravel Model Repository will automatically store it in cache and database (inserting or updating depending on the case).


Write-Through without expiration time:
```php
# create a new user in cache and database
$user = app( UserRepository::class )->rememberForever()->create([
	'firstname' => 'Krazy',
	'lastname'  => 'Danny',
	'email'	    => 'somecrazy@email.com',
	'active'    => true,
]);

# update an existing user in cache and database
$user->active = false;

app( UserRepository::class )->rememberForever()->save( $user );

```

Write-Through with expiration time (TTL):
```php
# create a new user in cache and database
$user = app( UserRepository::class )->remember()->during( 3600 )->create([
	'firstname' => 'Krazy',
	'lastname'  => 'Danny',
	'email'	    => 'somecrazy@email.com',
	'active'    => true,
]);

# update an existing user in cache and database
$user->active = false;

app( UserRepository::class )->remember()->during( 3600 )->save( $user );

```
<br>

### Write-Back Cache

<p align=""center"">
  <img alt=""Write Back Caching"" src=""https://github.com/krazydanny/laravel-repository/blob/master/write-back-cache.png"">
</p>

**How it works?**

Models are stored only in cache until they are massively persisted in database.


**Use cases**

Used in heavy write load scenarios and database-cache consistency is not a priority.


**Pros**

Very performant and resilient to database failures and downtimes 

**Cons**

In some cache failure scenarios data may be permanently lost.


**Usage**

*IMPORTANT!! THIS STRATEGY IS AVAILABLE FOR REDIS CACHE STORES ONLY (at the moment)*

With the buffer() or index() method Laravel Model Repository will store data in cache untill you call the persist() method which will iterate many (batch) of cached models at once, allowing us to persist them the way our project needs through a callback function.


First write models in cache:

Using **buffer()**

Stores models in cache in a way only accesible within the persist() method callback. Useful for optimizing performance and storage when you don't need to access them until they are persisted in database.

```php
$model = app( TransactionsRepository::class )->buffer( new Transactions( $data ) );

```

Using **index()**

Stores models in a way that they are available to be loaded from cache by get() method too. Useful when models need to be accesible before they are persisted.

```php
$model = app( TransactionsRepository::class )->index( new Transactions( $data ) );

```

Then massively persist models in database:

Using **persist()** 

The persist() method could be called later in a separate job or scheduled task, allowing us to manage how often we need to persist models into the database depending on our project's traffic and infrastructure.


```php
app( TransactionsRepository::class )->persist( 

    // the first param is a callback which returns true if models were persisted successfully, false otherwise
    function( $collection ) {
        
        foreach ( $collection as $model ) {

            // do database library custom and optimized logic here

            // for example: you could use bulk inserts and transactions in order to improve both performance and consistency
        }        

        if ( $result )
            return true; // if true remove model ids from persist() queue
        
        return false; // if false keeps model ids in persist() queue and tries again next time persist() method is called
    },

    // the second param (optional) is an array with one or many of the following available options
    [
        'written_since' => 0, // process only models written since ths specified timestamp in seconds
        'written_until' => \time(), // process only models written until the given timestamp in seconds
        'object_limit'  => 500, // the object limit to be processed at the same time (to prevent memory overflows)
        'clean_cache'   => true, // if true and callback returns true, marks models as persisted
        'method'        => 'buffer' // buffer | index
    ] 
);

```

The **method** parameter:

It has two possible values.

- **buffer** (default)

Performs persist() only for those models stored in cache with the buffer() method;

- **index**

Performs persist() only for those models stored in cache with the index() method;


<br>

Pretty Queries
----------------------------------

You can create human readable queries that represent your business logic in an intuititve way and ensures query criteria consistency encapsulating it's code.

For example:

```php
namespace App\Repositories;

use App\User;
use KrazyDanny\Laravel\Repository\BaseRepository;

class UserRepository extends BaseRepository {

	public function __construct ( ) {

		parent::__construct(
			User::class, // Model's class name
			'Users' // the name of the cache prefix
		);
	}

	public function findByState ( string $state ) {

		return $this->find(
			User::where([
				'state'      => $state,
				'deleted_at' => null,
			])
		);
	}

}

```

Then call a pretty query :)

```php
$activeUsers = app( UserRepository::class )->findByState( 'active' );

$activeUsers = app( UserRepository::class )->remember()->during( 3600 )->findByState( 'active' );

$activeUsers = app( UserRepository::class )->rememberForever()->findByState( 'active' );

```

<br>

Cache invalidation techniques
------------------------------------------

In some cases  we will need to remove models or queries from cache even if we've set an expiration time for them.

### Saving cache storage

To save storage we need data to be removed from cache, so we'll use the forget() method. Remember?


**For specific models:**
```php
app( UserRepository::class )->forget( $user );

```
**For queries:**

```php
$user->active = false;
$user->save();

$query = User::where( 'active', true );
app( UserRepository::class )->forget( $query );

```

**On events**

Now let's say we want to invalidate some specific queries when creating or updating a model. We could do something like this:

```php
namespace App\Repositories;

use App\User;
use KrazyDanny\Laravel\Repository\BaseRepository;

class UserRepository extends BaseRepository {

	public function __construct ( ) {

		parent::__construct(
			User::class, // Model's class name
			'Users' // the name of the cache prefix
		);
	}

	// then call this to invalidate active users cache and any other queries or models cache you need.
	public function forgetOnUserSave ( User $user ) {

		// let's use a queue to make only one request with all operations to the cache server
		$invalidations = [];

		// invalidates that specific user model cache
		$this->forget( $user, $invalidations );

		// invalidates the active users query cache
		$this->forget(
			User::where([
				'state'      => 'active',
				'deleted_at' => null,
			]),
			$invalidations
		);

		// makes request to the server and invalidates all cache entries at once

		$this->forget( $invalidations );
	}

}

```

Then, in the user observer...

```php
namespace App\Observers;

use App\User;
use App\Repositories\UserRepository;

class UserObserver {   

    public function saved ( User $model ) {

    	app( UserRepository::class )->forgetOnUserSave( $model );
    }

    # here other observer methods
}

```

### For real-time scenarios

To keep real-time cache consistency we want model data to be updated in the cache instead of being removed.


**For specific models:**

We will simply use remember(), during() and rememberForever() methods:

```php
app( UserRepository::class )->rememberForever( $user );
// or
app( UserRepository::class )->remember( $user )->during( 3600 );

```

**For queries:**

We would keep using forget() method as always, otherwise it would be expensive anyway getting the query from the cache, updating it somehow and then overwriting cache again.

**On events**

Let's assume we want to update model A in cache when model B is updated.

We could do something like this in the user observer:

```php
namespace App\Observers;

use App\UserSettings;
use App\Repositories\UserRepository;

class UserSettingsObserver {   

    public function saved ( UserSettings $model ) {

    	app( UserRepository::class )->remember( $model )->during( 3600 );
    }

    # here other observer methods
}

```

<br>

Repository Events
-----------------

We can also observe the following repository-level events.

- afterGet
- afterFirst
- afterFind
- afterCount


**On each call**

```php
$callback = function ( $cacheHit, $result ) {

	if ( $cacheHit ) {
		// do something when the query hits the cache
	}
	else {
		// do something else when the query hits the database
		// this is not for storing the model in cache, remember the repository did it for you.
	}
}

app( UserRepository::class )->observe( $callback )->rememberForever()->get( $user_id );

```

**On every call**

```php
$callback = function ( $cacheHit, $result ) {

	if ( $cacheHit ) {
		// do something when the query hits the cache
	}
	else {
		// do something else when the query hits the database
		// this is not for storing the model in cache, remember the repository did it for you.
	}
}

app( UserRepository::class )->observeAlways( 'afterGet', $callback);

app( UserRepository::class )->rememberForever()->get( $user_A_id );

app( UserRepository::class )->rememberForever()->get( $user_B_id );

```

**Some use cases...**

- Monitoring usage of our caching strategy in production environments.
- Have a special treatment for models or query results loaded from cache than those retrieved from database.


<br>

Exceptions handling
-------------------

### Cache Exceptions

```php
app( UserRepository::class )->handleCacheExceptions(function( $e ){
	// here we can do something like log the exception silently
})

```

### Database Exceptions

```php
app( UserRepository::class )->handleDatabaseExceptions(function( $e ){
	// here we can do something like log the exception silently
})

```

### The silently() method

When called before any method, that operation will not throw database nor cache exceptions. Unless we've thrown them inside handleDatabaseExceptions() or handleCacheStoreExceptions() methods.

For example:

```php
app( UserRepository::class )->silently()->rememberForever()->get( $user_id );

```


<br>

Some things I wish somebody told me before
------------------------------------------

### ""Be shapeless, like water my friend"" (Bruce Lee) 

There's no unique, best or does-it-all-right caching technique.

Every caching strategy has it's own advantages and disadvantages. Is up to you making a good analysis of what you project needs and it's priorities.

Even in the same project you may use different caching strategies for different models. For example: Is not the same caching millons of transaction logs everyday than registering a few new users in your app.

Also this library is designed to be implemented on the go. This means you can progressively apply caching techniques on specific calls.

Lets say we currently have the following line in many places of our project:

```php
$model = SomeModel::create( $data );

```

Now assume we want to implement write-back strategy for that model only in some critical places of our project and see how it goes. Then we should only replace those specifice calls with:

```php
$model = app( SomeModelRepository::class )->buffer( new SomeModel( $data ) );

```

And leave those calls we want out of the caching strategy alone, they are not affected at all. Besides some things doesn't really need to be cached. 

Be like water my friend... ;)

<br>

Bibliography
------------

Here are some articles which talk in depth about caching strategies:

- https://bluzelle.com/blog/things-you-should-know-about-database-caching
- https://zubialevich.blogspot.com/2018/08/caching-strategies.html
- https://codeahoy.com/2017/08/11/caching-strategies-and-how-to-choose-the-right-one/
- https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/BestPractices.html
"
247,downthemall/downthemall-legacy,JavaScript,"DownThemAll!
==================

The first and only download manager/accelerator built inside Firefox!

Developing
-------------------

https://developer.mozilla.org/en-US/docs/Setting_up_extension_development_environment
Just clone the repository and use an extension proxy file. No additional build step required.

- Pull requests welcome. By submitting code you agree to license it under MPL v2 unless explicitly specified otherwise. 
- Please stick to the general coding style.
- Please also always add unit tests for all new js modules and new module functions.
- Unit tests for UI (overlays) aren't required at the moment, but welcome. There is currently no infrastructure to run those, though.

Building an XPI
-------------------

See `make.py`.

Important bits of code
-------------------

- `modules/glue.jsm` - This is basically the main module, also specifying the general environment for all modules and window scopes.
- `modules/main.js` - General setup.
- `modules/loaders/` - ""overlay"" scripts. Different to traditional Firefox add-ons, DownThemAll! does not use real overlays and overlay scripts, but kind of simulates overlays via modules.
- `chrome/content/` - UI. Right now, due to historical reasons and some too-tight coupling the UI JS also contains some of the important data structures such as `QueueItem` (representing a single queued download)

- Please note that being restartless requires code to clean up after itself, i.e. if you modify something global you need to reverse the modifications when the add-on is unloaded. See `unload()`and `unloadWindow()` (in glue.jsm and/or support/overlays.js)
- Please make use of the niceties Firefox JS (ES6) and of the global helpers from glue.jsm, in particular:
  - `for of` loops
  - Sets and (weak) maps
  - generators
  - comprehensions and destructoring assignment
  - `Object.freeze()`, `Object.defineProperties()`, etc.
  - `log()`
  - `lazy()`/`lazyProto()`
  - `Services` and `Instances`
"
248,ejwa/gitinspector,Python,"[![Latest release](https://img.shields.io/github/release/ejwa/gitinspector.svg?style=flat-square)](https://github.com/ejwa/gitinspector/releases/latest)
[![License](https://img.shields.io/github/license/ejwa/gitinspector.svg?style=flat-square)](https://github.com/ejwa/gitinspector/blob/master/LICENSE.txt)
<h2>
 <img align=""left"" height=""65px""
      src=""https://raw.githubusercontent.com/ejwa/gitinspector/master/gitinspector/html/gitinspector_piclet.png""/>
      &nbsp;About Gitinspector
</h2>
<img align=""right"" width=""30%"" src=""https://raw.github.com/wiki/ejwa/gitinspector/images/html_example.jpg"" /> 
Gitinspector is a statistical analysis tool for git repositories. The default analysis shows general statistics per author, which can be complemented with a timeline analysis that shows the workload and activity of each author. Under normal operation, it filters the results to only show statistics about a number of given extensions and by default only includes source files in the statistical analysis.

This tool was originally written to help fetch repository statistics from student projects in the course Object-oriented Programming Project (TDA367/DIT211) at Chalmers University of Technology and Gothenburg University.

Today, gitinspector is used as a grading aid by universities worldwide.

A full [Documentation](https://github.com/ejwa/gitinspector/wiki/Documentation) of the usage and available options of gitinspector is available on the wiki. For help on the most common questions, please refer to the [FAQ](https://github.com/ejwa/gitinspector/wiki/FAQ) document.

### Some of the features
  * Shows cumulative work by each author in the history.
  * Filters results by extension (default: java,c,cc,cpp,h,hh,hpp,py,glsl,rb,js,sql).
  * Can display a statistical timeline analysis.
  * Scans for all filetypes (by extension) found in the repository.
  * Multi-threaded; uses multiple instances of git to speed up analysis when possible.
  * Supports HTML, JSON, XML and plain text output (console).
  * Can report violations of different code metrics.

### Example outputs
Below are some example outputs for a number of famous open source projects. All the statistics were generated using the *""-HTlrm""* flags.

| Project name | | | | |
|---|---|---|---|---|
| Django | [HTML](http://githubproxy.ejwa.se/wiki/ejwa/gitinspector/examples/django_output.html) | [HTML Embedded](http://githubproxy.ejwa.se/wiki/ejwa/gitinspector/examples/django_output.emb.html) | [Plain Text](http://githubproxy.ejwa.se/wiki/ejwa/gitinspector/examples/django_output.txt) | [XML](http://githubproxy.ejwa.se/wiki/ejwa/gitinspector/examples/django_output.xml) |
| JQuery | [HTML](http://githubproxy.ejwa.se/wiki/ejwa/gitinspector/examples/jquery_output.html) | [HTML Embedded](http://githubproxy.ejwa.se/wiki/ejwa/gitinspector/examples/jquery_output.emb.html) | [Plain Text](http://githubproxy.ejwa.se/wiki/ejwa/gitinspector/examples/jquery_output.txt) | [XML](http://githubproxy.ejwa.se/wiki/ejwa/gitinspector/examples/jquery_output.xml) |
| Pango | [HTML](http://githubproxy.ejwa.se/wiki/ejwa/gitinspector/examples/pango_output.html) | [HTML Embedded](http://githubproxy.ejwa.se/wiki/ejwa/gitinspector/examples/pango_output.emb.html) | [Plain Text](http://githubproxy.ejwa.se/wiki/ejwa/gitinspector/examples/pango_output.txt) | [XML](http://githubproxy.ejwa.se/wiki/ejwa/gitinspector/examples/pango_output.xml) |

### The Team
  * Adam Waldenberg, Lead maintainer and Swedish translation
  * Agustín Cañas, Spanish translation
  * Bart van Andel, npm package maintainer
  * Bill Wang, Chinese translation
  * Christian Kastner, Debian package maintainer
  * Jiwon Kim, Korean translation
  * Kamila Chyla, Polish translation
  * Luca Motta, Italian translation
  * Philipp Nowak, German translation
  * Sergei Lomakov, Russian translation
  * Yannick Moy, French translation

*We need translations for gitinspector!* If you are a gitinspector user, feel willing to help and have good language skills in any unsupported language we urge you to contact us. We also happily accept code patches. Please refer to [Contributing](https://github.com/ejwa/gitinspector/wiki/Contributing) for more information on how to contribute to the project.

### Packages
The Debian packages offered with releases of gitinspector are unofficial and very simple packages generated with [stdeb](https://github.com/astraw/stdeb). Christian Kastner is maintaining the official Debian packages. You can check the current status on the [Debian Package Tracker](https://tracker.debian.org/pkg/gitinspector).  Consequently, there are official packages for many Debian based distributions installable via *apt-get*.

An [npm](https://npmjs.com) package is provided for convenience as well. To install it globally, execute `npm i -g gitinspector`.

### License
gitinspector is licensed under the *GNU GPL v3*. The gitinspector logo is partly based on the git logo; based on the work of Jason Long. The logo is licensed under the *Creative Commons Attribution 3.0 Unported License*.
"
249,CoreWCF/CoreWCF,C#,"### What is Core WCF? 

Core WCF is a port of Windows Communication Framework (WCF) to .NET Core. The goal of this project is to enable existing WCF projects to move to .NET Core.

### Announcements

To keep up to date on what's going on with CoreWCF, you can subscribe to the [announcements](https://github.com/CoreWCF/announcements) repo to be notified about major changes and other noteworthy announcements.

### How do I get started?

There are pre-release packages available from a NuGet feed hosted in Azure DevOps. You can download the packages by adding the following package source to your list of feeds.

    https://pkgs.dev.azure.com/dotnet/CoreWCF/_packaging/CoreWCF/nuget/v3/index.json

If you are using a nuget.config file with only the default nuget.org package source, after adding the CoreWCF feed it would like like this:
```xml
<?xml version=""1.0"" encoding=""utf-8""?>
<configuration>
  <packageSources>
    <clear />
    <add key=""nuget.org"" value=""https://api.nuget.org/v3/index.json"" />
    <add key=""CoreWCF"" value=""https://pkgs.dev.azure.com/dotnet/CoreWCF/_packaging/CoreWCF/nuget/v3/index.json"" />
  </packageSources>
</configuration>
```
### How do I contribute?

Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for details.


### License, etc.

This project has adopted the code of conduct defined by the Contributor Covenant to clarify expected behavior in our community.
For more information see the [.NET Foundation Code of Conduct](https://dotnetfoundation.org/code-of-conduct).

Core WCF is Copyright &copy; 2019 .NET Foundation and other contributors under the [MIT license](LICENSE.txt).

### .NET Foundation

This project is supported by the [.NET Foundation](https://dotnetfoundation.org).
"
250,SOCI/soci,C++,"# SOCI - The C++ Database Access Library

[![GitHub release](https://img.shields.io/github/tag/SOCI/soci.svg)](https://github.com/SOCI/soci/releases/tag/v4.0.2)
[![GitHub commits](https://img.shields.io/github/commits-since/SOCI/soci/v4.0.2.svg)](https://github.com/SOCI/soci/tree/release/4.0)

[![Website](https://img.shields.io/website-up-down-green-red/http/shields.io.svg?label=soci.sourceforge.net)](http://soci.sourceforge.net)
[![SourceForge](https://img.shields.io/sourceforge/dm/soci.svg)](https://sourceforge.net/projects/soci/files/)

[![Gitter](https://img.shields.io/gitter/room/SOCI/soci.svg)](https://gitter.im/SOCI/soci)
[![Mailing Lists](https://img.shields.io/badge/mailing--lists-ok-yellowgreen.svg)](https://sourceforge.net/p/soci/mailman/)
[![StackExchange](https://img.shields.io/stackexchange/stackoverflow/t/soci.svg)](https://stackoverflow.com/questions/tagged/soci)

## Build Status

| Branches    | Travis-CI | AppVeyor-CI | Documentation | Coverity Scan  |
|-------------|-----------|-------------|---------------|----------------|
| master      | [![Build Status](https://travis-ci.org/SOCI/soci.svg?branch=master)](https://travis-ci.org/SOCI/soci/branches) | [![Build status](https://ci.appveyor.com/api/projects/status/dtp5mvbeyu9aqupr/branch/master?svg=true)](https://ci.appveyor.com/project/SOCI/soci/branch/master) | [![Docs Status](https://circleci.com/gh/SOCI/soci/tree/master.svg?style=svg&circle-token=5d31c692ed5fcffa5c5fc6b7fe2257b34d78f3c9)](https://circleci.com/gh/SOCI/soci/tree/master) | [![Coverage](https://scan.coverity.com/projects/6581/badge.svg)](https://scan.coverity.com/projects/soci-soci) |
| release/4.0 | [![Build Status](https://travis-ci.org/SOCI/soci.svg?branch=release%2F4.0)](https://travis-ci.org/SOCI/soci/branches) | [![Build status](https://ci.appveyor.com/api/projects/status/dtp5mvbeyu9aqupr/branch/release/4.0?svg=true)](https://ci.appveyor.com/project/SOCI/soci/branch/release/4.0) | [![Docs Status](https://circleci.com/gh/SOCI/soci/tree/release%2F4.0.svg?style=svg&circle-token=5d31c692ed5fcffa5c5fc6b7fe2257b34d78f3c9)](https://circleci.com/gh/SOCI/soci/tree/release%2F4.0) | |
| release/3.2 | [![Build Status](https://travis-ci.org/SOCI/soci.svg?branch=release%2F3.2)](https://travis-ci.org/SOCI/soci/branches) | [![Build status](https://ci.appveyor.com/api/projects/status/dtp5mvbeyu9aqupr/branch/release/3.2?svg=true)](https://ci.appveyor.com/project/SOCI/soci/branch/release/3.2) | | |

## History

Originally, SOCI was developed by [Maciej Sobczak](http://www.msobczak.com/)
at [CERN](http://www.cern.ch/) as abstraction layer for Oracle,
a **Simple Oracle Call Interface**.
Later, several database backends have been developed for SOCI,
thus the long name has lost its practicality.
Currently, if you like, SOCI may stand for **Simple Open (Database) Call Interface**
or something similar.

> ""CERN is also a user of the SOCI library, which serves as a database access
> layer in some of the control system components.""

-- Maciej Sobczak at [Inspirel](http://www.inspirel.com/users.html)

## License

SOCI library is distributed under the terms of the [Boost Software License](http://www.boost.org/LICENSE_1_0.txt).

## Requirements

Core:

* C++ compiler
* Boost C++ Libraries (optional, headers and Boost.DateTime)

Backend specific client libraries for:

* DB2
* Firebird
* MySQL
* ODBC with specific database driver
* Oracle
* PostgreSQL
* SQLite 3

See documentation at [soci.sourceforge.net](http://soci.sourceforge.net) for details

[BSL](http://www.boost.org/LICENSE_1_0.txt) &copy;
[Maciej Sobczak](http://github.com/msobczak) and [contributors](https://github.com/SOCI/soci/graphs/contributors).
"
251,foundeo/cfdocs,ColdFusion,"# CFDocs

CFDocs is a community maintained CFML reference tool available at [cfdocs.org](http://cfdocs.org). It features:
* Hosting on Amazon CloudFront CDN for fast responses around the globe. Sponsored by [Foundeo Inc.](http://foundeo.com).
* Easy to use urls like: [cfdocs.org/hash](http:cfdocs.org/hash) just hit /tag-name or /function-name.
* Publicly maintained on [GitHub](http://github.com/foundeo/cfdocs)

[![Build Status](https://travis-ci.org/foundeo/cfdocs.svg?branch=master)](https://travis-ci.org/foundeo/cfdocs)


## How reference data is structured

All of the tag and function documentation are defined in json files under [*data/en/functionname.json*](https://github.com/foundeo/cfdocs/tree/master/data/en)  [function example](https://github.com/foundeo/cfdocs/blob/master/data/en/sessioninvalidate.json) [tag example](https://github.com/foundeo/cfdocs/blob/master/data/en/cfhtmltopdf.json).

This makes the documentation super easy to edit and allows developers to run a local version of the site ""out of the box"" since no database setup is required.

## Running CFDocs locally for dev or pleasure


### Running CFDocs Locally

The cfdocs.org site can run locally very easily thanks to CommandBox.

1. Go download [CommandBox](https://www.ortussolutions.com/products/commandbox) if you do not have it already.
2. Download this repository and extract it to a folder, or clone it from this repository. If you are going to be making changes to the docs I would suggest you fork it, see Adam Tuttle's guide: [GitHub tip for your first pull request](https://adamtuttle.codes/your-first-github-pull-request/)
3. Run `box server start` from Command Prompt or Terminal in the root directory.

CommandBox will start an embedded CFML server on port 8411, and you can browse to [http://localhost:8411/](http://localhost:8411/) to view the docs.

If you don't want to go the commandbox route you can simply download it and unzip it to a web root and try it out (requires CF10+ or Lucee 4.5+), but you will probably spend more time trying to get it working than the 1-2 minutes it will take to download and learn how to use commandbox.

## How to contribute

If you are interested in helping you can just copy a tag or function and write up some documentation. The documentation doesn't need to be super wordy and should not be a copy of Adobe's documentation word for word.

CFDocs.org is meant to be a quick reference so keep it short and sweet. E.g. attribute names / function arguments and one two sentence descriptions.

#### The easy way to edit the docs

1. Browse the [data/en/](https://github.com/foundeo/cfdocs/tree/master/data/en) folder of this repository on github and find the tag or function you want to edit.
2. Click the edit button (pencil)
3. Commit / Send a pull request.

#### The better way to edit the docs

1. fork the project to your GitHub account *need help? read this: [GitHub tip for your first pull request](http://adamtuttle.codes/blog/2014/your-first-github-pull-request/)*
2. clone it locally
3. make your changes
4. test it locally using commandbox (see above)
4. once you are done you can send a pull request, and I'll merge it into the main repository.

### Not sure what you can contribute?

We need help expanding existing tag and function documentation. Look through the reference pages till you find one needing more definitions or examples. Also see:

* [Missing Descriptions](http://cfdocs.org/reports/missing-descriptions.cfm)
* [Missing Examples](http://cfdocs.org/reports/missing-examples.cfm)
* [Missing Related Links](http://cfdocs.org/reports/missing-related.cfm)
* [Missing Functions](http://cfdocs.org/reports/todo.cfm)

### JSON File Documentation

    {
        ""name"":""NameOfTagOrFunction"",
        ""type"":""function|tag"",
        ""syntax"":""Tag(arg)|<cftag attr=1>"",
        ""member"":""item.memberFunction([args])"",
        ""script"":""cftag(attr=1);"",
        ""returns"":""void"",
        ""related"":[
            ""tag"",
            ""function""
        ],
        ""description"":""A short description that describes what the tag or function does."",
        ""discouraged"":""Only add this key if this tag/function is discouraged by the community."",
        ""params"":[
            {""name"":""funcArgNameOrTagAttributeName"", ""description"":""What it does"", ""required"":true, ""default"":""false"", ""type"":""boolean"", ""values"":[]}
        ],
        ""engines"":{
            ""coldfusion"":{""minimum_version"":""10"", ""notes"":""CF Specific Info Here"", ""docs"":""http://learn.adobe.com/wiki/display/coldfusionen/function""},
            ""railo"":{""minimum_version"":""4.1"", ""notes"":""Railo Specific Here"", ""docs"":""http://railodocs.org/index.cfm/function/sessionrotate""},
            ""lucee"":{""minimum_version"":""4.5"", ""notes"":""Lucee Specific Info Here"", ""docs"":""https://docs.lucee.org/reference/functions/name.html""}
        },
        ""links"":[
            {
                ""title"":""Title of a blog entry that has good info about this."",
                ""description"":""Description of the link"",
                ""url"":""http://www.example.com/a/b.cfm""
            }
        ],
        ""examples"":[
            {
                ""title"":""Name of the code example"",
                ""description"":""Description of the code example"",
                ""code"":""<cf_examplecodehere>"",
                ""result"":""The expected output of the code example"",
                ""runnable"":true
            }
        ]
    }


##### name

The name of the tag or function, use lowercase.

##### type

Either `function` or `tag` or `listing` a *listing* is how categories are made, they simply contain a `name`, `description`, and a list of `related`

##### syntax

The basic syntax of the tag or function

##### script

For tags, shows how the tag would be invoked from cfscript.

##### member

For functions, shows the available member function syntax.

##### returns

The returntype of a function.

##### related

An array of tag or function names that are related to this item.

##### description

A short description of the item.

##### discouraged

If this key exists and has content a warning is displayed stating that the tag or function is discouraged by the CFML community.

##### params

Array of structures containing information about the attributes of a tag, or arguments of a function.

##### engines

CFML engine implementation specific info goes here, for example if it was added in CF10 and Railo 4.1 you can add that in `minimum_version` if something was changed in CF11, you can add notes about what changed. The `docs` key should point to a url for vendor documentation.

##### links

Use this to link to blog entries or other useful related content.

##### examples

Show example code. It is very helpful to readers to use the `result` to show the expected result of the code sample when applicable. This has to be JSON, so  you can to do `\n` for newline, double quotes must be escaped `\""`. The `runnable` is a boolean that determines if the _Run Code_ button shows up next to the example.

We have an [example JSON utility](http://cfdocs.org/utilities/json/) that can be used to create the JSON by filling out a form.

Please see the [cfdocs contributor guide](CONTRIBUTING.md) for frequently asked questions.

### IDE Integration

There are several CFML code editors that utilize the cfdocs repository to provide documentation within the editor.

#### Sublime Text

Install the CFML Package for Sublime Text 3: <https://packagecontrol.io/packages/CFML> the plugin will provide inline documentation when hovering the mouse over a tag or function, or when or typing code.

#### Visual Studio Code

Install the CFML Plugin from KamasamaK: <https://marketplace.visualstudio.com/items?itemName=KamasamaK.vscode-cfml>

#### VIM

Requires the vim-shell plugin:

```
function! CFDocsSearch()
   let wordUnderCursor = expand(""<cword>"")
   execute 'Open https://cfdocs.org/' . wordUnderCursor
endfunction

command! -nargs=0 CFdocs :call CFDocsSearch()
```

#### CFEclipse

Hit `F1` when the cursor is on a tag or function to be taken to the cfdocs.org doc. <http://cfeclipse.org/>


"
252,sitemesh/sitemesh2,Java,"*************************************
** OpenSymphony SiteMesh 2.5       **
*************************************

SiteMesh is a web-page layout system that can be used to abstract common look
and feel from the functionality of a web-application and to assemble large
webpages from smaller components. Pages and components can have meta-data
extracted from them (such as body, title and meta-tags) which can be used by
decorators (skins) that are applied.

SiteMesh won't tread on your toes or force you to work in a certain way (except
for cleaner) - you install and carry on working as before. It seamlessly fits in
with existing frameworks.

Forget the hype - just try it! You'll be impressed with how it can simplify
things.

--------------------------
-- Obtaining            --
--------------------------

The latest version of SiteMesh can be obtained from:

    http://www.opensymphony.com/sitemesh/

--------------------------
-- Requirements         --
--------------------------

SiteMesh requires a Java Servlet container conforming to the Servlet 2.3
specification. Versions prior to 2.3 are not enough.

Currently known containers that support this and SiteMesh was tested with:

* Orion 1.5.4 and up                         - http://www.orionserver.com
* Tomcat 4.0, 4.1 and 5.0.19                 - http://jakarta.apache.org/tomcat
* Resin 2.1.11, 2.1.12, 2.1.13 and 3.0.7     - http://www.caucho.com
* Oracle OC4J 2                              - http://www.oracle.com
* WebLogic 7.0 SP2, 8.1 and 8.1 SP2          - http://www.bea.com
* WebSphere 5.0                              - http://www.ibm.com
* Jetty 4.2.20                               - http://jetty.mortbay.org

--------------------------
-- Installation         --
--------------------------

* Copy sitemesh-@VERSION@.jar to the WEB-INF/lib/ directory of your web-app.

* OPTIONAL: Copy sitemesh.xml to the WEB-INF/ directory if you need to specify a custom
  decorator mapper configuration then the default configuration.

* Add the following to WEB-INF/web.xml:

    <filter>
        <filter-name>sitemesh</filter-name>
        <filter-class>com.opensymphony.sitemesh.webapp.SiteMeshFilter</filter-class>
    </filter>

    <filter-mapping>
        <filter-name>sitemesh</filter-name>
        <url-pattern>/*</url-pattern>
    </filter-mapping>

 * ORION USERS ONLY. For performance reasons, Orion does not auto-load tab library descriptors
   from Jars by default. To get passed this you will also have to copy sitemesh-decorator.tld
   and sitemesh-page.tld to WEB-INF/lib and add the following to WEB-INF/web.xml:

    <taglib>
        <taglib-uri>http://www.opensymphony.com/sitemesh/decorator</taglib-uri>
        <taglib-location>/WEB-INF/lib/sitemesh-decorator.tld</taglib-location>
    </taglib>

    <taglib>
        <taglib-uri>http://www.opensymphony.com/sitemesh/page</taglib-uri>
        <taglib-location>/WEB-INF/lib/sitemesh-page.tld</taglib-location>
    </taglib>


--------------------------
-- Getting started      --
--------------------------

Ok, let's assume you have some basic JSPs already on the site.
These should contain vanilla HTML.

If you don't, here's a JSP to get you started (test.jsp).

    <html>
        <head>
            <title>Hello world</title>
        </head>
        <body>
            <p>Today is <%= new java.util.Date() %>.</p>
        </body>
    </html>

Once you have some content (preferably more imaginative than the example above),
a decorator should be created (decorator.jsp).

    <%@ taglib uri=""http://www.opensymphony.com/sitemesh/decorator"" prefix=""decorator"" %>
    <html>
        <head>
            <title>My Site - <decorator:title default=""Welcome!"" /></title>
            <decorator:head />
        </head>
        <body>
            <decorator:body />
        </body>
    </html>

Now you need tell SiteMesh about that decorator and when to use it. Create the
file WEB-INF/decorators.xml:

    <decorators>

        <decorator name=""mydecorator"" page=""/decorator.jsp"">
            <pattern>/*</pattern>
        </decorator>

    </decorators>

Access your original JSP (test.jsp) though your web-browser and it should look
pretty normal. Now if you add some styling to your decorator it shall
automatically be applied to all the other pages in your web-app.

You can define as many decorators as you want in decorators.xml. Example:

    <decorators defaultdir=""/decorators"">

        <decorator name=""default"" page=""default.jsp"">
            <pattern>/*</pattern>
        </decorator>

        <decorator name=""anotherdecorator"" page=""decorator2.jsp"">
            <pattern>/subdir/*</pattern>
        </decorator>

        <decorator name=""htmldecorator"" page=""html.jsp"">
            <pattern>*.html</pattern>
            <pattern>*.htm</pattern>
        </decorator>

        <decorator name=""none"">
            <!-- These files will not get decorated. -->
            <pattern>/anotherdir/*</pattern>
        </decorator>

    </decorators>

--------------------------
-- Further support      --
--------------------------

You get the idea. Play around. See the SiteMesh website for
full documentation.

    http://www.opensymphony.com/sitemesh/

--------------------------
-- Credits              --
--------------------------

Thank these guys:
* Mathias Bogaert         <NOSPAMm.bogaert@memenco.com>
* Mike Cannon-Brookes     <mikeNOSPAM@atlassian.com>
* Victor Salaman          <salamanNOSPAM@teknos.com>
* Joseph Ottinger         <joeoNOSPAM@adjacency.org>
* Hani Suleiman           <NOSPAMfate@users.sourceforge.net>
* Scott Farquhar          <scottNOSPAM@atlassian.com>
* James Roper             <jamesNOSPAM@jazzy.id.au>
* Matt Quail              <mquailNOSPAM@atlasian.com>
* Charles Miller          <cmillerNOSPAM@atlassian.com>

                                            - Joe Walnes <joe@NOSPAMtruemesh.com>
"
253,foyzulkarim/GenericComponents,C#,"
# GenericComponents
Abstract
Every application, let it be a desktop, web, mobile or any other you can image, either sends data to database or retrieves data from database to show it to the user. What if we automate the data sending and fetching process and focus only on the business logics? So that we don't need to write the common framework related hundreads of thousands lines of code for each of our projects, and the projects have been written all over the world? 

<a href=""https://www.youtube.com/playlist?list=PLEYpvDF6qy8YjfgSh8cH63M8sNvMh4qvJ"">How you can use this library Video Playlist</a> 
"
254,OoliteProject/oolite,Objective-C,"[![Build Status (Mac)](https://api.travis-ci.org/OoliteProject/oolite.svg)](https://travis-ci.org/OoliteProject/oolite)

[![GitHub release](https://img.shields.io/github/release/OoliteProject/Oolite.svg)](https://github.com/OoliteProject/Oolite/releases/latest)
     

| Windows             | Linux               | OSX            |
|---------------------|---------------------|----------------|
| [![Github release](https://img.shields.io/github/downloads/OoliteProject/Oolite/latest/Oolite-1.90_x64.exe.svg)](https://github.com/OoliteProject/oolite/releases/latest) | [![Github release](https://img.shields.io/github/downloads/OoliteProject/Oolite/latest/oolite-1.90.linux-x86_64.tgz.svg)](https://github.com/OoliteProject/oolite/releases/latest) | [![Github release](https://img.shields.io/github/downloads/OoliteProject/Oolite/latest/oolite-1.90.zip.svg)](https://github.com/OoliteProject/oolite/releases/latest) |
[![Github release](https://img.shields.io/github/downloads/OoliteProject/Oolite/latest/Oolite-1.90_x86.exe.svg)](https://github.com/OoliteProject/oolite/releases/latest)| [![Github release](https://img.shields.io/github/downloads/OoliteProject/Oolite/latest/oolite-1.90.linux-x86.tgz.svg)](https://github.com/OoliteProject/oolite/releases/latest) | [![Github release](https://img.shields.io/github/downloads/OoliteProject/Oolite/latest/Oolite-1.90-Mac-TestRelease.zip.svg)](https://github.com/OoliteProject/oolite/releases/latest) |
| | [![Github release](https://img.shields.io/github/downloads/OoliteProject/Oolite/latest/oolite-1.90-test.linux-x86_64.tgz.svg)](https://github.com/OoliteProject/oolite/releases/latest) | |
| | [![Github release](https://img.shields.io/github/downloads/OoliteProject/Oolite/latest/oolite-1.90-test.linux-x86.tgz.svg)](https://github.com/OoliteProject/oolite/releases/latest) | |

![Oolite](http://oolite.org/images/gallery/large/another_commander-210210_LeavingCoriolisAgain.png)

Oolite for all platforms can be built from this repository. Here is a quick
guide to the source tree.
 
For end-user documentation, see [oolite.org](http://www.oolite.org/) and
[Elite Wiki](http://wiki.alioth.net/index.php/Oolite_Main_Page).

## Contents
- **debian**:  Files to enable automatic setup under Linux using dpkg (Debian package manager) tools
- **DebugOXP**:  [Debug.oxp](http://wiki.alioth.net/index.php/Debug_OXP), the expansion pack that enables console support in debug and test release builds
- **deps**
   - **Cocoa-deps**:  Dependencies for Mac OS X
   - **Cross-platform-deps**:  Dependencies for platforms other than Mac OS X
   - **Linux-deps**:  Dependencies for Linux on x86 and x86_64 processors
   - **URLs**:  URLs used for binary dependencies on Mac OS X
   - **Windows-deps**:  Dependencies for Windows on x86 and x86_64 processors
- **Doc**:  Documentation (including user guides)
- **installers**:  Files used to create various installers
- **Mac-specific**:  Additional projects used only on Mac OS X
   - **DataFormatters**:  Debugger configurations for Xcode
   - **DebugBundle**:  Implements the [Debug menu and in-app console](http://wiki.alioth.net/index.php/Debug_OXP#Mac_OS_X-specific_features)
   - **OCUnitTest**:  A small number of unit tests
   - **Oolite-docktile:**  An embedded plug-in which implements the Oolite dock menu when Oolite is not running
   - **Oolite-importer**:  A Spotlight importer to make saved games and OXPs searchable
- **Oolite.xcodeproj**:  The OS X Xcode project to build Oolite
- **Resources**:  Game assets and resource files for Mac and GNUstep application bundles
- **Schemata**:  Plist schema files for the [OXP Verifier](http://wiki.alioth.net/index.php/OXP_howto#OXP_Verifier)
- **src**:  Objective-C and C sources, incuding header files
   - **BSDCompat**:  Support for BSDisms that gnu libc doesn't have (strl*)
   - **Cocoa**:  Files that are only compiled on Mac OS X
   - **Core**:  Files that are compiled on all platforms
   - **SDL**:  Files that are only compiled for platforms that use SDL
- **tests**:  A mixed bag of test cases for manual testing and ad-hoc code tests.
- **tools**:  Various tools for preparing files, builds, releases etc.

## Building
### Mac OS X
You will need the latest version of Xcode from the App Store.
Then double click on the Xcode project in the Finder, select one of the Oolite
targets from the Scheme pop-up, and hit Build and Run (the play button in the
toolbar).

### Windows
See the Oolite wiki:
http://wiki.alioth.net/index.php/Running_Oolite-Windows

### Linux

If you have the Debian package tools (installed by default with
Debian and Ubuntu), use dpkg-buildpackage.

On Linux, BSD and other Unix platforms without dpkg tools, you will need to
get GNUstep and SDL development libraries in addition to what is usually
installed by default if you choose to install the development
headers/libraries etc. when initially installing the OS. For most Linux
distros, GNUstep and SDL development libraries come prepackaged - just
apt-get/yum install the relevant files. You may also need to install Mozilla
Spidermonkey (libmozjs). On others you may need to build them from source. In
particular, you need the SDL_Mixer library, which doesn't always come with the
base SDL development kit.

Then just type `make`, or, if you're using GNU make,
`make -f Makefile release`. On some systems, such as Gentoo, you may need to run
`make -f Makefile release OBJCFLAGS=-fobjc-exceptions`.  
If you get errors like `make[1]: *** No rule to make target '/objc.make'.  Stop.` it might help if you run `source /usr/share/GNUstep/Makefiles/GNUstep.sh` (the exact path to `GNUstep.sh` might differ).  
If you have problems with missing textures you can try to delete `deps/Linux-deps/include/png.h` and `deps/Linux-deps/include/pngconf.h` before compiling.

Also remember to first fetch the various submodules, see [Git](#Git).

## Running
On OS X, you can run from Xcode by clicking on the appropriate icon
(or choosing 'Run' from the 'Product' menu).  
On Linux/BSD/Unix, in a terminal, type `openapp oolite`, or if you compiled it yourself you can run it with `./oolite.app/oolite`.

## Git
The Oolite source is available from github.
Use `git clone https://github.com/OoliteProject/oolite`
to retrieve. Then `git submodule update --init`
to fetch the various submodules.

If you've cloned the source from a forked repository instead, this may
not work - due to relative directory paths in .gitmodules, git tries
to download the submodules from the fork instead of the original oolite
repository.  A workaround is to copy the file .absolute_gitmodules
onto .gitmodules, then perform the submodules init, then replace
.gitmodules with the relative path version.  eg, on Unix:

```
$ cp .absolute_gitmodules .gitmodules
$ git submodule update --init
$ git checkout -- .gitmodules
```

You should now have access to the submodules, without git complaining
that .gitmodules has changed or including .gitmodules in pull requests.
"
255,fptudelft/FP101x-Content-2015,Haskell,"# FP101x - Functional Programming MOOC 2015 Content

This repository will host the content for the [Functional Programming course on EdX](https://www.edx.org/course/introduction-functional-programming-delftx-fp101x-0).

As the course progresses we will put up more material here.

We encourage contributions from the community (e.g. translations, fixes, etc.).


Lectures
--------

| Title            | 360p | 720p | 1080p | Slides (PDF) | Slides (PPT) |
|------------------|------|------|-------|-------|-------|
| Book presentation | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-BookPresentation-ProgrammingInHaskell-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-BookPresentation-ProgrammingInHaskell-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-BookPresentation-ProgrammingInHaskell-video.mp4) | | |
| 0. Introduction Part 1 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week0/FP101x-chapter0-part1-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week0/FP101x-chapter0-part1-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week0/FP101x-chapter0-part1-video.mp4) | [0 (pdf)](slides/Chapter0.pdf) | [0 (ppt)](slides/Chapter0.pptx) |  
| 0. Introduction Part 2 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week0/FP101x-chapter0-part2-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week0/FP101x-chapter0-part2-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week0/FP101x-chapter0-part2-video.mp4) | | |
| 1. First Steps Part 1 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week0/FP101x-chapter1-part1-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week0/FP101x-chapter1-part1-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week0/FP101x-chapter1-part1-video.mp4) | [1 (pdf)](slides/Chapter1.pdf) | [1 (ppt)](slides/Chapter1.pptx) | 
| 1. First Steps Part 2 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week0/FP101x-chapter1-part2-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week0/FP101x-chapter1-part2-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week0/FP101x-chapter1-part2-video.mp4) | | |
| 2. Types and Classes Part 1 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter2-part1-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter2-part1-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter2-part1-video.mp4) |  [2 (pdf)](slides/Chapter2.pdf) | [2 (ppt)](slides/Chapter2.pptx) |
| 2. Types and Classes Part 2 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter2-part2-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter2-part2-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter2-part2-video.mp4) | | |
| 2. Types and Classes Part 3 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter2-part3-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter2-part3-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter2-part3-video.mp4) | | |
| 3. Defining Functions Part 1 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter3-part1-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter3-part1-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter3-part1-video.mp4) | [3 (pdf)](slides/Chapter3.pdf) | [3 (ppt)](slides/Chapter3.pptx) |
| 3. Defining Functions Part 2 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter3-part2-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter3-part2-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-chapter3-part2-video.mp4) | | |
| Jam Session - Functional Programming in Dart | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-ProgramLanguages-Dart-video.360.mp4) |  [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-ProgramLanguages-Dart-video.720.mp4) |  [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-ProgramLanguages-Dart-video.mp4) | | |
| 4. List Comprehensions Part 1 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-chapter4-part1-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-chapter4-part1-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-chapter4-part1-video.mp4) | [4 (pdf)](slides/Chapter4.pdf) | [4 (ppt)](slides/Chapter4.pptx) |
| 4. List Comprehensions Part 2 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-chapter4-part2-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-chapter4-part2-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-chapter4-part2-video.mp4) | | |
| 5. Recursive Functions Part 1 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-chapter5-part1-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-chapter5-part1-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-chapter5-part1-video.mp4) | [5 (pdf)](slides/Chapter5.pdf) | [5 (ppt)](slides/Chapter5.pptx) |
| 5. Recursive Functions Part 2 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-chapter5-part2-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-chapter5-part2-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-chapter5-part2-video.mp4) | | |
| Jam Session - Functional Programming in C# | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-ProgramLanguages-CSharp-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-ProgramLanguages-CSharp-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-ProgramLanguages-CSharp-video.mp4) | | |
| Jam Session - Functional Programming in Hack | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-ProgramLanguages-Hack-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-ProgramLanguages-Hack-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week2/FP101x-ProgramLanguages-Hack-video.mp4) | | |
| 6. Higher Order Functions Part 1 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week3/FP101x-chapter6-part1-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week3/FP101x-chapter6-part1-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week3/FP101x-chapter6-part1-video.mp4) | [6 (pdf)](slides/Chapter6.pdf) | [6 (ppt)](slides/Chapter6.pptx) |
| 6. Higher Order Functions Part 2 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week3/FP101x-chapter6-part2-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week3/FP101x-chapter6-part2-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week3/FP101x-chapter6-part2-video.mp4) | | |
| 6. Higher Order Functions Part 3 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week3/FP101x-chapter6-part3-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week3/FP101x-chapter6-part3-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week3/FP101x-chapter6-part3-video.mp4) | | |
| Jam Session - Church Numerals | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-StudentDemo-ExerciseHighOrderFunctions-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-StudentDemo-ExerciseHighOrderFunctions-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-StudentDemo-ExerciseHighOrderFunctions-video.mp4) | | |
| 7. Functional Parsers and Monads Part 1 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter7-part1-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter7-part1-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter7-part1-video.mp4) | [7 (pdf)](slides/Chapter7.pdf) | [7 (ppt)](slides/Chapter7.pptx) |
| 7. Functional Parsers and Monads Part 2 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter7-part2-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter7-part2-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter7-part2-video.mp4) | | |
| 7. Functional Parsers and Monads Part 3 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter7-part3-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter7-part3-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter7-part3-video.mp4) | | |
| 8. Interactive Programs Part 1 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter8-part1-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter8-part1-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter8-part1-video.mp4) | [8 (pdf)](slides/Chapter8.pdf) | [8 (ppt)](slides/Chapter8.pptx) |
| 8. Interactive Programs Part 2 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter8-part2-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter8-part2-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-chapter8-part2-video.mp4) | | |
| Jam Session - Functional Programming in Kotlin | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-ProgramLanguages-Kotlin-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-ProgramLanguages-Kotlin-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week4/FP101x-ProgramLanguages-Kotlin-video.mp4) | | |
| 9. Declaring Types and Classes Part 1 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter9-part1-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter9-part1-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter9-part1-video.mp4) | [9 (pdf)](slides/Chapter9.pdf) | [9 (ppt)](slides/Chapter9.pptx) |
| 9. Declaring Types and Classes Part 2 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter9-part2-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter9-part2-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter9-part2-video.mp4) | | |
| 9. Declaring Types and Classes Part 3 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter9-part3-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter9-part3-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter9-part3-video.mp4) | | |
| 10. The Countdown Problem Part 1 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter10-part1-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter10-part1-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter10-part1-video.mp4) |  [10 (pdf)](slides/Chapter10.pdf) | [10 (ppt)](slides/Chapter10.pptx) |
| 10. The Countdown Problem Part 2 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter10-part2-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter10-part2-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter10-part2-video.mp4) | | |
| 10. The Countdown Problem Part 3 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter10-part3-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter10-part3-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week5/FP101x-chapter10-part3-video.mp4) | | |
| Jam Session - Testing and Debugging with QuickCheck | [video](https://courses.edx.org/c4x/DelftX/FP101x/asset/Randomized_Testing.mp4) | | | | |
| Jam Session - Functional Programming in Scala | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-ProgramLanguages-Scala-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-ProgramLanguages-Scala-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week1/FP101x-ProgramLanguages-Scala-video.mp4) | | |
| 11. Lazy Evaluation Part 1 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week6/FP101x-chapter11-part1-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week6/FP101x-chapter11-part1-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week6/FP101x-chapter11-part1-video.mp4) |  [11 (pdf)](slides/Chapter11.pdf) | [11 (ppt)](slides/Chapter11.pptx) |
| 11. Lazy Evaluation Part 2 | [360p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week6/FP101x-chapter11-part2-video.360.mp4) | [720p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week6/FP101x-chapter11-part2-video.720.mp4) | [1080p](http://delftxdownloads.tudelft.nl/FP101x-FunctionalProgramming/Week6/FP101x-chapter11-part2-video.mp4) | | |
"
256,host505/repository.host505,,"# TheOath

--- For testing and learning purposes - Not for public use (use at your own risk) ---"
257,razorpay/ifsc,PHP,"# ifsc

This is part of the IFSC toolset released by Razorpay.
You can find more details about the entire release at
[ifsc.razorpay.com](https://ifsc.razorpay.com).

[![wercker status](https://app.wercker.com/status/bc9b22047e1b8eb55ce98ba451d7b504/s/master 'wercker status')](https://app.wercker.com/project/byKey/bc9b22047e1b8eb55ce98ba451d7b504) [![](https://images.microbadger.com/badges/image/razorpay/ifsc:1.5.13.svg)](https://microbadger.com/images/razorpay/ifsc:1.5.13) [![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

[![](https://images.microbadger.com/badges/version/razorpay/ifsc:1.5.13.svg)](https://microbadger.com/images/razorpay/ifsc:1.5.13) [![npm version](https://badge.fury.io/js/ifsc.svg)](https://badge.fury.io/js/ifsc) [![Gem Version](https://badge.fury.io/rb/ifsc.svg)](https://badge.fury.io/rb/ifsc) [![PHP version](https://badge.fury.io/ph/razorpay%2Fifsc.svg)](https://badge.fury.io/ph/razorpay%2Fifsc) [![Hex pm](http://img.shields.io/hexpm/v/ifsc.svg)](https://hex.pm/packages/ifsc)

## Dataset

If you are just looking for the dataset, go to
the [releases][releases] section and download
the latest release.

The latest [`build` pipeline][buildlist] on Wercker should result in a container
with the complete dataset as well.

### Source

The source for the dataset are the following files:

-  List of NEFT IFSCs from [RBI website][combined]
-  List of RTGS IFSCs from [RBI website][rtgs]
-  List of ACH Live Banks from [NPCI website][ach] used for IFSC sublet branches

#### SWIFT

SWIFT/BIC codes are supported for a few banks.

##### SBI
-  https://sbi.co.in/web/nri/quick-links/swift-codes
-  https://sbi.co.in/documents/16012/263663/sbinri_merged_bran_swfcodet.xlsx
-  Branch codes from above are checked against the [SBI Branch Locator](https://www.sbi.co.in/web/home/locator/branch) to get the IFSC.

##### PNB
- https://pnbindia.com/downloadprocess.aspx?fid=Zb7ImdUNlz9Ge73qn1nXQg==
- https://www.pnbindia.in/document/PNB-helpdesk/bic_code.pdf

##### HDFC
- https://www.hdfcbank.com/nri-banking/correspondent-banks

## Installation

## Ruby

Add this line to your application's Gemfile:

```ruby
gem ""ifsc""
```

And then execute:

```bash
$ bundle
```

Or install it yourself as:

```bash
$ gem install ifsc
```

Inside of your Ruby program do:

```ruby
require ""ifsc""
```

...to pull it in as a dependency.

## PHP

`composer require php-http/curl-client razorpay/ifsc`

The PHP package has a dependency on the virtual package `php-http/client-implementation` which requires you to install an adapter, but we do not care which one. That is an implementation detail in your application. You do not have to use the `php-http/curl-client` if you do not want to. You may use the `php-http/guzzle6-adapter`. Read more about the virtual packages, why this is a good idea and about the flexibility it brings at the [HTTPlug docs](http://docs.php-http.org/en/latest/httplug/users.html). You can find a list of suported providers on [packagist](https://packagist.org/providers/php-http/client-implementation).

The minimum [PHP version supported is 7.2](https://endoflife.date/php).

## Node.js

`$ npm install ifsc`

## Support Matrix

Only the latest version of each SDK is considered.

| Language | Validation | API Client | Sublet Support (Custom) | Bank Constants |
| -------- | ---------- | ---------- | ----------------------- | -------------- |
| PHP      | ✅         | ✅         | ✅ (✅)                 | ✅             |
| Ruby     | ✅         | ✅         | ✅ (✅)                 | ✅             |
| Elixir   | ✅         | ✅         | ✅ (❎)                 | ❎             |
| Node.js  | ✅         | ✅         | ❎ (❎)                 | ✅             |

## API Documentation

This repository also hosts the source code for 3 modules: PHP/Node.js/Ruby as of now.
The API is documented below:

### PHP

```php
<?php

use Razorpay\IFSC\Bank;
use Razorpay\IFSC\IFSC;
use Razorpay\IFSC\Client;

IFSC::validate('KKBK0000261'); // Returns true
IFSC::validate('BOTM0XEEMRA'); // Returns false

IFSC::validateBankCode('PUNB'); // Returns true
IFSC::validateBankCode('ABCD'); // Returns false

IFSC::getBankName('PUNB'); // Returns 'Punjab National Bank'
IFSC::getBankName('ABCD'); // Returns null

IFSC::getBankName(Bank::PUNB); //Returns Punjab National Bank

Bank::getDetails(Bank::PUNB);
Bank::getDetails('PUNB');

// Returns an array:
// [
//    'code' => 'PUNB',
//    'type' => 'PSB',
//    'ifsc' => 'PUNB0244200',
//    'micr' => '110024001',
//    'iin' => '508568',
//    'apbs' => true,
//    'ach_credit' => true,
//    'ach_debit' => true,
//    'nach_debit' => true,
//    'name' => 'Punjab National Bank',
//    'bank_code' => '024',
//    'upi' => true
// ]

$client = new Client();
$res = $client->lookupIFSC('KKBK0000261');

echo $res->bank; // 'KOTAK MAHINDRA BANK LIMITED'
echo $res->branch; // 'GURGAON'
echo $res->address; // 'JMD REGENT SQUARE,MEHRAULI GURGAON ROAD,OPPOSITE BRISTOL HOTEL,'
echo $res->contact; // '4131000'
echo $res->city; // 'GURGAON'
echo $res->district; // 'GURGAON'
echo $res->state; // 'HARYANA'
echo $res->getBankCode(); // KKBK
echo $res->getBankName(); // 'Kotak Mahindra Bank'

// lookupIFSC may throw `Razorpay\IFSC\Exception\ServerError`
// in case of server not responding in time
// or Razorpay\IFSC\Exception\InvalidCode in case
// the IFSC code is invalid
```

### Node.js

```js
var ifsc = require('ifsc');

ifsc.validate('KKBK0000261'); // returns true
ifsc.validate('BOTM0XEEMRA'); // returns false

ifsc.fetchDetails('KKBK0000261').then(function(res) {
   console.log(res);
});

console.log(ifsc.bank.PUNB); // prints PUNB
// Prints the entire JSON from https://ifsc.razorpay.com/KKBK0000261
// res is an object, not string
```

### Ruby

Make sure you have `require 'ifsc'` in your code.
Validating a code offline. (Remember to keep the gem up to date!)

```rb
# valid?

Razorpay::IFSC::IFSC.valid? 'KKBK0000261' # => true
Razorpay::IFSC::IFSC.valid? 'BOTM0XEEMRA' # => false

# validate!

Razorpay::IFSC::IFSC.validate! 'KKBK0000261' # => true
Razorpay::IFSC::IFSC.validate! 'BOTM0XEEMRA' # => Razorpay::IFSC::InvalidCodeError

# bank_name_for(code) gets you the bank name offline
Razorpay::IFSC::IFSC.bank_name_for 'PUNB0026200' -> ""Punjab National Bank""
Razorpay::IFSC::IFSC.bank_name_for 'KSCB0006001' -> ""Tumkur District Central Bank""

# get_details gets you the bank details from `banks.json`
Razorpay::IFSC::Bank.get_details 'PUNB'
{
   code: 'PUNB',
   type: 'PSB',
   ifsc: 'PUNB0244200',
   micr: '110024001',
   bank_code: '024',
   iin: '508568',
   apbs: true,
   ach_credit: true,
   ach_debit: true,
   nach_debit: true
}

# constants

Razorpay::IFSC::Bank::PUNB
'PUNB'
```

Validate online and retrieve details from the server

If you call `code.valid?` before calling `code.get`, the validation will be performed offline.

```rb
# 1. using find
code = Razorpay::IFSC::IFSC.find 'KKBK0000261'

# 2. using new(...).get
code = Razorpay::IFSC::IFSC.new 'KKBK0000261'
code.get

# result
code.valid?
# => true
code.bank
# => ""Kotak Mahindra Bank""
code.branch
# => ""GURGAON""
code.address
# => ""JMD REGENT SQUARE,MEHRAULI GURGAON ROAD,OPPOSITE BRISTOL HOTEL,""
code.contact
# => ""4131000""
code.city
# => ""GURGAON""
code.district
# => ""GURGAON""
code.state
# => ""HARYANA""
```

#### Sublet Branches

You can use the `code.bank_name` method to get the bank name considering sublet branches.

```rb
code = Razorpay::IFSC::IFSC.find 'HDFC0CKUB01'
code.bank_name ""Khamgaon Urban Co-operative Bank""
```

This works offline, and doesn't need a network call. This information is stored across 2 files:

1. `src/sublet.json` - Autogenerated from the [NPCI website](https://www.npci.org.in/national-automated-clearing-live-members-1)
2. `src/custom-sublets.json` - Maintained manually. Coverage is not 100%. PRs are welcome.

Sublet (or Sub-Member) branches are IFSC codes belonging to a large bank, but leased out to
smaller banks. In some cases, entire ranges are given to a specific bank.
For eg, all IFSCs starting with `YESB0TSS` belong to `Satara Shakari Bank`. These are
maintained manually in `custom-sublets.json`.

#### Error handling

```rb
# all these `Razorpay::IFSC::InvalidCodeError` for an invalid code
Razorpay::IFSC::IFSC.validate! '...'
Razorpay::IFSC::IFSC.find '...'
code = Razorpay::IFSC::IFSC.new '...'; code.get

# these raise `Razorpay::IFSC::ServerError` if there is an error
# communicating with the server
Razorpay::IFSC::IFSC.find '...'
code = Razorpay::IFSC::IFSC.new '...'; code.get
```

### Elixir

Documentation: [https://hexdocs.pm/ifsc](https://hexdocs.pm/ifsc)

Online validation

```elixir
iex> IFSC.get(""KKBK0000261"")
{:ok,
 %Razorpay.IFSC{
   address: ""JMD REGENT SQUARE,MEHRAULI GURGAON ROAD,OPPOSITE BRISTOL HOTEL,"",
   bank: ""Kotak Mahindra Bank"",
   bank_code: ""KKBK"",
   branch: ""GURGAON"",
   city: ""GURGAON"",
   contact: ""4131000"",
   district: ""GURGAON"",
   ifsc: ""KKBK0000261"",
   rtgs: true,
   state: ""HARYANA""
 }}

iex> IFSC.get(""foobar"")
{:error, :invalid_ifsc}
```

Offline validation

```elixir
iex> IFSC.validate(""KKBK0000261"")
{:ok,
 %Razorpay.IFSC{
   address: nil,
   bank: ""Kotak Mahindra Bank"",
   bank_code: ""KKBK"",
   branch: nil,
   city: nil,
   contact: nil,
   district: nil,
   ifsc: ""KKBK0000261"",
   rtgs: nil,
   state: nil
 }}

iex> IFSC.validate(""foobar"")
{:error, :invalid_format}

iex> IFSC.validate(""AAAA0000000"")
{:error, :invalid_bank_code}

iex(> IFSC.validate(""HDFC0000000"")
{:error, :invalid_branch_code}
```

### Code Notes

Both the packages ship with a 300kb JSON file, that
includes the entire list of IFSC codes, in a compressed,
but human-readable format.

The Bank Code and Names list is maintained manually, but verified
with tests to be accurate as per the latest RBI publications. This
lets us add older Bank codes to the name list, without worrying
about them getting deleted in newer builds.

## API Development

The IFSC API is maintained in a separate repository at <https://github.com/razorpay/ifsc-api>.

## License

The code in this repository is licensed under the MIT License. License
text is available in the `LICENSE` file. The dataset itself
is under public domain.

[combined]: https://rbidocs.rbi.org.in/rdocs/content/docs/68774.xlsx
[releases]: https://github.com/razorpay/ifsc/releases
[buildlist]: https://app.wercker.com/razorpay/ifsc/runs?view=runs&q=pipeline%3Abuild
[rtgs]: https://rbidocs.rbi.org.in/rdocs/RTGS/DOCs/RTGEB0815.xlsx
[ach]: https://www.npci.org.in/what-we-do/nach/live-members/live-banks
"
258,JoeyDeVries/LearnOpenGL,C++,"# learnopengl.com code repository
Contains code samples for all chapters of Learn OpenGL and [https://learnopengl.com](https://learnopengl.com). 

## Windows building
All relevant libraries are found in /libs and all DLLs found in /dlls (pre-)compiled for Windows. 
The CMake script knows where to find the libraries so just run CMake script and generate project of choice.
Note that you still have to manually copy the required .DLL files from the /dlls folder to your binary folder for the binaries to run.

Keep in mind the supplied libraries were generated with a specific compiler version which may or may not work on your system (generating a large batch of link errors). In that case it's advised to build the libraries yourself from the source.

## Linux building
First make sure you have CMake, Git, and GCC by typing as root (sudo) `apt-get install g++ cmake git` and then get the required packages:
Using root (sudo) and type `apt-get install libsoil-dev libglm-dev libassimp-dev libglew-dev libglfw3-dev libxinerama-dev libxcursor-dev  libxi-dev` .
Next, run CMake (preferably CMake-gui). The source directory is LearnOpenGL and specify the build directory as LearnOpenGL/build. Creating the build directory within LearnOpenGL is important for linking to the resource files (it also will be ignored by Git). Hit configure and specify your compiler files (Unix Makefiles are recommended), resolve any missing directories or libraries, and then hit generate. Navigate to the build directory (`cd LearnOpenGL/build`) and type `make` in the terminal. This should generate the executables in the respective chapter folders.

Note that CodeBlocks or other IDEs may have issues running the programs due to problems finding the shader and resource files, however it should still be able to generate the exectuables. To work around this problem it is possible to set an environment variable to tell the tutorials where the resource files can be found. The environment variable is named LOGL_ROOT_PATH and may be set to the path to the root of the LearnOpenGL directory tree. For example:

    `export LOGL_ROOT_PATH=/home/user/tutorials/LearnOpenGL`

Running `ls $LOGL_ROOT_PATH` should list, among other things, this README file and the resources direcory.

### Linux building in Docker
Using [this project](https://github.com/01e9/docker-ide) you can start IDE in docker:
```
.../docker-ide/ide cpp-gpu ~/.../clion/bin/clion.sh -x11docker ""--gpu""
```

## Mac OS X building
Building on Mac OS X is fairly simple (thanks [@hyperknot](https://github.com/hyperknot)):
```
brew install cmake assimp glm glfw
mkdir build
cd build
cmake ../.
make -j8
```
## Create Xcode project on Mac platform
Thanks [@caochao](https://github.com/caochao):
After cloning the repo, go to the root path of the repo, and run the command below:
```
mkdir xcode
cd xcode
cmake -G Xcode ..
```

## Glitter
Polytonic created a project called [Glitter](https://github.com/Polytonic/Glitter) that is a dead-simple boilerplate for OpenGL. 
Everything you need to run a single LearnOpenGL Project (including all libraries) and just that; nothing more. 
Perfect if you want to follow along with the chapters, without the hassle of having to manually compile and link all third party libraries!
"
259,danmunn/redmine_dmsf,Ruby,"Redmine DMSF Plugin
===================

The current version of Redmine DMSF is **2.4.6** [![Build Status](https://api.travis-ci.org/danmunn/redmine_dmsf.png)](https://travis-ci.org/danmunn/redmine_dmsf)

Redmine DMSF is Document Management System Features plugin for Redmine issue tracking system; It is aimed to replace current Redmine's Documents module.

Redmine DMSF now comes bundled with Webdav functionality: if switched on within plugin settings this will be accessible from /dmsf/webdav.

Webdav functionality is provided through DAV4Rack library.

Initial development was for Kontron AG R&D department and it is released as open source thanks to their generosity.  
Project home: <https://code.google.com/p/redmine-dmsf/>

Redmine Document Management System ""Features"" plugin is distributed under GNU General Public License v2 (GPL).  
Redmine is a flexible project management web application, released under the terms of the GNU General Public License v2 (GPL) at <https://www.redmine.org/>

Further information about the GPL license can be found at
<https://www.gnu.org/licenses/old-licenses/gpl-2.0.html#SEC1>

Features
--------

  * Directory structure
  * Document versioning / revision history
  * Email notifications for directories and/or documents
  * Document locking
  * Multi (drag/drop depending on browser) upload/download  
  * Direct document or document link sending via email
  * Configurable document approval workflow
  * Document access auditing
  * Integration with Redmine's activity feed
  * Wiki macros for quick content linking
  * Full read/write webdav functionality
  * Optional document content full-text search
  * Documents and folders symbolic links  
  * Trash bin
  * Documents attachable to issues
  * Compatible with Redmine 4.2.x

Dependencies
------------
  
  * Redmine 4.0.0 or higher

### Full-text search (optional)

If you want to use fulltext search abilities, install xapian packages. In case of using of Bitnami 
stack or Ruby installed via RVM it might be necessary to install Xapian bindings from sources. See https://xapian.org
 for details. 

To index some files with omega you may have to install some other packages like
xpdf, antiword, ...

From Omega documentation:

   * HTML (.html, .htm, .shtml, .shtm, .xhtml, .xhtm)
   * PHP (.php) - our HTML parser knows to ignore PHP code
   * text files (.txt, .text)
   * SVG (.svg)
   * CSV (Comma-Separated Values) files (.csv)
   * PDF (.pdf) if pdftotext is available (comes with poppler or xpdf)
   * PostScript (.ps, .eps, .ai) if ps2pdf (from ghostscript) and pdftotext (comes with poppler or xpdf) are available
   * OpenOffice/StarOffice documents (.sxc, .stc, .sxd, .std, .sxi, .sti, .sxm, .sxw, .sxg, .stw) if unzip is available
   * OpenDocument format documents (.odt, .ods, .odp, .odg, .odc, .odf, .odb, .odi, .odm, .ott, .ots, .otp, .otg, .otc, .otf, .oti, .oth) if unzip is available
   * MS Word documents (.dot) if antiword is available (.doc files are left to libmagic, as they may actually be RTF (AbiWord saves RTF when asked to save as .doc, and Microsoft Word quietly loads RTF files with a .doc extension), or plain-text).
   * MS Excel documents (.xls, .xlb, .xlt, .xlr, .xla) if xls2csv is available (comes with catdoc)
   * MS Powerpoint documents (.ppt, .pps) if catppt is available (comes with catdoc)
   * MS Office 2007 documents (.docx, .docm, .dotx, .dotm, .xlsx, .xlsm, .xltx, .xltm, .pptx, .pptm, .potx, .potm, .ppsx, .ppsm) if unzip is available
   * Wordperfect documents (.wpd) if wpd2text is available (comes with libwpd)
   * MS Works documents (.wps, .wpt) if wps2text is available (comes with libwps)
   * MS Outlook message (.msg) if perl with Email::Outlook::Message and HTML::Parser modules is available
   * MS Publisher documents (.pub) if pub2xhtml is available (comes with libmspub)
   * AbiWord documents (.abw)
   * Compressed AbiWord documents (.zabw)
   * Rich Text Format documents (.rtf) if unrtf is available
   * Perl POD documentation (.pl, .pm, .pod) if pod2text is available
   * reStructured text (.rst, .rest) if rst2html is available (comes with docutils)
   * Markdown (.md, .markdown) if markdown is available
   * TeX DVI files (.dvi) if catdvi is available
   * DjVu files (.djv, .djvu) if djvutxt is available
   * XPS files (.xps) if unzip is available
   * Debian packages (.deb, .udeb) if dpkg-deb is available
   * RPM packages (.rpm) if rpm is available
   * Atom feeds (.atom)
   * MAFF (.maff) if unzip is available
   * MHTML (.mhtml, .mht) if perl with MIME::Tools is available
   * MIME email messages (.eml) and USENET articles if perl with MIME::Tools and HTML::Parser is available
   * vCard files (.vcf, .vcard) if perl with Text::vCard is available
    
You can use following commands to install some of the required indexing tools:    

On Debian use:

```
sudo apt-get install xapian-omega ruby-xapian libxapian-dev poppler-utils antiword unzip catdoc libwpd-tools \
libwps-tools gzip unrtf catdvi djview djview3 uuid uuid-dev xz-utils libemail-outlook-message-perl
```

On Ubuntu use:

```
sudo apt-get install xapian-omega ruby-xapian libxapian-dev poppler-utils antiword  unzip catdoc libwpd-tools \
libwps-tools gzip unrtf catdvi djview djview3 uuid uuid-dev xz-utils libemail-outlook-message-perl
```

On CentOS use:
```
sudo yum install xapian-core xapian-bindings-ruby libxapian-dev poppler-utils antiword unzip catdoc libwpd-tools \
libwps-tools gzip unrtf catdvi djview djview3 uuid uuid-dev xz libemail-outlook-message-perl
```

Usage
-----

DMSF is designed to act as project module, so it must be checked as an enabled module within the project settings.

Search will now automatically search DMSF content when a Redmine search is performed, additionally a ""Documents"" and ""Folders"" check box will be visible, allowing you to search DMSF content exclusively.

Linking DMSF files from Wiki entries
------------------------------------

Link to a document with id 17: `{{dmsf(17)}}`

Link to a document with id 17 with link text ""File"": `{{dmsf(17, File)}}`

Link to the details of a document with id 17: `{{dmsfd(17)}}`

Link to the details of a document with id 17 with link text ""Details"": `{{dmsfd(17, Details)}}`

Text of the description of a document with id 17: `{{dmsfdesc(17)}}`

Text referring to the version of a document with id 17: `{{dmsfversion(17)}}`

Text referring to the last update date of a document with id 17: `{{dmsflastupdate(17)}}`

Link to the preview of 5 lines from a document with id 17: `{{dmsft(17, 5)}}`

Inline picture of the file with id 8; it must be an image file such as JPEG, PNG,...: `{{dmsf_image(8)}}`

Inline picture with custom size: `{{dmsf_image(8, size=300)}}`

Inline picture with custom size: `{{dmsf_image(8, size=50%)}}`

Inline picture with custom height: `{{dmsf_image(8, height=300)}}`

Inline picture with custom width: `{{dmsf_image(8, width=300)}}`

Inline picture with custom size: `{{dmsf_image(8, size=640x480)}}`

Thumbnail with height of 200px: `{{dmsftn(8)}}`

Thumbnail with custom size: `{{dmsftn(8, size=300)}}`

Inline video of the file with id 8; it must be a video file such as MP4: `{{dmsf_video(9)}}`

Inline video with custom size: `{{dmsf_video(9, size=300)}}`

Inline video with custom size: `{{dmsf_video(9, size=50%)}}`

Inline video with custom height: `{{dmsf_video(9, height=300)}}`

Inline video with custom width: `{{dmsf_video(9, width=300)}}`

Inline video with custom size: `{{dmsf_video(9, size=640x480)}}`

Approval workflow status of a document with id 8: `{{dmsfw(8)}}`

The DMSF document/revision id can be found in document details.

Linking DMSF folders from Wiki entries
--------------------------------------

Link to a folder with id 5: `{{dmsff(5)}}`

Link to a folder with id 5 with link text ""Folder"": `{{dmsff(5, Folder)}}`

The DMSF folder id can be found in the link when opening folders within Redmine.

You can also publish Wiki help description: 

In the file <redmine_root>/public/help/<language>/wiki_syntax_detailed.html, after the document link description/definition:

    <ul>
      <li>
        DMSF:
        <ul>
          <li><strong>{{dmsf(17)}}</strong> (a link to the file with id 17)</li>
          <li><strong>{{dmsf(17, File)}}</strong> (a link to the file with id 17 with the link text ""File"")</li>
          <li><strong>{{dmsf(17, File, 10)}}</strong> (a link to the file with id 17 with the link text ""File"" and the link pointing to the revision 10)</li>
          <li><strong>{{dmsfd(17)}}</strong> (a link to the details of the file with id 17)</li>
          <li><strong>{{dmsfdesc(17)}}</strong> (a link to the description of the file with id 17)</li>
          <li><strong>{{dmsff(5)}}</strong> (a link to the folder with id 5)</li>
          <li><strong>{{dmsff(5, Folder)}}</strong> (a link to the folder with id 5 with the link text ""Folder"")</li>
          <li><strong>{{dmsf_image(8)}}</strong> (an inline picture of the file with id 8; it must be an image file such as JPEG, PNG,...)</li>
          <li><strong>{{dmsf_image(8, size=300)}}</strong> (an inline picture with custom size)</li>
          <li><strong>{{dmsf_image(8, size=640x480)}}</strong> (an inline picture with custom size)</li>                    
          <li><strong>{{dmsf_image(8, size=50%)}}</strong> (an inline picture with custom size)</li>          
          <li><strong>{{dmsf_image(8, height=300)}}</strong> (an inline picture with custom size)</li>
          <li><strong>{{dmsf_image(8, width=300)}}</strong> (an inline picture with custom size)</li>
          <li><strong>{{dmsftn(8)}}</strong> (a thumbnail with height of 200px)</li>
          <li><strong>{{dmsftn(8, size=300)}}</strong> (a thumbnail with custom size)</li>
          <li><strong>{{dmsfw(8)}}</strong> (approval workflow status of a document with id 8)</li>
        </ul>
        The DMSF file/revision id can be found in the link for file/revision download from within Redmine.<br />
        The DMSF folder id can be found in the link when opening folders within Redmine.
      </li>
    </ul>

In the file <redmine_root>/public/help/<language>/wiki_syntax.html, at the end of the Redmine links section:

    <tr><th></th><td>{{dmsf(83)}}</td><td>Document <a href=""#"">#83</a></td></tr>    

There's a patch (tested with Redmine 3.4.2) that helps you to modify all help files at once. In your Redmine folder:

`cd redmine`

`patch -p0 < plugins/redmine_dmsf/extra/help_files_dmsf.diff`


Setup / Upgrade
---------------

You can either clone the master branch or download the latest zipped version. Before installing ensure that the Redmine instance is stopped.

    git clone git@github.com:danmunn/redmine_dmsf.git
       
    wget https://github.com/danmunn/redmine_dmsf/archive/master.zip

1. In case of upgrade **BACKUP YOUR DATABASE, ORIGINAL PLUGIN AND THE FOLDER WITH DOCUMENTS** first!!!
2. Put redmine_dmsf plugin directory into plugins. The plugins sub-directory must be named just **redmine_dmsf**. In case
   of need rename _redmine_dmsf-x.y.z_ to *redmine_dmsf*.
3. **Go to the redmine directory** `cd redmine`   
3. Install dependencies: `bundle install`.
4. Initialize/Update database:
    
    `RAILS_ENV=production bundle exec rake db:migrate`
    
    `RAILS_ENV=production bundle exec rake redmine:plugins:migrate NAME=redmine_dmsf`
5. The access rights must be set for web server, example: `chown -R www-data:www-data plugins/redmine_dmsf`.
6. Restart the web server, e.g. `systemctl restart apache2`
7. You should configure the plugin via Redmine interface: Administration -> Plugins -> DMSF -> Configure.
8. Don't forget to grant permissions for DMSF in Administration -> Roles and permissions
9. Assign DMSF permissions to appropriate roles.
10. There are a few rake tasks:

    I) To convert documents from the standard Redmine document module

        Available options:

            * project  => id or identifier of project (defaults to all projects)
            * dry  => true or false (default false) to perform just check without any conversion
            * invalid=replace  => to perform document title invalid characters replacement for '-'

        Example:
            
            rake redmine:dmsf_convert_documents project=test RAILS_ENV=""production""

            (If you don't run the rake task as the web server user, don't forget to change the ownership of the imported files, e.g.
              chown -R www-data:www-data /redmine/files/dmsf
            afterwards)

    II) To alert all users who are expected to do an approval in the current approval steps

        Example:
            
            rake redmine:dmsf_alert_approvals RAILS_ENV=""production""   
                        
    III) To create missing checksums for all document revisions
            
        Available options:
        
          *dry_run - test, no changes to the database          
          *forceSHA256 - replace old MD5 with SHA256
        
        Example:
        
          bundle exec rake redmine:dmsf_create_digests RAILS_ENV=""production""
          bundle exec rake redmine:dmsf_create_digests forceSHA256=1 RAILS_ENV=""production""
          bundle exec rake redmine:dmsf_create_digests dry_run=1 RAILS_ENV=""production""
          
    IV) To maintain DMSF
        
        * Remove all files with no database record from the document directory
        * Remove all links project_id = -1 (added links to an issue which hasn't been created)
        
        Available options:
        
          *dry_run - No physical deletion but to list of all unused files only
        
        Example:
        
          rake redmine:dmsf_maintenance RAILS_ENV=""production""
          rake redmine:dmsf_maintenance dry_run=1 RAILS_ENV=""production""

### Installation in a sub-uri

In order to documents and folders are available via WebDAV in case that the Redmine is configured to be run in a sub-uri 
it's necessary to add the following configuration option into your `config/additional_environment.rb`:

```ruby
config.relative_url_root = '/redmine'
```

### Full-text search
If you want to use full-text search features, you must setup file content indexing.

It is necessary to index DMSF files with omindex before searching attempts to receive some output:

  1. Change the configuration part of redmine_dmsf/extra/xapian_indexer.rb file according to your environment.
     (The path to the index database set in xapian_indexer.rb must corresponds to the path set in the plugin's settings.)   
  2. Run `ruby redmine_dmsf/extra/xapian_indexer.rb -v`

This command should be run on regular basis (e.g. from cron)

Example of cron job (once per hour at 8th minute):
    
    8 * * * * root /usr/bin/ruby redmine_dmsf/extra/xapian_indexer.rb

See redmine_dmsf/extra/xapian_indexer.rb for help.

Uninstalling DMSF
-----------------
Before uninstalling the DMSF plugin, please ensure that the Redmine instance is stopped.

1. `cd [redmine-install-dir]`
2. `rake redmine:plugins:migrate NAME=redmine_dmsf VERSION=0 RAILS_ENV=production`
3. `rm plugins/redmine_dmsf -Rf`

After these steps re-start your instance of Redmine.

Contributing
------------

If you've added something, why not share it. Fork the repository (github.com/danmunn/redmine_dmsf), 
make the changes and send a pull request to the maintainers.

Changes with tests, and full documentation are preferred.

Additional Documentation
------------------------

[CHANGELOG.md](CHANGELOG.md) - Project changelog
[dmsf_user_guide.odt](dmsf_user_guide.odt) - User's guide

---

Special thanks to <a href=""https://jetbrains.com""><img src=""jetbrains-variant-3.svg"" alt=""JetBrains logo"" width=""59""  height=""68""/></a> for providing an excelent IDE.
"
260,IseHayato/GitTestRepository,,"# GitTestRepository
"
261,winhu/MongoDB.Repository,C#,"MongoDB.Repository
============================================================================================================
宗旨：简化MongoDB的操作，实现类似Entity Framework风格的代码

============================================================================================================
使用方式：

	//定义Student类型
	public class Student : Entity
	{
		[BsonIndex]		//设置索引
		public string Name { get; set; }
		public int Age { get; set; }
	}    
	//定义Teacher类型
	public class Teacher : Entity
	{
        	public string Name { get; set; }
        	public int Age { get; set; }
	}
	//定义RefEntity类型，该类型中含有子集及其操作
	public class Grade : RefEntity
	{
        	public string Name { get; set; }
	}

	//定义上下文
	public class TestDBContext : MongoDBContext
	{
		public TestDBContext() : base(""TestDBContext"")		//TestDBContext为配置文件中的节点
		{ }

		public override void OnRegisterModel(ITypeRegistration registration)
		{
			registration.RegisterType<Student>().RegisterType<Student>(Teacher).RegisterType<Grade>();
		}
	}
	
	//在程序运行伊始，写入如下两段代码进行注册
	MongoDBRepository.RegisterMongoDBContext(new TestDBContext());		//注册上下文
	MongoDBRepository.RegisterMongoIndex();					//注册索引

	//配置文件中的MongoDB连接字符串节点
	<configuration>
		<connectionStrings>
			<add name=""TestDBContext"" connectionString=""mongodb://localhost:27017/TestMongo""/>
		</connectionStrings>
	</configuration>

	//Entity使用
	//单实体保存
	Student student = new Student()
	student.Name = ""hyf"";
	student.Age = 30;
	student.Save();

	//集合保存
	MongoEntity.Save(new List<Student>() {
	    new Student{ Name=""hyf"", Age=33 },
	    new Student{ Name=""zhc"", Age=30 }
	});

	//查询
	MongoEntity.Get<Student>(student.Id);
	MongoEntity.Get<Student>(s => s.Name == ""hyf"" && s.Age > 33);
	MongoEntity.Select<Student>(s => s.Age == 30).ToList();
	MongoEntity.Select<Student>(s => s.Age >= 19 && s.Age <= 22, s => s.Age, pageIndex=1, pageSize=2, out pageCount, out allCount).ToList();
	
	//删除
	MongoEntity.RemoveAll<Student>(e => e.Name == ""hyf"");
	
	//统计
	MongoEntity.Count<Student>(s => s.Age == 30)
	
	//更多操作请参考MongoEntity
	
	//=================================================================================================================
	//RefEntity使用
	//保存
	grade = new Grade();
	grade.Name = ""No1"";
	foreach (Student student in students)
	    grade.Add<Student>(student);
	foreach (Teacher teacher in teachers)
	    grade.Add<Teacher>(teacher);
	grade.Update();		//保存grade实例及其子集，如使用grade.save()，则只保存grade实例，不保存子集。
	
	//查询子集
	grade.Pick<Student>(""BsonId string"").Name
	
	//统计子集
	grade.Count<Student>()	//统计当前子集中的Student类型
	
	//更多关于自己的操作如查询，判断，删除等请参考IRefEntity接口


=================================================================================================================

Auther: WinHu

Blog: http://www.cnblogs.com/winhu/

欢迎大家参与并指正，提出更好的意见。
"
262,friendly-telegram/friendly-telegram,Python,"# [Repository moved to GitLab](https://gitlab.com/friendly-telegram/friendly-telegram)

"
263,asquarezone/AnsibleZone,Python,"# AnsibleZone
## This repository for Ansible
### It consists of Class Room Samples
"
264,sous-chefs/nagios,Ruby,"# nagios cookbook

[![Cookbook Version](https://img.shields.io/cookbook/v/nagios.svg)](https://supermarket.chef.io/cookbooks/nagios)
[![CI State](https://github.com/sous-chefs/nagios/workflows/ci/badge.svg)](https://github.com/sous-chefs/nagios/actions?query=workflow%3Aci)
[![OpenCollective](https://opencollective.com/sous-chefs/backers/badge.svg)](#backers)
[![OpenCollective](https://opencollective.com/sous-chefs/sponsors/badge.svg)](#sponsors)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)

Installs and configures Nagios server. Chef nodes are automatically discovered using search, and Nagios host groups are created based on Chef roles and optionally environments as well.

## Maintainers

This cookbook is maintained by the Sous Chefs. The Sous Chefs are a community of Chef cookbook maintainers working together to maintain important cookbooks. If you’d like to know more please visit [sous-chefs.org](https://sous-chefs.org/) or come chat with us on the Chef Community Slack in [#sous-chefs](https://chefcommunity.slack.com/messages/C2V7B88SF).

## Requirements

### Chef

Chef Infra Client version 14+ is required

Because of the heavy use of search, this recipe will not work with Chef Solo, as it cannot do any searches without a server.

This cookbook relies heavily on multiple data bags. See --Data Bag-- below.

The system running this cookbooks should have a role named 'monitoring' so that NRPE clients can authorize monitoring from that system. This role name is configurable via an attribute. See --Attributes-- below.

The functionality that was previously in the nagios::client recipe has been moved to its own NRPE cookbook at <https://github.com/sous-chefs/nrpe>

### Platform

- Debian 10+
- Ubuntu 18.04+
- Red Hat Enterprise Linux (CentOS) 7+

--Notes--: This cookbook has been tested on the listed platforms. It may work on other platforms with or without modification.

### Cookbooks

- apache2 ~> 5.0
- nginx ~> 9.0
- nrpe
- php >= 4.0
- yum-epel
- zap >= 0.6.0

## Attributes

### config

[The config file](https://github.com/sous-chefs/nagios/blob/master/attributes/config.rb) contains the Nagios configuration options. Consult the [nagios documentation](http://nagios.sourceforge.net/docs/3_0/configmain.html) for available settings and allowed options. Configuration entries of which multiple entries are allowed, need to be specified as an Array.

Example: `default['nagios']['conf']['cfg_dir'] = [ '/etc/nagios/conf.d' , '/usr/local/nagios/conf.d' ]`

### default

- `node['nagios']['user']` - Nagios user, default 'nagios'.
- `node['nagios']['group']` - Nagios group, default 'nagios'.
- `node['nagios']['plugin_dir']` - location where Nagios plugins go, default '/usr/lib/nagios/plugins'.
- `node['nagios']['multi_environment_monitoring']` - Chef server will monitor hosts in all environments, not just its own, default 'false'
- `node['nagios']['monitored_environments']` - If multi_environment_monitoring is 'true' nagios will monitor nodes in all environments. If monitored_environments is defined then nagios will monitor only hosts in the list of environments defined. For ex: ['prod', 'beta'] will monitor only hosts in 'prod' and 'beta' chef_environments. Defaults to '[]' - and all chef environments will be monitored by default.
- `node['nagios']['monitoring_interface']` - If set, will use the specified interface for all nagios monitoring network traffic. Defaults to `nil`
- `node['nagios']['exclude_tag_host']` - If set, hosts tagged with this value will be excluded from nagios monitoring.  Defaults to ''

- `node['nagios']['server']['install_method']` - whether to install from package or source. Default chosen by platform based on known packages available for Nagios: debian/ubuntu 'package', redhat/centos/scientific: source
- `node['nagios']['server']['install_yum-epel']` - whether to install the EPEL repo or not (only applies to RHEL platform family). The default value is `true`. Set this to `false` if you do not wish to install the EPEL RPM; in this scenario you will need to make the relevant packages available via another method e.g. local repo, or install from source.
- `node['nagios']['server']['service_name']` - name of the service used for Nagios, default chosen by platform, debian/ubuntu ""nagios3"", redhat family ""nagios"", all others, ""nagios""
- `node['nagios']['home']` - Nagios main home directory, default ""/usr/lib/nagios3""
- `node['nagios']['conf_dir']` - location where main Nagios config lives, default ""/etc/nagios3""
- `node['nagios']['resource_dir']` - location for recources, default ""/etc/nagios3""
- `node['nagios']['config_dir']` - location where included configuration files live, default ""/etc/nagios3/conf.d""
- `node['nagios']['log_dir']` - location of Nagios logs, default ""/var/log/nagios3""
- `node['nagios']['cache_dir']` - location of cached data, default ""/var/cache/nagios3""
- `node['nagios']['state_dir']` - Nagios runtime state information, default ""/var/lib/nagios3""
- `node['nagios']['run_dir']` - where pidfiles are stored, default ""/var/run/nagios3""
- `node['nagios']['docroot']` - Nagios webui docroot, default ""/usr/share/nagios3/htdocs""
- `node['nagios']['enable_ssl']` - boolean for whether Nagios web server should be https, default false
- `node['nagios']['ssl_cert_file']` = Location of SSL Certificate File. default ""/etc/nagios3/certificates/nagios-server.pem""
- `node['nagios']['ssl_cert_chain_file']` = Optional location of SSL Intermediate Certificate File. No default.
- `node['nagios']['ssl_cert_key']`  = Location of SSL Certificate Key. default ""/etc/nagios3/certificates/nagios-server.pem""
- `node['nagios']['ssl_protocols']` = The SSLProtocol string to pass to apache, defaults to ""all -SSL3 -SSL2""
- `node['nagios']['ssl_ciphers']` = The SSLCipherSuite string to pass to apache, defaults to empty (which will result in this setting not being included in the apache config)
- `node['nagios']['http_port']` - port that the Apache/Nginx virtual site should listen on, determined whether ssl is enabled (443 if so, otherwise 80). Note:  You will also need to configure the listening port for either NGINX or Apache within those cookbooks.
- `node['nagios']['server_name']` - common name to use in a server cert, default ""nagios""
- `node['nagios']['server']['server_alias']` - alias name for the webserver for use with Apache.  Defaults to nil
- `node['nagios']['ssl_req']` - info to use in a cert, default `/C=US/ST=Several/L=Locality/O=Example/OU=Operations/CN=#{node['nagios']['server_name']}/emailAddress=ops@#{node['nagios']['server_name']}`

- `node['nagios']['server']['version']` - version of the server source to download
- `node['nagios']['server']['checksum']` - checksum of the source files
- `node['nagios']['server']['patch_url']` - url to download patches from if installing from source
- `node['nagios']['server']['patches']` - array of patch filenames to apply if installing from source
- `node['nagios']['url']` - URL to host Nagios from - defaults to nil and instead uses  FQDN

- `node['nagios']['conf']['enable_notifications']` - set to 1 to enable notification.
- `node['nagios']['conf']['interval_length']` - minimum interval. Defaults to '1'.
- `node['nagios']['conf']['use_timezone']` - set the timezone for nagios AND apache.  Defaults to UTC.
- `node['nagios']['conf']['use_large_installation_tweaks']` - Attribute to enable [large installation tweaks](http://nagios.sourceforge.net/docs/3_0/largeinstalltweaks.html). Defaults to 0.

- `node['nagios']['check_external_commands']`
- `node['nagios']['default_contact_groups']`
- `node['nagios']['default_user_name']` - Specify a defaut guest user to allow page access without authentication.  --Only-- use this if nagios is running behind a secure webserver and users have been authenticated in some manner.  You'll likely want to change `node['nagios']['server_auth_require']` to `all granted`.  Defaults to `nil`.
- `node['nagios']['sysadmin_email']` - default notification email.
- `node['nagios']['sysadmin_sms_email']` - default notification sms.
- `node['nagios']['server_auth_method']` - authentication with the server can be done with openid (using `apache2::mod_auth_openid`), cas (using `apache2::mod_auth_cas`),ldap (using `apache2::mod_authnz_ldap`), or htauth (basic). The default is htauth. ""openid"" will utilize openid authentication, ""cas"" will utilize cas authentication, ""ldap"" will utilize LDAP authentication, and any other value will use htauth (basic).
- `node['nagios']['cas_login_url']` - login url for cas if using cas authentication.
- `node['nagios']['cas_validate_url']` - validation url for cas if using cas authentication.
- `node['nagios']['cas_validate_server']` - whether to validate the server cert. Defaults to off.
- `node['nagios']['cas_root_proxy_url']` - if set, sets the url that the cas server redirects to after auth.
- `node['nagios']['ldap_bind_dn']` - DN used to bind to the server when searching for ldap entries.
- `node['nagios']['ldap_bind_password']` - bind password used with the DN provided for searching ldap.
- `node['nagios']['ldap_url']` - ldap url and search parameters.
- `node['nagios']['ldap_authoritative']` - accepts ""on"" or ""off"". controls other authentication modules from authenticating the user if this one fails.
- `node['nagios']['ldap_group_attribute']` - Set the Apache AuthLDAPGroupAttribute directive to a non-default value.
- `node['nagios']['ldap_group_attribute_is_dn']` - accepts ""on"" or ""off"". Set the Apache AuthLDAPGroupAttributeIsDN directive. Apache's default behavior is currently ""on.""
- `node['nagios']['ldap_verify_cert']` - accepts ""on"" or ""off"". Set the Apache mod_ldap LDAPVerifyServerCert directive. Apache's default behavior is currently ""on.""
- `node['nagios']['ldap_trusted_mode']` - Set the Apache mod_ldap LDAPTrustedMode directive.
- `node['nagios']['ldap_trusted_global_cert']` - Set the Apache mod_ldap LDAPTrustedGlobalCert directive.
- `node['nagios']['users_databag']` - the databag containing users to search for. defaults to users
- `node['nagios']['users_databag_group']` - users databag group considered Nagios admins.  defaults to sysadmin
- `node['nagios']['services_databag']` - the databag containing services to search for. defaults to nagios_services
- `node['nagios']['servicegroups_databag']` - the databag containing servicegroups to search for. defaults to nagios_servicegroups
- `node['nagios']['templates_databag']` - the databag containing templates to search for. defaults to nagios_templates
- `node['nagios']['hostgroups_databag']` - the databag containing hostgroups to search for. defaults to nagios_hostgroups
- `node['nagios']['hosttemplates_databag']` - the databag containing host templates to search for. defaults to nagios_hosttemplates
- `node['nagios']['eventhandlers_databag']` - the databag containing eventhandlers to search for. defaults to nagios_eventhandlers
- `node['nagios']['unmanagedhosts_databag']` - the databag containing unmanagedhosts to search for. defaults to nagios_unmanagedhosts
- `node['nagios']['serviceescalations_databag']` - the databag containing serviceescalations to search for. defaults to nagios_serviceescalations
- `node['nagios']['hostescalations_databag']` - the databag containing hostescalations to search for. defaults to nagios_hostescalations
- `node['nagios']['contacts_databag']` - the databag containing contacts to search for. defaults to nagios_contacts
- `node['nagios']['contactgroups_databag']` - the databag containing contactgroups to search for. defaults to nagios_contactgroups
- `node['nagios']['servicedependencies_databag']` - the databag containing servicedependencies to search for. defaults to nagios_servicedependencies
- `node['nagios']['host_name_attribute']` - node attribute to use for naming the host. Must be unique across monitored nodes. Defaults to hostname
- `node['nagios']['regexp_matching']` - Attribute to enable [regexp matching](http://nagios.sourceforge.net/docs/3_0/configmain.html#use_regexp_matching). Defaults to 0.
- `node['nagios']['templates']` - These set directives in the default host template. Unless explicitly overridden, they will be inherited by the host definitions for each discovered node and `nagios_unmanagedhosts` data bag. For more information about these directives, see the Nagios documentation for [host definitions](http://nagios.sourceforge.net/docs/3_0/objectdefinitions.html#host).
- `node['nagios']['hosts_template']` - Host template you want to inherit properties/variables from, default 'server'. For more information, see the nagios doc on [Object Inheritance](http://nagios.sourceforge.net/docs/3_0/objectinheritance.html).
- `node['nagios']['brokers']` - Hash of broker modules to include in the config. Hash key is the path to the broker module, the value is any parameters to pass to it.
- `node['nagios']['nagios_config']['template_cookbook']` - Look for template file in the cookbook mentioned in the attribute. Defaults to nagios
- `node['nagios']['resources']['template_cookbook']` - Look for template file in the cookbook mentioned in the attribute. Defaults to nagios
- `node['nagios']['htauth']['template_cookbook']` - Look for template file in the cookbook mentioned in the attribute. Defaults to nagios
- `node['nagios']['nagios_config']['template_file']` - Template file to be rendered. Defaults to nagios.cfg.erb
- `node['nagios']['resources']['template_file']` - Template file to be rendered. Defaults to resource.cfg.erb
- `node['nagios']['htauth']['template_file']` - Template file to be rendered. Defaults to htpasswd.users.erb

- `node['nagios']['default_host']['flap_detection']` - Defaults to `true`.
- `node['nagios']['default_host']['process_perf_data']` - Defaults to `false`.
- `node['nagios']['default_host']['check_period']` - Defaults to `'24x7'`.
- `node['nagios']['default_host']['check_interval']` - In seconds. Must be divisible by `node['nagios']['interval_length']`. Defaults to `15`.
- `node['nagios']['default_host']['retry_interval']` - In seconds. Must be divisible by `node['nagios']['interval_length']`. Defaults to `15`.
- `node['nagios']['default_host']['max_check_attempts']` - Defaults to `1`.
- `node['nagios']['default_host']['check_command']` - Defaults to the pre-defined command `'check-host-alive'`.
- `node['nagios']['default_host']['notification_interval']` - In seconds. Must be divisible by `node['nagios']['interval_length']`. Defaults to `300`.
- `node['nagios']['default_host']['notification_options']` - Defaults to `'d,u,r'`.
- `node['nagios']['default_host']['action_url']` - Defines a action url.  Defaults to `nil`.

- `node['nagios']['default_service']['process_perf_data']` - Defaults to `false`.
- `node['nagios']['default_service']['action_url']` - Defines a action url. Defaults to `nil`.

- `node['nagios']['server']['web_server']` - web server to use. supports Apache or Nginx, default ""apache""
- `node['nagios']['server']['nginx_dispatch']` - nginx dispatch method. supports cgi or php, default ""cgi""
- `node['nagios']['server']['stop_apache']` - stop apache service if using nginx, default false
- `node['nagios']['server']['redirect_root']` - if using Apache, should [http://server/](http://server/) redirect to [http://server/nagios3](http://server/nagios3) automatically, default false
- `node['nagios']['server']['normalize_hostname']` - If set to true, normalize all hostnames in hosts.cfg to lowercase. Defaults to false.

 These are nagios cgi.config options.

- `node['nagios']['cgi']['show_context_help']`                         - Defaults to 1
- `node['nagios']['cgi']['authorized_for_system_information']`         - Defaults to '-'
- `node['nagios']['cgi']['authorized_for_configuration_information']`  - Defaults to '-'
- `node['nagios']['cgi']['authorized_for_system_commands']`            - Defaults to '-'
- `node['nagios']['cgi']['authorized_for_all_services']`               - Defaults to '-'
- `node['nagios']['cgi']['authorized_for_all_hosts']`                  - Defaults to '-'
- `node['nagios']['cgi']['authorized_for_all_service_commands']`       - Defaults to '-'
- `node['nagios']['cgi']['authorized_for_all_host_commands']`          - Defaults to '-'
- `node['nagios']['cgi']['default_statusmap_layout']`                  - Defaults to 5
- `node['nagios']['cgi']['default_statuswrl_layout']`                  - Defaults to 4
- `node['nagios']['cgi']['result_limit']`                              - Defaults to 100
- `node['nagios']['cgi']['escape_html_tags']`                          - Defaults to 0
- `node['nagios']['cgi']['action_url_target']`                         - Defaults to '_blank'
- `node['nagios']['cgi']['notes_url_target']`                          - Defaults to '_blank'
- `node['nagios']['cgi']['lock_author_names']`                         - Defaults to 1
- `node['nagios']['cgi']['template_cookbook']`                         - Look for template file in the cookbook mentioned in the attribute. Defaults to nagios
- `node['nagios']['cgi']['template_file']`                             - Template file to be rendered. Defaults to cgi.cfg.erb

## Recipes

### default

Includes the correct client installation recipe based on platform, either `nagios::server_package` or `nagios::server_source`.

The server recipe sets up Apache as the web front end by default. This recipe also does a number of searches to dynamically build the hostgroups to monitor, hosts that belong to them and admins to notify of events/alerts.

Searches are confined to the node's `chef_environment` unless multi-environment monitoring is enabled.

The recipe does the following:

1. Searches for users in 'users' databag belonging to a 'sysadmin' group, and authorizes them to access the Nagios web UI and also to receive notification e-mails.
1. Searches all available roles/environments and builds a list which will become the Nagios hostgroups.
1. Places nodes in Nagios hostgroups by role / environment membership.
1. Installs various packages required for the server.
1. Sets up configuration directories.
1. Moves the package-installed Nagios configuration to a 'dist' directory.
1. Disables the 000-default VirtualHost present on Debian/Ubuntu Apache2 package installations.
1. Templates configuration files for services, contacts, contact groups, templates, hostgroups and hosts.
1. Enables the Nagios web UI.
1. Starts the Nagios server service

### server_package

Installs the Nagios server from packages. Default for Debian / Ubuntu systems.

### server_source

Installs the Nagios server from source. Default for Red Hat based systems as native packages for Nagios are not available in the default repositories.

### pagerduty

Installs pagerduty plugin for nagios. If you only have a single pagerduty key, you can simply set a `node['nagios']['pagerduty_key']` attribute on your server. For multiple pagerduty key configuration see Pager Duty under Data Bags.

This recipe was written based on the [Nagios Integration Guide](http://www.pagerduty.com/docs/guides/nagios-integration-guide) from PagerDuty which explains how to get an API key for your Nagios server.

## Data Bags

[See Wiki for more databag information](https://github.com/sous-chefs/nagios/wiki/config)

### Pager Duty

You can define pagerduty contacts and keys by creating nagios_pagerduty data bags that contain the contact and the relevant key. Setting admin_contactgroup to ""true"" will add this pagerduty contact to the admin contact group created by this cookbook.

```javascript
{
  ""id"": ""pagerduty_critical"",
  ""admin_contactgroup"": ""true"",
  ""key"": ""a33e5ef0ac96772fbd771ddcccd3ccd0""
}
```

You can add these contacts to any contactgroups you create.

## Monitoring Role

Create a role to use for the monitoring server. The role name should match the value of the attribute ""`node['nrpe']['server_role']`"" on your clients. By default, this is '`monitoring`'. For example:

```ruby
# roles/monitoring.rb
name 'monitoring'
description 'Monitoring server'
run_list(
  'recipe[nagios::default]'
)

default_attributes(
  'nagios' => {
    'server_auth_method' => 'htauth'
  }
)
```

```bash
knife role from file monitoring.rb
```

## Usage

### server setup

Create a role named '`monitoring`', and add the nagios server recipe to the `run_list`. See --Monitoring Role-- above for an example.

Apply the nrpe cookbook to nodes in order to install the NRPE client

By default the Nagios server will only monitor systems in its same environment. To change this set the `multi_environment_monitoring` attribute. See --Attributes--

Create data bag items in the `users` data bag for each administer you would like to be able to login to the Nagios server UI. Pay special attention to the method you would like to use to authorization users (openid or htauth). See --Users-- and --Atttributes--

At this point you now have a minimally functional Nagios server, however the server will lack any service checks outside of the single Nagios Server health check.

### defining checks

NRPE commands are defined in recipes using the nrpe_check LWRP provider in the nrpe cookbooks. For base system monitoring such as load, ssh, memory, etc you may want to create a cookbook in your environment that defines each monitoring command via the LWRP.

With NRPE commands created using the LWRP you will need to define Nagios services to use those commands. These services are defined using the `nagios_services` data bag and applied to roles and/or environments. See --Services--

### enabling notifications

You need to set `default['nagios']['notifications_enabled'] = 1` attribute on your Nagios server to enable email notifications.

For email notifications to work an appropriate mail program package and local MTA need to be installed so that /usr/bin/mail or /bin/mail is available on the system.

Example:

Include [postfix cookbook](https://github.com/opscode-cookbooks/postfix) to be installed on your Nagios server node.

Add override_attributes to your `monitoring` role:

```ruby
# roles/monitoring.rb
name 'monitoring'
description 'Monitoring Server'
run_list(
  'recipe[nagios:default]',
  'recipe[postfix]'
)

override_attributes(
  'nagios' => { 'notifications_enabled' => '1' },
  'postfix' => { 'myhostname':'your_hostname', 'mydomain':'example.com' }
)

default_attributes(
  'nagios' => { 'server_auth_method' => 'htauth' }
)
```

```bash
knife role from file monitoring.rb
```

## Contributors

This project exists thanks to all the people who [contribute.](https://opencollective.com/sous-chefs/contributors.svg?width=890&button=false)

### Backers

Thank you to all our backers!

![https://opencollective.com/sous-chefs#backers](https://opencollective.com/sous-chefs/backers.svg?width=600&avatarHeight=40)

### Sponsors

Support this project by becoming a sponsor. Your logo will show up here with a link to your website.

![https://opencollective.com/sous-chefs/sponsor/0/website](https://opencollective.com/sous-chefs/sponsor/0/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/1/website](https://opencollective.com/sous-chefs/sponsor/1/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/2/website](https://opencollective.com/sous-chefs/sponsor/2/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/3/website](https://opencollective.com/sous-chefs/sponsor/3/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/4/website](https://opencollective.com/sous-chefs/sponsor/4/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/5/website](https://opencollective.com/sous-chefs/sponsor/5/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/6/website](https://opencollective.com/sous-chefs/sponsor/6/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/7/website](https://opencollective.com/sous-chefs/sponsor/7/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/8/website](https://opencollective.com/sous-chefs/sponsor/8/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/9/website](https://opencollective.com/sous-chefs/sponsor/9/avatar.svg?avatarHeight=100)
"
265,UNC-Libraries/Carolina-Digital-Repository,Java,"![Build](https://github.com/UNC-Libraries/Carolina-Digital-Repository/workflows/DcrBuild/badge.svg)

Building the project
---------------------
```
# clone the project
git clone https://github.com/UNC-Libraries/Carolina-Digital-Repository
# initialize submodules
git submodule update --init --recursive
# Install SASS parser to build CSS
gem install sass
# Install Homebrew, if not already installed
See https://brew.sh/ for instructions.
# Install Node.js to build JavaScript and run JavaScript tests
brew install node
# Build the project
mvn clean install -DskipTests
```

Eclipse IDE Developer Setup
---------------------------
A version of Eclipse with m2e is required

To set the environment variable you'll need for running unit tests in Eclipse, go to Preferences > Java > Installed JREs. Select your JRE and click Edit, then type  ```-Dfcrepo.baseUri=http://example.com/rest```` in the Default VM Arguments box in the Default VM Arguments box

Running Tests
-------------

All tests run automatically in Travis.
All Java tests run automatically when building the project.
JavaScript test don't run on a maven build, but can be run manually using the NPM command below.

```
# Java Tests
mvn clean install

# JavaScript Tests
npm --prefix static/js/vue-cdr-access run test:unit
```
"
266,sharpdx/SharpDX-Samples,C#,"# SharpDX Samples

This repository contains more than 100+ code samples using [SharpDX](http://sharpdx.org) ([github](https://github.com/sharpdx/SharpDX))

The purpose of these samples is to give a grasp of each API/platform and how to access DirectX API from C#.


> Disclaimer:
> - Quality of all code samples here are not meant for production (precisely, all code for Dispose() on COM objects is not always written while this is critical for production)
> - Samples are not exhaustive and not meant to demonstrate all DirectX API usages but only how to access DirectX from C#
>      

## Installing

1. Create a local directory `SharpDX` and clone this repository in it under `Samples`
1. Download latest 3.x dev zip package from http://sharpdx.org and unzip it under `SharpDX` directory.
2. You should have `SharpDX\Bin` along the `SharpDX\Samples`

## Content

TODO

## Licensing
MIT
"
267,unitycontainer/unity,,"
# Overview

The Unity Container (Unity) is a full featured, extensible dependency injection container. It facilitates building loosely coupled applications and provides developers with the following advantages:

* Simplified object creation, especially for hierarchical object structures and dependencies
* Abstraction of requirements; this allows developers to specify dependencies at run time or in configuration and simplify management of crosscutting concerns
* Increased flexibility by deferring component configuration to the container
* Service location capability; this allows clients to store or cache the container
* Instance and type interception
* Registration by convention

## Installation

Install Unity with the following command:

```shell
Install-Package Unity
```

Unity 5.x loosely follows Semantic Versioning — minor releases may introduce breaking changes. [Floating version references](https://docs.microsoft.com/en-us/nuget/consume-packages/package-references-in-project-files#floating-versions) should lock in the minor version in addition to the major version:

```xml
<PackageReference Include=""Unity.Container"" Version=""5.x.*"" />
```

## [Documentation](https://unitycontainer.github.io)

The documentation is a work in progress. Some info is available [here](https://unitycontainer.github.io) but more is coming...
Feel free to [open issues](https://github.com/unitycontainer/documentation/issues) in [Documentation](https://github.com/unitycontainer/documentation) project with all the questions you would like to be covered or questions you might have.

## New Features

[**Suggest**](https://feathub.com/unitycontainer/unity/features/new) new features or vote for the proposals you like, [**ADD**](https://feathub.com/unitycontainer/unity/features/new) your comments:

[![Feature Requests](http://feathub.com/unitycontainer/unity?format=svg)](http://feathub.com/unitycontainer/unity)

## Packages & Status

Unity library consists of multiple packages. For information about each package please follow the links

---
Package  | License       | Version       | Downloads
-------- | :------------ | :------------ | :------------
Unity (Composite) | [![License](https://img.shields.io/github/license/unitycontainer/unity.svg)](https://github.com/unitycontainer/unity/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.svg)](https://www.nuget.org/packages/Unity) | [![NuGet](https://img.shields.io/nuget/dt/Unity.svg)](https://www.nuget.org/packages/Unity)
[Unity.Abstractions](https://github.com/unitycontainer/abstractions) | [![License](https://img.shields.io/github/license/unitycontainer/abstractions.svg)](https://github.com/unitycontainer/abstractions/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.Abstractions.svg)](https://www.nuget.org/packages/Unity.Abstractions) | [![NuGet](https://img.shields.io/nuget/dt/Unity.Abstractions.svg)](https://www.nuget.org/packages/Unity.Abstractions)
[Unity.Container](https://github.com/unitycontainer/container) | [![License](https://img.shields.io/github/license/unitycontainer/container.svg)](https://github.com/unitycontainer/container/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.Container.svg)](https://www.nuget.org/packages/Unity.Container) | [![NuGet](https://img.shields.io/nuget/dt/Unity.Container.svg)](https://www.nuget.org/packages/Unity.Container)
[Unity.Configuration](https://github.com/unitycontainer/configuration) | [![License](https://img.shields.io/github/license/unitycontainer/configuration.svg)](https://github.com/unitycontainer/configuration/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.Configuration.svg)](https://www.nuget.org/packages/Unity.Configuration) | [![NuGet](https://img.shields.io/nuget/dt/Unity.Configuration.svg)](https://www.nuget.org/packages/Unity.Configuration)
[Unity.Interception](https://github.com/unitycontainer/interception) | [![License](https://img.shields.io/github/license/unitycontainer/interception.svg)](https://github.com/unitycontainer/interception/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.Interception.svg)](https://www.nuget.org/packages/Unity.Interception) | [![NuGet](https://img.shields.io/nuget/dt/Unity.Interception.svg)](https://www.nuget.org/packages/Unity.Interception)
[Unity.Interception.Configuration](https://github.com/unitycontainer/interception-configuration) | [![License](https://img.shields.io/github/license/unitycontainer/interception-configuration.svg)](https://github.com/unitycontainer/interception-configuration/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.Interception.Configuration.svg)](https://www.nuget.org/packages/Unity.Interception.Configuration) | [![NuGet](https://img.shields.io/nuget/dt/Unity.Interception.Configuration.svg)](https://www.nuget.org/packages/Unity.Interception.Configuration)
[Unity.RegistrationByConvention](https://github.com/unitycontainer/registration-by-convention) | [![License](https://img.shields.io/github/license/unitycontainer/registration-by-convention.svg)](https://github.com/unitycontainer/registration-by-convention/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.RegistrationByConvention.svg)](https://www.nuget.org/packages/Unity.RegistrationByConvention) | [![NuGet](https://img.shields.io/nuget/dt/Unity.RegistrationByConvention.svg)](https://www.nuget.org/packages/Unity.RegistrationByConvention)
[Unity.log4net](https://github.com/unitycontainer/log4net) | [![License](https://img.shields.io/github/license/unitycontainer/log4net.svg)](https://github.com/unitycontainer/log4net/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.log4net.svg)](https://www.nuget.org/packages/Unity.log4net) | [![NuGet](https://img.shields.io/nuget/dt/Unity.log4net.svg)](https://www.nuget.org/packages/Unity.log4net)
[Unity.NLog](https://github.com/unitycontainer/NLog) | [![License](https://img.shields.io/github/license/unitycontainer/NLog.svg)](https://github.com/unitycontainer/NLog/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.NLog.svg)](https://www.nuget.org/packages/Unity.NLog) | [![NuGet](https://img.shields.io/nuget/dt/Unity.NLog.svg)](https://www.nuget.org/packages/Unity.NLog)
[Unity.Microsoft.Logging](https://github.com/unitycontainer/microsoft-logging) | [![License](https://img.shields.io/github/license/unitycontainer/microsoft-logging.svg)](https://github.com/unitycontainer/microsoft-logging/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.Microsoft.Logging.svg)](https://www.nuget.org/packages/Unity.Microsoft.Logging) | [![NuGet](https://img.shields.io/nuget/dt/Unity.Microsoft.Logging.svg)](https://www.nuget.org/packages/Unity.Microsoft.Logging)
[Unity.Microsoft.DependencyInjection](https://github.com/unitycontainer/microsoft-dependency-injection) | [![License](https://img.shields.io/github/license/unitycontainer/microsoft-dependency-injection.svg)](https://github.com/unitycontainer/microsoft-dependency-injection/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.Microsoft.DependencyInjection.svg)](https://www.nuget.org/packages/Unity.Microsoft.DependencyInjection)| [![NuGet](https://img.shields.io/nuget/dt/Unity.Microsoft.DependencyInjection.svg)](https://www.nuget.org/packages/Unity.Microsoft.DependencyInjection)
[Unity.AspNet.WebApi](https://github.com/unitycontainer/aspnet-webapi) | [![License](https://img.shields.io/github/license/unitycontainer/aspnet-webapi.svg)](https://github.com/unitycontainer/aspnet-webapi/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.AspNet.WebApi.svg)](https://www.nuget.org/packages/Unity.AspNet.WebApi) | [![NuGet](https://img.shields.io/nuget/dt/Unity.AspNet.WebApi.svg)](https://www.nuget.org/packages/Unity.AspNet.WebApi)
[Unity.Mvc](https://github.com/unitycontainer/aspnet-mvc) | [![License](https://img.shields.io/github/license/unitycontainer/aspnet-mvc.svg)](https://github.com/unitycontainer/aspnet-mvc/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.Mvc.svg)](https://www.nuget.org/packages/Unity.Mvc) | [![NuGet](https://img.shields.io/nuget/dt/Unity.Mvc.svg)](https://www.nuget.org/packages/Unity.Mvc)
[Unity.WCF](https://github.com/unitycontainer/wcf) | [![License](https://img.shields.io/github/license/unitycontainer/wcf.svg)](https://github.com/unitycontainer/wcf/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.wcf.svg)](https://www.nuget.org/packages/Unity.wcf) | [![NuGet](https://img.shields.io/nuget/dt/Unity.wcf.svg)](https://www.nuget.org/packages/Unity.wcf)
[Unity.ServiceLocation](https://github.com/unitycontainer/service-location) | [![License](https://img.shields.io/github/license/unitycontainer/service-location.svg)](https://github.com/unitycontainer/service-location/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/Unity.ServiceLocation.svg)](https://www.nuget.org/packages/Unity.ServiceLocation) | [![NuGet](https://img.shields.io/nuget/dt/Unity.ServiceLocation.svg)](https://www.nuget.org/packages/Unity.ServiceLocation)
[CommonServiceLocator](https://github.com/unitycontainer/commonservicelocator) | [![License](https://img.shields.io/github/license/unitycontainer/commonservicelocator.svg)](https://github.com/unitycontainer/commonservicelocator/blob/master/LICENSE) | [![NuGet](https://img.shields.io/nuget/v/commonservicelocator.svg)](https://www.nuget.org/packages/CommonServiceLocator) | [![NuGet](https://img.shields.io/nuget/dt/commonservicelocator.svg)](https://www.nuget.org/packages/CommonServiceLocator)

## Code of Conduct

This project has adopted the code of conduct defined by the [Contributor Covenant](https://www.contributor-covenant.org/) to clarify expected behavior in our community. For more information, see the [.NET Foundation Code of Conduct](https://www.dotnetfoundation.org/code-of-conduct)

## Contributing

See the [Contributing guide](https://github.com/unitycontainer/unity/blob/master/CONTRIBUTING.md) for more information.

## .NET Foundation

Unity Container is a [.NET Foundation](https://dotnetfoundation.org/projects/unitycontainer) project.
"
268,Potential17/Hacktoberfest-2020,HTML,"# Welcome to HacktoberFest 2020!

## Follow these steps to make your first pull request-

### 1. Fork this repository.

### 2. Clone your forked repository to your local machine.

### 3. In the index file, look for the 'ol' tag. Then insert a 'li' tag with your link to your profile.

### 4. Add your name in index.html file.

### 5. Write these command on your terminal-

```sh
git add index.html
```

```sh
git commit -m ""your name""
```

```sh
git remote add [remote-name] [put the github link here]
```

```sh
git push [remote-name] master
```

--> Then create your pull request.

--> Star my repository.

--> Congratulations!! You have successfully created your pull request.

-->Check your progress here (https://hacktoberfest.digitalocean.com/profile)

### ✨ A beginner friendly repository made specifically for open source beginners,
### The sole purpose of this repository is to help beginners learn Git & GitHub through easy first contribution.
"
269,libfann/fann,C++,"# Fast Artificial Neural Network Library
## FANN

**Fast Artificial Neural Network (FANN) Library** is a free open source neural network library, which implements multilayer artificial neural networks in C with support for both fully connected and sparsely connected networks.

Cross-platform execution in both fixed and floating point are supported. It includes a framework for easy handling of training data sets. It is easy to use, versatile, well documented, and fast. 

Bindings to more than 15 programming languages are available. 

An easy to read introduction article and a reference manual accompanies the library with examples and recommendations on how to use the library. 

Several graphical user interfaces are also available for the library.

## FANN Features

* Multilayer Artificial Neural Network Library in C
* Backpropagation training (RPROP, Quickprop, Batch, Incremental)
* Evolving topology training which dynamically builds and trains the ANN (Cascade2)
* Easy to use (create, train and run an ANN with just three function calls)
* Fast (up to 150 times faster execution than other libraries)
* Versatile (possible to adjust many parameters and features on-the-fly)
* Well documented (An easy to read introduction article, a thorough reference manual, and a 50+ page university report describing the implementation considerations etc.)
* Cross-platform (configure script for linux and unix, dll files for windows, project files for MSVC++ and Borland compilers are also reported to work)
* Several different activation functions implemented (including stepwise linear functions for that extra bit of speed)
* Easy to save and load entire ANNs
* Several easy to use examples
* Can use both floating point and fixed point numbers (actually both float, double and int are available)
* Cache optimized (for that extra bit of speed)
* Open source, but can still be used in commercial applications (licenced under LGPL)
* Framework for easy handling of training data sets
* Graphical Interfaces
* Language Bindings to a large number of different programming languages
* Widely used (approximately 100 downloads a day)

## To Install

### On Linux

#### From Source

First you'll want to clone the repository:

`git clone https://github.com/libfann/fann.git`

Once that's finished, navigate to the Root directory. In this case it would be ./fann:

`cd ./fann`

Then run CMake

`cmake .`

After that, you'll need to use elevated privileges to install the library:

`sudo make install`

That's it! If everything went right, you should see a lot of text, and FANN should be installed!

### Building fann - Using vcpkg

You can download and install fann using the [vcpkg](https://github.com/Microsoft/vcpkg) dependency manager:

    git clone https://github.com/Microsoft/vcpkg.git
    cd vcpkg
    ./bootstrap-vcpkg.sh
    ./vcpkg integrate install
    ./vcpkg install fann

The fann port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.

## To Learn More

To get started with FANN, go to the [FANN help site](http://leenissen.dk/fann/wp/help/), which will include links to all the available resources. 

For more information about FANN, please refer to the [FANN website](http://leenissen.dk/fann/wp/)
"
270,foyzulkarim/GenericComponents,C#,"
# GenericComponents
Abstract
Every application, let it be a desktop, web, mobile or any other you can image, either sends data to database or retrieves data from database to show it to the user. What if we automate the data sending and fetching process and focus only on the business logics? So that we don't need to write the common framework related hundreads of thousands lines of code for each of our projects, and the projects have been written all over the world? 

<a href=""https://www.youtube.com/playlist?list=PLEYpvDF6qy8YjfgSh8cH63M8sNvMh4qvJ"">How you can use this library Video Playlist</a> 
"
271,VolodyaVechirko/RepositoryApp,Kotlin,
272,Wiznet/ioLibrary_Driver,C,"# ioLibrary Driver
The ioLibrary means “Internet Offload Library” for WIZnet chip. It includes drivers and application protocols.
The driver (ioLibrary) can be used for the application design of WIZnet TCP/IP chips as [W5500](http://wizwiki.net/wiki/doku.php?id=products:w5500:start), W5300, W5200, W5100 [W5100S](http://wizwiki.net/wiki/doku.php?id=products:w5100s:start).

## ioLibrary
This driver provides the Berkeley Socket type APIs.
- Directory Structure
<!-- ioLibrary pic -->
![ioLibrary](http://wizwiki.net/wiki/lib/exe/fetch.php?media=products:w5500:iolibrary_bsd.jpg ""ioLibrary"")

- Ethernet : SOCKET APIs like BSD & WIZCHIP([W5500](http://wizwiki.net/wiki/doku.php?id=products:w5500:start) / W5300 /  W5200 / W5100 / [W5100S](http://wizwiki.net/wiki/doku.php?id=products:w5100s:start)) Driver
- Internet :
  - DHCP client
  - DNS client
  - FTP client
  - FTP server
  - SNMP agent/trap
  - SNTP client
  - TFTP client
  - HTTP server
  - MQTT Client
  - Others will be added.

## How to add an ioLibrary in project through github site.
  - Example, refer to https://www.youtube.com/watch?v=mt815RBGdsA
  - [ioLibrary Doxygen doument](https://github.com/Wiznet/ioLibrary_Driver/blob/master/Ethernet/Socket_APIs_V3.0.3.chm) : Refer to **TODO** in this document
    - Define what chip is used in **wizchip_conf.h**
    - Define what Host I/F mode is used in **wizchip_conf.h**

## Revision History
  * ioLibrary V4.0.0 Released : 29, MAR, 2018
    * New features added: Library for W5100S added.
  * ioLibrary V3.1.1 Released : 14, Dec, 2016
    * Bug fixed : In Socket.c Fixed MACraw & IPraw sendto function.
  * ioLibrary V3.1.0 Released : 05, Dec, 2016
    * Internet application protocol add to MQTT Client (using paho MQTT 3.11)
  * ioLibrary V3.0.3 Released : 03, May, 2016
    * In W5300, Fixed some compile errors in close(). Refer to M20160503
    * In close(), replace socket() with some command sequences.
  * ioLibrary V3.0.2 Released : 26, April, 2016
    * Applied the erratum #1 in close() of socket.c (Refer to A20160426)
  * ioLibrary V3.0.1 Released : 15, July, 2015
    * Bug fixed : In W5100, Fixed CS control problem in read/write buffer with SPI. Refer to M20150715.
  * ioLibrary V3.0 Released : 01, June, 2015
    * Add to W5300
    * Typing Error in comments
    * Refer to 20150601 in sources.

  * Type casting error Fixed : 09, April. 2015
    In socket.c, send() : Refer to M20150409

  * ioLibrary V2.0 released : April. 2015
    * Added to W5100, W5200
    * Correct to some typing error
    * Fixed the warning of type casting.

  * Last release : Nov. 2014

"
273,rg-engineering/ioBroker.heatingcontrol,JavaScript,"![Logo](admin/heatingcontrol.png)
# ioBroker.HeatingControl
![Number of Installations](http://iobroker.live/badges/heatingcontrol-installed.svg) ![Number of Installations](http://iobroker.live/badges/heatingcontrol-stable.svg) 

[![NPM version](https://img.shields.io/npm/v/iobroker.heatingcontrol.svg)](https://www.npmjs.com/package/iobroker.heatingcontrol)
[![Downloads](https://img.shields.io/npm/dm/iobroker.heatingcontrol.svg)](https://www.npmjs.com/package/iobroker.heatingcontrol)
[![Tests](https://travis-ci.org/rg-engineering/ioBroker.heatingcontrol.svg?branch=master)](https://travis-ci.org/rg-engineering/ioBroker.heatingcontrol)

[![NPM](https://nodei.co/npm/iobroker.heatingcontrol.png?downloads=true)](https://nodei.co/npm/iobroker.heatingcontrol/)

**This adapter uses Sentry libraries to automatically report exceptions and code errors to the developers.** 
For more details and for information how to disable the error reporting see [Sentry-Plugin Documentation](https://github.com/ioBroker/plugin-sentry#plugin-sentry)! Sentry reporting is used starting with js-controller 3.0.


**If you like it, please consider a donation:**
                                                                          
[![paypal](https://www.paypalobjects.com/en_US/DK/i/btn/btn_donateCC_LG.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=YBAZTEBT9SYC2&source=url) 

## Adapter for controlling your heating system.

Features:
* Control the setpoint temperature levels of all thermostats per schedules
* Configure multiple heating periods for each day and night 
* Supports all kind of thermostats (precondition: it must be available in ioBroker)
* Homematic device autodetection 
* supports multiple profiles
* If there is no direct connection between the thermostat and the actuator, the actuator can be switched directly out of the adapter
* Currently, the actuator is switched off directly when the setpoint temperature is reached. As soon as the setpoint temperature is below the actual temperature, the actuator will be switched on. (To do: implement improved control)
* unlimited number of thermostats, actuators and sonsors per room are supported
* Thermostat, actuator and sensor are automatically detected per room. The function (eg ""heating"") is used for this.
* Rooms can be excluded within the admin interface, if a room contains a thermostat but should not be controlled
* sensor is used to reduce target temperature (e.g. if a window is open); optionally with SensorDelay
* interface to Feiertag-Adapter or any others to detect public holiday. Public holiday can be a normal day or like sundays. (admin setting)
* manual temperature override for a certain time
* predefined heating period
* take over changes from thermostat (optional)
* visualization from [Pittini](https://github.com/Pittini/iobroker-heatingcontrol-vis) is supported. Thank you!

[FAQ](doc/FAQ.md)


## Installation

## Settings
### main
* Function = Function to be used to detect thermostats, actuators and sensors per room. It's one of the sytem enums
* timezone = to be used for cron to adjust cron jobs
* Path to Feiertag - Adapter = if you wnat to use Feiertag-Adapter to dectect automatically public holiday for today then set the path here (e.g. feiertage.0)
* delete all devices when admin opens = should be disabled. Enable it only when you need to delete all room, actuator and sensor settings. A device search will be executed when adapter admin opens
* sensor used = if you have window sensors and you want to decrease target temperature when window is open then enable that option
* actuators used = if you want to control actuators directly from adapter. Just in case there is no direct connection between thermostat and actuator.
* use actuators if no heating period = only valid with actuators. Defines how actuators are set when no heating period is active
* use actuators if no thermostat available = only valid with actuators. If you have rooms without thermostat but with heating actuator you can switche them on or off permanantly


### profile
* Profile Type = three different profile types (Monday - Sunday, or Monday - Friday and Suturday/Sunday or every day) are supported
* number of profiles = if you need more then on profile increase that value. You can then select which profile will be used.
* number of periods = define how many daily sections with different temperature you need. As more you set as more datapoints will be created. Better to use a low value (e.g. 5)
* ""public holiday like sunday = if you want to set target temperatures on public holiday like sunday enable that option. Otherwise public holiday settings are the same as on normal days
* HeatingPeriod = start and end date of heating period. Used to set ""HeatingPeriodActive"" 



### devices
* a list of all rooms. You can disable a room here. 
* press edit button on right hand side to open settings window for thermostats, actuators and sensors for that room

### Edit Room
* here you can verifay and set object ID's for thermostats, actuators and sensors
* you can add manually new thermostats, actuators or sensors. Just press + button. Then you get an empty line which needs to filled up. The Edit-Button opens a list of available devices on the system
* thermostats:
** name, temperature target OID and current temperature OID should be set. 
* actuators
** name and OID for state should be set
* sensors
** name and OID for current state should be set

## datapoints

| DP name             | description                                                                                         |
|---------------------|-----------------------------------------------------------------------------------------------------|
| HeatingPeriodActive | if off, the profiles will not be used                                                               | 
| CurrentProfile      | select current profile (1 based, means profile 1 use datapoints under heatingcontrol.0.Profiles.0 ) | 
| LastProgramRun      | shows last time when adapter run                                                                    | 

### temperature decrease / increase

| DP name           | description                                                | target temperature for relative decrease                                       | target temperature for absolute decrease                      |
|-------------------|------------------------------------------------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------|
| GuestsPresent     | increase temperature because guests wants it warmer        | increase current profile temperature by Profiles.0.room.GuestIncrease          | set target to Profiles.0.room.absolute.GuestIncrease          | 
| PartyNow          | decrease temperature because it's becoming hot'            | decrease current profile temperature by Profiles.0.room.PartyDecrease          | set target to Profiles.0.room.absolute.PartyDecrease          | 
| Present           | we are present, if we are not present decrease temperature | decrease current profile temperature by Profiles.0.room.AbsentDecrease         | set target to Profiles.0.room.absolute.AbsentDecrease         | 
| VacationAbsent    | we are absent, so decrease also on weekend                 | decrease current profile temperature by Profiles.0.room.VacationAbsentDecrease | set target to Profiles.0.room.absolute.VacationAbsentDecrease | 


* in both szenarious only one lowering is used (in previous version of adapter more then one degreases could be used)
* in absolute degrease szenario only target values not equal 0°C are used. If you do not need any lowering for a certain room then keep decrease-values at 0°C

### no heating period
there are three options

* fix Temperature per room
if this option is selected, a new datapoint in object tree appears for every room. Here you can set a fix target temperature which is set when heating period is not active.
* fix Temperature for all rooms
with this option you can use one target temperature for every room when heating period is not active
* nothing
with this option nothing will be sent to thermostat if no heating period is active. Target temperature remain from last taget when heating period still was active.
In that case and if you use actuators from the adapter then you have the possibilty to define how actuators should be set (off, on, or leave it as it is) 


## others

* HolidayPresent / PublicHolidyToday
If you enable ""Holiday present like sunday"" or ""public holiday like sunday"" in admin, the profile for sunday is used
when adapter is informed that today is a public holiday or you are at home in holiday.

### window open
if ""use sensors"" is active and sensor(s) for a room is / are configured then

* decrease current profile temperature when window is open (true) by Profiles.0.room.WindowOpenDecrease if relative decrease is configured
* set target to Profiles.0.room.absolute.WindowOpenDecrease when window is open (true) if  absolute decrease is configured

optionally a delay can be used. If window is opened only for a short time sensor delay can avoid from reduce and back to normal in very short times.

## ical support
you can use your calendar or any other datapoint to change datapoints in adapter.
Just configure events from ical or other datapoints in admin. Supported are

| datapoint                           | description
|-------------------------------------|----------------------------------------------------------------------------
|heatingcontrol.0.Present             | set it to true (in case of boolean) or to a number higher then limit (in case of number)
|heatingcontrol.0.HolidayPresent      | set it to true when you at home in your holiday
|heatingcontrol.0.VacationAbsent      | set it to true when you not at home in your holiday
|heatingcontrol.0.GuestsPresent       | set it to true (in case of boolean) or to a number higher then limit (in case of number)
|heatingcontrol.0.PartyNow            | set it to true (in case of boolean) or to a number higher then limit (in case of number)

hint: with number datapoints you could count how many people are in the house and then decide, e.g. we have enough for a party...

## use changes from thermostat

Many user asked for an option to take over changes from thermostat into adapter. Now a four options are implemented:

| option                   | description                                                
|--------------------------|---------------------------------------------------------------------------------------
| no                       | changes from thermostat are ignored
| as override              | changes from thermostat are taken as override; override time must be set in advance in heatingcontrol.0.Rooms.RoomName.TemperaturOverrideTime
|                          | if override time is not set, than override is not executed
| as new profile setting   | changes from thermostat are taken as target temperature for current profile period
| until next profile point | changes from thermostat are taken as target temperature until next profile point. This is a manual mode, so only Window sensors are used. All other 
|                          | increases / decreases are ignored. There is a datapoint in every room to disable manual mode before reaching next profile point.

## extend override when temperature is changed
The standard behavior for override is, when you change temperature the override time is not changed. E.g if you start override for 20 minutes with 25°C
and you change to 28°C after 15 minutes then 28°C is only used for the last 5 minutes. With that option you restart override whenever you change override temperature.
In example above 28°C would then be used for 20 minutes which leads to 15 minutes 25°C and 20 minutes 28°C 

## override mode
There are two mode adjustable in admin for all rooms.
* timer controlled
this is the wellknown function, which uses a temperature and a duration. The given temperature is used for the duration and then temperature target will set back to value in auto mode
* until next profile point
this is a new function. Here we can use a temperature override until next profile point. The duration will be ignored but must be non-zero!


## Thermostat handles ""window is open""
Some thermostats can handle ""window is open"" by itself. In those cases a direct connection between window sensor and thermostat is configured and thermostat reduces
target temperature by itslef when a window is opened.
In combination with option ""use of changes from thermostat""  / ""until next profil point"" will lead this to an unexpected manual state. In this situation the reduced 
temperature  would be used until next profil point.
But the adpater can handle this behavior. You must enable option ""Thermostat handles 'Window is Open'"" and you can configure window sensors also in adapter.
When window is opened the adapter waits for max. 3 seconds for new target temperature from thermostat. If it receives a new target temperature in that time it will be used
as a reduced absolut temperature. Status will then be ""auto window open"". As soon as the window is closed the status goes back to auto and thermostat sets back the
original target temperature
**Attention** do not use Sensor Open Delay in that case. If you use it, the Window open event appears after target temperature received from thermostat. This ends up in 
manual state. 


## Copy period and copy profile
``
heatingcontrol.0.Profiles.1.CopyProfile
heatingcontrol.0.Profiles.1.Room.CopyProfile
``

and

``
heatingcontrol.0.Profiles.1.Küche.Fri.CopyPeriods
``

CopyProfile copies the entire content of the profile where the button is pressed to the next profile. In the above example, the button is in profile 1. The button copies everything from profile 1 to profile 2.
If you want to copy only one room, use the button in a certain room.

The CopyPeriods are available per day or Mon-Fri per room. This copies the periods to the next section. In the above example, the CopyPeriods copies all periods from Friday in the kitchen room to the periods on Saturday in the kitchen room.
So you can e.g. in the profile ""every day separately"", copy the periods from Monday to Sunday ...

## Issues and Feature Requests
* If you are faced with any bugs or have feature requests for this adapter, please create an issue within the GitHub issue section of the adapter at [github](https://github.com/rg-engineering/ioBroker.heatingcontrol/issues). Any feedback is appreciated and will help to improve this adapter.

## known issues

### Adapter with Homematic IP Fußbodenheizungsaktor HmIP-FAL230-C10 – 10fach, 230 V 
It seems that HmIP-FAL230-C10 can not be used directly as an actuator in combination with that adapter. If you use HmIP-FAL230-C10 together with Homematic thermostats it should work.
see also [Forum](https://forum.iobroker.net/topic/22579/test-adapter-heatingcontrol-v1-0-x/1553)

### Window-open function of HM thermostats
HM thermostats have an open window function in two variants. On the one hand as temperature drop detection and on the other hand in connection with a window contact.
This function causes the adapter to switch to manual mode when the window is opened. Ideally, this function should be deactivated so as not to interfere with the functionality of the adapter.
If thermostat use information from window sensor then ""thermostat handles window open"" option should be enabled.

When the adapter crashes or an other Code error happens, this error message that also appears in the ioBroker log is submitted to Sentry.  All of this helps me to provide error free adapters that basically never crashs.

## Changelog

### 2.3.2 (2021-04-18)
* (ericsboro) vis translation to russian
* (René) see issue #231: bug fix detect heating period

### 2.3.1 (2021-04-05)
* (René) some optimisations for vis translation

### 2.3.0 (2021-03-20)
* (René) see issue #187: show remaining override timeConverter
* (René) see issue #225: support different languages for vis
* (René) see issue #223: new overide mode ""until next profile point""
* (René) bug fix to calculate average for temperatur offset

### 2.2.0 (2021-02-15)
* (René) see issue #146: different type of window sensor and also adjustable comparative value
* (René) see issue #110: optionally every room can be set to ""no heating"" with separate datapoint
* (René) see issue #185: maintenance function: Delete all unused datapoints (e.g. profiles) is implemented now for admin
* (René) see issue #185: maintenance function: Delete all devices related to a room, when a room is deleted is implemented now for admin
* (René) see issue #207: copy buttons for vis added
* (René) see issue #219: bug fix: DecreaseValues and ProfilName are copied in CopyProfile now

### 2.1.1 (2021-02-08)
* (René) bug fix Temperatur Offset: invert sign of TemperatureOffset 

### 2.1.0 (2021-01-31)
* (René) see issue #198: add name to profile as a datapoint, used to be shown in visualisation
* (René) see issue #194: limit and step width for increase / decrease values adjustable in admin 
* (René) see issue #182: Temperatur Offset
* (René) see issue #212: ActiveTimeSlot inkorrekt for vis

### 2.0.4 (2021-01-28)
* (René) bug fix for issue #213: Warnung ""!!! Statechange not handled""

### 2.0.3 (2021-01-24)
* (René) bug fix for issue #211: endless change of temperatures

### 2.0.2 (2021-01-22)
* (René) bug fix for issue #208: exception ""undefined is not a valid state value""
* (René) bug fix for issue #209: Not all open windows are recognized

### 2.0.1 (2021-01-19)
* (René) bug fix for issue #204: do not take over reduced temperature in manual mode
* (René) bug fix for issue #203: Warnings ""has no existing object, this might lead to an error""
* (René) bug fix for issue #205: override start

### 2.0.0 (2021-01-16)
* (René) internal refactoring

**ATTENTION: breaking changes !!!!**
* complete internal refactoring (new source files, internal data structures, code review, ...)
* **Periods and Profils count from 1 instead 0**
* ChangesFromThermostat adjustable per room is removed
* recalculation of room temperature is performed only for the room where necessary (in previous versions all rooms were recalculated and new value transmitted)
* SensorOpenDelay / SensorCloseDelay renamed
* ResetButton to disable manual mode (and go back to auto)
* status log per room
* complete profile can be saved and loaded in admin
* copy profile (complete or for a single room) and periods (for a certain profile and room) by button supported
* datapoint selector for external datapoints added in admin
* autodectection for thermostats, sensors and actuators completely overworked
* room detection overworked
* limits and step widh for profil temperatures adjustable in admin for Pittini vis
* simple window status view (in html) for Pittini vis added
* room state as simple html table for vis added
* (optionally) extend override when temperature is changed; in standard new temperature is set, but timer is not changed
* (optionally) Thermostat handles ""window is open""
* issues in github: 
	* #161 Profil springt zur angegebenen Zeit nicht um
	* #153 cron Probleme beim ändern eines Profils mittels Javascript
	* #152 Fenstererkennung im manuellen Modus
	* #148 Bei Änderung vom Thermostat bis zum nächsten Profilpunkt müssen Sensoren berücksichtigt werden


### 1.1.2 (2020-11-11)
* (René) bug fix: activate actors after temperatur change

### 1.1.0 (2020-11-01)
* (René) see issue #149: bug fix: calculate current period in case we are still in last period from yesterday

### 1.1.0 (2020-10-20)
* (René) see issue #132: timer before on and off for actuators 
* (René) see issue #143: additional checks to avoid unneccessary override 
* (René) see issue #140: use guests present and party now DP's also as counter like present (as a option); add adjustable counter limit for present, party now and guest present
* (René) see issue #145: avoid reset of target temperatur by profile settings in option ""until next profil point"" when set by thermostat 

### 1.0.0 (2020-10-09)
* (matida538) added better Handling of strings in HandleThermostat (convert to Number, instead of warn) (e.g. fhem connector for fht80)
* (matida538) changed Check4ValidTemperature to convert strings to Number instead of Int (else we lose information e.g. 18.5 will be 18)
* (René) some smaller code optimisations

### 0.6.0 (2020-09-15)
* (René) see issue #123: use window open / close delay only when window state changed
* (René) see issue #122: better log for different type warning
* (René) see issue #120: override from thermostat only if it's different to current settings
* (René) see issue #126: TestThermostat should not be checked for correct configuration
* (René) see issue #124: vis from Pittini: Image for open / closed window adjustabel (as an option, if nothing is configured the original will be used)
* (René) see issue #127: use value from thermostat until next profile point 
* (René) see issue #128: try to convert string data to number

### 0.5.7 (2020-07-07)
* (René) see issue #116: get MinimumTemperature for vis only if enabled

### 0.5.6 (2020-06-14)
* (René) see issue #113: re-order of rooms added
* (René) see issue #112: bug fix ""Fensterübersicht""

### 0.5.4 (2020-06-04)
* (René) bug fix: HeatingControlVis avoid exceptions like ""Cannot read property 'val' of null""

### 0.5.3 (2020-06-03)
* (René) bug fix: new temperatures set when current profile is changed
* (René) refactoring HeatingControlVis to avoid exceptions like ""Cannot read property 'val' of null""

### 0.5.2 (2020-05-25)
* (René) bug fix: log a warning if actors are configured but UseActors are off

### 0.5.1 (2020-05-22)
* (René) log a warning if actors are configured but UseActors are off
* (René) sentry added
* (René) some hints in admin

### 0.5.0 (2020-05-03)
* (René) see issue #101: sensor close delay added (similar to already existing sensor open delay)
* (René) see issue #103: date/time format string corrected for vis
* (René) see issue #104: bug fix to take over changes from vis
* (René) see issue #102: bug fix change current time period to be shown on vis

### 0.4.0 (2020-05-02)
* (René) see issue #70: use changes from thermostat
* (René) see issue #91 bug fix: if the same sensor is configured for more than one room thermostat target temperature will be set for all configured rooms
* (René) script from Pittini integrated to support his visualization [Pittini](https://github.com/Pittini/iobroker-heatingcontrol-vis) 
* (Dutchman) some refactoring 

### 0.3.19 (2020-03-15)
* (René) create correct cron job for sunday if profile type ""every day"" is used
* (René) see issue #87: change type of time data points to string
* (René) see issue #87: set correct roles for data points
* (René) see issue #84: set default value for minimum temperature
* (René) see issue #86: all ""float"" converted to ""number""""

### 0.3.18 (2020-03-08)
* (René) fix issues reported by adapter checker

### 0.3.17 (2020-03-01)
* (René) check datapoint configuration: if datapoint points to itself then error messages
* (René) support of new vis see issue  #76
* (Rene) thermostat mode if no heating period

### 0.3.16 (2020-02-09)
* (René) deccrease/increase-handling also when Override is active (see issue #72)
* (René) priority handling for temperature increase / decrease overworked (use only values not equal zero)

### 0.3.15 (2020-01-18)
* (René) bug fix: avoid exception when go to override if MinTemperature-check is active

### 0.3.14 (2020-01-12)
* (René) format conversion for temperatures in string to number
* (René) ack for MinTemperature

### 0.3.13 (2019-12-28)
* (René) bugfix create cron jobs for profile type 3 (daily)

### 0.3.12 (2019-12-27)
* (René) bugfix exception in CheckTemperatureChange [ReferenceError: RoomState is not defined] 

### 0.3.11 (2019-12-27)
* (René) option: minimum temperature per room
* (René) bugfix exception in CheckTemperatureChange [ReferenceError: PublicHolidyToday is not defined] 

### 0.3.10 (2019-12-26)
* (René) see issue #54: stop override with OverrideTemperature =0
* (René) new priority for lowering reasons
* (René) handling of actuators without thermostat
* (René) see issue #66: handle lowering in time between 0:00 and first period
* (René) see issue #64: import of configuration fixed

### 0.3.9 (2019-12-14)
* (René) see issue #60: sensor delay
* (René) see issue #57: support of the same sensor for different rooms
* (René) bug fix: ""AbsentDecrease not defined"" for relative lowering

### 0.3.8 (2019-12-12)
* (René) see issue #59: TemperaturOverride: acceppt hh:mm and hh:mm:ss
* (René) PartyNow support by iCal 
* (René) if useActuators: show how many actuators are active (as a datapoint)

### 0.3.7 (2019-11-29)
Attention: some changes in datapoints!!
* (René) see issue  #53: moved datapoints for relative lowering into ""relative""
* (René) new datapoint to show lowering decrease mode (heatingcontrol.0.TemperatureDecreaseMode)
* (René) guest present as interface to ical
* (René) see issue #52: support radar adapter
* (René) all external states checked when adapter starts

### 0.3.6 (2019-11-23)
Attention: some changes in datapoints!!
* (René) moved some datapoints from ""profile"" to ""rooms""
* (René) see issue #50: support absolute and relative decrease of target temperature
* (René) do not check all rooms everytime: when data only for one room changed then check only one room
* (René) only one event is used to lower temperature
* (René) add interface to ical (path to vacation and path to holiday present datapoints)
* (René) support of more then one instance

### 0.3.4 (2019-11-09)
* (René) bug fix in data point name

### 0.3.3 (2019-11-08)
Attention: some changes in datapoints!!
* (René) in admin: new buttons to add search new rooms
* (René) bug fix: in profil type Mo-Fr / Sa- So period order check failed  
* (René) see issue #38: new datapoint for WindowIsOpen
* (René) change datapoint ""CurrentTimePeriod"" to ""CurrentTimePeriodFull"", ""CurrentTimePeriod"" and ""CurrentTimePeriodTime""
* (René) bugfix datapoint name ""Sa-Su""
* (René) see issue #16: new datapoint ""state"" per room to show reason for temperatur change 
* (René) change format of LastProgramRun date / time

### 0.3.2 (2019-11-01)
* (René) try to convert temperature to number if NaN
* (René) see issue #33: check for heating period when adapter starts
* (René) fix a problem in subscription function when room can not be found 

### 0.3.1 (2019-10-31)
* (René) see issue #42 and #44: check all sensors per room and set state when adapter starts
* (René) show message in admin when adapter is not online
* (René) pre-define devicelist; add dummy thermostat, if list is empty

### 0.3.0 (2019-10-27)
* (René) see issue #20 + #24: start and end of heating period is configurable in admin 
* (René) see issue #24: use external data point to set internal ""present"" data point 
* (René) see issue #15: manual temperatur override
* (René) see issue #35: delete of devices
* (René) reset DeleteAll at next admin start 

### 0.2.3 (2019-09-20)
* (René) see issue #19: handling of enums created in iobroker admin fixed
* (René) see issue #13: check order of periods; if order is wrong (next time is smaller than previous) then time si not used for cron and a warning appears in log
* (René) see issue #21: check temperatures after changing of period settings (e.g. time)
* (René) see issue #25: select OID for target and current of thermostat in admin overworked
* (René) change datapoint type from bool to boolean

### 0.2.2 (2019-09-13)
* (René) see issue #14: description of datapoint time changed ('from' instead 'until')
* (René) see issue #12: unnecessary warnings removed
* (René) see issue #17: seconds removed from time list
* (René) datepoint change handling reworked
* (René) see issue #18: take over values from external PublicHoliday-datapoint

### 0.2.1 (2019-09-08)
* (René) bug fixes in actuator handling

### 0.2.0 (2019-09-05)
* (René) path to Feiertag-Adapter can also include a complete datapoint path 

### 0.1.0 (2019-08-25)
* (René) redesign of data structure
	- more then one actuator, sensor and thermostat per room
	- three different profile types
	- manual configuration of devices (if device is not detected automatically)
	- interface to Feiertag-Adapter
	- public holiday as normal day or like sunday (setting in admin)
	- window sensor support. Reduce target temperature when window is open
	- !!ATTENTION!! data structure/objects has been changed. You need to update your visualisation settings

### 0.0.5 (2019-07-08)
* (René) support for max! thermostats

### 0.0.4 (2019-06-23)
* (René) debugging

### 0.0.3 (2019-06-02)
* (René) ready to publish

### 0.0.2 (2019-05-19)
* (René) actuator handling added

### 0.0.1 (2019-04-27)
* (René) initial release

## License

Copyright (C) <2019-2021>  <info@rg-engineering.eu>

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"
274,webanno/webanno,Java,"# WebAnno

[![Build Status](https://zoidberg.ukp.informatik.tu-darmstadt.de:443/jenkins/job/WebAnno%20(GitHub)%20(master)/badge/icon)](https://zoidberg.ukp.informatik.tu-darmstadt.de:443/jenkins/job/WebAnno%20(GitHub)%20(master)/) [![Join the chat at https://gitter.im/webanno/webanno](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/webanno/webanno?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

WebAnno is a general purpose web-based annotation tool for a wide range of linguistic annotations.

For more information, visit the [WebAnno website](https://webanno.github.io/webanno/).
"
275,sqlmapproject/udfhack,C,"# Database takeover user-defined functions for MySQL and PostgreSQL

Files in this folder can be compiled as shared libraries. These define some user-defined functions (UDF) for MySQL and PostgreSQL. They are licensed under the terms of the GNU Lesser General Public License and their compiled versions are available on the official sqlmap repository, https://github.com/sqlmapproject/sqlmap.
"
276,mixu/gr,JavaScript,"## Features

- Tag all the things! `gr @work foo` will run the command `foo` in all the paths tagged `@work`.
- Auto-discovery of git repositories for easy setup and tag management.
- `gr` does not reinvent any git operations: instead, it passes through and runs any unknown commands. All your git-fu will still work! e.g. `gr @work git fetch` is the same as running `git fetch` in each all the paths tagged `@work`.
- Built-in commands for common pain points:
  - `status` for one-line summaries of repos (modified, behind/ahead, tags)
- Extensible via plugins and middleware: REST API/[Connect](http://senchalabs.github.com/connect)-style request handlers (`function(req, res, next) { ... }`

-----

## Changelog

- Looking for more committers! Let me know if you're interested; gr currently meets my fairly limited needs but I know it can become even more useful given a larger core team and/or a stronger vision around how it can support usage in a team working on multiple repos.
- `0.5.5`: Submodule directory detection fixes, thanks @bimlas! Better behavior when the EDITOR environment variable is not set, thanks @petercoulton!
- `0.5.4`: Added `gr export` and `gr import` for sharing `.grconfig`, thanks @timja-kainos! Added current branch to status, thanks @n1ywb!
- `0.5.3`: minor improvements to internals, thanks @timja-kainos!
- `0.5.2`: `gr` now handles git submodules better, thanks @cojomojo! `gr` also prints out tags with newlines which makes for a more readable output, thanks @nwinkler!
- `0.5.1`: `gr discover` now handles paths outside the home directory as expected, thanks @farmerchris!
- `0.5.0`: `gr discover` is now an alias for `gr tag discover`; `gr discover` now accepts path arguments, shows progress during a scan and only scans five levels deep by default. Thanks @coderjoe for the patches!
- `0.4.1`: `gr status` now only invokes git once per directory, thanks @coderjoe!
- `0.4.0`: Added several usability improvements and bug fixes, courtesy of @nichtich (better handling of missing directories, support for simple paths). Added a fix that improves errors related to directory permissions, courtesy of @pnxs.
- `0.3.0`: Switched from `#foo` to `@foo` for tags; while the `#foo` syntax looks cool, most shells will treat it as a comment unless the tag is surrounded by quotes. Looking back at the design, I'd rather go for usability over pretty looking commands. Updated the documentation to match this change.

## Example

`gr` works by tagging directories with tags:

    gr +@work ~/mnt/gluejs ~/mnt/microee

After this, you can run any commands on the tagged directories simply by prefixing them with `gr @work`. For example:

    gr @work status

Outputs (actual output is colorized):

    ~/mnt/gluejs           2 modified [ahead 2]      @work
    ~/mnt/microee          Clean                     @work

E.g. path, modified, ahead/behind remote, tags. Alternatively, you can use plain git commands for a more verbose report:

    gr @work git status -sb

Outputs:

    in ~/mnt/gluejs

    ## glue2
     M lib/runner/package-commonjs/index.js
     M index.js

    in ~/mnt/microee

    ## master

`gr` doesn't do any command rewriting, or introduce any new commands - I like `git` as it is.

## Getting started

First, [install Node.js](http://nodejs.org/download/). Node.js adds the `node` and the `npm` command. On Ubuntu you need the `nodejs-legacy` package.

Next, to install `gr` (the name was already taken on npm):

    npm install -g git-run

You may need to prefix this with `sudo`.

### Setting up tags
Use the auto-discovery feature to set up tags quickly:

    gr tag discover

By default auto-discovery searches all paths under your home directory. `gr discover` is also an alias for `gr tag discover` (since v0.5.0).

Note that discover only scans up to five levels deep by default (since v0.5.0); if you need to scan even deeper in your path tree you should just pass in an explicit set of starting points to `gr tag discover`.

If you'd prefer, you can specify a directory under which auto-discovery should search (since in v0.5.0):

    gr tag discover /mnt/external/projects

Once auto-discovery completes, it will generate a list, and open it in your default console editor.

It will look like this:

    # Found the following directories with `.git` directories.
    #
    # Please add any tags you want by adding one or more `@tag`
    # entries after the path name.
    # For example:
    #   ~/foo @work @play
    # will tag ~/foo with @work and @play.
    #
    ~/foo
    ~/bar/baz

Add tags after each path, save the file, and exit.

Your tags are now set up!

Verify with `gr status` or `gr tag list`. Use `gr @work status` or `gr @work ls -lah` to see how commands are executed. (`status` is a built-in command; `ls -lah` is not, so it is run in each of the paths.)

You can run auto-discovery multiple times. It makes tag-related bulk changes quite easy.

## Tab completion

To add tab completion:

- open your `~/.zshrc` or `~/.bashrc` (`~/.bash_profile` on OS X)
- add the line `. <(gr completion)` at the end of the file
- then, to apply this change to your current session, run `source ~/.zshrc` (or `source ~/.bashrc` or `source ~/bash_profile`)

Now, when you type `gr <tab>`, you'll see the list tags you've created. If you notice any bugs, let me know via an issue.

## How I use gr

Some examples:

COMMAND                                                         | TASK
-------                                                         | ----
`gr @work git fetch` and then `gr @work status`                 | Update all my work repos. This fetches the newest information from the remote, and then prints a one-line-at-a-time summary.
`gr @work git diff` or `gr @work git diff --cached`             | See diffs
`gr @work jshint . --exclude=**/node_modules`                   | Run `jshint`
`gr @write make`                                                | Rebuild all my writing via `make`
`gr @work npm ls`                                               | List install npm modules
`gr @work git --no-pager log --decorate --graph --oneline -n 3` | Print a graph-like log


Of course, I don't actually type these out; I'm using `zsh` aliases instead. `grd` is for diff, `grdc` is for `diff --cached`; `grl` is for the log. For example, in `.zshrc`:

    alias grs=""gr status""
    alias grl=""gr git --no-pager log --decorate --graph --oneline -n 3""

You can set up similar aliases for `bash`; Google is your friend here.

## Usage

Usage:

    gr <options> <targets> <cmd>

## Options

Currently, there is just one option: `--json`, which switched to a machine-readable output and is used for integration tests.

## Targets

Targets can be *paths* or *tags*. For example:

    gr ~/foo ~/bar status
    gr @work ls -lah

- **Path** targets should be directories.
- **Tags** refer to sets of directories. They managed using the `tag` built-in.

If no targets are given, then all tagged paths are used. For example, `gr status` will report the status of all repositories.

## Tagging

Short form:

    @tag            List directories associated with ""tag""
    @tag <cmd>      Run a command in the directories associated with ""tag""
    -t <tag> <cmd>  Run a command in the directories associated with ""tag""
    +@tag           Add a tag to the current directory
    -@tag           Remove a tag from the current directory
    +@tag <path>    Add a tag to <path>
    -@tag <path>    Remove a tag from <path>

Long form:

    tag add <tag>   Alternative to +@tag
    tag rm <tag>    Alternative to -@tag
    tag add <t> <p> Alternative to +@tag <path>
    tag rm <t> <p>  Alternative to -@tag <path>
    tag list        List all tags (default action)
    tag discover    Auto-discover git paths under ~/

Example:

    gr +@work ~/bar

Internally, the tags are stored in the config file `~/.grconfig.json`. For example, the tag `@books` for the path `/home/m/mnt/css-book` would be stored as:

    {
     ""tags"": {
      ""books"": [
        ""/home/m/mnt/css-book""
      ]
    }
For some use cases, it may be easier to just edit this file rather than use the commands `tag add` and `tag rm`.

## Commands

The command can be either one of the built-in commands, or a shell command. For example:

    gr @work status
    gr ~/foo ~/bar ls -lah

To explicitly set the command, use `--`:

    gr ~/foo -- ~/bar.sh
    gr @work -- git remote -v

Tags can also be specified more explicitly. For example `gr -t work -t play` is the same as `gr @work @play`.

## Built-in commands:

    gr tag ..
      add <t>         Add a tag to the current directory
      rm <t>          Remove a tag from the current directory
      add <t> <path>  Add a tag to <path>
      rm <t> <path>   Remove a tag from <path>

    gr tag discover <paths> Auto-discover git paths under  the list of <paths>
                           (If omitted, <paths> defaults to ~/)

    gr tag list         List all known repositories and their tags

    gr list        List all known repositories and their tags

    gr status       Displays the (git) status of the selected directories.
    gr status -v    Runs ""git status -sb"" for a more verbose status.

    gr config ..
      get <k>       Get a config key (can also be a path, e.g. ""tags.foo"")
      set <k> <v>   Set a config key (overwrites existing value)
      add <k> <v>   Add a value to a config key (appends rather than overwriting)
      rm <k> <v>    Remove a value from a config key (if it exists)
      list          List all configuration (default action)

    gr help        Show this help
    gr version     Version info

## Plugins

TODO:

- [`bootstrap`](#TODO): bootstraps a set of repositories from a config file.

## Installing plugins

Generally speaking, you need to do two things:

1. install the plugin globally via npm: `npm install -g foo`
2. configure `gr` to use the plugin: `gr config add plugins foo`

The new commands should now be available.

## Writing plugins

Plugins are functions which are invoked once for each repository path specified by the user. This makes it easier to write plugins, since they do not need to handle executing against multiple repository paths explicitly.

Plugins are treated a bit like a REST API: they are defined as ""routes"" on the `gr` object.

Each plugin consists of an index file which is loaded when gr is started, and which should add new ""routes"":

    module.exports = function(gr) {
      // set up new commands on the gr object
      gr.use(['foo', 'help'], function(req, res, next) {
        console.log('Hello world');
        req.exit(); // stop processing
      });
      gr.use('foo', function(req, res, next) {
        console.log(req.argv, req.path);
        req.done(); // can be called multiple times
      });
    };

Of course, `req` and `res` in the handlers are not HTTP requests, but rather objects representing the target directory (a regular object) and `process.stdout`.

Each ""route"" is called multiple times, each with one path. Thus, assuming `@work` matches two paths, `gr @work status` is translated into multiple indvidual function calls; one for each directory/repository tagged `@work`.

      status({ path: '/home/m/foo', argv: ... }, process.stdout, next);
      status({ path: '/home/m/bar', argv: ... }, process.stdout, next);

There are three ways to stop processing:

1. Call `res.done()`. This means that the command should be called again for the next path. This is useful for processing commands that target directories.
2. Call `res.exit()`. This means that the command is complete, and `gr` should exit. For example, we don't want to show a help text multiple times if the user calls `gr @work help`.
3. Call `next`. This means that the current handler does not want to handle the current request. Similar to how Connect works, this is mostly used for writing middleware or falling back on a different action.

The `req` object describes the current request.

- `req.argv`: the command line arguments passed to the command, excluding ones that have already matched. For example: given `app.use(['foo', 'add'], ...)` and `gr foo add bar`, `argv = [ 'bar' ]`.
- `req.config`: the configuration object used by `gr`; allows you to read and write configuration values (see the code for details).
- `req.path`: the full path to the repository directory for this call
- `req.gr`: the instance of `gr` (see `index.js`)
- `req.format`: The desired output format, either `human` or `json`.

The `res` object controls

- `res.done`: Call this function when you have completed processing the task.

The `next` function is used if you decide not to handle the current request. Calling `next` will make the next matching request handler run. If you encounter an error, call `next(err)` to output the error.

## Writing middleware

Middleware are functions that extract additional metadata.

- `req.git.remotes`: TODO - a hash of remotes (for example: `{ origin: 'git...'}`). Extracted via the git middleware.

## A list of plugin ideas

Here are some plugin ideas:

- extend auto-discovery beyond git
- run tests
- run jshint / gjslint (only modified?)
- do npm / `package.json` linting
- generate docs
- fetch and report status as one action
- report `npm` versions (and whether the version of the package on npm is up to date)
- generate a changelog (between tagged versions)
- check that npm modules are up to date
- run `npm link` on all modules
- generate a list of authors
- generate a list of licences
- `xargs` compatibility
- Ability to confirm each command (to make it possible to skip)
- Ability expose other statuses (e.g., `npm outdated`)

## Make your plugin searchable

If you write a plugin, make sure to add the `gr` keyword to (in [`package.json`](https://npmjs.org/doc/json.html#keywords)). This makes it easy to find plugins by [searching `npm` by tag](https://npmjs.org/browse/keyword/gr).

Also, file a PR against this README if you want to have your plugin listed here.

## Status matching idea

(This is just a random idea) Using ""meta-tags"" to target commands based on `git ls-files`.

- `clean`: Clean working directory - in other words, no tracked files are modified; no untracked files exist.
- `untracked`: Has files that are not tracked (but that have not been added to tracking)
- `modified`: Has files that are tracked and modified (but that have not been staged)
- `deleted`: Has files that are tracked and deleted
- staged: Has files that are staged for commit (but that have not been committed)
- `unmerged`: Has files that have not been merged
- `unclean`: Does not have a clean working directory.

For example: `gr @clean git fetch`

## Inspired by

- http://manpages.ubuntu.com/manpages/jaunty/man1/mr.1.html
- https://github.com/fabioz/mu-repo
- http://source.android.com/source/developing.html
"
277,akeneo/pim-community-dev,PHP,"# Akeneo PIM Development Repository
Welcome to Akeneo PIM Product.

This repository is used to develop the Akeneo PIM product.

Practically, it means the Akeneo PIM source code is present in the src/ directory.

**If you want to create a new PIM project based on Akeneo PIM, please use https://www.github.com/akeneo/pim-community-standard**

If you want to contribute to the Akeneo PIM (and we will be pleased if you do!), you can fork this repository and submit a pull request.

Scrutinizer | Crowdin
----------- | -------
[![Scrutinizer Quality Score](https://scrutinizer-ci.com/g/akeneo/pim-community-dev/badges/quality-score.png?s=05ef3d5d2bbfae2f9a659060b21711d275f0c1ff)](https://scrutinizer-ci.com/g/akeneo/pim-community-dev/) | [![Crowdin](https://d322cqt584bo4o.cloudfront.net/akeneo/localized.svg)](https://crowdin.com/project/akeneo)

## Application Technical Information

The following documentation is designed for both clients and partners and provides all technical information required to define required server(s) to run Akeneo PIM application and check that end users workstation is compatible with Akeneo PIM application:
https://docs.akeneo.com/master/install_pim/manual/system_requirements/system_requirements.html

## Installation instructions

To install Akeneo PIM for a PIM project or for evaluation, please follow:
https://docs.akeneo.com/master/install_pim/index.html

## Upgrade instructions

To upgrade Akeneo PIM to a newer version, please follow:
https://docs.akeneo.com/master/migrate_pim/index.html

## Testing instructions

To run the tests of Akeneo PIM, please follow:
https://github.com/akeneo/pim-community-dev/blob/master/internal_doc/tests/running_the_tests.md
"
278,jawi/ols,Java,"# OpenBench LogicSniffer

![OpenBench LogicSniffer](http://s1.directupload.net/images/140913/bca9sg2g.png ""Screenshot of ols"")

This is the public [Git][git] repository for the OpenBench LogicSniffer (*ols*
for short) written by J.W. Janssen. It is an alternative client for an open
source [logic analyzer](https://en.wikipedia.org/wiki/Logic_analyzer) called
the [Open Bench Logic Sniffer](http://dangerousprototypes.com/docs/Open_Bench_Logic_Sniffer).
Refer to this [page](https://lxtreme.nl/projects/ols/) for more information about the
project.

## FEATURES

The alternative Java client provides the following features:

- Cross platform: The client runs on Mac OS X (32/64-bit), Windows (32/64 bit),
  Linux (32/64 bit) and Solaris (32 bit)

- Simple installation: No longer fiddling with the serial libraries (RXTX) in
  order to get the client up and running. The client embeds the suitable serial
  libraries for several operating systems

- Pluggable: Adding new functionality on the fly is possible and as easy as
  copying files to a single directory

- Looks and feels good: The client has a good look and feel, aiming at being as
  usable as possible and adhering to the human interface guidelines of the
  platform it is running on.

## DOCUMENTATION

All documentation is maintained in the [wiki](https://github.com/jawi/ols/wiki).

## COMPILING THE SOURCES

In case you are interested in cloning this repository and compile it for
yourself, you should do the following:

  $ git clone http://github.com/jawi/ols.git

For compiling the sources, you need to have at least a valid JDK (1.6+) and
[Maven][maven] installed. For developing, I recommend
[Eclipse][eclipse] as development environment.

  $ cd ols/
  $ mvn clean install

After this, you should find the latest binary ZIP or tarball in
`ols.distribution/target`.

## DEVELOPING FOR OLS

Developing for ols can be done with any *modern* IDE, like [Eclipse][eclipse],
[Netbeans][netbeans], or even [Emacs][emacs]. Keep in mind that IDE-specific
stuff is not committed to the repository as this would clutter it unnecessarily.

Keep in mind that your IDE should provide support for [Maven][maven], otherwise
it most probably will not compile out of the box. In addition, you probably
want support for [Git][git] in your IDE as well. To create the needed project
files for [Eclipse][eclipse], for example, you can use the following
[Maven][maven] command:

  $ mvn eclipse:eclipse

There are similar commands for other IDEs. See the [Maven][maven] site for more
details on this.

Some notes for [Eclipe][eclipse]: After having imported the projects into your
workspace, you might need to enable [Maven][maven] support by hand. Simply
select all projects, right click on them and choose
`Enable Dependency Management` from the [Maven][maven] menu should be
sufficient. For running the OLS client in [Eclipse][eclipse], you can make use
of the launch configurations found in the `eclipse/` subdirectory of the OLS
repository. The code formatting rules and cleanup rules can be found there, too.

## CONTRIBUTIONS

Drop me a line if you want to contribute code to the OLS repository. If needed
I can give you write-access to the GitHub or apply your patch directly.

## CONTACT

You can reach me at: `j dot w dot janssen at lxtreme.nl`

## DONATIONS

You can support and encourage further development of this project through the
following means:

[![Flattr This!](http://api.flattr.com/button/flattr-badge-large.png ""Flattr This!"")](https://flattr.com/thing/61272/OpenBench-LogicSniffer-alternative-Java-Client)

[![Bountysource](https://d2bbtvgnhux6eq.cloudfront.net/assets/Bountysource-green-712770df4397a3bc6f5b56b90402763c.png ""Bountysource logo"")](https://www.bountysource.com/trackers/315759-jawi-ols)

## LICENSE

[![GNU GPLv2](https://www.gnu.org/graphics/heckert_gnu.small.png ""GNU GPLv2"")](https://www.gnu.org/licenses/gpl-2.0.html)

    This program is free software; you can redistribute it and/or
    modify it under the terms of the GNU General Public License
    as published by the Free Software Foundation; either version 2
    of the License, or (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

[eclipse]: https://www.eclipse.org/
[maven]: https://maven.apache.org/
[netbeans]: https://netbeans.org/
[emacs]: https://www.gnu.org/software/emacs/
[git]: http://git-scm.com/

"
279,nullism/clickingbad,JavaScript,"Clicking Bad
============

Official Clicking Bad Repository

About
=====

System Requirements
-------------------

### Supported browsers

* IE9 & 10
* FF20-24
* Chrome (all recent)
* Android

Contributing
============

Code
----

### Style

* 4 spaces indent, no tabs
* Comments should appear on their own line or lines
    * This is due to the comment-stripper regex
* No Windows carriage returns (`\r\n`)
    * `dos2unix` can be run on these, but we'd prefer not to.
* No ""special"" characters in code, use standard ASCII where possible.

#### Sample .vimrc

    set shiftwidth=4
    set tabstop=4
    set expandtab

Look and Feel
-------------

There are a few things that should be kept in mind:

1. **Responsive design.** One CSS file should allow `index.html` to look good on mobile phones, too.
2. **Cross browser.** This means no `div` overflow, since it's not supported by Android. See the [Supported Browsers](#supported-browsers) section for more information.
3. **Valid.** Try to keep the HTML5 as valid as possible. 

Testing
-------

### Caching

The following command will cache templates to `./docroot`.   

    $ ./control.py cache
    ...

### Testing

    $ cd ./docroot
    $ python -m SimpleHTTPServer 5000

 
"
280,openpmix/openpmix,C,"Copyright (c) 2004-2007 The Trustees of Indiana University and Indiana
                        University Research and Technology
                        Corporation.  All rights reserved.
Copyright (c) 2004-2007 The University of Tennessee and The University
                        of Tennessee Research Foundation.  All rights
                        reserved.
Copyright (c) 2004-2008 High Performance Computing Center Stuttgart,
                        University of Stuttgart.  All rights reserved.
Copyright (c) 2004-2007 The Regents of the University of California.
                        All rights reserved.
Copyright (c) 2006-2020 Cisco Systems, Inc.  All rights reserved.
Copyright (c) 2006-2011 Mellanox Technologies. All rights reserved.
Copyright (c) 2006-2012 Oracle and/or its affiliates.  All rights reserved.
Copyright (c) 2007      Myricom, Inc.  All rights reserved.
Copyright (c) 2008-2021 IBM Corporation.  All rights reserved.
Copyright (c) 2010      Oak Ridge National Labs.  All rights reserved.
Copyright (c) 2011      University of Houston. All rights reserved.
Copyright (c) 2013-2020 Intel, Inc.  All rights reserved.
$COPYRIGHT$

Additional copyrights may follow

$HEADER$

===========================================================================
The best way to report bugs, send comments, or ask questions is to
post them on the OpenPMIx GitHub issue tracker:

     https://github.com/openpmix/openpmix/issues

When submitting questions and problems, be sure to include as much
extra information as possible.

Alternatively, you can sign up on the PMIx mailing list, which is
hosted by Google Groups:

     pmix@googlegroups.com

Because of spam, only subscribers are allowed to post to this list
(ensure that you subscribe with and post from exactly the same e-mail
address -- joe@example.com is considered different than
joe@mycomputer.example.com!).  You can subscribe to the list here:

     https://groups.google.com/d/forum/pmix

Thanks for your time.

===========================================================================

More information is available in the PMIx FAQ:

    https://openpmix.org/support/faq/

We are in early days, so please be patient - info will grow as questions
are addressed.

===========================================================================

The following abbreviated list of release notes applies to this code
base as of this writing (12 November 2015):

General notes
-------------

- The majority of PMIx's documentation is here in this file, the
  included man pages, and on the web site FAQ
  (https://openpmix.org/support/faq/).  This will eventually be
  supplemented with cohesive installation and user documentation files.

- Systems that have been tested are:
  - Linux (various flavors/distros), 32 bit, with gcc
  - Linux (various flavors/distros), 64 bit (x86), with gcc, Intel,
    and Portland (*)
  - OS X (10.7 and above), 32 and 64 bit (x86_64), with gcc (*)

- OpenPMIx has taken some steps towards Reproducible Builds
  (https://reproducible-builds.org/).  Specifically, OpenPMIx's
  ""configure"" and ""make"" process, by default, records the build date
  and some system-specific information such as the hostname where OpenPMIx
  was built and the username who built it.  If you desire a
  Reproducible Build, set the $SOURCE_DATE_EPOCH, $USER and $HOSTNAME
  environment variables before invoking ""configure"" and ""make"", and
  OpenPMIx will use those values instead of invoking ""whoami"" and/or
  ""hostname"", respectively.  See
  https://reproducible-builds.org/docs/source-date-epoch/ for
  information on the expected format and content of the
  $SOURCE_DATE_EPOCH variable.


(*) Compiler Notes
--------------

- The Portland Group compilers prior to version 7.0 require the
  ""-Msignextend"" compiler flag to extend the sign bit when converting
  from a shorter to longer integer.  This is is different than other
  compilers (such as GNU).  When compiling PMIx with the Portland
  compiler suite, the following flags should be passed to PMIx's
  configure script:

  shell$ ./configure CFLAGS=-Msignextend ...

  This will compile PMIx with the proper compile flags

- Running on nodes with different endian and/or different datatype
  sizes within a single parallel job is supported in this release.
  However, PMIx does not resize data when datatypes differ in size
  (for example, sending a 4 byte double and receiving an 8 byte
  double will fail).


===========================================================================

Building OpenPMIx
-----------------

OpenPMIx uses a traditional configure script paired with ""make"" to
build.  Typical installs can be of the pattern:

---------------------------------------------------------------------------
shell$ ./configure [...options...]
shell$ make all install
---------------------------------------------------------------------------

There are many available configure options (see ""./configure --help""
for a full list); a summary of the more commonly used ones follows:

INSTALLATION OPTIONS

--prefix=<directory>
  Install PMIx into the base directory named <directory>.  Hence,
  OpenPMIx will place its executables in <directory>/bin, its header
  files in <directory>/include, its libraries in <directory>/lib, etc.

--disable-shared
  By default, libpmix is built as a shared library.  This switch disables
  this default; it is really only useful when used with
  --enable-static.  Specifically, this option does *not* imply
  --enable-static; enabling static libraries and disabling shared
  libraries are two independent options.

--enable-static
  Build libpmix as a static library.  Note that this option does *not* imply
  --disable-shared; enabling static libraries and disabling shared
  libraries are two independent options.

 --disable-show-load-errors-by-default
   Set the default value of the mca_base_component_show_load_errors MCA
   variable: the --enable form of this option sets the MCA variable to
   true, the --disable form sets the MCA variable to false.  The MCA
   mca_base_component_show_load_errors variable can still be overridden
   at run time via the usual MCA-variable-setting mechanisms; this
   configure option simply sets the default value.

   The --disable form of this option is intended for OpenPMIx packagers
   who tend to enable support for many different types of networks and
   systems in their packages.  For example, consider a packager who
   includes support for both the FOO and BAR networks in their PMIx
   package, both of which require support libraries (libFOO.so and
   libBAR.so).  If an end user only has BAR hardware, they likely only
   have libBAR.so available on their systems -- not libFOO.so.
   Disabling load errors by default will prevent the user from seeing
   potentially confusing warnings about the FOO components failing to
   load because libFOO.so is not available on their systems.

   Conversely, system administrators tend to build an OpenPMIx that is
   targeted at their specific environment, and contains few (if any)
   components that are not needed.  In such cases, they might want
   their users to be warned that the FOO network components failed to
   load (e.g., if libFOO.so was mistakenly unavailable), and thus
   some PMIx calls might unexpectedly return ""not supported"".

--with-platform=FILE
  Load configure options for the build from FILE.  Options on the
  command line that are not in FILE are also used.  Options on the
  command line and in FILE are replaced by what is in FILE.

--enable-python-bindings
  Build the Python bindings for PMIx. Note the following packages
  are required to be installed.
    yum install Cython python3 python3-devel
    pip3 install Cython

Once OpenPMIx has been built and installed, it is safe to run ""make
clean"" and/or remove the entire build tree.

VPATH and parallel builds are fully supported.

Generally speaking, the only thing that users need to do to use OpenPMIx
is ensure that <prefix>/lib is in their LD_LIBRARY_PATH.  Users may
need to ensure to set LD_LIBRARY_PATH in their shell setup files (e.g.,
.bashrc, .cshrc) so that non-interactive rsh/ssh-based logins will
be able to find the OpenPMIx library.

===========================================================================

OpenPMIx Version Numbers and Binary Compatibility
-------------------------------------------------

OpenPMIx has two sets of version numbers that are likely of interest
to end users / system administrator:

    * Software version number
    * Shared library version numbers

Both are described below, followed by a discussion of application
binary interface (ABI) compatibility implications.

Software Version Number
-----------------------

OpenPMIx's version numbers are the union of several different values:
major, minor, release, and an optional quantifier.

  * Major: The major number is the first integer in the version string
    (e.g., v1.2.3). Changes in the major number typically indicate a
    significant change in the code base and/or end-user
    functionality. The major number is always included in the version
    number.

  * Minor: The minor number is the second integer in the version
    string (e.g., v1.2.3). Changes in the minor number typically
    indicate a incremental change in the code base and/or end-user
    functionality. The minor number is always included in the version
    number:

  * Release: The release number is the third integer in the version
    string (e.g., v1.2.3). Changes in the release number typically
    indicate a bug fix in the code base and/or end-user
    functionality.

  * Quantifier: OpenPMIx version numbers sometimes have an arbitrary
    string affixed to the end of the version number. Common strings
    include:

    o aX: Indicates an alpha release. X is an integer indicating
      the number of the alpha release (e.g., v1.2.3a5 indicates the
      5th alpha release of version 1.2.3).
    o bX: Indicates a beta release. X is an integer indicating
      the number of the beta release (e.g., v1.2.3b3 indicates the 3rd
      beta release of version 1.2.3).
    o rcX: Indicates a release candidate. X is an integer
      indicating the number of the release candidate (e.g., v1.2.3rc4
      indicates the 4th release candidate of version 1.2.3).

Although the major, minor, and release values (and optional
quantifiers) are reported in OpenPMIx nightly snapshot tarballs, the
filenames of these snapshot tarballs follow a slightly different
convention.

Specifically, the snapshot tarball filename contains three distinct
values:

   * Most recent Git tag name on the branch from which the tarball was
     created.

   * An integer indicating how many Git commits have occurred since
     that Git tag.

   * The Git hash of the tip of the branch.

For example, a snapshot tarball filename of
""pmix-v1.0.2-57-gb9f1fd9.tar.bz2"" indicates that this tarball was
created from the v1.0 branch, 57 Git commits after the ""v1.0.2"" tag,
specifically at Git hash gb9f1fd9.

OpenPMIx's Git master branch contains a single ""dev"" tag.  For example,
""pmix-dev-8-gf21c349.tar.bz2"" represents a snapshot tarball created
from the master branch, 8 Git commits after the ""dev"" tag,
specifically at Git hash gf21c349.

The exact value of the ""number of Git commits past a tag"" integer is
fairly meaningless; its sole purpose is to provide an easy,
human-recognizable ordering for snapshot tarballs.

Shared Library Version Number
-----------------------------

OpenPMIx uses the GNU Libtool shared library versioning scheme.

NOTE: Only official releases of OpenPMIx adhere to this versioning
      scheme. ""Beta"" releases, release candidates, and nightly
      tarballs, developer snapshots, and Git snapshot tarballs likely
      will all have arbitrary/meaningless shared library version
      numbers.

The GNU Libtool official documentation details how the versioning
scheme works.  The quick version is that the shared library versions
are a triple of integers: (current,revision,age), or ""c:r:a"".  This
triple is not related to the PMIx software version number.  There
are six simple rules for updating the values (taken almost verbatim
from the Libtool docs):

 1. Start with version information of ""0:0:0"" for each shared library.

 2. Update the version information only immediately before a public
    release of your software. More frequent updates are unnecessary,
    and only guarantee that the current interface number gets larger
    faster.

 3. If the library source code has changed at all since the last
    update, then increment revision (""c:r:a"" becomes ""c:r+1:a"").

 4. If any interfaces have been added, removed, or changed since the
    last update, increment current, and set revision to 0.

 5. If any interfaces have been added since the last public release,
    then increment age.

 6. If any interfaces have been removed since the last public release,
    then set age to 0.

Application Binary Interface (ABI) Compatibility
------------------------------------------------

OpenPMIx provides forward ABI compatibility in all versions of a given
feature release series and its corresponding
super stable series.  For example, on a single platform, a PMIx
application linked against OpenPMIx v1.3.2 shared libraries can be
updated to point to the shared libraries in any successive v1.3.x or
v1.4 release and still work properly (e.g., via the LD_LIBRARY_PATH
environment variable or other operating system mechanism).

OpenPMIx reserves the right to break ABI compatibility at new feature
release series.  For example, the same PMIx application from above
(linked against PMIx v1.3.2 shared libraries) will *not* work with
PMIx v1.5 shared libraries.

===========================================================================

Common Questions
----------------

Many common questions about building and using OpenPMIx are answered
on the FAQ:

    https://openpmix.org/support/faq/

===========================================================================

Got more questions?
-------------------

Found a bug?  Got a question?  Want to make a suggestion?  Want to
contribute to OpenPMIx?  Please let us know!

When submitting questions and problems, be sure to include as much
extra information as possible.  This web page details all the
information that we request in order to provide assistance:

     https://openpmix.org/support/

Questions and comments should generally be posted to the OpenPMIx
GitHub issue tracker:

    https://github.com/openpmix/openpmix/issues

Alternatively, question can also be sent to the OpenPMIx mailing list
(pmix@googlegroups.com).  Because of spam, only subscribers are
allowed to post to this list (ensure that you subscribe with and post
from *exactly* the same e-mail address -- joe@example.com is
considered different than joe@mycomputer.example.com!).  Visit this
page to subscribe to the user's list:

     https://groups.google.com/d/forum/pmix

Make today a PMIx day!
"
281,brendt/settings-repository,PHP,
282,rhiever/Data-Analysis-and-Machine-Learning-Projects,Jupyter Notebook,"![Python 2.7](https://img.shields.io/badge/python-2.7-blue.svg)
![Python 3.5](https://img.shields.io/badge/python-3.5-blue.svg)
![License](https://img.shields.io/badge/license-MIT%20License-blue.svg)

# Randy Olson's data analysis and machine learning projects

© 2016 - current, Randal S. Olson

This is a repository of teaching materials, code, and data for my data analysis and machine learning projects.

Each repository will (usually) correspond to one of the blog posts on my [web site](http://www.randalolson.com/blog/).

Be sure to check the documentation (usually in IPython Notebook format) in the directory you're interested in for the notes on the analysis, data usage terms, etc.

If you don't have the necessary software installed to run IPython Notebook, don't fret. You can use [nbviewer](http://nbviewer.ipython.org/) to view a notebook on the web.

For example, if you want to view the notebook in the `wheres-waldo-path-optimization` directory, copy the [full link](https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/wheres-waldo-path-optimization/Where's%20Waldo%20path%20optimization.ipynb) to the notebook then paste it into [nbviewer](http://nbviewer.ipython.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/wheres-waldo-path-optimization/Where%27s%20Waldo%20path%20optimization.ipynb).

## License

### Instructional Material

All instructional material in this repository is made available under the [Creative Commons Attribution license](https://creativecommons.org/licenses/by/4.0/). The following is a human-readable summary of (and not a substitute for) the [full legal text of the CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/legalcode).

You are free to:

* **Share**—copy and redistribute the material in any medium or format
* **Adapt**—remix, transform, and build upon the material

for any purpose, even commercially.

The licensor cannot revoke these freedoms as long as you follow the license terms.

Under the following terms:

* **Attribution**—You must give appropriate credit (mentioning that your work is derived from work that is © Randal S. Olson and, where practical, linking to http://www.randalolson.com/), provide a [link to the license](https://creativecommons.org/licenses/by/4.0/), and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.

**No additional restrictions**—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.

**Notices:**

* You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.
* No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.

### Software

Except where otherwise noted, the example programs and other software provided in this repository are made available under the [OSI](http://opensource.org/)-approved [MIT license](http://opensource.org/licenses/mit-license.html).

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"
283,Brewtarget/brewtarget,C++,"# Brewtarget

![Linux Workflow](https://github.com/brewtarget/brewtarget/actions/workflows/linux-ubuntu.yml/badge.svg)

![Windows Workflow](https://github.com/brewtarget/brewtarget/actions/workflows/windows.yml/badge.svg)

Brewtarget is free open-source brewing software, and a beer recipe creation
tool available for Linux, Mac, and Windows. It automatically calculates color,
bitterness, and other parameters for you while you drag and drop ingredients
into the recipe. Brewtarget also has many other tools such as priming sugar
calculators, OG correction help, and a unique mash designing tool. It also can
export and import recipes in BeerXML, allowing you to easily share recipes with
friends who use BeerSmith or other programs. All of this means that Brewtarget
is your single, free, go-to tool when crafting your beer recipes.

## Authors

* Adam Hawes <ach@hawes.net.au>
* Aidan Roberts <aidanr67@gmail.com>
* A.J. Drobnich <aj.drobnich@gmail.com>
* André Rodrigues <andre@sabayon.local>
* Blair Bonnett <blair.bonnett@gmail.com>
* Brian Rower <brian.rower@gmail.com>
* Carles Muñoz Gorriz <carlesmu@internautas.org>
* Chris Pavetto <chrispavetto@gmail.com>
* Chris Speck <cgspeck@gmail.com>
* Dan Cavanagh <dan@dancavanagh.com>
* Daniel Moreno <danielm5@users.noreply.github.com>
* Daniel Pettersson <pettson81@gmail.com>
* David Grundberg <individ@acc.umu.se>
* Eric Tamme <etamme@gmail.com>
* Greg Greenaae <ggreenaae@gmail.com>
* Greg Meess <Daedalus12@gmail.com>
* Idar Lund <idarlund@gmail.com>
* Jamie Daws <jdelectronics1@gmail.com>
* Jean-Baptiste Wons <wonsjb@gmail.com>
* Jeff Bailey <skydvr38@verizon.net>
* Jerry Jacobs <jerry@xor-gate.org>
* Jonatan Pålsson <jonatan.p@gmail.com>
* Jonathon Harding <github@jrhardin.net>
* Julian Volodia <julianvolodia@gmail.com>
* Kregg Kemper <gigatropolis@yahoo.com>
* Luke Vincent <luke.r.vincent@gmail.com>
* Marcel Koek <koek.marcel@gmail.com>
* Mark de Wever <koraq@xs4all.nl>
* Markus Mårtensson <mackan.90@gmail.com>
* Matt Anderson <matt.anderson@is4s.com>
* Mattias Måhl <mattias@kejsarsten.com>
* Matt Young <mfsy@yahoo.com>
* Maxime Lavigne <duguigne@gmail.com>
* Medic Momcilo <medicmomcilo@gmail.com>
* Mike Evans <mikee@saxicola.co.uk>
* Mik Firestone <mikfire@gmail.com>
* Mikhail Gorbunov <mikhail@sirena2000.ru>
* Mitch Lillie <mitch@mitchlillie.com>
* Padraic Stack <padraic.stack@gmail.com>
* Peter Buelow <goballstate@gmail.com>
* Peter Urbanec <git.user@urbanec.net>
* Philip Greggory Lee <rocketman768@gmail.com> -- Original developer
* Rob Taylor <robtaylor@floopily.org>
* Samuel Östling <MrOstling@gmail.com>
* Scott Peshak <scott@peshak.net>
* Théophane MARTIN <theophane.m@gmail.com>
* Tyler Cipriani <tcipriani@wikimedia.org>

Author list created with the help of the following command:

    $ git log --raw | grep ""^Author: "" | sort -u

## Websites

### For Users

* [Main website](http://www.brewtarget.org) (No longer updated and has some out-of-date links)
* [Help group](https://groups.google.com/forum/?fromgroups=#!forum/brewtarget-help) (Linked to from the website, but it's better to raise issues on GitHub than post here)
* [Latest builds](https://github.com/Brewtarget/brewtarget/actions)
* [Brewtarget PPA](https://launchpad.net/~brewtarget-devs/+archive/ubuntu/brewtarget-releases) (out of date)
* [Bug tracker](https://github.com/Brewtarget/brewtarget/issues)

Latest builds are available by logging into Github, following the ""Latest builds"" link above, drilling down into the relevant OS and downloading the installer package.

### For Developers

* [Source code repository](https://github.com/Brewtarget/brewtarget)
* [Developers team](https://launchpad.net/~brewtarget-devs) (No longer used)
* [Developers wiki](https://github.com/Brewtarget/brewtarget/wiki)

## Compiling and Installing

### Dependencies

On Debian systems like Ubuntu, the packages for dependencies are:

* cmake (>= 2.8.11)
* git
* libxerces-c-dev
* libxerces-c-doc
* libxalan-c-dev
* libxalan-c-doc
* qtbase5-dev
* qttools5-dev
* qttools5-dev-tools
* qtmultimedia5-dev
* libqt5sql5-sqlite
* libqt5sql5-psql
* libqt5svg5-dev
* libqt5multimedia5-plugins
* doxygen (optional, for source documentation)

### Compiling

We do not do any in-source builds. You will create a separate directory
for the build.

    $ mkdir brewtarget-build
    $ cd brewtarget-build
    $ cmake /path/to/brewtarget-src
    $ make

### Installing

Linux-like systems may simply do:

    $ sudo make install

Systems that use .deb or .rpm packages may also create a package first:

    $ make package

Then either

    $ sudo dpkg -i brewtarget*.deb

or

    $ sudo rpm -i brewtarget*.rpm

On Mac and Windows environments, the `package` target will create an installer
that may be executed to finish the installation.

### Make targets

* `make package`
  Makes .deb, .rpm, NSIS Installer, and .tar.bz2 binary packages.
* `make package_source`
  Makes a .tar.bz2 source package.
* `make source_doc`
  Makes html documentation of the source in doc/html.

### Cmake options

These options are passed to `cmake` with the `-D` flag before compiling. For
example:

    $ cmake /path/to/brewtarget -DCMAKE_INSTALL_PREFIX=/usr -DDO_RELEASE_BUILD=ON

* `CMAKE_INSTALL_PREFIX` - `/usr/local` by default. Set this to `/usr` on
  Debian-based systems like Ubuntu.
* `BUILD_DESIGNER_PLUGINS` - `OFF` by default. If set to `ON`, builds the Qt Designer
  plugins instead of brewtarget.
* `DO_RELEASE_BUILD` - `OFF` by default. If `ON`, will do a release build.
  Otherwise, debug build.
* `NO_MESSING_WITH_FLAGS` - `OFF` by default. `ON` means do not add any build
   flags whatsoever. May override other options.
"
284,efreesen/active_repository,Ruby,"# ActiveRepository (Discontinued)

[[![Coverage Status](https://coveralls.io/repos/efreesen/active_repository/badge.png)](https://coveralls.io/r/efreesen/active_repository)![Build Status](https://secure.travis-ci.org/efreesen/active_repository.png)](http://travis-ci.org/efreesen/active_repository)[![Dependency Status](https://gemnasium.com/efreesen/active_repository.png)](https://gemnasium.com/efreesen/active_repository) [![Code Climate](https://codeclimate.com/github/efreesen/active_repository.png)](https://codeclimate.com/github/efreesen/active_repository)

ActiveRepository was designed so you can build your Business Models without any dependence on persistence. It by default saves your data in memory using ActiveHash (https://github.com/zilkey/active_hash). Then when you decide which kind of persistence you want to use, all you have to do is connect ActiveRepository with it.

Currently it only works with ActiveRecord and/or Mongoid, but it is easy to add adaptors. If you need any adaptor, just open an issue!

With it you can always run your tests directly into memory, boosting your suite's speed. If you need to run all tests or a single spec using persistence you can do it too.

Check out our benchmark:

* **ActiveRepository:**
  Finished in **2.96** seconds
  90 examples, 0 failures

* **ActiveRecord:**
  Finished in **6.29** seconds
  90 examples, 0 failures

* **Mongoid:**
  Finished in **7.01** seconds
  90 examples, 0 failures

In ActiveRepository you will always work with ActiveRepository objects, so you can create relations between ActiveRecord and Mongoid seamlessly. You can even use Mongoid's Origin query format or keep with the SQL format no matter what kind of persistence you are using, we convert it for you!

## Requirements

### Ruby

ActiveRepository requires Ruby version **>= 1.9.3**.

## Installation

Add this line to your application's Gemfile:

    gem 'active_repository'

And then execute:

    $ bundle

Or install it yourself as:

    $ gem install active_repository

## Usage

To use it your class should inherit ActiveRepository::Base:

    class UserRepository < ActiveRepository::Base
      persistence_class = User
      save_in_memory = false
    end

ActiveRepository::Base has two class attributes to help it identify where it is going to persist data.

### persistence_class

This attribute is used to identify the class responsible for persisting data, it should be the ActiveRecord model or the Mongoid Document. Let's say your ActiveRecord Model is called User, using the example above, all database actions would be passed to User class, and you can extract all your business logic to the UserRepository class.

### save_in_memory

This attribute is used to persist data directly into memory. When set to true, it ignores the persistence_class attribute and save in memory. If set to false it goes back to persistence_class. You can use it to keep your tests saving in memory, or set it to false manually if a test need to touch the database.

If using Rails you can even tie it to your environment, so in tests it is set to true and otherwise it is set to false, like this:

    class UserRepository < ActiveRepository::Base
      persistence_class = User
      save_in_memory = Rails.env.test?
    end

### postfix

ActiveRepository also has an attribute to help you keep your code clean, the postfix. It can be used to define a pattern for Persistence classes so you don't need to keep declaring it everywhere. When using it, your persistence_class name would be \<repository_class_name\> + \<postfix\>.

Here is an example, let's say you have a bunch of Mongoid Documents and you don't want to declare persistence_class for each repository. So you can create a Base Repository and declare the postfix:

    class BaseRepository < ActiveRepository::Base
      # Defines the postfix
      postfix ""Document""

      save_in_memory = false
    end

You have to rename your Mongoid Documents to the defined pattern, like this:

    class UserDocument
      include Mongoid::Document
    end

And you can create your repositories inheriting from BaseRepository:

    class User < BaseRepository
    end

Then you are good to go!!!

### Setting fields

After defining the persistence options, you can set the fields it is going to use:

    class UserRepository < ActiveRepository::Base
      # Defines the fields of the class
      fields :name, :email, :birthdate

      persistence_class = User
      save_in_memory = false
    end

Now you are all set and ready to go. Your business logic is decoupled from the persistence tier!

You can check an example project here: https://github.com/efreesen/sports_betting_engine

## Contributing

1. Fork it
2. Create your feature branch (`git checkout -b my-new-feature`)
3. Commit your changes (`git commit -am 'Added some feature'`)
4. Push to the branch (`git push origin my-new-feature`)
5. Create new Pull Request
"
285,disconnectme/disconnect-tracking-protection,,"The Tracker Protection lists are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/ 

Please contact support@disconnect.me if you’d like to license the list for commercial use. 

Copyright (c) 2020 Disconnect, Inc.

Please submit an issue or email us if you have feedback, or suggestions. Pull requests are not reviewed and will be closed.
"
286,src-d/hercules,Go,"<p align=""center"">

</p>
<h1 align=""center"">Hercules</h1>
<p align=""center"">
      Fast, insightful and highly customizable Git history analysis.<br><br>
      <a href=""http://godoc.org/gopkg.in/src-d/hercules.v10""><img src=""https://godoc.org/gopkg.in/src-d/hercules.v10?status.svg"" alt=""GoDoc""></a>
      <a href=""https://travis-ci.com/src-d/hercules""><img src=""https://travis-ci.com/src-d/hercules.svg?branch=master"" alt=""Travis build Status""></a>
      <a href=""https://ci.appveyor.com/project/vmarkovtsev/hercules""><img src=""https://ci.appveyor.com/api/projects/status/49f0lm3v2y6xyph3?svg=true"" alt=""AppVeyor build status""></a>
      <a href=""https://pypi.python.org/pypi/labours""><img src=""https://img.shields.io/pypi/v/labours.svg"" alt=""PyPi package status""></a>
      <a href=""https://hub.docker.com/r/srcd/hercules""><img src=""https://img.shields.io/docker/build/srcd/hercules.svg"" alt=""Docker build status""></a>
      <a href=""https://codecov.io/gh/src-d/hercules""><img src=""https://codecov.io/github/src-d/hercules/coverage.svg"" alt=""Code coverage""></a>
      <a href=""https://goreportcard.com/report/github.com/src-d/hercules""><img src=""https://goreportcard.com/badge/github.com/src-d/hercules"" alt=""Go Report Card""></a>
      <a href=""https://opensource.org/licenses/Apache-2.0""><img src=""https://img.shields.io/badge/License-Apache%202.0-blue.svg"" alt=""Apache 2.0 license""></a>
</p>
<p align=""center"">
  <a href=""#overview"">Overview</a> •
  <a href=""#usage"">How To Use</a> •
  <a href=""#installation"">Installation</a> •
  <a href=""#contributions"">Contributions</a> •
  <a href=""#license"">License</a>
</p>

--------


Table of Contents
=================

  * [Overview](#overview)
  * [Installation](#installation)
     * [Build from source](#build-from-source)
     * [GitHub Action](#github-action)
  * [Contributions](#contributions)
  * [License](#license)
  * [Usage](#usage)
    * [Caching](#caching)
    * [GitHub Action](#github-action-1)
    * [Docker image](#docker-image)
    * [Built-in analyses](#built-in-analyses)
      * [Project burndown](#project-burndown)
      * [Files](#files)
      * [People](#people)
      * [Churn matrix](#overwrites-matrix)
      * [Code ownership](#code-ownership)
      * [Couples](#couples)
      * [Structural hotness](#structural-hotness)
      * [Aligned commit series](#aligned-commit-series)
      * [Added vs changed lines through time](#added-vs-changed-lines-through-time)
      * [Efforts through time](#efforts-through-time)
      * [Sentiment (positive and negative comments)](#sentiment-positive-and-negative-comments)
      * [Everything in a single pass](#everything-in-a-single-pass)
    * [Plugins](#plugins)
    * [Merging](#merging)
    * [Bad unicode errors](#bad-unicode-errors)
    * [Plotting](#plotting)
    * [Custom plotting backend](#custom-plotting-backend)
    * [Caveats](#caveats)
    * [Burndown Out-Of-Memory](#burndown-out-of-memory)
  * [Roadmap](#roadmap)

## Overview

Hercules is an amazingly fast and highly customizable Git repository analysis engine written in Go. Batteries are included.
Powered by [go-git](https://github.com/go-git/go-git).

*Notice (November 2020): the main author is back from the limbo and is gradually resuming the development. See the [roadmap](#roadmap).*

There are two command-line tools: `hercules` and `labours`. The first is a program
written in Go which takes a Git repository and executes a Directed Acyclic Graph (DAG) of [analysis tasks](doc/PIPELINE_ITEMS.md) over the full commit history.
The second is a Python script which shows some predefined plots over the collected data. These two tools are normally used together through
a pipe. It is possible to write custom analyses using the plugin system. It is also possible
to merge several analysis results together - relevant for organizations. 
The analyzed commit history includes branches, merges, etc.

Hercules has been successfully used for several internal projects at [source{d}](https://sourced.tech).
There are blog posts: [1](https://blog.sourced.tech/post/hercules-v4), [2](https://blog.sourced.tech/post/hercules) and
a [presentation](http://vmarkovtsev.github.io/gowayfest-2018-minsk/). Please [contribute](#contributions)
by testing, fixing bugs, adding [new analyses](https://github.com/src-d/hercules/issues?q=is%3Aissue+is%3Aopen+label%3Anew-analysis), or coding swagger!

![Hercules DAG of Burndown analysis](doc/dag.png)
<p align=""center"">The DAG of burndown and couples analyses with UAST diff refining. Generated with <code>hercules --burndown --burndown-people --couples --feature=uast --dry-run --dump-dag doc/dag.dot https://github.com/src-d/hercules</code></p>

![git/git image](doc/linux.png)
<p align=""center"">torvalds/linux line burndown (granularity 30, sampling 30, resampled by year). Generated with <code>hercules --burndown --first-parent --pb https://github.com/torvalds/linux | labours -f pb -m burndown-project</code> in 1h 40min.</p>

## Installation

Grab `hercules` binary from the [Releases page](https://github.com/src-d/hercules/releases).
`labours` is installable from [PyPi](https://pypi.org/):

```
pip3 install labours
```

[`pip3`](https://pip.pypa.io/en/stable/installing/) is the Python package manager.

Numpy and Scipy can be installed on Windows using http://www.lfd.uci.edu/~gohlke/pythonlibs/

### Build from source
You are going to need Go (>= v1.11) and [`protoc`](https://github.com/google/protobuf/releases).
```
git clone https://github.com/src-d/hercules && cd hercules
make
pip3 install -e ./python
```

### GitHub Action

It is possible to run Hercules as a [GitHub Action](https://help.github.com/en/articles/about-github-actions):
[Hercules on GitHub Marketplace](https://github.com/marketplace/actions/hercules-insights).
Please refer to the [sample workflow](.github/workflows/main.yml) which demonstrates how to setup.

## Contributions

...are welcome! See [CONTRIBUTING](CONTRIBUTING.md) and [code of conduct](CODE_OF_CONDUCT.md).

## License
[Apache 2.0](LICENSE.md)

## Usage

The most useful and reliably up-to-date command line reference:

```
hercules --help
```

Some examples:

```
# Use ""memory"" go-git backend and display the burndown plot. ""memory"" is the fastest but the repository's git data must fit into RAM.
hercules --burndown https://github.com/go-git/go-git | labours -m burndown-project --resample month
# Use ""file system"" go-git backend and print some basic information about the repository.
hercules /path/to/cloned/go-git
# Use ""file system"" go-git backend, cache the cloned repository to /tmp/repo-cache, use Protocol Buffers and display the burndown plot without resampling.
hercules --burndown --pb https://github.com/git/git /tmp/repo-cache | labours -m burndown-project -f pb --resample raw

# Now something fun
# Get the linear history from git rev-list, reverse it
# Pipe to hercules, produce burndown snapshots for every 30 days grouped by 30 days
# Save the raw data to cache.yaml, so that later is possible to labours -i cache.yaml
# Pipe the raw data to labours, set text font size to 16pt, use Agg matplotlib backend and save the plot to output.png
git rev-list HEAD | tac | hercules --commits - --burndown https://github.com/git/git | tee cache.yaml | labours -m burndown-project --font-size 16 --backend Agg --output git.png
```

`labours -i /path/to/yaml` allows to read the output from `hercules` which was saved on disk.

### Caching

It is possible to store the cloned repository on disk. The subsequent analysis can run on the
corresponding directory instead of cloning from scratch:

```
# First time - cache
hercules https://github.com/git/git /tmp/repo-cache

# Second time - use the cache
hercules --some-analysis /tmp/repo-cache
```

### GitHub Action

The action produces the artifact named
`hercules_charts`. Since it is currently impossible to pack several files in one artifact, all the
charts and Tensorflow Projector files are packed in the inner tar archive. In order to view the embeddings,
go to [projector.tensorflow.org](https://projector.tensorflow.org), click ""Load"" and choose the two TSVs. Then use UMAP or T-SNE.

### Docker image

```
docker run --rm srcd/hercules hercules --burndown --pb https://github.com/git/git | docker run --rm -i -v $(pwd):/io srcd/hercules labours -f pb -m burndown-project -o /io/git_git.png
```

### Built-in analyses

#### Project burndown

```
hercules --burndown
labours -m burndown-project
```

Line burndown statistics for the whole repository.
Exactly the same what [git-of-theseus](https://github.com/erikbern/git-of-theseus)
does but much faster. Blaming is performed efficiently and incrementally using a custom RB tree tracking
algorithm, and only the last modification date is recorded while running the analysis.

All burndown analyses depend on the values of *granularity* and *sampling*.
Granularity is the number of days each band in the stack consists of. Sampling
is the frequency with which the burnout state is snapshotted. The smaller the
value, the more smooth is the plot but the more work is done.

There is an option to resample the bands inside `labours`, so that you can
define a very precise distribution and visualize it different ways. Besides,
resampling aligns the bands across periodic boundaries, e.g. months or years.
Unresampled bands are apparently not aligned and start from the project's birth date.

#### Files

```
hercules --burndown --burndown-files
labours -m burndown-file
```

Burndown statistics for every file in the repository which is alive in the latest revision.

Note: it will generate separate graph for every file. You don't want to run it on repository with many files.

#### People

```
hercules --burndown --burndown-people [--people-dict=/path/to/identities]
labours -m burndown-person
```

Burndown statistics for the repository's contributors. If `--people-dict` is not specified, the identities are
discovered by the following algorithm:

0. We start from the root commit towards the HEAD. Emails and names are converted to lower case.
1. If we process an unknown email and name, record them as a new developer.
2. If we process a known email but unknown name, match to the developer with the matching email,
and add the unknown name to the list of that developer's names.
3. If we process an unknown email but known name, match to the developer with the matching name,
and add the unknown email to the list of that developer's emails.

If `--people-dict` is specified, it should point to a text file with the custom identities. The
format is: every line is a single developer, it contains all the matching emails and names separated
by `|`. The case is ignored.

#### Overwrites matrix

![Wireshark top 20 overwrites matrix](doc/wireshark_overwrites_matrix.png)
<p align=""center"">Wireshark top 20 devs - overwrites matrix</p>

```
hercules --burndown --burndown-people [--people-dict=/path/to/identities]
labours -m overwrites-matrix
```

Beside the burndown information, `--burndown-people` collects the added and deleted line statistics per
developer. Thus it can be visualized how many lines written by developer A are removed by developer B.
This indicates collaboration between people and defines expertise teams.

The format is the matrix with N rows and (N+2) columns, where N is the number of developers.

1. First column is the number of lines the developer wrote.
2. Second column is how many lines were written by the developer and deleted by unidentified developers
(if `--people-dict` is not specified, it is always 0).
3. The rest of the columns show how many lines were written by the developer and deleted by identified
developers.

The sequence of developers is stored in `people_sequence` YAML node.

#### Code ownership

![Ember.js top 20 code ownership](doc/emberjs_people.png)
<p align=""center"">Ember.js top 20 devs - code ownership</p>

```
hercules --burndown --burndown-people [--people-dict=/path/to/identities]
labours -m ownership
```

`--burndown-people` also allows to draw the code share through time stacked area plot. That is,
how many lines are alive at the sampled moments in time for each identified developer.

#### Couples

![Linux kernel file couples](doc/tfprojcouples.png)
<p align=""center"">torvalds/linux files' coupling in Tensorflow Projector</p>

```
hercules --couples [--people-dict=/path/to/identities]
labours -m couples -o <name> [--couples-tmp-dir=/tmp]
```

**Important**: it requires Tensorflow to be installed, please follow [official instructions](https://www.tensorflow.org/install/).

The files are coupled if they are changed in the same commit. The developers are coupled if they
change the same file. `hercules` records the number of couples throughout the whole commit history
and outputs the two corresponding co-occurrence matrices. `labours` then trains
[Swivel embeddings](https://github.com/src-d/tensorflow-swivel) - dense vectors which reflect the
co-occurrence probability through the Euclidean distance. The training requires a working
[Tensorflow](http://tensorflow.org) installation. The intermediate files are stored in the
system temporary directory or `--couples-tmp-dir` if it is specified. The trained embeddings are
written to the current working directory with the name depending on `-o`. The output format is TSV
and matches [Tensorflow Projector](http://projector.tensorflow.org/) so that the files and people
can be visualized with t-SNE implemented in TF Projector.

#### Structural hotness

```
      46  jinja2/compiler.py:visit_Template [FunctionDef]
      42  jinja2/compiler.py:visit_For [FunctionDef]
      34  jinja2/compiler.py:visit_Output [FunctionDef]
      29  jinja2/environment.py:compile [FunctionDef]
      27  jinja2/compiler.py:visit_Include [FunctionDef]
      22  jinja2/compiler.py:visit_Macro [FunctionDef]
      22  jinja2/compiler.py:visit_FromImport [FunctionDef]
      21  jinja2/compiler.py:visit_Filter [FunctionDef]
      21  jinja2/runtime.py:__call__ [FunctionDef]
      20  jinja2/compiler.py:visit_Block [FunctionDef]
```

Thanks to Babelfish, hercules is able to measure how many times each structural unit has been modified.
By default, it looks at functions; refer to [Semantic UAST XPath](https://docs.sourced.tech/babelfish/using-babelfish/uast-querying)
manual to switch to something else.

```
hercules --shotness [--shotness-xpath-*]
labours -m shotness
```

Couples analysis automatically loads ""shotness"" data if available.

![Jinja2 functions grouped by structural hotness](doc/jinja.png)
<p align=""center""><code>hercules --shotness --pb https://github.com/pallets/jinja | labours -m couples -f pb</code></p>

#### Aligned commit series

![tensorflow/tensorflow](doc/devs_tensorflow.png)
<p align=""center"">tensorflow/tensorflow aligned commit series of top 50 developers by commit number.</p>

```
hercules --devs [--people-dict=/path/to/identities]
labours -m devs -o <name>
```

We record how many commits made, as well as lines added, removed and changed per day for each developer.
We plot the resulting commit time series using a few tricks to show the temporal grouping. In other words,
two adjacent commit series should look similar after normalization.

1. We compute the distance matrix of the commit series. Our distance metric is
[Dynamic Time Warping](https://en.wikipedia.org/wiki/Dynamic_time_warping).
We use [FastDTW](https://cs.fit.edu/~pkc/papers/tdm04.pdf) algorithm which has linear complexity
proportional to the length of time series. Thus the overall complexity of computing the matrix is quadratic.
2. We compile the linear list of commit series with
[Seriation](http://nicolas.kruchten.com/content/2018/02/seriation/) technique.
Particularly, we solve the [Travelling Salesman Problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem) which is NP-complete.
However, given the typical number of developers which is less than 1,000, there is a good chance that
the solution does not take much time. We use [Google or-tools](https://developers.google.com/optimization/routing/tsp) solver.
3. We find 1-dimensional clusters in the resulting path with [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)
algorithm and assign colors accordingly.
4. Time series are smoothed by convolving with the [Slepian window](https://en.wikipedia.org/wiki/Window_function#DPSS_or_Slepian_window).

This plot allows to discover how the development team evolved through time. It also shows ""commit flashmobs""
such as [Hacktoberfest](https://hacktoberfest.digitalocean.com/). For example, here are the revealed
insights from the `tensorflow/tensorflow` plot above:

1. ""Tensorflow Gardener"" is classified as the only outlier.
2. The ""blue"" group of developers covers the global maintainers and a few people who left (at the top).
3. The ""red"" group shows how core developers join the project or become less active.

#### Added vs changed lines through time

![tensorflow/tensorflow](doc/add_vs_changed.png)
<p align=""center"">tensorflow/tensorflow added and changed lines through time.</p>

```
hercules --devs [--people-dict=/path/to/identities]
labours -m old-vs-new -o <name>
```

`--devs` from the previous section allows to plot how many lines were added and how many existing changed
(deleted or replaced) through time. This plot is smoothed.

#### Efforts through time

![kubernetes/kubernetes](doc/k8s_efforts.png)
<p align=""center"">kubernetes/kubernetes efforts through time.</p>

```
hercules --devs [--people-dict=/path/to/identities]
labours -m devs-efforts -o <name>
```

Besides, `--devs` allows to plot how many lines have been changed (added or removed) by each developer.
The upper part of the plot is an accumulated (integrated) lower part. It is impossible to have the same scale
for both parts, so the lower values are scaled, and hence there are no lower Y axis ticks.
There is a difference between the efforts plot and the ownership plot, although changing lines correlate
with owning lines.

#### Sentiment (positive and negative comments)

![Django sentiment](doc/sentiment.png)
<p align=""center"">It can be clearly seen that Django comments were positive/optimistic in the beginning, but later became negative/pessimistic.<br><code>hercules --sentiment --pb https://github.com/django/django | labours -m sentiment -f pb</code></p>

We extract new and changed comments from source code on every commit, apply [BiDiSentiment](https://github.com/vmarkovtsev/bidisentiment)
general purpose sentiment recurrent neural network and plot the results. Requires
[libtensorflow](https://www.tensorflow.org/install/install_go).
E.g. [`sadly, we need to hide the rect from the documentation finder for now`](https://github.com/pygame/pygame/commit/b6091d38c8a5639d311858660b38841d96598509#diff-eae59f175858fcef57cb17e733981c73R27) is negative and
[`Theano has a built-in optimization for logsumexp (...) so we can just write the expression directly`](https://github.com/keras-team/keras/commit/7d52af64c03e71bcd23112a7086dc8aab1b37ed2#diff-ff634bb5c5441d7052449f20018872b8R549)
is positive. Don't expect too much though - as was written, the sentiment model is
general purpose and the code comments have different nature, so there is no magic (for now).

Hercules must be built with ""tensorflow"" tag - it is not by default:

```
make TAGS=tensorflow
```

Such a build requires [`libtensorflow`](https://www.tensorflow.org/install/install_go).

#### Everything in a single pass

```
hercules --burndown --burndown-files --burndown-people --couples --shotness --devs [--people-dict=/path/to/identities]
labours -m all
```

### Plugins

Hercules has a plugin system and allows to run custom analyses. See [PLUGINS.md](PLUGINS.md).

### Merging

`hercules combine` is the command which joins several analysis results in Protocol Buffers format together.

```
hercules --burndown --pb https://github.com/go-git/go-git > go-git.pb
hercules --burndown --pb https://github.com/src-d/hercules > hercules.pb
hercules combine go-git.pb hercules.pb | labours -f pb -m burndown-project --resample M
```

### Bad unicode errors

YAML does not support the whole range of Unicode characters and the parser on `labours` side
may raise exceptions. Filter the output from `hercules` through `fix_yaml_unicode.py` to discard
such offending characters.

```
hercules --burndown --burndown-people https://github.com/... | python3 fix_yaml_unicode.py | labours -m people
```

### Plotting

These options affects all plots:

```
labours [--style=white|black] [--backend=] [--size=Y,X]
```

`--style` sets the general style of the plot (see `labours --help`).
`--background` changes the plot background to be either white or black.
`--backend` chooses the Matplotlib backend.
`--size` sets the size of the figure in inches. The default is `12,9`.

(required in macOS) you can pin the default Matplotlib backend with

```
echo ""backend: TkAgg"" > ~/.matplotlib/matplotlibrc
```

These options are effective in burndown charts only:

```
labours [--text-size] [--relative]
```

`--text-size` changes the font size, `--relative` activate the stretched burndown layout.

### Custom plotting backend

It is possible to output all the information needed to draw the plots in JSON format.
Simply append `.json` to the output (`-o`) and you are done. The data format is not fully
specified and depends on the Python code which generates it. Each JSON file should
contain `""type""` which reflects the plot kind.

### Caveats

1. Processing all the commits may fail in some rare cases. If you get an error similar to https://github.com/src-d/hercules/issues/106
please report there and specify `--first-parent` as a workaround.
1. Burndown collection may fail with an Out-Of-Memory error. See the next session for the workarounds.
1. Parsing YAML in Python is slow when the number of internal objects is big. `hercules`' output
for the Linux kernel in ""couples"" mode is 1.5 GB and takes more than an hour / 180GB RAM to be
parsed. However, most of the repositories are parsed within a minute. Try using Protocol Buffers
instead (`hercules --pb` and `labours -f pb`).
1. To speed up yaml parsing
   ```
   # Debian, Ubuntu
   apt install libyaml-dev
   # macOS
   brew install yaml-cpp libyaml

   # you might need to re-install pyyaml for changes to make effect
   pip uninstall pyyaml
   pip --no-cache-dir install pyyaml
   ```

### Burndown Out-Of-Memory

If the analyzed repository is big and extensively uses branching, the burndown stats collection may
fail with an OOM. You should try the following:

1. Read the repo from disk instead of cloning into memory.
2. Use `--skip-blacklist` to avoid analyzing the unwanted files. It is also possible to constrain the `--language`.
3. Use the [hibernation](doc/HIBERNATION.md) feature: `--hibernation-distance 10 --burndown-hibernation-threshold=1000`. Play with those two numbers to start hibernating right before the OOM.
4. Hibernate on disk: `--burndown-hibernation-disk --burndown-hibernation-dir /path`.
5. `--first-parent`, you win.

## Roadmap

* [ ] Switch from `src-d/go-git` to `go-git/go-git`. Upgrade the codebase to be compatible with the latest Go version.
* [ ] Update the docs regarding the copyrights and such.
* [ ] Fix the reported bugs.
* [ ] Remove the dependency on Babelfish for parsing the code. It is abandoned and a better alternative should be found.
* [ ] Remove the ad-hoc analyses added while source{d} was agonizing.

"
287,ionic-team/ionic-bower,JavaScript,"# ionic-bower

Bower repository for [Ionic Framework](http://github.com/driftyco/ionic) v1 (Ionic 2+ uses npm)

### Usage

Include `js/ionic.bundle.js` to get ionic and all of its dependencies.

Alternatively, include the individual ionic files with the dependencies separately.

### Versions

To install the latest stable version, `bower install driftyco/ionic-bower`

To install the latest nightly release, `bower install driftyco/ionic-bower#master`
"
288,flowable/flowable-examples,Java,
289,georgy/nexus-npm-repository-plugin,,"
NPM plugin is now a part of nexus-oss (https://github.com/sonatype/nexus-oss). See https://github.com/sonatype/nexus-oss/pull/906

"
290,tier4/AutowareArchitectureProposal.iv,C++,
291,xrootd/xrootd,C++,"
--------------------------------------------------------------------------------
                    _    _ ______                   _____
                   \ \  / (_____ \             _   (____ \
                    \ \/ / _____) ) ___   ___ | |_  _   \ \
                     )  ( (_____ ( / _ \ / _ \|  _)| |   | |
                    / /\ \      | | |_| | |_| | |__| |__/ /
                   /_/  \_\     |_|\___/ \___/ \___)_____/

--------------------------------------------------------------------------------

1. S U P P O R T E D   O P E R A T I N G   S Y S T E M S

   XRootD is supported on the following platforms:

  * RedHat Enterprise Linux 5 and 6 and derivatives (Scientific Linux)
    compiled with gcc
  * Solaris 10 compiled with SunCC
  * MacOSX 10.6 and 10.7 compiled with gcc or clang

2. B U I L D   I N S T R U C T I O N S

2.0 Build dependecies

  XRootD requires at minimum following packages (RHEL distro):

  * gcc-c++, cmake(3), krb5-devel, libuuid-devel, libxml2-devel, openssl-devel, systemd-devel, zlib-devel
  * devtoolset-7 (only RHEL7)

2.1 Build system

  XRootD uses CMake to handle the build process. It should build fine with
cmake 2.6, however, on some platforms, this version of cmake has problems
handling the perl libraries, therefore version 2.8 or newer is recommended.

2.2 Build parameters

  The build process supports the following parameters:

  * CMAKE_INSTALL_PREFIX - indicates where the XRootD files should be installed,
                           (default: /usr)
  * CMAKE_BUILD_TYPE     - type of the build: Release/Debug/RelWithDebInfo
  * FORCE_32BITS         - Force building 32 bit binaries when on Solaris AMD64
                           (default: FALSE)
  * ENABLE_PERL          - enable the perl bindings if possible (default: TRUE)
  * ENABLE_FUSE          - enable the fuse filesystem driver if possible
                           (default: TRUE)
  * ENABLE_CRYPTO        - enable the OpenSSL cryprography support (including
                           the X509 authentication) if possible (default: TRUE)
  * ENABLE_KRB5          - enable the Kerberos 5 authentication if possible
                           (default: TRUE)
  * ENABLE_READLINE      - enable the lib readline support in the commandline
                           utilities (default: TRUE)
  * OPENSSL_ROOT_DIR     - path to the root of the openssl installation if it
                           cannot be detected in a standard location
  * KERBEROS5_ROOT_DIR   - path to the root of the kerberos installation if it
                           cannot be detected in a standard location
  * READLINE_ROOT_DIR    - path to the root of the readline installation if it
                           cannot be detected in a standard location
  * CMAKE_C_COMPILER     - path to the c compiler that should be used
  * CMAKE_CXX_COMPILER   - path to the c++ compiler that should be used

2.3 Build steps

  * on REHL7 only: scl enable devtoolset-7 /bin/bash

  * Create an empty build directory:

    mkdir build
    cd build

  * Generate the build system files using cmake, ie:

    cmake /path/to/the/xrootd/source -DCMAKE_INSTALL_PREFIX=/opt/xrootd \
                                     -DENABLE_PERL=FALSE

  * Build the source:

    make

  * Install the source:

    make install

3. P L A T F O R M   N O T E S

3.1 Solaris

  * On Solaris x86 the Sun Studio <= 12.1 compiler optimization algorithms
    are broken, only Debug build is supported. For the optimized mode upgrade
    the compiler to 12.2 or later.
"
292,objcio/functional-swift,Swift,"# Functional Swift by [objc.io](https://www.objc.io)

This repository contains sample code from the [Functional Swift book](https://www.objc.io/books/functional-swift).
"
293,mspnp/elasticsearch,Java,"# Run Elasticsearch on Azure
Microsoft patterns & practices

This repository has been archived.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

"
294,xinu-os/xinu,C,"# Embedded Xinu #

Embedded Xinu, Copyright (C) 2008, 2009, 2010.  All rights reserved.

Version: 2.01

 1. What is Embedded Xinu?
 2. Directory Structure
 3. Prerequisites
 4. Installation Instructions
    1. Build Embedded Xinu
    2. Make serial and network connections
    3. Enter Common Firmware Environment prompt
    4. Set IP address
    5. Load image over TFTP
 5. Links


## 1. What is Embedded Xinu? ##

Xinu (""Xinu is not unix"", a recursive acronym) is a UNIX-like operating
system originally developed by Douglas Comer for instructional purposes at
Purdue University in the 1980s.

Embedded Xinu is a reimplementation of the original Xinu operating system
on the MIPS processor which is able to run on inexpensive wireless routers
and is suitable for courses and research in the areas of Operating Systems,
Hardware Systems, Embedded Systems, Networking, and Compilers.

## 2. Directory Structure ##

Once you have downloaded and extracted the xinu tarball, you will see a
basic directory structure:

	AUTHORS   device/   lib/     mem/      loader/   README  system/
	compile/  include/  LICENSE  mailbox/  network/  shell/  test/

 * `AUTHORS`  a brief history of contributors to the Xinu operating system
              in it's varying iterations.
 * `compile/` contains the Makefile and other necessities for building the
              Xinu system once you have a cross-compiler.
 * `device/`  contains directories with source for all device drivers in Xinu.
 * `include/` contains all the header files used by Xinu.
 * `lib/`     contains library folders (e.g., `libxc/`) with a Makefile and 
              source for the library
 * `LICENSE`  the license under which this project falls.
 * `loader/`  contains assembly files and is where the bootloader will begin
              execution of O/S code.
 * `mailbox/` contains source for the mailbox message queuing system.
 * `mem/`     contains source for page-based memory protection.
 * `network/` contains code for the TCP/IP networking stack.
 * `README`   this document.
 * `RELEASE`  release notes for the current version.
 * `shell/`   contains the source for all Xinu shell related functions.
 * `system/`  contains the source for all Xinu system functions such as the
              nulluser process (`initialize.c`) as well as code to set up a C
              environment (`startup.S`).
 * `test/`    contains a number of testcases (which can be run using the shell
              command `testsuite`).

## 3. Prerequisites ##

### 3.1 Supported platform with hardware modification ###

To run Embedded Xinu you need a supported router or virtual machine.
Currently, Embedded Xinu supports Linksys WRT54GL, Linksys WRT160NL,
and the Qemu-mipsel virtual machine.  For an updated list
of supported platforms, visit:

http://xinu.mscs.mu.edu/List_of_supported_platforms

In order to communicate with the router, you need to perform a hardware
modification that will expose the serial port that exists on the PCB.  For
more information on this process, see:

http://xinu.mscs.mu.edu/Modify_the_Linksys_hardware

### 3.2 Cross-compiler ###

To build Embedded Xinu you will need a cross-compiler from your host
computer's architecture to MIPSEL (little endian MIPS for the 54GL router)
or MIPS (big endian for the 160NL router).  Instructions on how to do this
can be found here:

http://xinu.mscs.mu.edu/Build_Xinu#Cross-Compiler

### 3.3 Serial communication software ###

Any serial communication software will do. The Xinu Console Tools include
a program called tty-connect which can serve the purpose for a UNIX 
environment.  More information about the Xinu Console Tools can be found 
at:

http://xinu.mscs.mu.edu/Console_Tools#Xinu_Console_Tools

### 3.4 TFTP server software ###

A TFTP server will provide the router with the ability to download and run
the compiled Embedded Xinu image.  

## 4. Installation Instructions ##

### 4.1 Build Embedded Xinu ###

Update the `MIPS_ROOT` and `MIPS_PREFIX` variables in compile/mipsVars to 
correctly point to the cross-compiler on your machine.

Then, from the compile directory, simply run make, which should leave you
with a xinu.boot file.  This is the binary image you need to transfer to
your router for it to run Embedded Xinu.  The default build is for the
WRT54GL router; change the compile/Makefile `PLATFORM` variable for other
builds.  See the compile/platforms directory for supported configurations.

### 4.2 Make serial and network connections ###

After creating the `xinu.boot` image you can connect the router's serial
port to your computer and open up a connection using the following
settings:

 - 8 data bits, no parity bit, and 1 stop bit (8N1)
 - 115200 bps

### 4.3 Enter Common Firmware Environment prompt ###

With the serial connection open, power on the router and immediately start
sending breaks (Control-C) to the device, if your luck holds you will be
greeted with a CFE prompt.

    CFE>

If the router seems to start booting up, you can powercycle and try again.

### 4.4 Set IP address ###

By default, the router will have a static IP address of 192.168.1.1.  If you
need to set a different address for your network, run one of the following
commands:

    ifconfig eth0 -auto                      if you are using a DHCP server 
    ifconfig eth0 -addr=*.*.*.*              for a static IP address

### 4.5 Load image over TFTP ###

On a computer that is network accessible from the router, start your TFTP
server and place the xinu.boot image in the root directory that the server
makes available.

Then, on the router type the command:

    CFE> boot -elf [TFTP server IP]:xinu.boot

If all has gone correctly the router you will be greeted with the Xinu Shell
(`xsh$ `), which means you are now running Embedded Xinu!

## 5. Links ##

### The Embedded Xinu Wiki ###

The home of the Embedded Xinu project

    http://xinu.mscs.mu.edu/

### Dr. Brylow's Embedded Xinu Lab Infrastructure Page ###

More information about the Embedded Xinu Lab at Marquette University

    http://www.mscs.mu.edu/~brylow/xinu/

"
295,fayland/dist-zilla-plugin-repository,Perl,
296,myopencart/ocStore,PHP,"# ocStore

## Краткое описание

ocStore - это онлайн магазин, который основан на базе OpenCart и имеет открытый исходный код. Очень простое и надёжное решение для желающих создать собственный онлайн бизнес при минимальных затратах..

## Баг-трекер

Прежде, чем создать сообщение об ошибке, пожалуйста, прочитайте инструкцию:

 1. Посетите [OpenCart форум](https://opencartforum.com/), возможно что там, Вы найдете решение.
 2. Проверьте все открытые (и закрытые) вопросы на [Баг-трекер GitHub] (https://github.com/myopencart/ocstore/issues).
 3. Если ошибка связана с движком OpenCart, пожалуйста, создайте отчет [на форуме](https://opencartforum.com/forum/15-отчёты-об-ошибках/).
 4. ПРОЧИТАЙТЕ [весь список изменений](https://github.com/myopencart/ocStore/blob/ocStore2/changelog.md).
 5. Что бы найти ответ на Ваш вопрос, используйте [поиск по форуму](https://opencartforum.com/index.php?app=core&module=search).
 6. Пожалуйста убедитесь, что эта проблема (или ошибка) не связана с Вашим сервером.

Если у Вас появились какие то дополнительные вопросы, пожалуйста, посетите наш [форум](https://opencartforum.com/)

**Важное замечание!**
- Если Ваше сообщение об ошибке не связаны с основным кодом OpenCart (например, некоректная работа модуля или конфигурация Вашего сервера), то вопрос будет закрыт без причины. По таким вопросам обращайтесь к автору дополнения, или воспользуйтесь разделом [платных услуг](https://opencartforum.com/forum/22-услуги/) на форуме.
- Если Вы хотите сообщить о серьёзной угрозе безопасности в коде OpenCart, пожалуйста, отправьте личное сообщение любому из модераторов форума, или, создайте тему в разделе [Вопросы безопасности](https://opencartforum.com/forum/41-вопросы-безопасности/).
- Если у Вас есть какие то новые идеи или советы, воспользуйтесь соответсвующим разделом на форуме [Предложения и пожелания](https://opencartforum.com/forum/31-предложения-и-пожелания/).

## Релизы

Для получения уведомлений о новых релизах, примечаниях администратора или сообщениях от группы OC Team, откройте на форуме [Новости и анонсы](https://opencartforum.com/forum/3-новости-и-анонсы/), и вверху справа - нажмите на кнопку ""Следить за форумом"".

## Установка и Обновление

Пожалуйста, прочитайте соответствующий файл (install.txt/upgrade.txt) внутри архива, или воспользуйтесь разделом на форуме [Установка, обновление, настройка](https://opencartforum.com/forum/6-установка-обновление-настройка/).

## Лицензия

[GNU General Public License версии 3 (GPLv3)] (https://github.com/myopencart/ocStore/blob/ocStore2/license.txt)

## Ссылки

- [Сайт ocStore] (https://ocstore.com/)
- [OpenCart форум] (https://opencartforum.com/)
- [Документация] (https://docs.ocstore.com/)
- [Facebook] (https://www.facebook.com/opencartforum)
- [Вконтакте] (https://vk.com/opencartforumcom)
- [Twitter] (https://twitter.com/opencartforum)
- [YouTube] (https://www.youtube.com/c/OpenCartForumCom)"
297,Futureazoo/TextureRepository,,"# Texture Repository
This repository contains various textures for use in Minecraft.

A Sample:

<img src=http://i.imgur.com/8pzEygJ.png></p>

These textures are free to use by mod authors, texture artists, and custom map makers. 
Please credit the artist for any texture you use from this repository.

-----------------

If you would like to contribute to the repository, submit a pull request. 
Create a new folder under your username and organize them based on blocks and items. (further internal organization is encouraged)

-----------------

I hope this comes in handy to any mod authors who don't enjoy texturing, or anyone looking for inspiration.

Enjoy!
-

 [<img src=""https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png"">](http://creativecommons.org/licenses/by-nc-sa/4.0/)

Textures are licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.](http://creativecommons.org/licenses/by-nc-sa/4.0/).
"
298,recalbox/recalbox-os,,"[![English](http://upload.wikimedia.org/wikipedia/commons/e/e1/Union_Jack_22x16.png ""English"")](README.md)
[![Italiano](http://upload.wikimedia.org/wikipedia/commons/7/70/Flag_of_italy.png ""Italiano"")](README-IT.md) 
[![Français](http://upload.wikimedia.org/wikipedia/commons/1/14/Flag_of_france.png ""Française"")](README-FR.md)
[![Deutsch](http://upload.wikimedia.org/wikipedia/commons/4/4b/Flag_of_germany.png ""Deutsch"")](README-DE.md)
[![Portugues](http://upload.wikimedia.org/wikipedia/commons/a/aa/Flag_of_Portugal_icon.png ""Portugues"")](README-PT.md)
[![Español](http://upload.wikimedia.org/wikipedia/commons/3/30/Flag_of_spain.png ""Español"")](README-ES.md)
[![Türkçe](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Flag_of_Turkey.svg/24px-Flag_of_Turkey.svg.png ""Türkçe"")](README-TR.md)
****
# recalbox-os
**Neu : V3.2.11 - Fehler mit ""gleicher Controller Name"" wurde korrigiert, Zoid Theme hinzugefügt**

**Neu : V3.2.4  - Neues recalbox.conf System, neues Boot-System**

**Neu : V3.2.3  - Fehlerkorrekturen, Analog Joystick und L2/R2 zu Joystick-Konfiguration hinzugefügt**

**Neu : recalboxOS ist jetzt kompatibel mit RPi2!**

**Neu : Alle Informationen zum recalbox-os im [WIKI](https://github.com/digitalLumberjack/recalbox-os/wiki)**

Das Super ""Repository"" für recalbox.

Ziel dieses ""Repository"" ist die Neu-Gruppierung der verschiedenen recalbox-Projekte zu einem Ganzen und die 
Vereinfachung das System zu kompilieren.
## Vorstellung
recalboxOS ist ein leichtes, integriertes System, entwickelt für RaspberryPi und RaspberryPI 2.

Du kannst Deinen RPI in eine Emulations Plattform verwandeln, es werden bis zu 16 Systeme unterstützt!

## Eigenschaften
- Unterstützung für Atari 2600, NES, Game Boy, Game Boy Color, Game Boy Advance, Super Nintendo, Master System, Megadrive (Genesis), FBA, iMame4all, PCEngine, MSX1/2, PSX, SegaCD, Sega 32x, Sega SG1000, Famicom Disk System
- Erstellt mit ""buildroot"", dadurch ist das Haupt-Datei-System auf nur 100MB komprimiert
- Rettungssystem basierend auf NOOBS : Neuinstallation direkt von Deiner SD-Karte oder hole Dir die letzte Version aus dem Netz
- Online-Aktualisierung
- Netzwerk-Zugriff auf ROM-Verzeichnis, Screenshots, Saves, etc.
- Fat32 ROM-Partition : Mit allen Systemen kompatibel
- Controller-Konfiguration im Front-End : Einmalige Konfiguration, überall spielen
- Hintergrundmusik im Front-End
- Integrierte PS3 und ShanWan Bluetooth Unterstützung (Einen Controller anschliessen, wieder entfernen und spielen)
- Französisch, Englisch, Portugiesisch (Dank an mgoulart), Spanisch, Deutsch und mit Unterstützung vielleicht noch mehr
- Front-End basierend auf dem großartigen EmulationStation2 von Aloshi
- FBA optimierte Version mit Unterstützung für bis zu 4 Spieler (Yeah Dungeons and Dragons)
- Benutze RPI GPIOs als Contoller

## Projekte
**recalboxOS** ist das Haupt-Projekt, bestehend aus 3 Unter-Projekten, die das System bilden :

- **recalbox-buildroot** :
https://github.com/digitalLumberjack/recalbox-buildroot (branch recalbox)                                                   
Das recalbox-buildroot Projekt ist ein ""buildroot"" System. Es erstellt ein komplettes Linux Betriebssystem, dass auf recalbox lauffähig ist.
Du kannst dieses Projekt kompilieren, dann die Ausgabe-Dateien auf eine manuell formartierte SD-Karte kopieren, um das System auf einem RaspberryPi zu starten. 
Es gibt aber noch einen besseren Weg, das recalbox-rescue System.

- **recalbox-rescue** : 
https://github.com/digitalLumberjack/recalbox-rescue (branch recalbox)                                                      
Basierend auf dem grandiosen NOOBS vom RPI Team, ermöglicht Dir das recalbox-rescue System eine einfache Installation von RecalboxOS und einer Rettungs-Partition auf Deiner SD-Karte. 
Es ist ein anderes, kleines Betriebssystem, welches retroboxOS runterlädt, Deine SD-Karte formatiert und das System für Dich installiert.  
Bevor die SD-Karten Version installiert wird, wird überprüft, ob eine neue Version im Internet verfügbar ist.

- **recalbox-emulationstation** :
https://github.com/digitalLumberjack/recalbox-emulationstation/tree/recalbox-buildroot
Basierend auf dem grandiosen EmulationStation2 von Aloshi, wurde das Front-End ein wenig modifiziert, 
um ogg Hintergrundmusik, Sprachauswahl, Update Unterstützung und Controller-Konfiguration zu ermöglichen.
"
299,objcode/v8,C++,
300,chef-boneyard/database,Ruby,"# Database Cookbook

[![Build Status](https://travis-ci.org/chef-cookbooks/database.svg?branch=master)](http://travis-ci.org/chef-cookbooks/database) [![Cookbook Version](http://img.shields.io/cookbook/v/database.svg)](https://supermarket.chef.io/cookbooks/database)

The main highlight of this cookbook is the `database` and `database_user` resources for managing databases and database users in a RDBMS. Providers for MySQL, PostgreSQL and SQL Server are also provided, see usage documentation below.

## DEPRECATION

This cookbook has been deprecated. The original intent was to abstract database implementation details away from the end user. In hindsight this was a mistake as it is difficult / impossible to make a generic interface for databases that works accross all systems. Instead we plan to move the individual providers into their respective cookbooks (mysql, postgresql, sqlite, sql_server) where they can fully utilize the feature set of the underlying DB systems.

## Requirements

### Platforms

- Debian / Ubuntu derivatives
- RHEL derivatives
- Fedora

### Chef

- Chef 12.1+

### Cookbooks

- postgresql

## Resources/Providers

These resources aim to expose an abstraction layer for interacting with different RDBMS in a general way. Currently the cookbook ships with providers for MySQL, PostgreSQL and SQL Server. Please see specific usage in the **Example** sections below. The providers use specific Ruby gems installed under Chef's Ruby environment to execute commands and carry out actions. These gems will need to be installed before the providers can operate correctly. Specific notes for each RDBS flavor:

- MySQL: leverages the `mysql2` gem, which can be installed with the `mysql2_chef_gem` resource prior to use (available on the Supermarket). You must depend on the `mysql2_chef_gem` cookbook, then use a `mysql2_chef_gem` resource to install it. The resource allows the user to select MySQL client library versions, as well as optionally select MariaDB libraries.

- PostgreSQL: leverages the `pg` gem which is installed as part of the `postgresql::ruby` recipe. You must declare `include_recipe ""database::postgresql""` to include this.

- SQL Server: leverages the `tiny_tds` gem which is installed as part of the `sql_server::client` recipe.

- SQLite: leverages the `sqlite3` gem which is installed as part of the `database::sqlite` recipe. You must declare `include_recipe ""database::sqlite""` to include this.

### database

Manage databases in a RDBMS. Use the proper shortcut resource depending on your RDBMS: `mysql_database`, `postgresql_database`, `sql_server_database` or `sqlite_database`.

#### Actions

- `:create`: create a named database
- `:drop`: drop a named database
- `:query`: execute an arbitrary query against a named database

#### Attribute Parameters

- database_name: name attribute. Name of the database to interact with
- connection: hash of connection info. valid keys include `:host`, `:port`, `:username`, and `:password`

  - only for MySQL DB*:

    - `:flags` (see `Mysql2::Client@@default_query_options[:connect_flags]`)
    - `:default_file`, `:default_group` (see <https://github.com/brianmario/mysql2#reading-a-mysql-config-file>)

  - only for PostgreSQL: `:database` (overwrites parameter `database_name`)

  - not used for SQLlite

- sql: string of sql or a block that executes to a string of sql, which will be executed against the database. used by `:query` action only

- The database cookbook uses the `mysql2` gem.

> ""The value of host may be either a host name or an IP address. If host is NULL or the string ""127.0.0.1"", a connection to the local host is assumed. For Windows, the client connects using a shared-memory connection, if the server has shared-memory connections enabled. Otherwise, TCP/IP is used. For a host value of ""."" on Windows, the client connects using a named pipe, if the server has named-pipe connections enabled. If named-pipe connections are not enabled, an error occurs.""

If you specify a `:socket` key and are using the mysql_service resource to set up the MySQL service, you'll need to specify the path in the form `/var/run/mysql-<instance name>/mysqld.sock`.

#### Providers

- `Chef::Provider::Database::Mysql`: shortcut resource `mysql_database`
- `Chef::Provider::Database::Postgresql`: shortcut resource `postgresql_database`
- `Chef::Provider::Database::SqlServer`: shortcut resource `sql_server_database`
- `Chef::Provider::Database::Sqlite`: shortcut resource `sqlite_database`

#### Examples

```ruby
# Create a mysql database
mysql_database 'wordpress-cust01' do
  connection(
    :host     => '127.0.0.1',
    :username => 'root',
    :password => node['wordpress-cust01']['mysql']['initial_root_password']
  )
  action :create
end
```

```ruby
# Create a mysql database on a named mysql instance
mysql_database 'oracle_rools' do
  connection(
    :host     => '127.0.0.1',
    :username => 'root',
    :socket   => ""/var/run/mysql-#{instance-name}/mysqld.sock""
    :password => node['mysql']['server_root_password']
  )
  action :create
end
```

```ruby
# Create a sql server database
sql_server_database 'mr_softie' do
  connection(
    :host     => '127.0.0.1',
    :port     => node['sql_server']['port'],
    :username => 'sa',
    :password => node['sql_server']['server_sa_password'],
    :options  => { 'ANSI_NULLS' => 'ON', 'QUOTED_IDENTIFIER' => 'OFF' }
  )
  action :create
end
```

```ruby
# create a postgresql database
postgresql_database 'mr_softie' do
  connection(
    :host      => '127.0.0.1',
    :port      => 5432,
    :username  => 'postgres',
    :password  => node['postgresql']['password']['postgres']
  )
  action :create
end
```

```ruby
# create a postgresql database with additional parameters
postgresql_database 'mr_softie' do
  connection(
    :host     => '127.0.0.1',
    :port     => 5432,
    :username => 'postgres',
    :password => node['postgresql']['password']['postgres']
  )
  template 'DEFAULT'
  encoding 'DEFAULT'
  tablespace 'DEFAULT'
  connection_limit '-1'
  owner 'postgres'
  action :create
end
```

```ruby
# Externalize conection info in a ruby hash
mysql_connection_info = {
  :host     => '127.0.0.1',
  :username => 'root',
  :password => node['mysql']['server_root_password']
}

sql_server_connection_info = {
  :host     => '127.0.0.1',
  :port     => node['sql_server']['port'],
  :username => 'sa',
  :password => node['sql_server']['server_sa_password']
}

postgresql_connection_info = {
  :host     => '127.0.0.1',
  :port     => node['postgresql']['config']['port'],
  :username => 'postgres',
  :password => node['postgresql']['password']['postgres']
}

# Same create commands, connection info as an external hash
mysql_database 'foo' do
  connection mysql_connection_info
  action :create
end

sql_server_database 'foo' do
  connection sql_server_connection_info
  action     :create
end

postgresql_database 'foo' do
  connection postgresql_connection_info
  action     :create
end

# Create database, set provider in resource parameter
database 'bar' do
  connection mysql_connection_info
  provider   Chef::Provider::Database::Mysql
  action     :create
end

database 'bar' do
  connection sql_server_connection_info
  provider   Chef::Provider::Database::SqlServer
  action     :create
end

database 'bar' do
  connection postgresql_connection_info
  provider   Chef::Provider::Database::Postgresql
  action     :create
end



# Drop a database
mysql_database 'baz' do
  connection mysql_connection_info
  action    :drop
end



# Query a database
mysql_database 'flush the privileges' do
  connection mysql_connection_info
  sql        'flush privileges'
  action     :query
end


# Query a database from a sql script on disk
mysql_database 'run script' do
  connection mysql_connection_info
  sql { ::File.open('/path/to/sql_script.sql').read }
  action :query
end



# Vacuum a postgres database
postgresql_database 'vacuum databases' do
  connection      postgresql_connection_info
  database_name 'template1'
  sql 'VACUUM FULL VERBOSE ANALYZE'
  action :query
end
```

```ruby
# Create, Insert, Query a SQLite database
# Note that inserting anything in to the database will create it automaticly.
sqlite_database 'mr_softie' do
  database_name '/path/to/database.db3'
  sql ""sql command""
  action :query
end

# Delete the database, will remove the file
sqlite_database 'mr_softie' do
  database_name '/path/to/database.db3'
  action :drop
end
```

### database_user

Manage users and user privileges in a RDBMS. Use the proper shortcut resource depending on your RDBMS: `mysql_database_user`, `postgresql_database_user`, or `sql_server_database_user`.

#### Actions

- `:create`: create a user
- `:drop`: drop a user
- `:grant`: manipulate user privileges on database objects

#### Attribute Parameters

- username: name attribute. Name of the database user
- password: password for the user account
- database_name: Name of the database to interact with
- connection: hash of connection info. valid keys include :host, :port, :username, :password
- privileges: array of database privileges to grant user. used by the :grant action. default is :all
- host: host where user connections are allowed from. used by MySQL provider only. default is '127.0.0.1'
- table: table to grant privileges on. used by :grant action and MySQL provider only. default is '*' (all tables)
- require_ssl: true or false to force SSL connections to be used for user
- require_x509: true or false to force SSL with client certificate verification

#### Providers

- `Chef::Provider::Database::MysqlUser`: shortcut resource `mysql_database_user`
- `Chef::Provider::Database::PostgresqlUser`: shortcut resource `postgresql_database_user`
- `Chef::Provider::Database::SqlServerUser`: shortcut resource`sql_server_database_user`

#### Examples

```ruby
# create connection info as an external ruby hash
mysql_connection_info = {
  :host     => '127.0.0.1',
  :username => 'root',
  :password => node['mysql']['server_root_password']
}

postgresql_connection_info = {
  :host     => '127.0.0.1',
  :port     => node['postgresql']['config']['port'],
  :username => 'postgres',
  :password => node['postgresql']['password']['postgres']
}

sql_server_connection_info = {
  :host     => '127.0.0.1',
  :port     => node['sql_server']['port'],
  :username => 'sa',
  :password => node['sql_server']['server_sa_password']
}

# Create a mysql user but grant no privileges
mysql_database_user 'disenfranchised' do
  connection mysql_connection_info
  password   'super_secret'
  action     :create
end

# Do the same but pass the provider to the database resource
database_user 'disenfranchised' do
  connection mysql_connection_info
  password   'super_secret'
  provider   Chef::Provider::Database::MysqlUser
  action     :create
end

# Create a postgresql user but grant no privileges
postgresql_database_user 'disenfranchised' do
  connection postgresql_connection_info
  password   'super_secret'
  action     :create
end

# The same as above but utilizing hashed password string instead of
# plain text one
postgresql_database_user 'disenfranchised' do
  connection    postgresql_connection_info
  password      hashed_password('md5eacdbf8d9847a76978bd515fae200a2a')
  action        :grant
end

# Do the same but pass the provider to the database resource
database_user 'disenfranchised' do
  connection postgresql_connection_info
  password   'super_secret'
  provider   Chef::Provider::Database::PostgresqlUser
  action     :create
end

# Create a sql server user but grant no privileges
sql_server_database_user 'disenfranchised' do
  connection sql_server_connection_info
  password   'super_secret'
  action     :create
end

# Drop a mysql user
mysql_database_user 'foo_user' do
  connection mysql_connection_info
  action     :drop
end

# Bulk drop sql server users
%w(disenfranchised foo_user).each do |user|
  sql_server_database_user user do
    connection sql_server_connection_info
    action     :drop
  end
end

# Grant SELECT, UPDATE, and INSERT privileges to all tables in foo db from all hosts
mysql_database_user 'foo_user' do
  connection    mysql_connection_info
  password      'super_secret'
  database_name 'foo'
  host          '%'
  privileges    [:select,:update,:insert]
  action        :grant
end

# The same as above but utilizing hashed password string instead of
# plain text one
mysql_database_user 'foo_user' do
  connection    mysql_connection_info
  password      hashed_password('*664E8D709A6EBADFC68361EBE82CF77F10211E52')
  database_name 'foo'
  host          '%'
  privileges    [:select,:update,:insert]
  action        :grant
end

# Grant all privileges on all databases/tables from 127.0.0.1
mysql_database_user 'super_user' do
  connection mysql_connection_info
  password   'super_secret'
  action     :grant
end

# grant all privileges on all tables, sequences and functions in public schema of foo db
postgresql_database_user 'foo_user' do
  connection    postgresql_connection_info
  database_name 'foo'
  schema_name 'public'
  tables [:all]
  sequences [:all]
  functions [:all]
  privileges    [:all]
  action        [:grant, :grant_schema, :grant_table, :grant_sequence, :grant_function]
end

# grant select,update,insert privileges to all tables in foo db
sql_server_database_user 'foo_user' do
  connection    sql_server_connection_info
  password      'super_secret'
  database_name 'foo'
  privileges    [:select,:update,:insert]
  action        :grant
end
```

## License & Authors

**Author:** Cookbook Engineering Team ([cookbooks@chef.io](mailto:cookbooks@chef.io))

**Copyright:** 2009-2016, Chef Software, Inc.

```
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```
"
301,microsoft/moodle-repository_onenote,PHP,"# Moodle Plugins for Microsoft Services
*including* **Office 365** *and other Microsoft services*

## Microsoft OneNote Repository Plugin

This plugin allows the user to browse their OneNote Online content, such as notebooks, sections, and pages using the Moodle file picker UI. It also allows them to download the content of their OneNote page. It uses the Microsoft OneNote API Local plugin to do some of these things.

## Design details

### Basic design
This plugin uses the API exposed by the Microsoft OneNote API local plugin for logging in, getting the list of notebooks, sections, and pages; and also for downloading the page when needed.

### Plugin dependencies
repository_onenote => local_onenote => local_msaccount

### Configuration
None. This plugin depends upon the Microsoft Account local plugin to be configured for accessing the appropriate Microsoft Live application.


This is part of the suite of Microsoft Services plugins for Moodle.

This repository is updated with stable releases. To follow active development, see: https://github.com/Microsoft/o365-moodle

## Installation

1. Unpack the plugin into /repository/onenote within your Moodle install.
2. From the Moodle Administration block, expand Site Administration and click ""Notifications"".
3. Follow the on-screen instuctions to install the plugin.

For more documentation, visit https://docs.moodle.org/30/en/Office365

## Support

If you are experiencing problems, have a feature request, or have a question, please open an issue on Github at https://github.com/Microsoft/o365-moodle.

To help developers debug problems, please include the following in all issues:
- Plugin versions.
- Moodle version.
- Detailed instructions of what went wrong and how to reproduce the problem.
- Any error messages encountered.
- PHP version.
- Database software and versions.
- Any other environmental information available.

Note that developers will triage issues and deal with more serious problems first. All issues will be addressed but some may not be addressed immediately.

## Contributing

We're looking for community contributions! Feel free to submit pull requests, but please do so against the development repository at https://github.com/Microsoft/o365-moodle. Pull requests submitted to individual plugin repositories cannot be accepted.

### Needed Contributions
Smaller issues that developers cannot address right away will be labeled with ""Help Wanted"" in the issue tracker in the development repository at https://github.com/Microsoft/o365-moodle/issues. These are only suggestions - we can also accept pull requests fixing other bugs, or even adding new features.

Pull requests adding new features are much appreciated but note that they may be rejected (even if technically sound) if they do not match the direction of the project. If you want to add a new feature, it's best to open an issue outlining your idea first, and get feedback from the maintainers.

Contributions to our documentation are especially appreciated! All documentation lives in the /local/o365docs folder of the development repository (https://github.com/Microsoft/o365-moodle). Updates to this documentation can be sent via pull request like any other contributions.

### Code Review
All pull requests go through a thorough examination from developers before they are merged. Please read our [code review process](https://github.com/Microsoft/o365-moodle/tree/master/local/o365docs/codereview.md) and ensure your code is consistent before submitting. A developer may respond with changes that are needed before a pull request can be accepted and it is up to the submitter to make those changes. If accepted, your commit will remain as-is to ensure you get credit, but developers may modify solutions slightly in subsequent commits.

### CLA
Finally, before we can accept your pull request, you'll need to electronically complete Microsoft's [Contributor License Agreement](https://cla.microsoft.com/). If you've done this for other Microsoft projects, then you're already covered.

[Why a CLA?](https://www.gnu.org/licenses/why-assign.html) (from the FSF)

# Copyright

&copy; Microsoft, Inc.  Code for this plugin is licensed under the GPLv3 license.

Any Microsoft trademarks and logos included in these plugins are property of Microsoft and should not be reused, redistributed, modified, repurposed, or otherwise altered or used outside of this plugin.
"
302,w3c/csswg-test,,"csswg-test has moved
====================

As of 28 March 2017, this repo is no more. RIP, csswg-test.

It has now been merged into [web-platform-tests][] and all tests formerly in
this repository can be found in the `css` subdirectory:
* https://github.com/w3c/web-platform-tests/tree/master/css

Issues and pull requests on this repository are kept for historical interest
only; no new issues should be opened: they will be as heard as a tree falling
in a abandoned forest.

If you have an existing pull request open here, feel free to push new changes
to it until it is ready to merge. If you were about to open a pull request
here, [rebase it onto web-platform-tests][moving-old-branches], and open it
there.


[web-platform-tests]: https://github.com/w3c/web-platform-tests
[moving-old-branches]: https://github.com/w3c/web-platform-tests/blob/master/css/README.md#importing-old-branches
"
303,devit-tel/mongoose-repository,TypeScript,"# mongoose-repository
A mongoose Repository based
Include Plugin:
- mongoose
- mongoose-delete (default options: { deletedAt: true, indexFields: true, overrideMethods: true })
- mongoose-history
- mongoose-paginate
- ~~mongoose-timestamp~~  <b><span style=""color:red"">Deprecated version 1.1.10 change use mongoose provide timestamps</span></b>
- mongoose-aggregate-paginate
- rascal

### install
```
npm install sendit-mongoose-repository --save
```

## Create repo Example
bar.repository.js file
```javascript
import mongoose from 'mongoose'
import RepositoryBuilder from 'sendit-mongoose-repository'

const schemaDefinition = {
  name: {
    type: String,
    require: true
  },
  foos: {
    type: [Number],
    require: true
  },
  company: { type: Mongoose.Schema.Types.ObjectId, ref: 'Company' },
}

export const builder = RepositoryBuilder('Bar', schemaDefinition)
export default builder.Repository
// builder provides:
//  {
//      Model,
//      Schema,
//      Repository,
//      schemaDefinition
//  }
```

## BaseRepostory provides functions
```javascript
.findOne(query: any, options: any)
.find(query: any = {}, options: any = {})
.create(data: any)
.update(query: any, data: any)
.upsert(query: any, data: any)
  (default options: {upsert: true, new: true})
.delete(data: any)
.aggregate(data: any)
.aggregatePaginate(query: any, options)

```


## Usage Example
Find one
```javascript
import BarRepository from './bar.repository.js'
export default async function list() {
  var filter = {
    name: 'default'
  }
  var options = {
    populate: 'company' //optional
  }
  return BarRepository.findOne(filter, options)
}
```

Find all
```javascript
import BarRepository from './bar.repository.js'

export default async function list() {
  var filter = {
    name: 'default'
  }
  var options = {
    populate: 'company' //optional
  }
  return BarRepository.find(filter, options)
}
```
Find with Paginate (required options.limit and options.page)

```javascript
var filter = {
  name: 'default'
}
var options = {
  limit: 10, // required
  page: 1, // required, start 1
  sort: {name: -1}, // optional, default: {_id: 1}, (ex. sort descending name)
  populate: 'company', // optional
  select: '-_id -__v -password' // optional omit _id, __v, password
}
return BarRepository.find(filter, options)
```

Create
```javascript
await BarRepository.create({ name: 'default' })
```

Update
```javascript
await BarRepository.update({ name: 'default' }, { foos: [12, 69] })
```

Delete
```javascript
await BarRepository.delete({ name: 'default' })
```

Aggregate
```javascript
import BarRepository from './bar.repository.js'

export default async function list() {
  var filter = {
    name: 'default'
  }
  var options = {
    populate: 'company' //optional
  }
  return BarRepository.find(filter, options)
}
```

Aggregate Paginate

```javascript
var aggregateQuery = [
  { $match : { name: 'default' } },
  { $project: { foos: 1 } }
]
var options = {
  limit: 10, // required
  page: 1, // required, start 1
  sort: {name: -1}
}
return BarRepository.aggregatePaginate(filter, options)
```

## AMQP

Publish queue after create or update <br/>

Pattern queue name
```
node_env.serviceName.create.model
node_env.serviceName.update.model
```

example
```
local.fleet.create.vehicles
```

```javascript
import { init } from 'sendit-mongoose-repository'

init({
  exchange: 'exchange-name',
  models: ['model'],
  ttl: 50000000000000, //millisecond
  service: 'serviceName',
  vhosts: 'local',
  connection: {
    slashes: true,
    protocol: 'amqp',
    hostname: '127.0.0.1',
    user: 'guest',
    password: 'guest',
    vhost: `//local`,
    port: 5672,
    options: {
      heartbeat: 5,
    },
    socketOptions: {
      timeout: 1000,
    },
  },
})
```

Example For cluster connections

```javascript
connections: [
        ""amqp://guest:guest@example1.com:5672/v1?heartbeat=10"",
        ""amqp://guest:guest@example2.com:5672/v1?heartbeat=10"",
        ""amqp://guest:guest@example3.com:5672/v1?heartbeat=10""
      ]
```"
304,QuickBlox/q-municate-ios,,"# Source code removed from the public access, if you are interested in Q-municate please [contact us](https://communication.quickblox.com/q-municate#rec209621870)


![](https://d2mxuefqeaa7sj.cloudfront.net/s_7BF69620C1058AA11632E980A66E2B94CAE1B1639FF018694E91270C4F3093C2_1517567392151_retina_cover-min.png)


Q-municate is an open source code of chat application with full range of communication features on board (such as messaging, file transfer, push notifications, audio/video calls, etc.).
We are inspired to give you chat application out of the box. You can customise this application depending on your needs. As always QuickBlox backend is at your service: https://quickblox.com/plans/
Find the source code and more information about Q-municate, as well as installation guide, in our Developers section: https://quickblox.com/developers/q-municate
This guide is brought to you from QuickBlox iOS team in order to explain how you can build a communication app on iOS using QuickBlox API.
It is a step by step guide designed for all developer levels including beginners as we move from simple to more complex implementation. Depending on your skills and your project requirements you may choose which parts of this guide are to follow. Enjoy and if you need assistance from QuickBlox iOS team feel free to let us know by creating an [issue](https://github.com/QuickBlox/q-municate-ios/issues).
Q-municate is a fully fledged chat application using the Quickblox API.

<!-- TOC -->

- [Q-municate 3.0.0](#q-municate-300)
  - [1. Requirements & Software Environment](#1-requirements--software-environment)
  - [2. QuickBlox modules](#2-quickblox-modules)
  - [3. Features](#3-features)
  - [4. Screens](#4-screens)
  - [5. Audio and Video Calls](#5-audio-and-video-calls)
  - [6. Extensions](#6-extensions)
  - [7. Code explanation](#7-code-explanation)
  - [8. How to build your own Chat app](#8-how-to-build-your-own-chat-app)
  - [9. Contributing](#9-contributing)
    - [Features](#features)
    - [Issues](#issues)
  - [10. License](#10-license)

<!-- /TOC -->


## 1. Requirements & Software Environment


- [Xcode 10](https://developer.apple.com/xcode/whats-new/) and later.
- iOS 10.3 and later.
- [QuickBlox iOS SDK](http://quickblox.com/developers/IOS) 2.17.4 and later.
- [QuickBlox WebRTC SDK](http://quickblox.com/developers/Sample-webrtc-ios) 2.7.4 and later.
- [Bolts](https://github.com/BoltsFramework/Bolts-ObjC#bolts) 1.9.0 version.
- [Facebook iOS SDK](https://developers.facebook.com/docs/ios) 5.6.0 version.
- [Firebase](https://fabric.io/kits/ios/digits) 6.9.0 version.

**1.1** [**QuickBlox iOS SDK**](http://quickblox.com/developers/IOS)

[![CocoaPods](https://img.shields.io/cocoapods/v/QuickBlox.svg)](https://cocoapods.org/pods/QuickBlox)
[![CocoaPods](https://img.shields.io/cocoapods/dt/QuickBlox.svg)](https://cocoapods.org/pods/QuickBlox)
[![CocoaPods](https://img.shields.io/cocoapods/dm/QuickBlox.svg)](https://cocoapods.org/pods/QuickBlox)

QuickBlox - Communication & cloud backend platform which brings superpowers to your mobile apps.

**1.2  WebRTC**

[![CocoaPods](https://img.shields.io/cocoapods/v/Quickblox-WebRTC.svg)](https://cocoapods.org/pods/Quickblox-WebRTC)
[![CocoaPods](https://img.shields.io/cocoapods/dt/Quickblox-WebRTC.svg)](https://cocoapods.org/pods/Quickblox-WebRTC)
[![CocoaPods](https://img.shields.io/cocoapods/dm/Quickblox-WebRTC.svg)](https://cocoapods.org/pods/Quickblox-WebRTC)

Q-municate uses [WebRTC](https://en.wikipedia.org/wiki/WebRTC) for video and audio calling meaning it’s a great cross-platform solution. WebRTC itself it open-source, so you can modify the code as much as you wish, or you can trust the thousands of skilled developers who contributed to it’s development.

[The VideoChat code sample](https://github.com/QuickBlox/quickblox-ios-sdk/tree/master/sample-videochat-webrtc) allows you to easily add video calling and audio calling features into your iOS app. Enable a video call function similar to FaceTime or Skype using code sample as a basis.


## 2. QuickBlox modules

Q-municate application uses following:

- [Authentication](http://quickblox.com/developers/Authentication_and_Authorization)
- [Users](http://quickblox.com/developers/Users)
- [Chat](http://quickblox.com/developers/Chat)
- [Video calling](http://quickblox.com/developers/VideoChat)
- [Content](http://quickblox.com/developers/Content)
- [Push Notifications](http://quickblox.com/developers/Messages)


## 3. Features

It includes such features as:

- The App supports both landscape and portrait mode.
- The iOS application has English language interface and easy to add localisation.
- Three sign-up methods as well as login – [Facebook](https://developers.facebook.com/docs/ios/), [Firebase](https://firebase.google.com/docs/ios/setup) (phone number) and with email/password
- Call Kit
- Share extension
- Siri extension for messaging
- View list of all active chat dialogs with message history (private and group chat dialogs)
- View, edit and leave group chat dialogs
- View and remove private chat dialogs
- Search: local dialogs search, contacts search and global users search
- Create and participate in private and group dialogs
- Managing, updating and removing dialogs
- Audio and Video calls (using QuickBlox WebRTC Framework)
- Edit own user profile
- Reset password and logout
- See other users profile
- Pull to refresh for dialogs list, contacts list and user info page


> Please note all these features are available in open source code, so you can customise your app depending on your needs.


## 4. Screens

**4.1 Welcome**


![Figure 4.1 Welcome screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977157374_welcome.png)


Available features:

- Connect with Phone – this button allows user to enter the App with his/her phone number using Firebase. If tapped will be shown User Agreement pop-up.
- Login by email or social button – By tapping this button action sheet with extra login methods will pop up. There is such methods as Facebook login and login by email/password.
- Login with Facebook allows user to enter the App with his/her Facebook credentials. If tapped will be shown User Agreement pop-up.
- If App has passed Facebook authorisation successfully, the App will redirect user into chat dialogs list screen.
- Login by email/password allows user to enter the App if he/she provides correct and valid email and password. By tapping on this button user will be redirected to the login screen.


> Please note, that there is no longer a possibility to sign up user using email and password method. You can only sign up using Phone number and/or Facebook credentials.

**4.2 Login with email/password**


![Figure 4.2 Login with email screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977175593_login_email.png)


Available features:

- Fields set:
  - Email – text/numeric/symbolic fields 3 chars min - no border, mandatory (email symbols validation included)
  - Password – text/numeric/symbolic field 8-40 chars (should contain alphanumeric and punctuation characters only) , mandatory
- Buttons:
  - Back - returns user back to welcome screen
  - Done - performing login after fields validation using provided email and password
  - Forgot password - opens forgot password screen

**4.3 Forgot password**


![Figure 4.3 Forgot password screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977185493_forgot_password.png)



- Fields set:
  - Email – text/numeric/symbolic fields 3 chars min - no border, mandatory (email symbols validation included)
- Buttons:
  - Back - returns user back to welcome screen
  - Reset - performing password reset

**4.4 Tab Bar**

Tab bar is a main controller of the application. It consists of such pages:

- Chat dialogs list (main page)
- Contacts list
- Settings

**4.5 Chat Dialogs List**


![Figure 4.4 Dialogs screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977194742_chats.png)



- Search:
  - Search allows user to filter existing dialogs in local cache by its names.
- Buttons:
  - Right bar button - redirects user to new dialog screen
  

**4.6 New Message**


![Figure 4.5 New message screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977203691_new_message.png)


If you will select only 1 contact - private chat will be opened (if existent) or created if needed. Otherwise group chat will be created.

- Search:
  - Tag field allows you to search through contacts full names.
- Buttons:
  - Right bar button - creates chat dialog
  - Back - return user back to chat dialogs page
  

**4.7 Chat**

There is a possibility to send:

- Text messages
- Images from gallery and camera
- Videos from gallery and camera
- Audio records using input toolbar right button

Available features:

- Sharing and forwarding
- Copying image attachmnets and text messages

**4.8 Private Chat**


![Figure 4.6 Private chat screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977220481_private_chat.png)


Buttons:

- Right bar buttons - Audio and Video call buttons, you can only call user if he is in your contact list
- Back - returns user back to chat dialogs list screen
- Navigation bar title - redirects user to opponent profile page

**4.9 Group Chat**


![Figure 4.7 Group chat screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977229548_group_chat.png)


Buttons:

- Right bar button and navigation bar title - redirects user to group chat info screen
- Back - return user to chat dialogs list screen
- Opponent user avatars - by tapping opponent user avatars in messages you will be redirected to the info page of that user

**4.10 Group Chat Info**


![Figure 4.8 Group chat info screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977239195_group_info.png)


Fields/Buttons:

- By tapping on group avatar you can change it by taking a new photo or selecting it from library
- By tapping on group name you will be redirected to group name change screen
- By tapping on Add member field you will be redirected to contacts screen in order to select users to add
- By tapping on any user in members list you will be redirected to their info page (except your own user in list)
- By tapping Leave and remove chat field - you will leave existent group chat and delete it locally

**4.11 Contacts List**


![Figure 4.9 Contacts list screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977255219_contacts.png)


Search:

- Search has two scopes buttons:
  - Local search - allows user to filter existing contacts by their names.
  - Global search - allows user to find users and see their profiles by full names.


![Figure 4.10 Search screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516968530350_ContactsSearch+1.png)


**4.12 User Info**


![Figure 4.11 User info screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977275044_profile.png)


Fields/Buttons

- Contacts actions:
- Send message - opens chat with user, if there is no chat yet - creates it
- Audio Call - audio call to user
- Video Call - video call to user
- Remove Contact and Chat - deleting user from contact list and chat with him

Other user actions:

- Add contact - sending a contact request to user or accepting existing one

**4.13 Settings**


![Figure 4.12 Settings screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977285595_settings.png)


Fields/Buttons:

- Full name, status and email fields will redirect you to update field screen, where you can change your info.


![Figure 4.13 User status screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977297040_status.png)



- By tapping on avatar action sheet will be opened. You can either take a new picture or choose it from library to update your user avatar.
- Push notification switch - you can either subscribe or unsubscribe from push notifications.
- Tell a friend - opens share controller where you can share this awesome app with your friends :)
- Give feedback - feedback screen, where you can send an email to us with bugs, improvements or suggestion information in order to help us make Q-municate better!


![Figure 4.14 Feedback screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977305837_feedback.png)



## 5. Audio and Video Calls

Q-municate using QuickBlox WebRTC SDK as call service. You can find more information on it [here](http://quickblox.com/developers/Sample-webrtc-ios).

**5.1 Calls controller**

Call controller has 6 states:

- Incoming audio call
- Incoming video call
- Outgoing audio call
- Outgoing video call
- Active audio call
- Active video call

Call controller is been managed by QMCallManager, basically call manager allocating it with a specific state, whether it is an incoming or outgoing call, then call controller changing its state to active one if required user accepts it.

**5.2 Audio Call**

You can see down below Incoming, outgoing and active audio call screens.


![Figure 5.1 Audio call screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516983533322_AudioCallScreens.png)


Toolbar buttons

- Incoming call:
  - Decline - declines call and closes received session and controller
  - Accept - accepts call and changes call controller state to Active audio call
- Outgoing and active call:
  - Microphone - disables microphone for current call
  - Speaker - whether sound should be played in speaker or receiver. Default for audio calls is receiver.
  - Decline - hanging up current all and closing controller

**5.3 Video Call**

You can see down below Incoming, outgoing and active video call screens.


![Figure 5.1 Video call screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516977747381_video_call.png)


By default sound for video calls is in speakers.

- Incoming call:
  - Decline - declines call and closes received session and controller
  - Accept - accepts call and changes call controller state to Active video call
- Outgoing and active call:
  - Camera - enables/disables camera for current call
  - Camera rotation - changes camera for current call (front/back)
  - Microphone - disables microphone for current call
  - Decline - hanging up current all and closing controller

**5.4 Call Kit**

CallKit allows to integrate calling services with other call-related apps on the system. CallKit provides the calling interface, and we handle the back-end communication with [our VoIP service](https://quickblox.com/developers/SimpleSample-messages_users-ios#Adding_support_for_VOIP_push_notifications). For incoming and outgoing calls, CallKit displays the same interfaces as the Phone app, giving Q-municate application a more native look and feel.

![Figure 5.1 Call controller screen](https://d2mxuefqeaa7sj.cloudfront.net/s_7BF69620C1058AA11632E980A66E2B94CAE1B1639FF018694E91270C4F3093C2_1517255409865_call_kit.png)



## 6. Extensions

**6.1 Share extension**

[Share extension](https://developer.apple.com/library/content/documentation/General/Conceptual/ExtensibilityPG/Share.html#//apple_ref/doc/uid/TP40014214-CH12-SW1) gives users a convenient way to share content with other entities.
Available types for sharing:

- Locations
- URL
- Images
- Videos
- Audios


![Figure 6.1 Share extension screen](https://d2mxuefqeaa7sj.cloudfront.net/s_93E53399630C968604A237F0EAB1B99A1C51B88BD402C4A305A46BBA63CA3D8B_1516980034230_Untitled-1.png)


**6.2 Intents App Extension(SiriKit for messaging)**

An *Intents app extension* receives user request to send a message from [SiriKit](https://developer.apple.com/documentation/sirikit) and turns it into app-specific actions.


![Figure 6.2 Sending message via SIRI screen](https://d2mxuefqeaa7sj.cloudfront.net/s_7BF69620C1058AA11632E980A66E2B94CAE1B1639FF018694E91270C4F3093C2_1517494955322_siri-min.png)



## 7. Code explanation

You can see basic code explanation down below. For detailed one please see our inline documentation for header files in most classes. We have tried to describe as detailed as possible the purpose of every class and its methods. If you have any questions, feel free to let us know by creating an [issue](https://github.com/QuickBlox/q-municate-ios/issues).

**7.1 Storyboards**

We have separated Q-municate for modules, such as:

- Auth
- Main
- Chat
- Settings

Each module has its own storyboard, all storyboards are linked with storyboard links (feature available since Xcode 7 and iOS 8+).

## 8. How to build your own Chat app

If you want to build your own app using Q-municate as a basis, please follow our [detailed guide here](http://quickblox.com/developers/Q-municate#How_to_build_your_own_Chat_app).

## 9. Contributing

### Features
1. Fork it ( https://github.com/[my-github-username]/q-municate-ios/fork )
2. Create your feature branch (`git checkout -b my-new-feature`)
3. Commit your changes (`git commit -am 'My new feature'`)
4. Push to the branch (`git push origin my-new-feature`)
5. Create a new Pull Request

### Issues

If you find an issue, please [create an issue](https://github.com/QuickBlox/q-municate-ios/issues).

## 10. License

Apache License, Version 2.0. See [LICENSE](LICENSE) file.
"
305,CoskunKurtuldu/GenericRepositoryPattern,C#,
306,chef-cookbooks/sudo,Ruby,"# sudo cookbook

[![Build Status](https://travis-ci.org/chef-cookbooks/sudo.svg?branch=master)](http://travis-ci.org/chef-cookbooks/sudo) [![Cookbook Version](https://img.shields.io/cookbook/v/sudo.svg)](https://supermarket.chef.io/cookbooks/sudo)

The default recipe configures the `/etc/sudoers` file. The cookbook also includes a sudo resource to adding and removing individual sudo entries.

NOTE: The `sudo` resource is now built into Chef 14+. When Chef 15 is released (April 2019) this resource will be removed from this cookbook as all users should be on Chef 14+.

## Requirements

### Platforms

- AIX
- Debian/Ubuntu
- RHEL/CentOS/Scientific/Amazon/Oracle
- Amazon Linux
- FreeBSD
- macOS
- openSUSE / SUSE Enterprise

### Chef

- Chef 12.21.3+

### Cookbooks

- None

## Resource

Use the sudo resource to add or remove individual sudo entries using sudoers.d files.

**Note** Sudo version 1.7.2 or newer is required to use the sudo resource as it relies on the ""#includedir"" directive introduced in version 1.7.2\. The resource does not enforce installing the version. Supported releases of Ubuntu, Debian and RHEL (6+) all support this feature.

### Actions

- `:create` - Create a sudoers config
- `:delete` - Delete a sudoers config

### Properties

Property            | Description                                                                                                                                                                                              | Example Value                            | Default Value
------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- | ---------------
`filename`          | name of the `/etc/sudoers.d` file                                                                                                                                                                        | restart-tomcat                           | resource's name
`commands`          | array of commands this sudoer can execute, they must contain a full path. Example: use `/usr/bin/tail` over `tail`                                                                                                                                                                | ['/etc/init.d/tomcat restart']           | ['ALL']
`groups`            | group(s) to provide sudo privileges to. This accepts either an array or a comma separated list. Leading % on group names is optional. This property was named 'group' prior to the 5.1 cookbook release. | %admin,superadmin                        | []
`nopasswd`          | allow running sudo without specifying a password sudo                                                                                                                                                    | true                                     | false
`noexec`            | prevents commands from shelling out                                                                                                                                                                      | true                                     | false
`runas`             | User the command(s) can be run as                                                                                                                                                                        | root                                     | ALL
`template`          | the erb template to render instead of the default                                                                                                                                                        | restart-tomcat.erb                       |
`users`             | user(s) to provide sudo privileges to. This accepts either an array or a comma separated. This property was named 'user' prior to the 5.1 cookbook release. list.                                        | [tomcat, webapp]                         | []
`defaults`          | array of defaults this user has                                                                                                                                                                          | ['!requiretty','env_reset']              |
`setenv`            | whether to permit the preserving of environment with `sudo -E`                                                                                                                                           | true                                     | false
`env_keep_add`      | array of strings to add to env_keep                                                                                                                                                                      | ['HOME', 'MY_ENV_VAR MY_OTHER_ENV_VAR']  |
`env_keep_subtract` | array of strings to remove from env_keep                                                                                                                                                                 | ['DISPLAY', 'MY_SECURE_ENV_VAR']         |
`variables`         | the variables to pass to the custom template. Ignored if not using a custom template.                                                                                                                    | commands: ['/etc/init.d/tomcat restart'] |

**If you use the template property, all other properties will be ignored except for the variables property.**

### Examples

#### user bob sudo privileges for any command

```ruby
sudo 'bob' do
  user 'bob'
end
```

#### group sysadmin passwordless sudo privileges for any command

```ruby
sudo ""sysadmin"" do
  group ""sysadmin""
  nopasswd true
end
```

#### group sysadmin/superadmin and user bob passwordless sudo privileges for any command

```ruby
sudo ""sysadmin"" do
  group ['sysadmin', 'superadmin']
  user ""bob""
  nopasswd true
end
```

### Built-In vs. Provided Templates

The resource provides two methods for templating the sudoers config files:

1. Using the built-in template
2. Using a custom, cookbook-level template

Both methods will create the `/etc/sudoers.d/#{resourcename}` files with the correct permissions.

The resource also performs **fragment validation**. If a sudoer-fragment is not valid, the Chef run will throw an exception and fail. This ensures that your sudoers file is always valid and cannot become corrupt (from this cookbook).

#### Using the Built-in Template

```ruby
sudo 'tomcat' do
  user      '%tomcat'    # or a username
  runas     'app_user'   # or 'app_user:tomcat'
  commands  ['/etc/init.d/tomcat restart']
end
```

#### Specifying Your Own Template

```ruby
sudo 'tomcat' do
  template    'my_tomcat.erb' # local cookbook template
  variables   cmds: ['/etc/init.d/tomcat restart']
end
```

In either case, the following file would be generated in `/etc/sudoers.d/tomcat`

```bash
# This file is managed by Chef for node.example.com
# Do NOT modify this file directly.

%tomcat ALL=(app_user) /etc/init.d/tomcat restart
```

## Usage

We highly recommend using the sudo resource to define individual sudo entries, but this cookbook also ships with a recipe that can be included on a run_list and controlled using attributes.

### Attributes

- `node['authorization']['sudo']['groups']` - groups to enable sudo access (default: `[]`)
- `node['authorization']['sudo']['users']` - users to enable sudo access (default: `[]`)
- `node['authorization']['sudo']['passwordless']` - use passwordless sudo (default: `false`)
- `node['authorization']['sudo']['include_sudoers_d']` - include and manage `/etc/sudoers.d` (default: `true` on Linux systems. Note: older / EOL distros do not support this feature)
- `node['authorization']['sudo']['agent_forwarding']` - preserve `SSH_AUTH_SOCK` when sudoing (default: `false`)
- `node['authorization']['sudo']['sudoers_defaults']` - Array of `Defaults` entries to configure in `/etc/sudoers`
- `node['authorization']['sudo']['setenv']` - Whether to permit preserving of environment with `sudo -E` (default: `false`)

### Using the Attributes

To use attributes for defining sudoers, set the attributes above on the node (or role) itself:

```json
{
  ""default_attributes"": {
    ""authorization"": {
      ""sudo"": {
        ""groups"": [""admin"", ""wheel"", ""sysadmin""],
        ""users"": [""jerry"", ""greg""],
        ""passwordless"": ""true""
      }
    }
  }
}
```

```json
{
  ""default_attributes"": {
    ""authorization"": {
      ""sudo"": {
        ""command_aliases"": [{
          ""name"": ""TEST"",
          ""command_list"": [
            ""/usr/bin/ls"",
            ""/usr/bin/cat""
          ]
        }],
        ""custom_commands"": {
          ""users"": [
            {
              ""user"": ""test_user"",
              ""passwordless"": true,
              ""command_list"": [
                ""TEST""
              ]
            }
          ],
          ""groups"": [
            {
              ""group"": ""test_group"",
              ""passwordless"": false,
              ""command_list"": [
                ""TEST""
              ]
            }
          ]
        }
      }
    }
  }
}
```

```ruby
# roles/example.rb
default_attributes(
  ""authorization"" => {
    ""sudo"" => {
      ""groups"" => [""admin"", ""wheel"", ""sysadmin""],
      ""users"" => [""jerry"", ""greg""],
      ""passwordless"" => true
    }
  }
)
```

**Note that the template for the sudoers file has the group ""sysadmin"" with ALL:ALL permission, though the group by default does not exist.**

### Sudoers Defaults

Configure a node attribute, `node['authorization']['sudo']['sudoers_defaults']` as an array of `Defaults` entries to configure in `/etc/sudoers`. A list of examples for common platforms is listed below:

_Debian_

```ruby
node.default['authorization']['sudo']['sudoers_defaults'] = ['env_reset']
```

_Ubuntu_

```ruby
node.default['authorization']['sudo']['sudoers_defaults'] = [
  'env_reset',
  'secure_path=""/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin""'
]
```

_FreeBSD_

```ruby
node.default['authorization']['sudo']['sudoers_defaults'] = [
  'env_reset',
  'secure_path=""/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin""'
]
```

_RHEL family 6.x_

```ruby
node.default['authorization']['sudo']['sudoers_defaults'] = [
  '!visiblepw',
  'env_reset',
  'env_keep =  ""COLORS DISPLAY HOSTNAME HISTSIZE INPUTRC KDEDIR LS_COLORS""',
  'env_keep += ""MAIL PS1 PS2 QTDIR USERNAME LANG LC_ADDRESS LC_CTYPE""',
  'env_keep += ""LC_COLLATE LC_IDENTIFICATION LC_MEASUREMENT LC_MESSAGES""',
  'env_keep += ""LC_MONETARY LC_NAME LC_NUMERIC LC_PAPER LC_TELEPHONE""',
  'env_keep += ""LC_TIME LC_ALL LANGUAGE LINGUAS _XKB_CHARSET XAUTHORITY""',
  'env_keep += ""HOME""',
  'always_set_home',
  'secure_path = /sbin:/bin:/usr/sbin:/usr/bin'
]
```

_macOS_

```ruby
node.default['authorization']['sudo']['sudoers_defaults'] = [
  'env_reset',
  'env_keep += ""BLOCKSIZE""',
  'env_keep += ""COLORFGBG COLORTERM""',
  'env_keep += ""__CF_USER_TEXT_ENCODING""',
  'env_keep += ""CHARSET LANG LANGUAGE LC_ALL LC_COLLATE LC_CTYPE""',
  'env_keep += ""LC_MESSAGES LC_MONETARY LC_NUMERIC LC_TIME""',
  'env_keep += ""LINES COLUMNS""',
  'env_keep += ""LSCOLORS""',
  'env_keep += ""TZ""',
  'env_keep += ""DISPLAY XAUTHORIZATION XAUTHORITY""',
  'env_keep += ""EDITOR VISUAL""',
  'env_keep += ""HOME MAIL""'
]
```

## Maintainers

This cookbook is maintained by Chef's Community Cookbook Engineering team. Our goal is to improve cookbook quality and to aid the community in contributing to cookbooks. To learn more about our team, process, and design goals see our [team documentation](https://github.com/chef-cookbooks/community_cookbook_documentation/blob/master/COOKBOOK_TEAM.MD). To learn more about contributing to cookbooks like this see our [contributing documentation](https://github.com/chef-cookbooks/community_cookbook_documentation/blob/master/CONTRIBUTING.MD), or if you have general questions about this cookbook come chat with us in #cookbok-engineering on the [Chef Community Slack](http://community-slack.chef.io/)

## License

**Copyright:** 2008-2018, Chef Software, Inc.

```
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```
"
307,tomgi/git_stats,Ruby,"# GitStats [![Build Status](https://secure.travis-ci.org/tomgi/git_stats.svg)](https://secure.travis-ci.org/tomgi/git_stats) [![Build Status](https://codeclimate.com/badge.png)](https://codeclimate.com/github/tomgi/git_stats)

GitStats is a git repository statistics generator.
It browses the repository and outputs html page with statistics.

## Examples
* [devise](http://tomgi.github.com/git_stats/examples/devise/index.html)
* [devise_invitable](http://tomgi.github.com/git_stats/examples/devise_invitable/index.html)
* [john](http://tomgi.github.com/git_stats/examples/john/index.html)
* [jquery](http://tomgi.github.com/git_stats/examples/jquery/index.html)
* [merit](http://tomgi.github.com/git_stats/examples/merit/index.html)
* [paperclip](http://tomgi.github.com/git_stats/examples/paperclip/index.html)
* [rails](http://tomgi.github.com/git_stats/examples/rails/index.html)

## Installation

### Existing ruby/gem environment

    $ gem install git_stats
    
### debian stretch (9.*)

    # apt-get install ruby ruby-nokogiri ruby-nokogiri-diff ruby-nokogumbo
    # gem install git_stats
    
### Ubuntu

    $ sudo apt-get install ruby ruby-dev gcc zlib1g-dev make
    $ sudo gem install git_stats

## Usage

### Generator

#### Print help

    $ git_stats
    Commands:
      git_stats generate        # Generates the statistics of a repository
      git_stats help [COMMAND]  # Describe available commands or one specific command

#### Print help of the generate command

    $ git_stats help generate
    Usage:
      git_stats generate

    Options:
      p, [--path=PATH]                          # Path to repository from which statistics should be generated.
                                                # Default: .
      o, [--out-path=OUT_PATH]                  # Output path where statistics should be written.
                                                # Default: ./git_stats
      l, [--language=LANGUAGE]                  # Language of written statistics.
                                                # Default: en
      f, [--first-commit-sha=FIRST_COMMIT_SHA]  # Commit from where statistics should start.
      t, [--last-commit-sha=LAST_COMMIT_SHA]    # Commit where statistics should stop.
                                                # Default: HEAD
      s, [--silent], [--no-silent]              # Silent mode. Don't output anything.
      d, [--tree=TREE]                          # Tree where statistics should be generated.
                                                # Default: .
      c, [--comment-string=COMMENT_STRING]      # The string which is used for comments.
                                                # Default: //

    Generates the statistics of a repository



#### Start generator with default settings

    $ git_stats generate
      git rev-list --pretty=format:'%h|%at|%ai|%aE' HEAD | grep -v commit
      git shortlog -se HEAD
      ...

#### Start generator with some parameters in long and short form.

    $ git_stats generate -o stats --langugage de
      git rev-list --pretty=format:'%h|%at|%ai|%aE' HEAD | grep -v commit
      git shortlog -se HEAD
      ...

### API usage example

    > repo = GitStats::GitData::Repo.new(path: '.', first_commit_sha: 'abcd1234', last_commit_sha: 'HEAD')
    > repo.authors
    => [...]
    > repo.commits
    => [...]
    > commit.files
    => [...]


## Contributing

1. Fork it
2. Create your feature branch (`git checkout -b my-new-feature`)
3. Commit your changes (`git commit -am 'Added some feature'`)
4. Make sure to add tests for it. This is important so I don't break it in a future version unintentionally.
5. Push to the branch (`git push origin my-new-feature`)
6. Create new Pull Request
"
308,Finesse/web-fonts-repository,PHP,"# Web fonts repository

[![Latest Stable Version](https://poser.pugx.org/finesse/web-fonts-repository/v/stable)](https://packagist.org/packages/finesse/web-fonts-repository)
[![Total Downloads](https://poser.pugx.org/finesse/web-fonts-repository/downloads)](https://packagist.org/packages/finesse/web-fonts-repository)
![PHP from Packagist](https://img.shields.io/packagist/php-v/finesse/web-fonts-repository.svg)
[![Test Status](https://github.com/finesse/web-fonts-repository/workflows/Test/badge.svg)](https://github.com/Finesse/web-fonts-repository/actions?workflow=Test)
[![Maintainability](https://api.codeclimate.com/v1/badges/db0fe71d28df4626145f/maintainability)](https://codeclimate.com/github/Finesse/web-fonts-repository/maintainability)
[![Test Coverage](https://api.codeclimate.com/v1/badges/db0fe71d28df4626145f/test_coverage)](https://codeclimate.com/github/Finesse/web-fonts-repository/test_coverage)

A simple webfont hosting inspired by [Google Fonts](http://fonts.google.com).
It runs on your server, stores and distributes webfont files and generates CSS on-the-go for embedding fonts on web pages.


## Quick start

### Requirements

1. HTTP server supporting PHP ≥ 7.0
2. [Composer](http://getcomposer.org) (required for installation)

### Installation

#### 1. Download the source code

Run the following code in the console:

```bash
composer create-project finesse/web-fonts-repository webfonts
```

Where `webfonts` is a path to a directory where the repository should be installed.

Or you can make some things manually:

1. Download [the source code from GitHub](http://github.com/Finesse/web-fonts-repository/archive/master.zip) and extract it.
2. Open a terminal and go to the source code root.
3. Install the libraries by running in the terminal:
    ```bash
    composer install
    ```
4. Prepare the repository by running in the terminal:
    ```bash
    composer run-script post-create-project-cmd
    ```

#### 2. File permissions

Give the user behalf which the web server runs permissions to write inside the `logs` directory.

You can just run this in the console:

```bash
# Don't do it in production!
chmod 777 logs
```

#### 3. Web server

Make the directory `public` be the document root of the web server.
Or just open [http://localhost/public](http://localhost/public) if you installed the repository to the web server root.

Make all the requests to not-existing files be handled by `public/index.php`. 
If your server is Apache, it's already done.

Make the server add the `Access-Control-Allow-Origin: *` HTTP-header to the font files. 
Otherwise some browsers will reject using fonts from the repository.
* Apache: all you need to do is to make sure that the `mod_header.c` module is on
  (run the `a2enmod headers` command and restart the server to turn it on).
* Nginx: use [this instruction](http://davidwalsh.name/cdn-fonts).

### Setup

Put your font files (woff, woff2, ttf, otf, eot, svg) to the `public/fonts` directory. You may separate them by subdirectories.
You can convert webfont files using [Transfonter](http://transfonter.org).

All settings go to the file `config/settings-local.php`.
If you don't have it, copy it from the file `config/settings-local.php.example`.

Parameters:

#### `displayErrorDetails`

Whether errors details should be sent to browser. Anyway errors are written to the file `logs/app.log`.
**You should turn it off on production server.**

#### `logger`/`level`

How many messages should be logged to the file.
The value is one of the [`\Psr\Log\LogLevel`](http://github.com/php-fig/fig-standards/blob/master/accepted/PSR-3-logger-interface.md#5-psrlogloglevel) constants.
You can read more about log levels [here](http://github.com/apix/log#log-levels).

#### `fonts`

The list of fonts available in the repository. Simple example:

```php
return [
    // ...
    'fonts' => [
        'Open Sans' => [
            'styles' => [
                '300' => 'OpenSans/opensans-light.*',
                '300i' => 'OpenSans/opensans-light-italic.*',
                '400' => 'OpenSans/opensans-regular.*',
                '400i' => 'OpenSans/opensans-regular-italic.*',
            ]
        ],
        'Roboto' => [
            'styles' => [
                '300' => 'Roboto/roboto-light.*',
                '400' => 'Roboto/roboto-regular.*',
                '500' => 'Roboto/roboto-medium.*',
                '700' => 'Roboto/roboto-bold.*',
            ]
        ]
    ]
];
```

The `fonts` array keys are the font families names. The `styles` arrays keys are the styles names.
The numbers in the style names are the font weights, `i` stands for italic.

The font file paths are given relative to the `public/fonts` directory. 
The file paths are the [glob](http://en.wikipedia.org/wiki/Glob_(programming)) search patterns.
It means that the repository should consider all files matching the pattern as font files.

You can find more examples and possibilities [here](docs/fonts-setup.md).

### Usage

Add a `<link>` tag to the HTML code of the page on which you want to embed a font:

```html
<link rel=""stylesheet"" href=""http://web-fonts-repository.local/css?family=Open+Sans:400,400i,700,700i|Roboto:300,400"" />
```

Where `http://web-fonts-repository.local` is the root URL of an installed web fonts repository.

The required fonts are specified the same way as on Google Fonts. Font families are divided by `|`, families styles
are divided by `,`, family name is separated from styles list using `:`.

You may omit the styles list. In this case the regular style (`400`) is used.

```html
<link rel=""stylesheet"" href=""http://web-fonts-repository.local/css?family=Open+Sans"" />
```

You can specify a value for the [font-display](https://developer.mozilla.org/en-US/docs/Web/CSS/@font-face/font-display)
style property using `display` parameter. Example:

```html
<link rel=""stylesheet"" href=""http://web-fonts-repository.local/css?family=Open+Sans&display=swap"" />
```

Then embed a font in a CSS code:

```css
body {
    font-family: 'Open Sans', sans-serif;
}
```

## Versions compatibility

The project follows the [Semantic Versioning](http://semver.org).

It means that patch versions are fully compatible (i.e. 1.2.1 and 1.2.2), minor versions are backward compatible 
(i.e. 1.2.1 and 1.3.2) and major versions are not compatible (i.e. 1.2.1 and 3.0).
The pre-release versions (0.*) are a bit different: patch versions are backward compatible and minor versions are not 
compatible.


## License

MIT. See [the LICENSE](LICENSE) file for details.
"
309,supermamon/Reposi3,JavaScript,"## Archived

I started this project to provide a non-complex way for IOS Jailbreak tweak developers to host packages and depictions on their personal repositories. What started as a hobby has become one of the most popular Cydia repo templates - 272 forks and 173 stars as of this writing.

But life catches up and what was a hobby now has reached it's time to wind down. 

Thank you for all the support.

---

# Reposi3

A Cydia repository template. This template contains sample on how you can easily make depiction pages without replicating your html pages. The pages are styled using [Bootstrap](http://getbootstrap.com/) which is really easy to use. You can see how it looks like by visiting [this sample repo](https://supermamon.github.io/Reposi3/) on your desktop or mobile phone.

Most data for this repo is stored on XML files and are loaded on the depiction page dynamically. See the guide below on how to set it up. Note that this guide doesn't cover creating .deb files but will briefly cover assiging depictions.

## How to use this template

### 1. Download

If you are *not* hosting your repo on [Github Pages](https://pages.github.com/), you can download the zip file [here](https://github.com/supermamon/Reposi3/archive/master.zip) and extract to a subfolder on your website.

There are 2 options for those using [Github Pages](https://pages.github.com/).

A. If you want to use your root `username.github.io` as your repo, fork this repo and rename it to `username.github.io`. So when adding it in Cydia, use `https://username.github.io`.

B. If you want to use a subfolder for your existing `username.github.io` as your repo (example `username.github.io/repo`), fork this repo and rename it to `repo`. So when adding it in Cydia, use `https://username.github.io/repo`.

You can change `repo` to anything you want, like `cydia` for example. So your repo url would be `https://username.github.io/cydia`.


#### 2. Personalize

**Release File**

Edit `Release` file. Modify the items pointed by `<--`

    Origin: Reposi3  <--
    Label: Reposi3   <--
    Suite: stable
    Version: 1.0
    Codename: ios
    Architectures: iphoneos-arm
    Components: main
    Description: Reposi3 - a cydia repo template  <--

**Branding**



Edit `index.html`
* Change the page title in the `<title>Reposi3</title>` tag
* See lines 20 and 21.
* Change line 20 into your own **brand** and line 21 to have your own URL.
* Line2 30-51 contains the list of featured packages. You can edit those or remove them totally.
* Replace CydiaIcon.png.


**Page Footers**

This data are the links that appear at the bottom of every depication. The data is stored in `repo.xml` at the root folder of your repo.

```xml
<repo>
    <footerlinks>
        <link>
            <name>Follow me on Twitter</name>
            <url>https://twitter.com/reposi3</url>
            <iconclass>glyphicon glyphicon-user</iconclass>
        </link>
        <link>
            <name>I want this depiction template</name>
            <url>https://github.com/supermamon/Reposi3</url>
            <iconclass>glyphicon glyphicon-thumbs-up</iconclass>
        </link>
    </footerlinks>
</repo>
```


#### 3. Your repo is _almost_ ready.
At this point your commit your changes to github and your repo is basically ready to be added into Cydia.
You can also visit your repo's homepage by going to `https://username.github.io/repo/`.
It will come with 2 sample packages, Old Package and New Package.
Each of the packages have a link on this page pointing to their depictions.
Next guide will show you how to assign and customize your depiction pages.

## Adding packages first package to your repo

#### 1. Adding a simple depiction page

Go to the depictions folder and duplicate the folder `com.supermamon.oldpackage`.
Rename the duplicate with the same name as your package name.
There are 2 files inside the folder - `info.xml` and `changelog.xml`.
Update the 2 files with information regading your package.
The tags are pretty much self-explanatory.
Contact [@reposi3](https://twitter.com/reposi3) for questions.

`info.xml`.
```xml
<package>
    <id>com.supermamon.oldpackage</id>
    <name>Old Package</name>
    <version>1.0.0-1</version>
    <compatibility>
        <firmware>
            <miniOS>5.0</miniOS>
            <maxiOS>7.0</maxiOS>
            <otherVersions>unsupported</otherVersions>
            <!--
            for otherVersions, you can put either unsupported or unconfirmed
            -->
        </firmware>
    </compatibility>
    <dependencies></dependencies>
    <descriptionlist>
        <description>This is an old package. Requires iOS 7 and below..</description>
    </descriptionlist>
    <screenshots></screenshots>
    <changelog>
        <change>Initial release</change>
    </changelog>
    <links></links>
</package>
```

`changelog.xml`.
```xml
<changelog>
    <changes>
        <version>1.0.0-1</version>
        <change>Initial release</change>
    </changes>
</changelog>
```


#### 2. Link the depiction page your tweak's `control` file

For the depictions to appear on Cydia, you will need to add the depictions url at the end of your package's `control` file before compiling it.
The control file should look like this:

```text
Package: com.supermamon.oldpackage
Name: Old Package
Section: Tweaks
Depends: firmware (<7.0)
Description: This is a sample old package. Firmware should be lower than 7.0
Depiction: https://username.github.io/repo/depictions/?p=[idhere]
```

Replace `[idhere]` with your actual package name.

```text
Depiction: https://username.github.io/repo/depictions/?p=com.supermamon.oldpackage
```

#### 3. Rebuilding the `Packages` file

With your updated `control` file, build your tweak.
Store the resulting `.deb.` file into the `/debs/` folder of your repo.
Build your `Packages` file and compress with `bzip2`.

```sh
user:~/ $ cd repo
user:~/repo $ dpkg-scanpackages -m ./debs > Packages
user:~/repo $ bzip2 Packages
```

_Windows users, see [dpkg-scanpackages-py](https://github.com/supermamon/dpkg-scanpackages-py) or [scanpkg](https://github.com/mstg/scanpkg)._

#### 5. Cydia at last!

Push your changes again to Github and if you haven't done yet, go ahead and add your repo to Cydia.
You should now be able to install your tweak into your own repo.

### Cleanup

Just a cleanup step, remove the debs that came with this template and re-run the commands on step 3. You can keep the sample depictions for reference by they're not needed for your repo.
"
310,tv42/gitosis,Python,"==========================================================
 ``gitosis`` -- software for hosting ``git`` repositories
==========================================================

	Manage ``git`` repositories, provide access to them over SSH,
	with tight access control and not needing shell accounts.

.. note::

	Documentation is still lacking, and non-default configurations
	(e.g. config file, repositories, installing in a location that
	is not in ``PATH``) basically have not been tested at all.
	Basic usage should be very reliable -- the project has been
	hosting itself for a long time. Any help is welcome.

``gitosis`` aims to make hosting ``git`` repos easier and safer. It
manages multiple repositories under one user account, using SSH keys
to identify users. End users do not need shell accounts on the server,
they will talk to one shared account that will not let them run
arbitrary commands.

``gitosis`` is licensed under the GPL, see the file ``COPYING`` for
more information.

You can get ``gitosis`` via ``git`` by saying::

    git clone https://github.com/tv42/gitosis.git

And install it via::

    python setup.py install

Though you may want to use e.g. ``--prefix=``.


Setting up
==========

First, we will create the user that will own the repositories. This is
usually called ``git``, but any name will work, and you can have more
than one per system if you really want to. The user does not need a
password, but does need a valid shell (otherwise, SSH will refuse to
work). Don't use an existing account unless you know what you're
doing.

I usually store ``git`` repositories in the subtree
``/srv/example.com/git`` (replace ``example.com`` with your own
domain). You may choose another location. Adjust to suit and run::

	sudo adduser \
	    --system \
	    --shell /bin/sh \
	    --gecos 'git version control' \
	    --group \
	    --disabled-password \
	    --home /srv/example.com/git \
	    git

This command is known to work in Debian and Ubuntu. Your mileage may
vary.

You will need an SSH public key to continue. If you don't have one,
you need to generate one. See the man page for ``ssh-keygen``, and you
may also be interested in ``ssh-agent``. Create it on your personal
computer, and protect the *private* key well -- that includes not
transferring it over the network.

Next, we need to set things up for this newly-created user. The
following command will create a ``~/repositories`` that will hold the
``git`` repositories, a ``~/.gitosis.conf`` that will be a symlink to
the actual configuration file, and it will add the SSH public key to
``~/.ssh/authorized_keys`` with a ``command=`` option that restricts
it to running ``gitosis-serve``. Run::

	sudo -H -u git gitosis-init <FILENAME.pub
	# (or just copy-paste the public key when prompted)

then just ``git clone git@SERVER:gitosis-admin.git``, and you get a
repository with SSH keys as ``keys/USER.pub`` and a ``gitosis.conf``
where you can configure who has access to what.

.. warning::

	For now, ``gitosis`` uses the ``HOME`` environment variable to
	locate where to write its files. If you use ``sudo -u``
	without ``-H``, ``sudo`` will leave the old value of ``HOME``
	in place, and this will cause trouble. There will be a
	workaround for that later on, but for now, always remember to
	use ``-H`` if you're sudoing to the account.

You should always edit the configuration file via ``git``. The file
symlinked to ``~/.gitosis.conf`` on the server will be overwritten
when pushing changes to the ``gitosis-admin.git`` repository.

Edit the settings as you wish, commit and push. That's pretty much it!
Once you push, ``gitosis`` will immediately make your changes take
effect on the server.


Managing it
===========

To add new users:

- add a ``keys/USER.pub`` file
- authorize them to read/write repositories as needed (or just
  authorize the group ``@all``)

To create new repositories, just authorize writing to them and
push. It's that simple! For example: let's assume your username is
``jdoe`` and you want to create a repository ``myproject``.
In your clone of ``gitosis-admin``, edit ``gitosis.conf`` and add::

	[group myteam]
	members = jdoe
	writable = myproject

Commit that change and push. Then create the initial commit and push
it::

	mkdir myproject
	cd mypyroject
	git init
	git remote add myserver git@MYSERVER:myproject.git
	# do some work, git add and commit files
	git push myserver master:refs/heads/master

That's it. If you now add others to ``members``, they can use that
repository too.


Example configuration
=====================

.. include:: example.conf
   :literal:


Using ``git daemon``
====================

Anonymous read-only access to ``git`` repositories is provided by
``git daemon``, which is distributed as part of ``git``. But
``gitosis`` will still help you manage it: setting ``daemon = yes`` in
your ``gitosis.conf``, either globally in ``[gitosis]`` or
per-repository under ``[repo REPOSITORYNAME]``, makes ``gitosis``
create the ``git-daemon-export-ok`` files in those repository, thus
telling ``git daemon`` that publishing those repositories is ok.

To actually run ``git daemon`` in Ubuntu, put this in
``/etc/event.d/local-git-daemon``:

.. include:: etc-event.d-local-git-daemon
   :literal:

For other operating systems, use a similar invocation in an ``init.d``
script, ``/etc/inittab``, ``inetd.conf``, ``runit``, or something like
that (good luck).

Note that this short snippet is not a substitute for reading and
understanding the relevant documentation.


Using gitweb
============

``gitweb`` is a CGI script that lets one browse ``git`` repositories
on the web. It is most commonly used anonymously, but you could also
require authentication in your web server, before letting people use
it. ``gitosis`` can help here by generating a list of projects that
are publicly visible. Simply add a section ``[repo REPOSITORYNAME]``
to your ``gitosis.conf``, and allow publishing with ``gitweb = yes``
(or globally under ``[gitosis]``). You should also set ``description``
and ``owner`` for each repository.

Here's a LightTPD_ config file snippet showing how to run ``gitweb``
as a CGI:

.. _LightTPD: http://www.lighttpd.net/

.. include:: lighttpd-gitweb.conf
   :literal:

And a simple ``gitweb.conf`` file:

.. include:: gitweb.conf
   :literal:

Note that this short snippet is not a substitute for reading and
understanding the relevant documentation.



Contact
=======

You can email the author at ``tv@eagain.net``, or hop on
``irc.freenode.net`` channel ``#git`` and hope for the best.

There will be more, keep an eye on http://eagain.net/ and/or the git
mailing list.
"
311,duorg/Scripts,Shell,"# Scripts
Repository for all the script files
"
312,fluent/fluentd-docs,Ruby,"# Fluentd Docs App

# Important Warnings

**THE OFFICIAL DOCUMENTATION FOR FLUENTD HAS BEEN MOVED TO GITBOOK**

This repository is kept as a historical archive. If you find something
wrong on https://docs.fluentd.org, please submit issues or PRs to the new
GitBook repository.

https://github.com/fluent/fluentd-docs-gitbook

# Overview

Ultrasimple CMS and content for Fluentd documentation. The production site is [here](https://docs.fluentd.org/).

If you'd like to propose an edit to the Fluentd docs, please fork this repo and send us a pull request.

Note that we require each commit to be signed off by the author as policy. Thus, when writing a patch, please sign your commit using the `-s` option.

    # Append `--amend` to sign the previous commit
    $ git commit -s

# Install

    $ gem install bundler
    $ bundle install --path vendor/bundle
    $ bundle exec rake server
    $ open ""http://localhost:9395/""

# Test

    $ bundle exec rake test

# Build Search Index

    $ heroku run rake index

# Deploy

We deploy fluentd-docs by Circle-CI automatically.
If you want to deploy it manually, run following command.

    $ git push heroku master

## NOTE

When you have updated an article, please update config/last_updated.json too.

    $ bundle exec rake last_updated
    $ git add config/last_updated.json

# Contributing docs for v1

v1 docs are under docs/v1. Currently, most articles are symlinked to the corresponding file in docs/.

- If you are updating an exiting article. Just replace the symlink with an actual article.
- If you are adding a new article, just add it under docs/v1.

Once v1 is released, this process will be updated.

# INCLUDE pragma

You can use the ""INCLUDE pragma"" to avoid copy-and-pasting the same content or updates on multiple pages.

The syntax is as follows:

    INCLUDE: <filename without extension>

    ... the rest of the document.

Please remember to include a blank line between ""INCLUDE..."" and the rest of the document.
The docs app will search for \<filename\>.txt in the `docs` directory and insert its contents into the current document.

For example, if you write

    INCLUDE: _buffer_parameters

    ... the rest of the document.

then the docs app will insert the contents of \_buffer\_parameters.txt into the current document.

# Acknowledgement

This program is forked from [heroku/heroku-docs](http://github.com/heroku/heroku-docs), and originally written by @rtomayko and @adamwiggins. Later, modified by @kzk and @doryokujin.

Code is released under the MIT License: http://www.opensource.org/licenses/mit-license.php

All rights reserved on the content (text files in the docs subdirectory), although you're welcome to modify these for the purpose of suggesting edits.
"
313,RocketChat/Docker.Official.Image,Dockerfile,"# Rocket.Chat

Rocket.Chat is a Web Chat Server, developed in JavaScript, using the Meteor fullstack framework.

It is a great solution for communities and companies wanting to privately host their own chat service or for developers looking forward to build and evolve their own chat platforms.

%%LOGO%%

# How to use this image
### Docker Compose
If you need both the mongo and Rocket.Chat containers, use a docker compose one-liner:

    docker-compose up --build -d

Which will run both containers, with Rocket.Chat listening on http://localhost:80
Then, access it via `http://localhost` in a browser.  Replace `localhost` in `ROOT_URL` with your own domain name if you are hosting at your own domain.

Stop the containers with:

    docker-compose down

### Individual containers
First, start an instance of mongo:

    docker run --name db -d mongo:4.0 mongod --smallfiles

Then start Rocket.Chat linked to this mongo instance:

    docker run --name rocketchat --link db:db -d rocket.chat

This will start a Rocket.Chat instance listening on the default Meteor port of 3000 on the container.

If you'd like to be able to access the instance directly at standard port on the host machine:

    docker run --name rocketchat -p 80:3000 --env ROOT_URL=http://localhost --link db:db -d rocket.chat

Then, access it via `http://localhost` in a browser.  Replace `localhost` in `ROOT_URL` with your own domain name if you are hosting at your own domain.

If you're using a third party Mongo provider, or working with Kubernetes, you need to override the `MONGO_URL` environment variable:

    docker run --name rocketchat -p 80:3000 --env ROOT_URL=http://localhost --env MONGO_URL=mongodb://mymongourl/mydb -d rocket.chat

"
314,PHPFusion/PHPFusion,PHP,"# PHPFusion Andromeda (v9)

This is the development of PHPFusion v9.XX, code name Andromeda.

**Cloning of Andromeda branch:**
```
C:\> git clone --branch Andromeda https://github.com/PHPFusion/PHPFusion.git
```
"
315,sous-chefs/tomcat,Ruby,"# tomcat Cookbook

[![Cookbook Version](https://img.shields.io/cookbook/v/tomcat.svg?style=flat)](https://supermarket.chef.io/cookbooks/tomcat)
[![CI State](https://github.com/sous-chefs/tomcat/workflows/ci/badge.svg)](https://github.com/sous-chefs/tomcat/actions?query=workflow%3Aci)
[![OpenCollective](https://opencollective.com/sous-chefs/backers/badge.svg)](#backers)
[![OpenCollective](https://opencollective.com/sous-chefs/sponsors/badge.svg)](#sponsors)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)

Provides resources for installing Tomcat and managing the Tomcat service for use in wrapper cookbooks. Installs Tomcat from tarballs on the Apache.org website and installs the appropriate configuration for your platform's init system.

## Requirements

### Platforms

- Debian / Ubuntu derivatives
- RHEL and derivatives
- Fedora
- openSUSE / SUSE Linux Enterprises

### Chef

- Chef 13+

### Cookbooks

- none

## Usage

Due to the complexity of Tomcat cookbooks it's not possible to create an attribute driven cookbook that solves everyone's problems. Instead this cookbook provides resources for installing Tomcat and managing the Tomcat service, which are best used in your own wrapper cookbook. The best way to understand how this could be used is to look at the helloworld test recipe located at <https://github.com/chef-cookbooks/tomcat/blob/master/test/cookbooks/test/recipes/helloworld_example.rb>

## Resources

### tomcat_install

tomcat_install installs an instance of the tomcat binary direct from Apache's mirror site. As distro packages are not used we can easily deploy per-instance installations and any version available on the Apache archive site can be installed.

#### properties

- `version`: The version to install. Default: 8.5.54
- `version_archive`: The filename of the versioned archive to install. Default: apache-tomcat-VERSION.tar.gz
- `install_path`: Full path to the install directory. Default: `/opt/tomcat_INSTANCENAME_VERSION`
- `tarball_base_uri`: The base uri to the apache mirror containing the tarballs. Default: `<http://archive.apache.org/dist/tomcat/>'
- `checksum_base_uri`: The base uri to the apache mirror containing the md5 or sha512 file. Default: '<http://archive.apache.org/dist/tomcat/>'
- `verify_checksum`: Whether the checksum should be verified against `checksum_base_uri`. Default: `true`.
- `dir_mode`: Directory permissions of the `install_path`. Default: `'0750'`.
- `tarball_uri`: The complete uri to the tarball. Default: `TARBALL_BASE_URI/tomcat-#{major_version(version)}/v#{version}/bin/#{version_archive}.#{version_checksum_algorithm(version)}`
- `checksum_uri`: The complete uri to the tarball checksum. Default: `CHECKSUM_BASE_URI/tomcat-#{major_version(version)}/v#{version}/bin/#{version_archive}.#{version_checksum_algorithm(version)}`
- `tarball_path`: Local path on disk to the tarball. If the file does not exist, or the checksum does not match, it will be downloaded from `tarball_uri`.
- `tarball_validate_ssl`: Validate the SSL certificate, if `tarball_uri` is using HTTPS. Default `true`.
- `exclude_docs`: Exclude ./webapps/docs from installation. Default `true`.
- `exclude_examples`: Exclude ./webapps/examples from installation. Default `true`.
- `exclude_manager`: Exclude ./webapps/manager from installation. Default: `false`.
- `exclude_hostmanager`: Exclude ./webapps/host-manager from installation. Default: `false`.
- `tomcat_user`: User to run tomcat as. Default: `tomcat_INSTANCENAME`
- `tomcat_group`: Group of the tomcat user. Default: `tomcat_INSTANCENAME`
- `tomcat_user_shell`: Shell of the tomcat user. Default: `/bin/false`
- `create_user`: Creates the specified tomcat_user within the OS.  Default `true`.
- `create_group`: Creates the specified tomcat_group within the OS. Default `true`.
- `service_template_source`: Source template file for the upstart/systemd service definition. Default: `init_#{node['init_package']}.erb`
- `service_template_cookbook`: Cookbook from which to source the upstart/systemd service definition template. Default: `tomcat`
- `create_symlink`: Creates symlink at SYMLINK_PATH to INSTALL_PATH. Default: `true`
- `symlink_path`: Full path to where the symlink will be created targetting INSTALL_PATH. Default: `/opt/tomcat_INSTANCE_NAME`

#### example

Install an Tomcat 8.0.36 instance named 'helloworld' to /opt/tomcat_helloworld_8_0_36/ with a symlink at /opt/tomcat_helloworld/

```ruby
tomcat_install 'helloworld' do
  version '8.0.36'
end
```

Install an Tomcat instance named 'helloworld' from a local tarball to /opt/tomcat_helloworld_8_0_36/ with a symlink at /opt/tomcat_helloworld/

```ruby
tomcat_install 'helloworld' do
  version '8.0.36'
  verify_checksum false
  tarball_path '/tmp/apache-tomcat-8.0.36.tar.gz'
end
```

### tomcat_service

tomcat_service sets up the installed tomcat instance to run using the appropriate init system (upstart or systemd)

#### properties

- `install_path`: Full path to the install directory. Default: /opt/tomcat_INSTANCENAME
- `env_vars`: An array of hashes containing the environmental variables for Tomcat's setenv.sh script. Note: If CATALINA_BASE is not passed it will automatically be added as the first item in the array. Default: [ {'CATALINA_BASE' => '/opt/INSTANCE_NAME/'}, {'CATALINA_PID' => '$CATALINA_BASE/bin/tomcat.pid'} ]
- `service_vars`: An array of hashes containing additional systemd directives when setting up a service under systemd.
- `sensitive`: Excludes diffs that may expose ENV values from the chef-client logs. Default: `false`
- `service_name`: The service name to configure. Default: `tomcat_INSTANCE_NAME`
- `tomcat_user`: The user the service runs under
- `tomcat_group`: The group the service runs under
- `service_template_source`: The service template source for the appropriate init system.
- `service_template_cookbook`: The cookbook that contains the service template source template. Default: `tomcat`
- `service_template_local`: Specifies if `service_template_source` is a local path rather than sourced from a cookbook. Default: `false`

#### actions

- `start`
- `create`
- `stop`
- `enable`
- `disable`
- `restart`

#### example

```ruby
tomcat_service 'helloworld' do
  action :start
  env_vars [{ 'CATALINA_PID' => '/my/special/path/tomcat.pid' }]
end
```

## Contributors

This project exists thanks to all the people who [contribute.](https://opencollective.com/sous-chefs/contributors.svg?width=890&button=false)

### Backers

Thank you to all our backers!

![https://opencollective.com/sous-chefs#backers](https://opencollective.com/sous-chefs/backers.svg?width=600&avatarHeight=40)

### Sponsors

Support this project by becoming a sponsor. Your logo will show up here with a link to your website.

![https://opencollective.com/sous-chefs/sponsor/0/website](https://opencollective.com/sous-chefs/sponsor/0/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/1/website](https://opencollective.com/sous-chefs/sponsor/1/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/2/website](https://opencollective.com/sous-chefs/sponsor/2/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/3/website](https://opencollective.com/sous-chefs/sponsor/3/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/4/website](https://opencollective.com/sous-chefs/sponsor/4/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/5/website](https://opencollective.com/sous-chefs/sponsor/5/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/6/website](https://opencollective.com/sous-chefs/sponsor/6/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/7/website](https://opencollective.com/sous-chefs/sponsor/7/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/8/website](https://opencollective.com/sous-chefs/sponsor/8/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/9/website](https://opencollective.com/sous-chefs/sponsor/9/avatar.svg?avatarHeight=100)
"
316,TelerikAcademy/HTML,HTML,"# HTML Basics Course Repository


Repository for the Course [HTML Basics](http://) at [Telerik Academy](http://)

In the free course [HTML Basics](http://) the students will make their first steps in the development of modern web applications. The course is introductionary and covers only the basics of web development. Yet it is a fundamental for everybody who wants to grow as a web professional, both front-end developer or web developer.

The course includes topics such as **HTML tags**, **using tables**, **forms and inputs** and more...

##  Course Program

### [00. Course Introduction](/Topics/00. Course-Introduction)

Course Program, Examination, Trainers

##### [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/presentation.png"" height=""18""/>Slides](https://rawgit.com/TelerikAcademy/HTML/master/Topics/00. Course-Introduction/slides/index.html) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/code.png"" height=""15""> Demos](/Topics/00. Course-Introduction/demos) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/homework.png"" height=""15"">Tasks](/Topics/00. Course-Introduction/homework) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/video.png"" height=""15""> Videos](/Topics/00. Course-Introduction/VIDEOS.md)


### [01. Web Basics](/Topics/01. Web-Basics)

Web sites, Web servers

##### [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/presentation.png"" height=""18""/>Slides](https://rawgit.com/TelerikAcademy/HTML/master/Topics/01. Web-Basics/slides/index.html) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/code.png"" height=""15""> Demos](/Topics/01. Web-Basics/demos) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/homework.png"" height=""15"">Tasks](/Topics/01. Web-Basics/homework) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/video.png"" height=""15""> Videos](/Topics/01. Web-Basics/VIDEOS.md)


### [02. HTML Fundamentals](/Topics/02. HTML-Fundamentals)

HyperText Markup Language, tags, attributes

##### [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/presentation.png"" height=""18""/>Slides](https://rawgit.com/TelerikAcademy/HTML/master/Topics/02. HTML-Fundamentals/slides/index.html) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/code.png"" height=""15""> Demos](/Topics/02. HTML-Fundamentals/demos) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/homework.png"" height=""15"">Tasks](/Topics/02. HTML-Fundamentals/homework) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/video.png"" height=""15""> Videos](/Topics/02. HTML-Fundamentals/VIDEOS.md)


### [03. HTML-Tables](/Topics/03. HTML-Tables)

Tables and why they are not used anymore

##### [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/presentation.png"" height=""18""/>Slides](https://rawgit.com/TelerikAcademy/HTML/master/Topics/03. HTML-Tables/slides/index.html) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/code.png"" height=""15""> Demos](/Topics/03. HTML-Tables/demos) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/homework.png"" height=""15"">Tasks](/Topics/03. HTML-Tables/homework) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/video.png"" height=""15""> Videos](/Topics/03. HTML-Tables/VIDEOS.md)


### [04. HTML Forms and Frames](/Topics/04. HTML-Forms-and-Frames)

HTML forms and sending data over the web

##### [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/presentation.png"" height=""18""/>Slides](https://rawgit.com/TelerikAcademy/HTML/master/Topics/04. HTML-Forms-and-Frames/slides/index.html) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/code.png"" height=""15""> Demos](/Topics/04. HTML-Forms-and-Frames/demos) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/homework.png"" height=""15"">Tasks](/Topics/04. HTML-Forms-and-Frames/homework) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/video.png"" height=""15""> Videos](/Topics/04. HTML-Forms-and-Frames/VIDEOS.md)

### [05. Semantic HTML](/Topics/05. Semantic-HTML)

HTML5 and semantics

##### [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/presentation.png"" height=""18""/>Slides](https://rawgit.com/TelerikAcademy/HTML/master/Topics/05. Semantic-HTML/slides/index.html) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/code.png"" height=""15""> Demos](/Topics/05. Semantic-HTML/demos) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/homework.png"" height=""15"">Tasks](/Topics/05. Semantic-HTML/homework) / [<img src=""https://raw.githubusercontent.com/TelerikAcademy/Common/master/icons/video.png"" height=""15""> Videos](/Topics/05. Semantic-HTML/VIDEOS.md)
"
317,sous-chefs/apt,Ruby,"# apt Cookbook

[![Cookbook Version](https://img.shields.io/cookbook/v/apt.svg)](https://supermarket.chef.io/cookbooks/apt)
[![CI State](https://github.com/sous-chefs/apt/workflows/ci/badge.svg)](https://github.com/sous-chefs/apt/actions?query=workflow%3Aci)
[![OpenCollective](https://opencollective.com/sous-chefs/backers/badge.svg)](#backers)
[![OpenCollective](https://opencollective.com/sous-chefs/sponsors/badge.svg)](#sponsors)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)

This cookbook includes recipes to execute apt-get update to ensure the local APT package cache is up to date. There are recipes for managing the apt-cacher-ng caching proxy and proxy clients. It also includes a custom resource for pinning packages via /etc/apt/preferences.d.

## Maintainers

This cookbook is maintained by the Sous Chefs. The Sous Chefs are a community of Chef cookbook maintainers working together to maintain important cookbooks. If you’d like to know more please visit [sous-chefs.org](https://sous-chefs.org/) or come chat with us on the Chef Community Slack in [#sous-chefs](https://chefcommunity.slack.com/messages/C2V7B88SF).

## Requirements

### Platforms

- Ubuntu 12.04+
- Debian 7+

May work with or without modification on other Debian derivatives.

### Chef

- Chef 13.3+

### Cookbooks

- None

## Recipes

### default

This recipe manually updates the timestamp file used to only run `apt-get update` if the cache is more than one day old.

This recipe should appear first in the run list of Debian or Ubuntu nodes to ensure that the package cache is up to date before managing any `package` resources with Chef.

This recipe also sets up a local cache directory for preseeding packages.

**Including the default recipe on a node that does not support apt (such as Windows or RHEL) results in a noop.**

### cacher-client

Configures the node to use a `apt-cacher-ng` server to cache apt requests. Configuration of the server to use is located in `default['apt']['cacher_client']['cacher_server']` which is a hash containing `host`, `port`, `proxy_ssl`, and `bypass` keys. Example:

```json
{
  ""apt"": {
    ""cacher_client"": {
      ""cacher_server"": {
        ""host"": ""cache_server.mycorp.dmz"",
        ""port"": 1234,
        ""proxy_ssl"": true,
        ""cache_bypass"": {
          ""download.oracle.com"": ""http""
        }
      }
    }
  }
}
```

#### Bypassing the cache

Occasionally you may come across repositories that do not play nicely when the node is using an `apt-cacher-ng` server. You can configure `cacher-client` to bypass the server and connect directly to the repository with the `cache_bypass` attribute.

To do this, you need to override the `cache_bypass` attribute with an hash of repositories, with each key as the repository URL and value as the protocol to use:

```json
{
  ""apt"": {
    ""cacher_client"": {
      ""cacher_server"": {
        ""cache_bypass"": {
          ""URL"": ""PROTOCOL""
        }
      }
    }
  }
}
```

For example, to prevent caching and directly connect to the repository at `download.oracle.com` via http and the repo at `nginx.org` via https

```json
{
  ""apt"": {
    ""cacher_client"": {
      ""cacher_server"": {
        ""cache_bypass"": {
          ""download.oracle.com"": ""http"",
          ""nginx.org"": ""https""
        }
      }
    }
  }
}
```

### cacher-ng

Installs the `apt-cacher-ng` package and service so the system can provide APT caching. You can check the usage report at <http://{hostname}:3142/acng-report.html>.

If you wish to help the `cacher-ng` recipe seed itself, you must now explicitly include the `cacher-client` recipe in your run list **after** `cacher-ng` or you will block your ability to install any packages (ie. `apt-cacher-ng`).

### unattended-upgrades

Installs and configures the `unattended-upgrades` package to provide automatic package updates. This can be configured to upgrade all packages or to just install security updates by setting `['apt']['unattended_upgrades']['allowed_origins']`.

To pull just security updates, set `origins_patterns` to something like `[""origin=Ubuntu,archive=trusty-security""]` (for Ubuntu trusty) or `[""origin=Debian,label=Debian-Security""]` (for Debian).

## Attributes

### General

- `['apt']['compile_time_update']` - force the default recipe to run `apt-get update` at compile time.
- `['apt']['periodic_update_min_delay']` - minimum delay (in seconds) between two actual executions of `apt-get update` by the `execute[apt-get-update-periodic]` resource, default is '86400' (24 hours)

### Caching

- `['apt']['cacher_client']['cacher_server']` - Hash containing server information used by clients for caching. See the example in the recipes section above for the full format of the hash.
- `['apt']['cacher_interface']` - interface to connect to the cacher-ng service, no default.
- `['apt']['cacher_port']` - port for the cacher-ng service (used by server recipe only), default is '3142'
- `['apt']['cacher_dir']` - directory used by cacher-ng service, default is '/var/cache/apt-cacher-ng'
- `['apt']['compiletime']` - force the `cacher-client` recipe to run before other recipes. It forces apt to use the proxy before other recipes run. Useful if your nodes have limited access to public apt repositories. This is overridden if the `cacher-ng` recipe is in your run list. Default is 'false'

### Unattended Upgrades

- `['apt']['unattended_upgrades']['enable']` - enables unattended upgrades, default is false
- `['apt']['unattended_upgrades']['update_package_lists']` - automatically update package list (`apt-get update`) daily, default is true
- `['apt']['unattended_upgrades']['allowed_origins']` - array of allowed apt origins from which to pull automatic upgrades, defaults to a guess at the system's main origin and should almost always be overridden
- `['apt']['unattended_upgrades']['origins_patterns']` - array of allowed apt origin patterns from which to pull automatic upgrades, defaults to none.
- `['apt']['unattended_upgrades']['package_blacklist']` - an array of package which should never be automatically upgraded, defaults to none
- `['apt']['unattended_upgrades']['auto_fix_interrupted_dpkg']` - attempts to repair dpkg state with `dpkg --force-confold --configure -a` if it exits uncleanly, defaults to false (contrary to the unattended-upgrades default)
- `['apt']['unattended_upgrades']['minimal_steps']` - Split the upgrade into the smallest possible chunks. This makes the upgrade a bit slower but it has the benefit that shutdown while a upgrade is running is possible (with a small delay). Defaults to false.
- `['apt']['unattended_upgrades']['install_on_shutdown']` - Install upgrades when the machine is shuting down instead of doing it in the background while the machine is running. This will (obviously) make shutdown slower. Defaults to false.
- `['apt']['unattended_upgrades']['mail']` - Send email to this address for problems or packages upgrades. Defaults to no email.
- `['apt']['unattended_upgrades']['sender']` - Send email from this address for problems or packages upgrades. Defaults to 'root'.
- `['apt']['unattended_upgrades']['mail_only_on_error']` - If set, email will only be set on upgrade errors. Otherwise, an email will be sent after each upgrade. Defaults to true.
- `['apt']['unattended_upgrades']['remove_unused_dependencies']` Do automatic removal of new unused dependencies after the upgrade. Defaults to false.
- `['apt']['unattended_upgrades']['automatic_reboot']` - Automatically reboots _without confirmation_ if a restart is required after the upgrade. Defaults to false.
- `['apt']['unattended_upgrades']['dl_limit']` - Limits the bandwidth used by apt to download packages. Value given as an integer in kb/sec. Defaults to nil (no limit).
- `['apt']['unattended_upgrades']['random_sleep']` - Wait a random number of seconds up to this value before running daily periodic apt actions. System default is 1800 seconds (30 minutes).
- `['apt']['unattended_upgrades']['syslog_enable']` - Enable logging to syslog. Defaults to false.
- `['apt']['unattended_upgrades']['syslog_facility']` - Specify syslog facility. Defaults to 'daemon'.
- `['apt']['unattended_upgrades']['dpkg_options']` An array of dpkg options to be used specifically only for unattended upgrades. Defaults to `[]` which will prevent it from being rendered from the template in the resulting file.

### Configuration for APT

- `['apt']['confd']['force_confask']` - Prompt when overwriting configuration files. (default: false)
- `['apt']['confd']['force_confdef']` - Don't prompt when overwriting configuration files. (default: false)
- `['apt']['confd']['force_confmiss']` - Install removed configuration files when upgrading packages. (default: false)
- `['apt']['confd']['force_confnew']` - Overwrite configuration files when installing packages. (default: false)
- `['apt']['confd']['force_confold']` - Keep modified configuration files when installing packages. (default: false)
- `['apt']['confd']['install_recommends']` - Consider recommended packages as a dependency for installing. (default: true)
- `['apt']['confd']['install_suggests']` - Consider suggested packages as a dependency for installing. (default: false)

## Libraries

There is an `interface_ipaddress` method that returns the IP address for a particular host and interface, used by the `cacher-client` recipe. To enable it on the server use the `['apt']['cacher_interface']` attribute.

## Usage

Put `recipe[apt]` first in the run list. If you have other recipes that you want to use to configure how apt behaves, like new sources, notify the execute resource to run, e.g.:

```ruby
template '/etc/apt/sources.list.d/my_apt_sources.list' do
  notifies :run, 'execute[apt-get update]', :immediately
end
```

The above will run during execution phase since it is a normal template resource, and should appear before other package resources that need the sources in the template.

Put `recipe[apt::cacher-ng]` in the run_list for a server to provide APT caching and add `recipe[apt::cacher-client]` on the rest of the Debian-based nodes to take advantage of the caching server.

If you want to cleanup unused packages, there is also the `apt-get autoclean` and `apt-get autoremove` resources provided for automated cleanup.

## Resources

### apt_preference

The apt_preference resource has been moved into chef-client in Chef 13.3.

See <https://docs.chef.io/resource_apt_preference.html> for usage details

### apt_repository

The apt_repository resource has been moved into chef-client in Chef 12.9.

See <https://docs.chef.io/resource_apt_repository.html> for usage details

### apt_update

The apt_update resource has been moved into chef-client in Chef 12.7.

See <https://docs.chef.io/resource_apt_update.html> for usage details

## Contributors

This project exists thanks to all the people who [contribute.](https://opencollective.com/sous-chefs/contributors.svg?width=890&button=false)

### Backers

Thank you to all our backers!

![https://opencollective.com/sous-chefs#backers](https://opencollective.com/sous-chefs/backers.svg?width=600&avatarHeight=40)

### Sponsors

Support this project by becoming a sponsor. Your logo will show up here with a link to your website.

![https://opencollective.com/sous-chefs/sponsor/0/website](https://opencollective.com/sous-chefs/sponsor/0/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/1/website](https://opencollective.com/sous-chefs/sponsor/1/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/2/website](https://opencollective.com/sous-chefs/sponsor/2/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/3/website](https://opencollective.com/sous-chefs/sponsor/3/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/4/website](https://opencollective.com/sous-chefs/sponsor/4/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/5/website](https://opencollective.com/sous-chefs/sponsor/5/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/6/website](https://opencollective.com/sous-chefs/sponsor/6/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/7/website](https://opencollective.com/sous-chefs/sponsor/7/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/8/website](https://opencollective.com/sous-chefs/sponsor/8/avatar.svg?avatarHeight=100)
![https://opencollective.com/sous-chefs/sponsor/9/website](https://opencollective.com/sous-chefs/sponsor/9/avatar.svg?avatarHeight=100)
"
318,Harvard-IACS/2018-CS109A,Jupyter Notebook,"# 2018 CS109A Course Repository
This is the public (available to students) repository for cs109a Fall 2018
"
319,Yenthe666/Odoo_Samples,Python,"<h3>Information</h3>
This repository contains samples in different modules.  
Every folder is a module which you can easily install to see how things work or you can browse the code!

<h3>Module button_action_demo</h3>
This module will learn you how to create buttons at the top of forms and how to make it perform actions.
All Python (model data) is under models/button_action_demo.py and you can find the views under views/button_view.xml.

<h3>Module default_data_demo</h3>
This module will learn you how to automatically insert default data in to a database. It will create a new model (demo.default.data) which is filled with records that are made in the file defaultdata.xml (under data/ folder).

<h3>Module default_filter_demo</h3>
This module will learn you how to add filters on search views.
It'll allow you to filter on a specific field by simply using the filter from the searchview.

<h3>Module inherit_report_demo</h3>
This module will learn you how to inherit existing QWeb reports and how to modify them.
In this example I will modify the default quotation/order report and only show the description with the total price.
To make it look a bit better I've added a table header color.

<h3>Module logging_demo</h3>
This module will learn you how to create log statements in Odoo. You can use these log statements to debug values
or to write unusual behaviour to the Odoo logfile.

<h3>Module mail_template_demo</h3>
This module will learn you how to create e-mail templates in Odoo.
It will add a new e-mail template for the model ```res.partner``` and will learn you the basic concepts of e-mail templates and jinja2 rendering.

<h3>Module send_mail_template_demo</h3>
This module will learn you how to send out e-mail templates from Python code.
In this example we'll add a send e-mail button. After clicking on it the e-mail template that we created in the module mail_template_demo will be used to send out an e-mail.

<h3>Module many2many_default_data_demo</h3>
This module is a new module from scratch that inherits sale.order. It will create a new many2many to the model sale.order.printorder and will automatically fill this many2many with all the data from the model sale.order.printorder.
In this sample you can see how to use default=, how to use functions and how to use self.pool.

<h3>Module many2many_handle_widget_demo</h3>
This module is a new module from scratch that inherits sale.order. In this module you can see how the handle widget (drag and drop) works
with many2many and how sequencing works.

<h3>Module on_change_function</h3>
This module is a new module from scratch which inherits the model product.template.
It adds the fields CostPrice and ShippingCost, which you can see in models.py. This is also where the on_change event is programmed. The fields are then added to the products view in the file templates.xml, which inherits the default product view.

<h3>Module project_task_agenda_colouring</h3>
<b>Note:</b> If you want to use this module you will also need to install the module web_widget_color. This module builds on top of that module.
This module will give you the ability to add custom colours to tasks which will then re-color the agenda view.
There is a new view created 'Agenda statuses' where you can create a status with a color from the color picker. On the project task you can then choose an agenda status. When you'd go to the agenda view you will see it being re-coloured.
This module does almost the same as ```project_task_kanban_colouring``` but this module colours the agenda view, not the Kanban view.
Agenda statusses view:<br/>
<img src=""http://i.imgur.com/ZlHcm9R.png"" alt=""Agenda statuses""/><br/>
Project task view:<br/>
<img src=""http://i.imgur.com/OhP9Vau.png"" alt=""Project task view""/><br/>
Tasks calendar view:<br/>
<img src=""http://i.imgur.com/3PwRhEG.png"" alt=""Project tasks calendar view""/>

<h3>Module project_task_kanban_colouring</h3>
<b>Note:</b> If you want to use this module you will also need to install the module web_widget_color. This module builds on top of that module.
This module will give you the ability to add custom colours to tasks which will then re-color the agenda view.
There is a new view created 'Agenda statuses' where you can create a status with a color from the color picker. On the project task you can then choose an agenda status. When you'd go to the kanban view you will see it being re-coloured.
This module does almost the same as ```project_task_agenda_colouring``` but this module colours the Kanban view, not the agenda view.
Agenda statusses view:<br/>
<img src=""http://i.imgur.com/ZlHcm9R.png"" alt=""Agenda statuses""/><br/>
Project task view:<br/>
<img src=""http://i.imgur.com/OhP9Vau.png"" alt=""Project task view""/><br/>
Tasks kanban view:<br/>
<img src=""http://i.imgur.com/zIy1zoX.png"" alt=""Project task kanban view""/>

<h3>Module sale</h3>
This module is the default sale module with a few modifications. All the changes can be seen in sale.py and sale_view.xml.<br />
Changes in sale.py:
```
    def _get_default_currency(self, cr, uid, context=None):
        res = self.pool.get('res.company').search(cr, uid, [('currency_id','=','EUR')], context=context)
        return res and res[0] or False

    _columns = {
        #This fills the Many2one with all data from res.currency
        'currency_id_invoices': fields.many2one('res.currency', string=""Valuta"", required=True),
      
    _defaults = {
        #This makes the function go off that sets EUR.
        'currency_id_invoices': _get_default_currency,
  ```
Changes in sale_view.xml:
  ```
    <field name=""currency_id_invoices""/>
  ```
  
<h3>Module scheduler_demo</h3>
This module is a new module, from scratch, which creates a new model (scheduler.demo), new views and an automated action (scheduler). This module will learn you how to create an automated action from scratch, how to loop over all records in a model and how to update those values from within a scheduler.

<h3>Module static_resources_demo</h3>
This module is a new module, from scratch which creates a new CSS and JavaScript file.
This module will learn you how to create new static files and how to add them to the main Odoo CSS / JavaScript.
 
<h3>Module statusbar_demo</h3>
This module is a new module, from scratch, which creates a new model (statusbar.demo) and new views.
This module will learn you how to create a statusbar (selection) and how to handle different states and writing on the current record. You will learn how to add buttons, how to trigger functions and how to change the state of your record.

<h3>Module upload_images</h3>
This module is a new module, from scratch, which creates a new model (upload_images.tutorial), a new report (report_images.xml) and a new menu_item named 'Images' under Sales. In this new menu item you can upload images in multiple sizes and you will see a new report detail here. With this report you will see the image printed in multiple sizes.

<h3>Module user_access_rights_demo</h3>
This module creates a new security selection on the user settings. This allows you to add users to groups under Settings > Users and gives you the ability to show views and fields only to specific user groups.

<h3>Module web_widget_color</h3>
This module adds a colour picker widget to Odoo. The picker itself is inspired on the <a href=""http://jscolor.com"">jsColor </a> library.
To use this widget you need to create a char field in with a size of atleast 7 characters in the database:
```
color = fields.Char(
    string=""Color"",
    help=""Choose your color""
)
```
Afterwards call it in the view with ```widget=""color""```:
```
<field name=""arch"" type=""xml"">
    <tree string=""View name"">
        ...
        <field name=""name""/>
        <field name=""color"" widget=""color""/>
        ...
    </tree>
</field>
```
Picker sample:<br/>
<img src=""/web_widget_color/images/picker.png""/><br/>
Tree view sample:<br/>
<img src=""/web_widget_color/images/list_view.png""/>

<h3>Module xpath_expressions</h3>
This module is a new module, from scratch which inherits the model product.template and inherits the product view (sale > products). In this sample you can see how to add new pages, groups or fields with xpath expressions. You can see the samples in templates.xml.
"
320,boochtek/activerecord-repository,Ruby,"ActiveRecord Repository
=======================

This is an implementation of the repository pattern for ActiveRecord.
Using this allows splitting the domain model and persistence classes.


WARNING: This is currently merely a proof of concept. This means that testing
and refactoring have often been put on the back burner.


TODO
----

* Loading
    * Loading relations
        * Avoiding N+1 queries
* Saving
    * Saving relations
* Keep a registry of the repository classes
    * Indexed by the classes they map to
    * Use this to find repository class for relations
        * Caveat: What if there are 2 repositories for a relation we're looking for?
            * Need a way to specify that somewhere
                * In the repository is the only place that makes sense
* Probably need to restrict other calls to Repository
    * `User::Repository.create`
* Entity#initialize and #update should basically be the same
    * Maybe the only difference is that initialize will set things to `nil`
    * They could use a lot of refactoring
* Relations between entities
    * belongs_to
    * has_many
    * has_one
    * has_many through:
    * Cascading deletions
* Entity#changed?
* Identity mapping
* Use module builder pattern in ActiveModel.entity
    * Add more options
* Use module factory (and module builder) pattern in ActiveRecord.repository
    * Add options
        * Setting entity class
        * Setting table_name
        * Setting a different primary key than `id`
        * Maybe define indexes
* More validations
"
321,sexym0nk3y/Laravel-5.3-Repository,PHP,"# Laravel 5.3 Repositories

Simple repository setup for Laravel 5.3

## Installation

1. Copy `BaseRepository.php` into `App/Repositories`
2. Copy `repository.stub` into `App/Console/Commands/stubs`
3. Copy `MakeRepositoryCommand.php` into `App/Console/Commands`
4. Add `MakeRepositoryCommand:class` into the `App/Console/Kernel.php`

```php
class Kernel extends ConsoleKernel
{
    /**
     * The Artisan commands provided by your application.
     *
     * @var array
     */
    protected $commands = [
        Commands\MakeRepositoryCommand::class,
    ];

```

## Usage
The `make:repository` command automatically creates a new Repository into `App\Repositories`

```terminal
php artisan make:repository NameRepo
```"
322,backmeupplz/voicy,JavaScript,"[![Voicybot](/img/logo.png?raw=true)](http://voicybot.com/)

# [@voicybot](https://t.me/voicybot) main repository

This repository contains the code for one of the most popular bots I've ever built for Telegram — [@voicybot](https://t.me/voicybot). Please, feel free to fork, add features and create pull requests so that everybody (over 600 000 chats) can experience the features you've built.

You can also help by translating the bot to other languages or fixing some texts in existing languages by modifying the `locales` folder.

# List of repositories

- [Voicy](https://github.com/backmeupplz/voicy) — the main [@voicybot](https://t.me/voicybot) code
- [Voicy payments](https://github.com/backmeupplz/voicy-payments) — payments service that used stripe to process payments for the Google Speech seconds of recognition; currently retired as the stats server for [voicybot.com](http://voicybot.com)
- [Voicy landing](https://github.com/backmeupplz/voicy-landing) — [voicybot.com](http://voicybot.com) landing page

# Installation and local launch

1. Clone this repo: `git clone https://github.com/backmeupplz/voicy`
2. Launch the [mongo database](https://www.mongodb.com/) locally
3. Create `.env` file with the environment variables listed below
4. Install ffmpeg on your machine
5. Run `yarn install` in the root folder
6. Run `yarn start`

# Environment variables in `.env` file

- `MONGO_URL` — Url for the mongo database used
- `TOKEN` — Telegram bot token
- `SALT` — Random salt to generate various encrypted stuff
- `ADMIN_ID` — Chat id of the person who shall receive valuable logs
- `WIT_LANGUAGES` — A map of language names to Wit.ai tokens

Extra info is available in `.env.sample` file.

# Continuous integration

Any commit pushed to master gets deployed to [@voicybot](https://t.me/voicybot) via [CI Ninja](https://github.com/backmeupplz/ci-ninja).

# License

MIT — use for any purpose. Would be great if you could leave a note about the original developers. Thanks!

# As seen on

[![Habrahabr](/img/habr.png?raw=true)](https://habrahabr.ru/post/316824/)
[![Spark](/img/spark.png?raw=true)](https://spark.ru/startup/voicy/blog/19008/kak-zapustit-proekt-v-odinochku/)
[![Reddit](/img/reddit.png?raw=true)](https://redd.it/5iduzy)
[![Bot Store](/img/bs.png?raw=true)](https://storebot.me/bot/voicybot)
[![Product Hunt](/img/ph.png?raw=true)](https://www.producthunt.com/posts/voicy)
"
323,xroche/httrack,C,"HTTrack Website Copier, Offline Browser for Windows and Unix
Copyright (C) 1998-2017 Xavier Roche and other contributors

Welcome to HTTrack Website Copier!


Information:

The folder html/ contains the documentation
You might want to read these files, especially for installing HTTrack:
  - See INSTALL file for installation information (NO WARRANTY)
  - See license.txt file for license information


Contacting us:

If you want to ask any question, feel free to contact us!
email: httrack@httrack.com


Donations:

HTTrack Website Copier is free software, but if you can, please donate to the Free Software Foundation (FSF) to support free software!

(Europe)
http://www.fsfeurope.org/help/donate.html

(World)
http://www.fsf.org/help/donate.html


Engine limits:

These are the principals limits of HTTrack for that moment. Note that we did not heard about any other utility
that would have solved them.

- Several scripts generating complex filenames may not find them (ex: img.src='image'+a+Mobj.dst+'.gif')
- Some java classes may not find some files on them (class included)
- Cgi-bin links may not work properly in some cases (parameters needed). To avoid them: use filters like -*cgi-bin*


Advanced options:

These options may not be necessary for a normal usage. But they can solve several problems.

- If you need more than 100,000 links: -#L1000000 (1,000,000 links)
- If you need more than 500 filters: -#F1000 (1,000 filters)
- If you need transfer rate statictics every minutes: -#Z
- If you need transfer operations statistics every minutes: -#T
- If you want log files to be refreshed after every line: -#f


Thank you!


Have fun with HTTrack Website Copier!
The authors

"
324,Shougo/neosnippet-snippets,Vim Snippet,"Neosnippet-snippets
===================

The standard snippets repository for
[neosnippet](https://github.com/Shougo/neosnippet.vim).

You can fork or send pull request.

"
325,lensfun/lensfun,C++,"Travis CI [![Travis CI Build Status](https://travis-ci.com/lensfun/lensfun.svg?branch=master)](https://travis-ci.com/lensfun/lensfun)

AppVeyor [![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/lensfun/lensfun?branch=master&svg=true)](https://ci.appveyor.com/project/seebk/lensfun)

WHAT IS IT
----------

The goal of the Lensfun library is to provide a open source database
of photographic lenses and their characteristics. In the past there
was a effort in this direction (see http://www.epaperpress.com/ptlens/),
but then author decided to take the commercial route and the database
froze at the last public stage. This database was used as the basement
on which Lensfun database grew, thanks to PTLens author which gave his
permission for this, while the code was totally rewritten from scratch
(and the database was converted to a totally new, XML-based format).

The Lensfun library not only provides a way to read the database
and search for specific things in it, but also provides a set of
algorithms for correcting images based on detailed knowledge of
lens properties. Right now Lensfun is designed to correct
distortion, transversal (also known as lateral) chromatic aberrations,
and vignetting.

The interface is defined both using C++ style and plain C.
The C interface is a wrapper around the C++ classes.

GitHub Project: https://github.com/lensfun/lensfun
GitHub Page: https://lensfun.github.io/


LICENSE
-------

The libraries which are part of this package are licensed under the terms
of the GNU Lesser General Public License, version 3. Libraries are located
under the subdirectory libs/ of the source package. A copy of the license
is available in the file COPYING.LESSER which can be found in the source
archive. You can read it here: http://www.gnu.org/licenses/lgpl-3.0.html

The documentation files, including those autogenerated with Doxygen,
are covered as well by GNU Lesser General Public License, version 3.

Applications which are part of this package are licensed under the terms
of the GNU General Public License, version 3. Applications are located
under the apps/ subdirectory of the source package. A copy of the license
can be found in the file COPYING which can be found in the source
archive. You can read it here: http://www.gnu.org/licenses/gpl-3.0.html

Test programs and tools are put into public domain, unless explicitly
specified otherwise in the header of the source files. Test programs
are located under the tests/ subdirectory, and tools are located in tools/.

The lens database is licensed under the Creative Commons Attribution-Share
Alike 3.0 license (CC BY-SA 3.0). The database is located under the data/ 
subdirectory of the source package and a copy of the license is available 
in the file data/COPYING.CC_BY-SA_3.0 which can be found in the source
archive. You can read it here: https://creativecommons.org/licenses/by-sa/3.0/


BUILD INSTRUCTIONS
------------------

The build system is based on CMake (http://www.cmake.org/). In order to
successfully configure and build the project the following tools are more
or less required:

 - CMake
 - Doxygen in order to generate the library documentation.
 - GLib > 2.26 which is used for low-level I/O and XML parsing.
 - GLib > 2.40 to build the unit tests with the GLib test framework.
 - libpng is required to build and run the example implementation.
 - xmllint to test the database XML validity

First enter the Lensfun root folder and create a build directory.

    cd lensfun
    mkdir build

Enter the build directory and run CMake to configure your sources and create
the build files

    cd build
    cmake ../

Run make/make install as usual

    make
    make install

After that, you may want to use ldconfig to add the necessary symlinks so that
programs can find and use Lensfun.

The following CMake options can be set (defaults are upper case):

    -DCMAKE_BUILD_TYPE=DEBUG|Release        select debug or release build mode
    -DINSTALL_HELPER_SCRIPTS=off|ON         install various helper scripts
    -DCMAKE_INSTALL_PREFIX=/USR/LOCAL       install prefix

    -DBUILD_STATIC=OFF|on       build static or shared lib
    -DBUILD_TESTS=OFF|on        build also the test programs
    -DBUILD_LENSTOOL=OFF|on     build Lensfun reference implementation
    -DBUILD_FOR_SSE=off|ON      build with SSE optimisation
    -DBUILD_FOR_SSE2=off|ON     build with SSE2 optimisaiton
    -DBUILD_DOC=OFF|on          build documentation

If you want to have more detailed output when running 'make' you can simply add
'VERBOSE=1' to the make command line.

You can also build packages with cmake:

    Add -DCPACK_BINARY_DEB:BOOL=ON or -DCPACK_BINARY_RPM:BOOL=ON to the
    command line and then ""make package"". (But this is not extensively tested.)

Please note that running cmake again does NOT reset all options to default or
reconfigure all variables. To restart with a clean configuration delete all files
in your cmake_build folder.

If you prefer setting the configuration with a GUI or want to get an extensive
overview of all available settings and cache values run cmake-gui.


DOCUMENTATION
-------------

The end-user documentation for the library can be built by issuing the
command:

    make docs

Also you can read it online at any time by pointing your browser to:

	https://lensfun.github.io/manual/latest

The documentation on the site is updated every night from Git, so it always
contains the latest info on the library.


CREDITS
-------

Here goes a full list of people who have contributed to this library:

CODE:
  > Andrew Zabolotny <zap@homelink.ru>

LENS DATA:
  > Tom Niemann: original open-source ptlens database.

THANKS:
  > Pablo d'Angelo for the idea of a open-source lens database.
  >
  > The whole PanoTools team, for all math and knowledge I have borrowed from PanoTools:
  >
  > Helmut Dersch - The father of most (all?) open-source panorama creation tools.
  > Daniel M. German
  > Kevin Kratzke
  > Rik Littlefield
  > Fulvio Senore
  > Jim Watters
  > Thomas Rauscher
  > Pablo d'Angelo (thanks once more :)
  > Bret McKee
  > Robert Platt

Also I would like to thank the people that made valuable contributions to Lensfun:
  > Niels Kristian Bech Jensen
  > Pascal de Bruijn
  > Thomas Modes
  > Torsten Bronger

And of course great thanks to all the people sending profiles for the database.
"
326,LGalaxiesPublicRelease/LGalaxies_PublicRepository,IDL,"Documentation about the code can be found at:
http://galformod.mpa-garching.mpg.de/public/LGalaxies/"
327,dspinellis/awesome-msr,,"# Awesome Empirical Software Engineering [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
A curated repository of data sets and tools that can be used for conducting evidence-based, data-driven research on software systems.
This research approach is often termed [experimental, or empirical software engineering](https://en.wikipedia.org/wiki/Experimental_software_engineering).
Many of the data sets can also be useful in research using [search-based software engineering](https://en.wikipedia.org/wiki/Search-based_software_engineering) methods.
The repository is named after the [Mining Software Repositories (MSR)](https://www.msrconf.org/) conference series.
For examples of such work see the MSR conference's [Hall of Fame](http://2016.msrconf.org/#/hall-of-fame).


- This list requires your input for its continuous improvement.
  Read the [contribution guide](contributing.md) for instructions on how
  you can contribute.
  Alternatively, you can send me an [email](mailto:dds@aueb.gr)
  if you find the process too cumbersome or confusing.
- For more awesome lists, see [awesome](https://github.com/sindresorhus/awesome).

## Contents
- [Repositories](#repositories)
- [Data Sets](#data-sets)
- [Tools](#tools)
- [Research Outlets](#research-outlets)

## Repositories

- [SIR](http://sir.unl.edu/portal/index.php) - Software-artifact infrastructure repository; Java, C, C++, and C# software together with test suites and fault data.
- [PROMISE](http://promise.site.uottawa.ca/SERepository/datasets-page.html) - About 20 datasets related to software engineering research.
- [FLOSSmole](https://flossmole.org/collection_details) - Collaborative collection and analysis of free/libre/open source project data.
- [Zenodo](http://zenodo.org/) - Software data collections in CERN's open-access repository.
  - [Software Engineering Artifacts Can Really Assist Future Tasks](http://zenodo.org/communities/seacraft)
  - [Empirical Software Engineering](https://zenodo.org/communities/empirical-software-engineering/)
  - [Mining Software Repositories](https://zenodo.org/communities/msr/)

## Data Sets

- [AndroidTimeMachine](https://androidtimemachine.github.io) - Graph-based dataset of commit history of 8,431 real-world Android apps.
- [AndroZoo](https://androzoo.uni.lu/) - Collection of Android Applications.
- [Bug Prediction Dataset](http://bug.inf.usi.ch/index.php) - Collection of models and metrics from Eclipse JDT Core, PDE UI, Equinox Framework, Lucene, Mylyn, and their histories.
- [Code Reviews](http://kin-y.github.io/miningReviewRepo/) - Code reviews of OpenStack, LibreOffice, AOSP, Qt, Eclipse.
- [CoREBench](http://www.comp.nus.edu.sg/%7Erelease/corebench/) - Collection of 70 realistically Complex Regression Errors that were systematically extracted from the repositories and bug reports of four open-source software projects: Make, Grep, Findutils, and Coreutils.
- [Cryptocurrency GitHub Activity and Market Cap Dataset](https://rvantonder.github.io/CryptOSS/) - Activity such as commits, stars, prices, and market cap of over 200 cryptocurrency projects on GitHub over time. Raw, historic data is also [available](https://zenodo.org/record/2595588#.XRuzuBNKhSM).
- [Defects4J](https://github.com/rjust/defects4j) - Collection of 395 reproducible bugs collected with the goal of advancing software testing research.
- [Eclipse AERI stacktraces](http://download.eclipse.org/scava/datasets/aeri_stacktraces/aeri_stacktraces.html) - Collection of stacktraces of Exceptions encountered by users of the Eclipse IDE, as retrieved by the AERI reporting system.
- [Enron Spreadsheets and Emails](https://figshare.com/articles/Enron_Spreadsheets_and_Emails/1221767) - All the spreadsheets and emails used in the paper 'Enron's Spreadsheets and Related Emails: A Dataset and Analysis'.
- [Findbugs-maven](https://github.com/istlab/maven_bug_catalog) - Set of FindBugs reports for the Java projects of the [Maven repository](https://maven.apache.org).
- [GHTorrent](http://ghtorrent.org/) - Scalable, queriable, offline mirror of data offered through the GitHub REST API.
- [GitHub Bug Dataset](http://www.inf.u-szeged.hu/~ferenc/papers/GitHubBugDataSet/) - Bug Dataset of 15 Java open-source projects characterized by static source code metrics.
- [GitHub on Google BigQuery](https://cloud.google.com/bigquery/public-data/github) - GitHub data accessible through Google's BigQuery platform.
- [Grammar Zoo](http://slebok.github.io/zoo/) - Collection of grammars of DSLs and GPLs, some extracted from metamodels and document schemata.
- [KaVE](http://www.kave.cc/datasets) - Developer tool interaction data.
- [Linux Kernel 4.21 Call Graphs](https://zenodo.org/record/2652487#.XRnvomUzb0o) - The Linux Kernel 4.21 Call Graphs produced using [CScout](https://github.com/dspinellis/cscout/). 
- [Maven metrics](https://github.com/bkarak/data_msr2015) - Collection of software complexity & sizing metrics for the [Maven Repository](https://maven.apache.org).
- [Maven Dependency Graph](https://zenodo.org/record/1489120) - Snapshot of the whole Maven Central taken on September 6, 2018, stored in a graph database.
- [mzdata](https://github.com/jxshin/mzdata) - Multi-extract and multi-level dataset of Mozilla issue tracking history.
- [npm-miner](https://github.com/AuthEceSoftEng/msr-2018-npm-miner) - The dataset contains the analysis results of 5 open source software quality tools eslint, escomplex, nsp, jsinspect and sonarjs for 2000 popular (in terms of stars and downloads) npm packages.
- [OCL Expressions on GitHub](https://github.com/tue-mdse/ocl-dataset) - Data set of 9188 OCL expressions originating from 504 EMF meta-models in 245 systematically selected GitHub repositories.
- [RepoReapers Data Set](https://reporeapers.github.io) - Data set containing a collection of _engineered software projects_ from GHTorrent.
- [Software Heritage Graph Dataset](https://doi.org/10.5281/zenodo.2583978) - Graph of the development history and file metadata of >80 million software projects from various forges (GitHub, Gitlab, Debian, PyPI, Google Code, etc) in a deduplicated and unified representation ([paper here](https://dl.acm.org/citation.cfm?id=3341907)).
- [STAMINA](http://stamina.chefbe.net/download) - (STAte Machine INference Approaches) data are used to benchmark techniques for learning deterministic finite state machines (FSMs).
- [Stack Exchange](https://archive.org/details/stackexchange) - Anonymized dump of all user-contributed content on the Stack Exchange network.
- [TravisTorrent](http://travistorrent.testroots.org) - Provides free and easy-to-use Traivs CI build analyses.
- [Ultimate Debian Database (UDD)](https://wiki.debian.org/UltimateDebianDatabase) - Data about various aspects of Debian (e.g. packages, bugs, mainteners) in the same SQL database.
- [Unified Bug Dataset](http://www.inf.u-szeged.hu/~ferenc/papers/UnifiedBugDataSet/) - Static source code based datasets which includes the Bugcatchers Bug Dataset, the [Bug Prediction Dataset](http://bug.inf.usi.ch/index.php), the [Eclipse Bug Dataset](https://www.st.cs.uni-saarland.de/softevo/bug-data/eclipse/), the [GitHub Bug Dataset](http://www.inf.u-szeged.hu/~ferenc/papers/GitHubBugDataSet/), some datasets from the [PROMISE](http://promise.site.uottawa.ca/SERepository/datasets-page.html) repository.
- [Unix history](https://github.com/dspinellis/unix-history-repo) - Git repository with 46 years of Unix history evolution.

## Tools
- [astminer](https://github.com/JetBrains-Research/astminer) - Library and tool for mining of path-based representations of code and other data derived from ASTs.
- [Boa](http://boa.cs.iastate.edu/) - Domain-specific language and infrastructure that eases mining software repositories.
- [buckwheat](https://github.com/JetBrains-Research/buckwheat) - Multi-language tokenizer for extracting identifiers from source code.
- [ckjm](http://www.spinellis.gr/sw/ckjm/) - Chidamber and Kemerer Java Metrics.
- [Coming](https://github.com/SpoonLabs/coming/) - A Java framework for analyzing code changes and mining instances of change patterns from Git repositories.
- [CryptOSS](https://github.com/rvantonder/CryptOSS) - Mine GitHub activity and market cap data for cryptocurrency projects.
- [DbDeo](https://github.com/tushartushar/DbDeo) - Extract embedded SQL statements and detect database schema smells.
- [Designite](http://www.designite-tools.com) - Compute source code metrics and detect a variety of implementation, design, and architecture smells for C#.
- [DesigniteJava](https://github.com/tushartushar/DesigniteJava) - Compute source code metrics and detect a variety of implementation and design smells for Java.
- [Diggit](https://github.com/jrfaller/diggit) - Agile Ruby Tool to analyze Git repositories.
- [GrimoireLab](http://grimoirelab.github.io/) - Free/Libre/Open Source tools for Software Development Analytics.
- [MetricMiner](http://www.github.com/mauricioaniche/metricminer2) - Lean Java DSL to mine and extract data (e.g. commits, developers, modifications, diffs) from Git and SVN repositories.
- [Maven-miner](https://github.com/diverse-project/maven-miner) - Java tools and infrastructure to resolve the whole Maven dependency graph, hosted in Maven Central, in the form of a [Neo4j](https://neo4j.com/) Graph.
- [Perceval](https://github.com/chaoss/grimoirelab-perceval) - Fetch repository data from tens of back-ends.
- [Puppeteer](https://github.com/tushartushar/Puppeteer) - Detect configuration smells in Puppet code.
- [PyDriller](https://github.com/ishepard/pydriller) - Python Framework to analyse Git repositories.
- [qmcalc](https://github.com/dspinellis/cqmetrics) - Calculate quality metrics from C source code.
- [reaper](https://github.com/RepoReapers/reaper) - Python tool to compute a score for a repository from GHTorrent. The score quantifies the extent to which the project contained within the repository is _engineered_.
- [RefactoringMiner](https://github.com/tsantalis/RefactoringMiner) - Library/API for detection of refactorings in changes of Java code.
- [VulData7](https://github.com/electricalwind/data7) - Java framework enabling the automated collection of commits fixing vulnerabilities that are reported in NVD (links NVD with Git).

## Research Outlets
- Outlets exclusively devoted to empirical software engineering research
  - [Empirical Software Engineering journal](https://link.springer.com/journal/10664)
  - [MSR: Mining Software Repositories conference](https://www.msrconf.org/)
  - [PROMISE: Predictive Models and Data Analytics in Software Engineering conference](http://promise.site.uottawa.ca/SERepository/)
- Outlets that publish empirical software engineering research
  - [ACM Transactions on Software Engineering and Methodology (TOSEM)](https://dl.acm.org/citation.cfm?id=J790)
  - [ESEC/FSE: ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering](https://www.esec-fse.org/)
  - [ICSE: International Conference on Software Engineering](http://www.icse-conferences.org/)
  - [IEEE Software magazine](https://publications.computer.org/software-magazine/)
  - [IEEE Transactions on Software Engineering](https://www.computer.org/csdl/journal/ts)
  - [Journal of Systems and Software](https://www.journals.elsevier.com/journal-of-systems-and-software)
  - [SANER: IEEE International Conference on Software Analysis, Evolution and Reengineering](https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000695)


## License

[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)

To the extent possible under law, [Diomidis Spinellis](http://www.spinellis.gr) has waived all copyright and related or neighboring rights to this work.
"
328,rudrakshpathak/laravel-service-repository-pattern,PHP,"# Laravel Service Repository pattern

[![version](https://img.shields.io/badge/version-v1.0.3-orange.svg)]()
[![PyPI](https://img.shields.io/badge/status-stable-brightgreen.svg)]()
[![license](https://img.shields.io/badge/license-MIT-blue.svg)]()
"
329,hacs/default,Python,"# Default repositories

This is where HACS gets its default repositories from to show in the ""store"".

If you want to publish your repositories as default in HACS have a look here:

- https://hacs.xyz/docs/publish/start
- https://hacs.xyz/docs/publish/include
"
330,chaotic-aur/infra,Shell,
331,DaveGut/DEPRECATED-TP-Link-SmartThings,Groovy,"# This integration is DEPRECATED.
SmartThings now has native integration of TP-Link/Kasa products.

https://www.smartthings.com/products/-/filter/brands/tp-link


# TP-Link / Kasa Smart Home Device SmartThings Integration
(Note:  The previous version is still available in the directory ""Old Version"")

# Current Versions:
1.  Service Manager: 4.0.04
2.  Device Handler: 4.0.01

# 2/21/2019 UPDATE
A significant update to version 4.0.  No added device functionality, so upgrade is purely optional.  Changes:

I.  Cleaned up device handlers to reduce code size and (very slightly) reduce execution cycles.

II.  Added a ""upgrade installation"" in the smart application to simplify the update process.  This new version is designed to upgrade from Versions 1, 2, 3, 3.5, and 3.6 and has been tested to do so.

III.  Created documentation for installation and upgrade in the Documentation Folder:

    1.	""0 - Upgrade Installation.pdf""
    2.	""1 - Installing DH and SmartApp into SmartThings.pdf""
    3.	""2 - Running the Service Manager for the First Time.pdf""
    4.	""3 - Adding Kasa Devices.pdf""
    5.	""4 - Setting Kasa Device Preferences.pdf""
    6.	""TP-Link SmartThing Implementation.pdf""
    7.  ""RE270_370 IFTTT Instructions.txt""

There is only one version supporting three integrations:

a.  Kasa Account.  Integration via the user's Kasa Account through the Cloud.  This uses a Smart Application for installation, device communications, and device management.

b.  Node Applet.  Smart Application integration via a home wifi Node.JS bridge (pc, fire tablet, android device, Raspberry Pi).  The Smart Application is used (with user entry of bridge IP) to install and manage the devices.  Especially useful in the new SmartThings phone app since it allows entry of user preferences via that app.

c.  Manual Node Installation.  Traditional Hub installation.  Does not use a Smart Application.

# Installation Prequisites:

A SmartThings Hub, IDE Account and SmartThings Classic are required for all original installations.  After installation, you may (if desired) transition to the new SmartThings phone app.

    a.	A SmartThing Hub
    b.	Kasa Account.  (1) Kasa Account, (2) TP-Link devices in remote mode.
    c.	Node Applet.  (1) Node.js Bridge, (2) Static IP address for Bridge 
    	(recommended for all devices).
    d.	Manual Node Installation.  (1)  Node.js Bridge, (2) Static IP addresses 
    	for the bridge and all devices.

# Installation.
    a.  Install the code to your Smart Things IDE per: ""1 - Installing DH and SmartApp into SmartThings.pdf"".
    b.  Run the Service Manager for the first time per: ""2 - Running the Service Manager for the First Time.pdf"".
    c.  Add your Kasa Devices per: ""3 - Adding Kasa Devices.pdf"".
    d.  OPTIONAL.  Set preferences for the devices per: ""4 - Setting Kasa Device Preferences.pdf"".
    
# Upgrade from previous versions.
See:  ""0 - Upgrade Installation.pdf"".

    a.  Replace the content of the device handlers and service manager in SmartThings  (NOTE: For the combined plug-switch device handler, use either the plug or the switch device handler.  They are the same except for an icon.)
    b.  Run the Service Manager and select Update Installation Data then ""Save"" in the right corner on the next page.

"
332,SkinsRestorer/SkinsRestorerX,Java,"# SkinsRestorerX
[![Build Status](https://travis-ci.org/SkinsRestorer/SkinsRestorerX.svg?branch=master)](https://travis-ci.org/SkinsRestorer/SkinsRestorerX)
[![Current Release](https://img.shields.io/github/release/SkinsRestorer/SkinsRestorerX.svg)](https://github.com/SkinsRestorer/SkinsRestorerX/releases/latest)
[![Downloads](https://img.shields.io/github/downloads/SkinsRestorer/SkinsRestorerX/latest/total.svg)](https://github.com/SkinsRestorer/SkinsRestorerX/releases/latest/download/SkinsRestorer.jar)
[![Contributors](https://img.shields.io/github/contributors/SkinsRestorer/SkinsRestorerX.svg)](https://github.com/SkinsRestorer/SkinsRestorerX/graphs/contributors)
[![Commits since last release](https://img.shields.io/github/commits-since/SkinsRestorer/SkinsRestorerX/latest.svg)](https://github.com/SkinsRestorer/SkinsRestorerX/commits/master)
[![License](https://img.shields.io/github/license/SkinsRestorer/SkinsRestorerX.svg)](https://github.com/SkinsRestorer/SkinsRestorerX/blob/master/LICENSE)
[![Discord](https://img.shields.io/discord/186794372468178944.svg?color=blue&label=discord&logo=discord)](https://discord.gg/sAhVsyU)
[![Spigot downloads](https://img.shields.io/spiget/downloads/2124?label=Spigot%20downloads)](https://www.spigotmc.org/resources/2124/)
[![Spigot rating](https://img.shields.io/spiget/rating/2124?label=Spigot%20rating)](https://www.spigotmc.org/resources/2124/)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[![download badge](https://img.shields.io/badge/DOWNLOAD-LATEST-success?style=for-the-badge)](https://github.com/SkinsRestorer/SkinsRestorerX/releases/latest/download/SkinsRestorer.jar)
&nbsp;&nbsp;&nbsp;[![download badge2](https://img.shields.io/badge/DOWNLOAD-DEV__BUILD-important?style=for-the-badge)](https://ci.codemc.io/job/SkinsRestorer/job/SkinsRestorerX/lastSuccessfulBuild/artifact/target/SkinsRestorer.jar)

This is the development repository for [SkinsRestorer](https://www.spigotmc.org/resources/skinsrestorer.2124/) (Minecraft plugin).

Restoring offline mode skins & changing skins for Bukkit/Spigot, BungeeCord/Waterfall, Sponge, catserver and Velocity servers.

## :telescope: Compatibility
- Java 8 till 12 (maybe newer also) ([AdoptOpenJDK](https://adoptopenjdk.net/) | [Oracle Java](https://www.oracle.com/de/java/technologies/javase-downloads.html))
- Minecraft 1.8.0 - 1.16.x

## :link: Links
- [Spigot Page](https://www.spigotmc.org/resources/skinsrestorer.2124/)
- [Velocitypowered Page](https://forums.velocitypowered.com/t/skinsrestorer-ability-to-restore-change-skins-on-servers/142)
- [Sponge ore Page](https://ore.spongepowered.org/SRTeam/SkinsRestorer)
- [PaperMC](https://papermc.io/forums/t/1-8-1-14-4-skinsrestorer/1996)
- [Wiki](https://github.com/SkinsRestorer/SkinsRestorerX/wiki/)
- [Jenkins](https://ci.codemc.io/job/SkinsRestorer/job/SkinsRestorerX/)
- [Discord](https://discord.me/skinsrestorer)
- [Website](https://skinsrestorer.net/)

## :scroll: License
SkinsRestorer is licensed under GNU General Public License v3.0. Please see [`LICENSE.txt`](https://github.com/SkinsRestorer/SkinsRestorerX/blob/master/LICENSE) for more info.

## :family: Authors
See [Contributors](https://github.com/SkinsRestorer/SkinsRestorerX/graphs/contributors) for a list of people that have suppported this project by contributing.

## :building_construction: SkinsRestorer API

:rotating_light: Please note that this API is still WIP. Expect breaking changes! :rotating_light:

##### Maven repository
````
<repository>
    <id>codemc-snapshots</id>
    <url>https://repo.codemc.org/repository/maven-snapshots/</url>
</repository>
````

##### SkinsRestorer API
````
<!-- SkinsRestorer API -->
<dependency>
    <groupId>net.skinsrestorer</groupId>
    <artifactId>skinsrestorer</artifactId>
    <version>14.0.0</version>
</dependency>
````

##### Example Bukkit plugin
https://github.com/SkinsRestorer/SkinsRestorerAPIExample

### How to install? / installation / setup
Installing SkinsRestorer can be tricky when using a proxy server.
Make sure to read on how to install [here](https://github.com/SkinsRestorer/SkinsRestorerX/wiki/Installing-SkinsRestorer#Basic-Installation)
"
333,buddhi1980/mandelbulber2,C++,"![Banner](https://raw.githubusercontent.com/buddhi1980/mandelbulber2/wiki/assets/images/mandelbulberBanner.png)
<table>
<tr>
    <th>Coverity Scan</th>
    <th>Build Status</th>
    <th>Gitter Chat</th>
</tr>
<tr>
    <td><a href=""https://scan.coverity.com/projects/mandelbulber-v2"">
        <img alt=""Coverity Scan"" src=""https://scan.coverity.com/projects/4723/badge.svg?flat=1""></a></td>
    <td>
        Linux: <a href=""https://travis-ci.com/github/buddhi1980/mandelbulber2"">
                <img alt=""Build Status Linux"" src=""https://travis-ci.com/buddhi1980/mandelbulber2.svg?branch=master""></a></br>
        Windows: <a href=""https://ci.appveyor.com/project/buddhi1980/mandelbulber2"">
                <img alt=""Build Status Windows"" src=""https://ci.appveyor.com/api/projects/status/urd2h30tu7reg4mp?svg=true""></a></br>
        OSX: <a href=""https://circleci.com/gh/buddhi1980/mandelbulber2"">
                <img alt=""Build Status OSX Circle CI"" src=""https://circleci.com/gh/buddhi1980/mandelbulber2.svg?style=shield"">
</a></br>
Azure: <a href=""https://dev.azure.com/buddhi19800328/Mandelbulber/_build?definitionId=1""><img alt=""Build status Azure"" src=""https://dev.azure.com/buddhi19800328/Mandelbulber/_apis/build/status/buddhi1980.mandelbulber2?branchName=master""></a>

</td>
    </td>
    <td><a href=""https://gitter.im/buddhi1980/mandelbulber2?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge"">
        <img alt=""Join the chat"" src=""https://badges.gitter.im/Join%20Chat.svg""></a></td>
</tr>
</table>




## Universal Idea

Mandelbulber creatively generates three-dimensional fractals.

Explore trigonometric, hyper-complex, Mandelbox, IFS, and many other 3D fractals.

Render with a great palette of customizable materials to create stunning images and videos.

The possibilities are literally **infinite**!

![Test Render](https://raw.githubusercontent.com/buddhi1980/mandelbulber2/wiki/assets/images/mandelbulberTestrender.jpg)

## Features

- High-Performance computing with multiple graphics accelerator cards (multi-GPU support via OpenCL)
- Mathematical Models and Monte Carlo Algorithms for photo-realistic scenes
- [Enlightening Documentation](https://github.com/buddhi1980/mandelbulber_doc/)
- Renders trigonometric, hyper-complex, Mandelbox, IFS, and many other 3D fractals
- Complex 3D raymarching: hard shadows, ambient occlusion, depth of field, translucency & refraction, etc.
- Rich GUI in Qt 5 environment
- Unlimited image resolution on 64-bit systems
- Program developed for ARM (experimental), x86 and x64 CPUs (Linux, Windows, macOS)
- Simple 3D navigator
- Distributed Network Rendering
- Key-frame animation for all parameters with different interpolations
- Material management
- Texture mapping (color, luminosity, diffusion, normal maps, displacement)
- Exporting of 3D objects
- Rendering queue
- Command line interface for headless systems


![image](https://cloud.githubusercontent.com/assets/11696990/13788910/173cf11a-eae2-11e5-884e-f1d03924a5f3.png)
![image](https://user-images.githubusercontent.com/11696990/52135525-4c3ad100-2646-11e9-920b-770747cb90c0.png)

## Keyboard shortcuts

In render window:

  - <kbd>Shift</kbd>+<kbd>Up</kbd> or <kbd>Q</kbd> / <kbd>Shift</kbd>+<kbd>Down</kbd> or <kbd>Z</kbd>: Move Camera Forward / Backward
  - <kbd>Shift</kbd>+<kbd>Left</kbd> or <kbd>A</kbd> / <kbd>Shift</kbd>+<kbd>Right</kbd> or <kbd>D</kbd>: Move Camera Left / Right
  - <kbd>W</kbd> / <kbd>S</kbd>: Move Camera Up / Down
  - <kbd>Up</kbd> / <kbd>Down</kbd> / <kbd>Left</kbd> / <kbd>Right</kbd>: Rotate Camera
  - <kbd>Ctrl</kbd>+(<kbd>Left</kbd> / <kbd>Right</kbd>): Roll Camera Left / Right

## Building and Deploying 

Download the latest stable version from [Releases](https://github.com/buddhi1980/mandelbulber2/releases) or clone git repository for actual development version.

After downloading, unpack the file and follow our instructions from the [README](https://raw.githubusercontent.com/buddhi1980/mandelbulber2/master/mandelbulber2/deploy/README) file.
Please see additional information in [mandelbulber2/deploy](mandelbulber2/deploy) folder.

## Easy Preparation for Development

The software is natively developed using Qt Creator for Linux (Debian or Ubuntu).

The file [mandelbulber2/qmake/mandelbulber.pro](https://github.com/buddhi1980/mandelbulber2/blob/master/mandelbulber2/qmake/mandelbulber.pro) specifies the build system configuration for Qt Creator. To be able to compile the program, the host operating system requires preparation:

Use the following scripts to prepare your Linux environment for development.
These scripts install all dependencies, compile the program, and create symbolic links in /usr/share/mandelbulber to your working directory.

[Prepare Debian for Development](https://github.com/buddhi1980/mandelbulber2/blob/master/mandelbulber2/tools/prepare_for_dev_debian_testing.sh)

[Prepare Ubuntu for Development](https://github.com/buddhi1980/mandelbulber2/blob/master/mandelbulber2/tools/prepare_for_dev_ubuntu.sh)

[Arch Linux AUR Package (Current release)](https://aur.archlinux.org/packages/mandelbulber2/)

[Arch Linux AUR Package (Current git snapshot)](https://aur.archlinux.org/packages/mandelbulber2-git/)

In addition, there exists an MSVC Solution located in this git package. The solution accommodates all dependencies with NUGET for autonomous compilation of OpenCL accelerated binary.

The system requires the QT5 framework in conjunction with LZO for real-time data compression and GSL for scientific resources.

## Official partners

[I-love-chaos](http://ilc.fractalforums.com)

[![I-love-chaos](http://ilc.fractalforums.com/img/thumbnail/img/ilc-128x128.png)](http://ilc.fractalforums.com)

## Resources
[Video tutorials](https://www.youtube.com/playlist?list=PLOwamUnstvZF0Y9sjxvwHNvrHHF1ZzFql)

[Image Gallery](http://krzysztofmarczak.deviantart.com/gallery/)

[Forum](https://fractalforums.org/mandelbulber/14)

[Forum Gallery](http://www.fractalforums.com/index.php?action=gallery;cat=51)

[Compiled Binaries](http://sourceforge.net/projects/mandelbulber/)

[Coverity Scan](http://scan.coverity.com/projects/4723?tab=overview)

![Open Hub](https://www.openhub.net/p/mandelbulber2/widgets/project_thin_badge.gif)

## License

GNU GPL v3
"
334,kubeflow/examples,Jsonnet,"# kubeflow-examples

A repository to share extended Kubeflow examples and tutorials to demonstrate machine learning
concepts, data science workflows, and Kubeflow deployments. The examples illustrate the happy path,
acting as a starting point for new users and a reference guide for experienced users.

This repository is home to the following types of examples and demos:
* [End-to-end](#end-to-end)
* [Component-focused](#component-focused)
* [Demos](#demos)

## End-to-end

### [Named Entity Recognition](./named_entity_recognition)
Author: [Sascha Heyer](https://github.com/saschaheyer)

This example covers the following concepts:
1. Build reusable pipeline components
2. Run Kubeflow Pipelines with Jupyter notebooks
1. Train a Named Entity Recognition model on a Kubernetes cluster
1. Deploy a Keras model to AI Platform
1. Use Kubeflow metrics
1. Use Kubeflow visualizations 

### [GitHub issue summarization](./github_issue_summarization)
Author: [Hamel Husain](https://github.com/hamelsmu)

This example covers the following concepts:
1. Natural Language Processing (NLP) with Keras and Tensorflow
1. Connecting to Jupyterhub
1. Shared persistent storage
1. Training a Tensorflow model
    1. CPU
    1. GPU
1. Serving with Seldon Core
1. Flask front-end

### [Pachyderm Example - GitHub issue summarization](./github_issue_summarization/Pachyderm_Example)
Author: [Nick Harvey](https://github.com/Nick-Harvey) & [Daniel Whitenack](https://github.com/dwhitena)

This example covers the following concepts:
1. A production pipeline for pre-processing, training, and model export
1. CI/CD for model binaries, building and deploying a docker image for serving in Seldon
1. Full tracking of what data produced which model, and what model is being used for inference
1. Automatic updates of models based on changes to training data or code
1. Training with single node Tensorflow and distributed TF-jobs

### [Pytorch MNIST](./pytorch_mnist)
Author: [David Sabater](https://github.com/dsdinter)

This example covers the following concepts:
1. Distributed Data Parallel (DDP) training with Pytorch on CPU and GPU
1. Shared persistent storage
1. Training a Pytorch model
    1. CPU
    1. GPU
1. Serving with Seldon Core
1. Flask front-end

### [MNIST](./mnist)

Author: [Elson Rodriguez](https://github.com/elsonrodriguez)

This example covers the following concepts:
1. Image recognition of handwritten digits
1. S3 storage
1. Training automation with Argo
1. Monitoring with Argo UI and Tensorboard
1. Serving with Tensorflow

### [Distributed Object Detection](./object_detection)

Author: [Daniel Castellanos](https://github.com/ldcastell)

This example covers the following concepts:
1. Gathering and preparing the data for model training using K8s jobs
1. Using Kubeflow tf-job and tf-operator to launch a distributed object training job
1. Serving the model through Kubeflow's tf-serving

### [Financial Time Series](./financial_time_series)

Author: [Sven Degroote](https://github.com/Svendegroote91)

This example covers the following concepts:
1. Deploying Kubeflow to a GKE cluster
2. Exploration via JupyterHub (prospect data, preprocess data, develop ML model)
3. Training several tensorflow models at scale with TF-jobs
4. Deploy and serve with TF-serving
5. Iterate training and serving
6. Training on GPU
7. Using Kubeflow Pipelines to automate ML workflow

### [Pipelines](./pipelines)

#### [Simple notebook pipeline](./pipelines/simple-notebook-pipeline)
Author: [Zane Durante](https://github.com/zanedurante)

This example covers the following concepts:
1. How to create pipeline components from python functions in jupyter notebook
2. How to compile and run a pipeline from jupyter notebook

#### [MNIST Pipelines](./pipelines/mnist-pipelines)
Author: [Dan Sanche](https://github.com/DanSanche) and [Jin Chi He](https://github.com/jinchihe)

This example covers the following concepts:
1. Run MNIST Pipelines sample on a Google Cloud Platform (GCP).
2. Run MNIST Pipelines sample for on premises cluster.

## Component-focused

### [XGBoost - Ames housing price prediction](./xgboost_ames_housing)
Author: [Puneith Kaul](https://github.com/puneith)

This example covers the following concepts:
1. Training an XGBoost model
1. Shared persistent storage
1. GCS and GKE
1. Serving with Seldon Core

## Demos

Demos are for showing Kubeflow or one of its components publicly, with the
intent of highlighting product vision, not necessarily teaching. In contrast,
the goal of the **examples** is to provide a self-guided walkthrough of
Kubeflow or one of its components, for the purpose of teaching you how to
install and use the product.

In an *example*, all commands should be embedded in the process and explained.
In a *demo*, most details should be done behind the scenes, to optimize for
 on-stage rhythm and limited timing.

You can find the demos in the [`/demos` directory](demos/).

## Third-party hosted

| Source | Example | Description |
| ------ | ------- | ----------- |
| | | | |

## Get Involved

* [Slack](https://join.slack.com/t/kubeflow/shared_invite/zt-cpr020z4-PfcAue_2nw67~iIDy7maAQ)
* [Twitter](http://twitter.com/kubeflow)
* [Mailing List](https://groups.google.com/forum/#!forum/kubeflow-discuss)

In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.

The Kubeflow community is guided by our [Code of Conduct](https://github.com/kubeflow/community/blob/master/CODE_OF_CONDUCT.md), which we encourage everybody to read before participating.
"
335,Azure/azure-policy,Open Policy Agent,"# Azure Policy Samples

This repository contains built-in samples of Azure Policies that can be used as reference for creating and assigning policies to your subscriptions and resource groups. For easy search of all built-in  with descriptions, see [Policy samples](https://docs.microsoft.com/azure/governance/policy/samples/) on docs.microsoft.com.

For custom policy samples, check out our Community repo! (https://github.com/Azure/Community-Policy)

## Contributing

To contribute, please submit your policies to our Community repo! (https://github.com/Azure/Community-Policy)

## Reporting Samples Issues

If you discover a problem with any of the samples published here that isn't already reported in [**Issues**](https://github.com/Azure/azure-policy/issues), open a [**New issue**](https://github.com/Azure/azure-policy/issues/new/choose).

# Azure Policy Known Issues

Check here for a current list of [**known issues**](#known-issues) for Azure Policy.

# Azure Policy Resources

## Articles

- [Azure Policy overview](https://docs.microsoft.com/azure/governance/policy/overview)
- [How to assign policies using the Azure portal](https://docs.microsoft.com/azure/governance/policy/assign-policy-portal)
- [How to assign policies using Azure PowerShell](https://docs.microsoft.com/azure/governance/policy/assign-policy-powershell)
- [How to assign policies using Azure CLI](https://docs.microsoft.com/azure/governance/policy/assign-policy-azurecli)
- [Export and manage Azure Policy resources as code with GitHub](https://docs.microsoft.com/azure/governance/policy/tutorials/policy-as-code-github)
- [Definition structure](https://docs.microsoft.com/azure/governance/policy/concepts/definition-structure)
- [Understand Policy effects](https://docs.microsoft.com/azure/governance/policy/concepts/effects)
- [Audit VMs with Guest Configuration](https://docs.microsoft.com/azure/governance/policy/concepts/guest-configuration)
- [Programmatically create policies](https://docs.microsoft.com/azure/governance/policy/how-to/programmatically-create)
- [Get compliance data](https://docs.microsoft.com/azure/governance/policy/how-to/get-compliance-data)
- [Remediate non-compliant resources](https://docs.microsoft.com/azure/governance/policy/how-to/remediate-resources)

## References

- [Azure CLI](https://docs.microsoft.com/cli/azure/policy)
- Azure PowerShell
  - [Policy](https://docs.microsoft.com/powershell/module/az.resources/#policies)
  - [Guest Configuration (preview)](https://www.powershellgallery.com/packages/AzureRM.GuestConfiguration)
- REST API
  - [Events](https://docs.microsoft.com/rest/api/policy-insights/policyevents)
  - [States](https://docs.microsoft.com/rest/api/policy-insights/policystates)
  - [Assignments](https://docs.microsoft.com/rest/api/resources/policyassignments)
  - [Policy Definitions](https://docs.microsoft.com/rest/api/resources/policydefinitions)
  - [Initiative Definitions](https://docs.microsoft.com/rest/api/resources/policysetdefinitions)
  - [Policy Tracked Resources](https://docs.microsoft.com/rest/api/policy-insights/policytrackedresources)
  - [Remediations](https://docs.microsoft.com/rest/api/policy-insights/remediations)
  - [Guest Configuration (preview)](https://docs.microsoft.com/rest/api/guestconfiguration/)


## Getting Support

The general Azure Policy support role of this repository has transitioned to standard Azure support channels. See below for information about getting support help for Azure Policy.

### Alias Requests

An alias enables you to restrict what values or conditions are permitted for a *property* on a resource. Each alias maps to the paths in different API versions for a given resource type. During policy evaluation, the policy engine gets the property path for that API version.
See the documentation page on aliases [**here**](https://docs.microsoft.com/azure/governance/policy/concepts/definition-structure#aliases). For additional information about Azure Policy and aliases, visit this [**blog post**](https://azure.microsoft.com/blog/more-resource-policy-aliases/).

Previously, this repository was the official channel to open requests for new aliases. Since the full set of aliases for most namespaces have now been published, support for requesting aliases is now handled by Azure Customer Support. Open a new [**Azure Customer Support ticket**](https://azure.microsoft.com/support/create-ticket/) if you believe you need new aliases to be published.

[**This page**](https://docs.microsoft.com/azure/governance/policy/concepts/definition-structure#aliases) documents the commands for discovering existing aliases.

### General Questions

If you have questions you haven't been able to answer from the [**Azure Policy documentation**](https://docs.microsoft.com/azure/governance/policy), there are a few places that host discussions on Azure Policy:

 - [Microsoft Tech Community](https://techcommunity.microsoft.com/) [**Azure Governance conversation space**](https://techcommunity.microsoft.com/t5/Azure-Governance/bd-p/AzureGovernance)
 - Join the Customer Call on Azure Governance (register [here](https://forms.office.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxn7UD7lweFDnmuLj72r6E1UN1dLNTBZUVMyNVpHUjJLRE5PVDVGNlkyOC4u)) Latest Customer call (Sep 3rd) recording can be found [here](https://youtu.be/ONKn9XMPZCs)
 - Search old [**issues in this repo**](https://github.com/Azure/azure-policy/issues)
 - Search or add to Azure Policy discussions on [**StackOverflow**](https://stackoverflow.com/questions/tagged/azure-policy+or+azure+policy)

If your questions are more in-depth or involve information that is not public, open a new [**Azure Customer Support ticket**](https://azure.microsoft.com/support/create-ticket/).

### Documentation Corrections

To report issues in the Azure Policy online documentation, look for a feedback area at the bottom of the page. If you don't see a place to enter feedback, you can also directly open a new issue at the [**Microsoft Docs GitHub**](https://github.com/MicrosoftDocs/feedback/issues).

### New built-in Policy Proposals

If you have ideas for new built-in policies you want to suggest to Microsoft, you can submit them to [**Azure Governance User Voice**](https://feedback.azure.com/forums/915958-azure-governance). These suggestions are actively reviewed and prioritized for implementation.

### Other Support for Azure Policy

If you are encountering livesite issues or difficulties in implementing new policies that may be due to problems in Azure Policy itself, open a support ticket at [**Azure Customer Support**](https://azure.microsoft.com/support/create-ticket/). If you want to submit an idea for consideration, add an idea or upvote an existing idea at [**Azure Governance User Voice**](https://feedback.azure.com/forums/915958-azure-governance).

## Known Issues

Azure Policy operates at a level above other Azure services by applying policy rules against PUT requests and GET responses of resource types going between Azure Resource Manager and the owning resource provider (RP). In a few cases, the behavior of a given RP is unexpected or incompatible in some way with Azure Policy. The Azure Policy team works with the RP teams to close these gaps as soon as possible after they are discovered. Usually aliases for properties of these resource types will be removed after the anomalous behavior is discovered. Issues of this nature will be documented here until final resolution.

All cases of known resource types with anomalous policy behavior are listed here. Currently there is no way to make these resource types invisible at policy authoring time, so writing policies that attempt to manage these resource types cannot be prevented, despite the fact that the results of such policies may be either incomplete or incorrect.

### Resource Type query results incomplete, missing, or non-standard format

In some cases, certain RPs may return incomplete or otherwise limited or missing information about resources of a given type. The Azure Policy engine is unable to determine the compliance of any resources of such a type. Below are listed the known resource types exhibiting this problem.

- Microsoft<span></span>.Web/sites/config/* (except Microsoft<span></span>.Web/sites/config/web)

Currently, there is no plan to change this behavior for the above Microsoft.Web resource types. If this scenario is important to you, please [open a support ticket](https://azure.microsoft.com/support/create-ticket/) with the Web team.

- Microsoft.HDInsights/clusters/computeProfile.roles[*].scriptActions
- Microsoft.Sql/servers/auditingSettings
  - This type will work correctly as the related resource in `AuditIfNotExists` and `DeployIfNotExists` policies, as long as a `name` for the resource is provided, e.g:
  ```       ""effect"": ""AuditIfNotExists"",
            ""details"": {
              ""type"": ""Microsoft.Sql/servers/auditingSettings"",
              ""name"": ""default""
            }
   ```
- Microsoft.DataLakeStore/accounts
  - This type behaves similarly to Microsoft.Sql/servers/autidintSettings. Compliance of some fields cannot be determined except in AuditIfNotExits and DeployIfNotExists.
- Microsoft.Sql 'master' database 
   - This type behaves similarly to Microsoft.Sql/servers/auditingSettings. Compliance of some fields cannot be determined except in `AuditIfNotExists` and `DeployIfNotExists` policies.
- Microsoft.Compute/virtualMachines/instanceView

The potential for fixing these resource types is still under investigation.

### Resource Type not correctly published by resource provider

In some cases, a resource provider may implement a resource type, but not correctly publish it to the Azure Resource Manager. The result of this is that Azure Policy is unable to discover the type in order to determine compliance. In some cases, this still allows deny policies to work, but compliance results will usually be incorrect. These resource types exhibit this behavior:

- Microsoft.Storage/storageAccounts/blobServices

These resource types previously exhibited this behavior, but are now removed:

- Microsoft.EventHub/namespaces/networkRuleSet (replaced by Microsoft.EventHub/namespaces/networkruleset**s**)
- Microsoft.ServiceBus/namespaces/networkRuleSet (replaced by Microsoft.ServiceBus/namespaces/networkruleset**s**)

In some cases the unpublished resource type is actually a subtype of a published type, which causes aliases to refer to a parent type instead of the unpublished type. Evaluation of such policies fails, causing the policy to never apply to any resource.

These resource types previously exhibited this behavior but have been fixed:

- Microsoft.EventHub/namespaces/networkrulesets
- Microsoft.ServiceBus/namespaces/networkrulesets
- Microsoft.Sql/servers/databases/backupShortTermRetentionPolicies
- Microsoft.ApiManagement/service/portalsettings/delegation

### Resource management that bypasses Azure Resource Manager

Resource providers are free to implement their own resource management operations outside of Azure Resource Manager (""dataplane"" operations). In almost every Azure resource type, the distinction between resource management and dataplane operations is clear and the resource provider only implements resource management one way. Occasionally, a resource provider may choose to implement a type that can be managed both ways. In this case, Azure Policy controls the standard Azure Resource Manager API normally, but operations on the direct resource provider API to create, modify and delete resources of that type bypass Azure Resource Manager so they are invisible to Azure Policy. Since policy enforcement is incomplete, we recommend that customers do not implement policies targeting such a resource type. This is the list of known such resource types:

- Microsoft.Storage/storageAccounts/blobServices/containers

The storage team has implemented blob public access control on storage accounts to address this scenario. Per-account public access control of blobs can be controlled by Azure Policy using the new alias ```Microsoft.Storage/storageAccounts/allowBlobPublicAccess```.

Note that Azure policies for dataplane operations of certain targeted resource providers is also supported or under active development.

 - Microsoft.Sql/servers/firewallRules

Firewall rules can be created/deleted/modified via T-SQL commands, which bypasses Azure Policy. There is currently no plan to address this.

- Microsoft.ServiceFabric/clusters/applications

Service Fabric applications created via direct requests to the Service Fabric cluster (i.e. via New-ServiceFabricApplication) will not appear in the Azure Resource Manager representation of the Service Fabric cluster. Policy will not be able to audit/enforce these applications.

### Nonstandard creation pattern

In a few instances, the creation pattern of a resource type doesn't follow normal REST patterns. In these cases, deny policies may not work or may only work for some properties. For example, certain resource types may PUT only a subset of the properties of the resource type to create the entire resource. With such types the resource provider selects the values for properties not provided in the payload. Such a resource might be created with a non-compliant value even though a deny policy exists to prevent it. A similar result may occur if a set of resource types can be created using a collection PUT. Known resource types that exhibit this class of behavior:

- Microsoft.Sql/servers/firewallRules
- Microsoft.Automation/certificates
- Microsoft.Security/securityContacts

There is currently no plan to change this behavior for these types. If this scenario is important to you, please [open a support ticket](https://azure.microsoft.com/support/create-ticket/) with the Azure SQL or Automation team.

### Provider pass-through to non Azure Resource Manager resources

There are examples where a resource provider publishes a resource type to Azure Resource Manager, but the resources it represents cannot be managed by Azure Resource Manager. For example, Microsoft.Web has published several resource types to Azure Resource Manager that actually represent resources of the customer's site rather than Azure Resource Manager resources. Such resources cannot or should not be managed by Azure policy, and are explicitly excluded. All known examples are listed here:

- Microsoft.Web/sites/deployments
- Microsoft.Web/sites/functions
- Microsoft.Web/sites/instances/deployments
- Microsoft.Web/sites/siteextensions
- Microsoft.Web/sites/slots/deployments
- Microsoft.Web/sites/slots/functions
- Microsoft.Web/sites/slots/instances/deployments
- Microsoft.Web/sites/slots/siteextensions

### Legacy or incorrect aliases

Since custom policies use aliases directly, it is usually not possible to update them without causing unintended side effects to existing custom policies. This means that aliases referring to incorrect information or following legacy naming conventions must be left in place, even though it may cause confusion. In certain cases where an alias is known to refer to the wrong information, another alias may be created as a corrected alternative to the known bad one. In these cases, the new alias will be given the name of the bad alias with .v2 appended. For example a bad alias named Microsoft.ResourceProvider/someType/someAlias would result in the addition of a corrected version named Microsoft.ResourceProvider/someType/someAlias.v2. If an alias is added to correct a .v2 alias it will be named by replacing v2 with v3. All known corrected aliases are listed here:

- Microsoft.Sql/servers/databases/requestedServiceObjectiveName.v2

### Optional or auto-generated resource property that bypasses policy evaluation

In a few instances, when creating a resource from Azure Portal, the property is not set in the PUT request payload. When the request reaches the resource provider, the resource provider generates the property and sets the value. Because the property is not in the request payload, the policy cannot evaluate the property. Known resource fields that exhibit this class of behavior:

- Microsoft.Storage/storageAccounts/networkAcls.defaultAction
- Microsoft.Authorization/roleAssignments/principalType
- Microsoft.Compute/virtualMachines/storageProfile.osDisk.diskSizeGB
- Microsoft.Compute/virtualMachineScaleSets/virtualMachineProfile.storageProfile.osDisk.diskSizeGB
- Microsoft.Compute/virtualMachineScaleSets/virtualMachines/storageProfile.osDisk.diskSizeGB

Using this type of alias in the existence condition of auditIfNotExists or deployIfNotExists policies works correctly. These two kinds of effects will get the full resource content to evaluate the existence condition. The property is always present in GET request payloads.

Using this type of alias in audit/deny/append effect policies works partially. The compliance scan result will be correct for existing resources. However, when creating/updating the resource, there will be no audit events for audit effect policies and no deny or append behaviors for deny/append effect policies because of the missing property in the request payload.

-Microsoft.Databricks/* (Creation time only)

All Databricks resources bypass policy enforcement at creation time. Databricks resources will have policy enforcement post-creation. To provide feedback on this, please leverage the [Databricks UserVoice] (https://feedback.azure.com/forums/909463-azure-databricks). 

### Resource types that exceed current enforcement and compliance scale

There some resource types that are generated at very high scale. These are not suitable for management by Azure Policy because the enforcement and compliance checks create overhead that can negatively impact the performance of the API itself. Most of these are not significant policy scenarios, but there are a few exceptions.

These are resource types that have significant policy scenarios, but are not supported by Azure Policy due to the above scalability considerations:

- Microsoft.ServiceBus/namespaces/topics
- Microsoft.ServiceBus/namespaces/topics/authorizationRules
- Microsoft.ServiceBus/namespaces/topics/subscriptions
- Microsoft.ServiceBus/namespaces/topics/subscriptions/rules

Work to increase the scale that policy can be performantly applied to resource types is in progress. Planned availability date is not yet determined.

### Azure Policy Add-on not compatible on AKS Kubernetes 1.19 (preview) version
1.19 clusters will return this error via gatekeeper controller and policy webhook pods: 
 certificate relies on legacy Common Name field, use SANs or temporarily enable Common Name matching 

Mitigation: Avoid using K8s 1.19 (preview) with the Azure Policy add-on. The add-on can be used with any GA supported version such as 1.16, 1.17, or 1.18. 
Feature team is actively working on fixing this issue. GitHub issue tracking this on AKS side https://github.com/Azure/AKS/issues/1869 


### Indexed Resource types always non-complaint to tagging policies 
As of February 2021, index resources that don't support tags aren't applicable to polices that inspect tags.

### Alias changes  

May 2020: Microsoft.DocumentDB/databaseAccounts/ipRangeFilter updated from a string property to an array.  Please re-author your custom definitions to support the property as an array.  
July 2020: The alias Microsoft.Sql/servers/securityAlertPolicies/emailAddresses[] and related policies were deprecated. 

*This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.*
"
336,exvim/main,Vim script,"![exVim Logo](http://exvim.github.io/images/logo.png)

[Documentation](http://exvim.github.io/docs/) |
[Downloads](http://exvim.github.io/downloads/) |
[Plugins](http://exvim.github.io/docs/plugins/) |
[Installation](http://exvim.github.io/docs/install/) |
[Getting Start](http://exvim.github.io/docs/getting-start/) |
[Known Issues](http://exvim.github.io/docs/known-issues/) |
[Community](#community)

# Intro

exVim is a project to turn Vim into a nice programming environment. exVim introduce the project file concept (.exvim) in Vim. By editing this project file (.exvim) through Vim, the exVim plugins will be invoked. In this way, it makes you possible to apply different Vim settings, plugin settings and even loading plugins on demand by different projects. In general, it makes Vim become the best IDE in the world!

**EVEN COOLER IS --- WE USE EXVIM TO DEVELOP EXVIM! (\\(-_-)/)**

## Features

- Manage your project with a `.exvim` setting file.
- Update your project files with a single command. (tags, cscope-db, search-index, makefile, ...)
- Project files are stored in one place (in the folder `./.exvim.your_project_name/` under your project).
- Load Vim plugins on demand for different projects based on your `.exvim` settings.
- Better management of plugin windows in Vim. (avoid getting multiple plugin windows messed up)  
- Browse and work on your project files and folders in the project window.
- Class, variable, and function tag jumping.
- Global search in project scope. 
- Global search engine customization (user can choose grep, idutils, or even his own)
- A powerful way to filter your global search results.
- Generate class hierarchy pictures. 
- Enhanced quick-fix window.
- Popular Vim plugin integrated.

## How does it work?

By editing and saving your project settings in a `your_project_name.exvim` file and opening it with Vim, the exVim plugins will be loaded. exVim will parse the `your_project_name.exvim` file and apply the settings for your project after Vim
has started.

The settings include:

- The window layout of your Vim. (Where to open the plugin window, initial opened window, last used layout...)
- File and Folder filter.
- Plugins you wish to use in the project.
- Plugin settings for the project.
- External tools. Such as grep, idutils, ctags, cscope....
- External tools settings for the project.
- Your extension settings.
- ...

exVim also makes sure project files are stored in one place (in the folder `./.exvim.your_project_name/` under your project). 
This makes your project clean and much better to work with external tools. These project files can be:

- global search index and results (idutils)
- tags
- cscope files
- hierarchy graph pictures
- error messages
- temporary files
- ...

After Vim has loaded `your_project_name.exvim` and started, exVim helps you update your project files so you 
can use your favorite plugins with these files.

## How does it integrate Vim plugins?

exVim aims to implement as many functions and features as possible in **pure Vim language**. 
We try to avoid re-inventing the wheel. As a result, we carefully select and integrate popular Vim plugins  
for some of the tasks. For those features that are lacking or for those we think we can do it better, 
we develop ourselves. They are put in [exVim organization](https://github.com/exvim) on GitHub.

Here are the standards we use for patches and development for Vim plugins:

- Developed in pure Vim language
- Follow the unix philosophy: do one thing well 
- Minimal dependencies 
- High quality code and good performance
- Highly active community
- Can be installed with a variety of plugin managers, Vundle or pathogen. (Repo in GitHub, standard runtime path structure)

Read the [Plugins](http://exvim.github.io/docs/plugins/) page for details of the plugins
in exVim.

## About exVim organization and this repository

The exVim is an organization in Github. The repositories under exVim are the plugins used in
exVim project. They follow the standard Vim plugin structures so that people can install them on
demand.

The [exvim/main](https://github.com/exvim/main#installation) is the repository for 
managing the plugins, external tools, and custom scripts. It is the main entry point for the
exVim project. 

The repository contains:

- An essential `.vimrc` settings for exVim.
- Configuration files for external tools.
- Templates for vim-plugins and external tools.
- exVim develop environment.

## Community

- [Chinese] QQ群: 319248144
- [English] Comming Soon...
"
337,OWASP/owasp.github.io,HTML,"# owasp.github.io
OWASP Foundation main site repository

[![CC BY-SA 4.0][cc-by-sa-shield]][cc-by-sa]

The website is licensed under a [Creative Commons Attribution-ShareAlike 4.0
International License][cc-by-sa].

[![CC BY-SA 4.0][cc-by-sa-image]][cc-by-sa]

[cc-by-sa]: https://creativecommons.org/licenses/by-sa/4.0/
[cc-by-sa-image]: https://licensebuttons.net/l/by-sa/4.0/88x31.png
[cc-by-sa-shield]: https://img.shields.io/badge/License-CC%20BY--SA%204.0-lightgrey.svg
"
338,hraban/tomono,Shell,"# Multi- To Mono-repository

Merge multiple repositories into one big monorepository. Migrates every branch in
every subrepo to the eponymous branch in the monorepo, with all files
(including in the history) rewritten to live under a subdirectory.

Features:

* Preserve full history and commit hashes of all repositories.
* Don't Stop The World: keep working in your other repositories during the
  migration and pull the changes into the monorepo as you go.
* No conflicts: Each original repository keeps their directory structure, no
  merging required. All files are moved into a subdirectory.

Requirements:
* git version 2.9+.

## Usage

Prepare a list of repositories to merge in a file. The format is
`<repository_url><space><new_name>`. If you try and use a slash in
`<new_name>` it will fail because it uses this as a `git remote`. If
you need to have a slash, i.e. some folder depth, pass a third
parameter, the format will then be:
`<repository_url><space><new_name><space><folder_name>`

Here is an example `repos.txt` where the services are directly in the
root of the repository and the libraries are in a `/lib` subfolder:

```
git@github.com:mycompany/service-one.git one
git@github.com:mycompany/service-two.git two
git@github.com:mycompany/library-three.git three lib/three
git@github.com:mycompany/library-four.git four lib/four
```

Now pipe the file to the tomono.sh script. Assuming you've downloaded this
program to your home directory, for example, you can do:

```sh
$ cat repos.txt | ~/tomono/tomono.sh
```

This will create a new repository called `core`, in your current directory.

If you already have a repository called `core` and wish to import more into it,
pass the `--continue` flag. Make sure you don't have any outstanding changes!

To change the name of the monorepo directory, set an envvar before any other
operations:

```sh
$ export MONOREPO_NAME=my_directory
$ ...
```

### Tags and namespacing

Note that all tags are namespaced by default: e.g. if your remote `foo` has tags
`v1` and `v2`, your new monorepo will have tags `foo/v1` and `foo/v2`. If you'd
rather not have this, and just risk the odd tag clash (not a big deal: worst
case one tag overrides the other), you can do the following _after_ running the
full script:

```sh
$ ....tomono.sh # after this
$ cd core
$ rm -rf .git/refs/tags
$ git fetch --all
```

That will re-fetch all tags for you, verbatim.

## Fluid migration: Don't Stop The World

New changes to the old repositories can be imported into the monorepo and
merged in. For example, in the above example, say repository `one` had a branch
`my_branch` which continued to be developed after the migration. To pull those
changes in:

```sh
# Fetch all changes to the old repositories
$ git fetch --all --no-tags
$ git checkout my_branch
$ git merge --strategy recursive --strategy-option subtree=one/ one/my_branch
```

This is a regular merge like you are used to (recursive is the default). The
only special thing about it is the `--strategy-option subtree=one/`: this tells
git that the files have all been moved to a subdirectory called `one`.

N.B.: new tags won't be merged, because they would not be namespaced if fetched
this way. If you don't mind having all your tags together in the same scope,
follow the ""no namespaced tags"" instructions from above, and remove the
`--no-tags` bit, here.

### Github branch protection

If:

* the changes have been made to master in the old repo, and
* your mono repo is stored on Github, and
* you have branch protection set up for master,

you could create a PR from the changes instead of directly merging into master:

```sh
$ git fetch --all --no-tags
# Checkout to master first to make sure we're basing this off the latest master
$ git checkout master
# Now the new ""some_branch"" will be where our current master is
$ git checkout -b new_one_master
$ git merge --strategy recursive --strategy-option subtree=one/ one/master
$ git push -b origin new_one_master
# Go to Github and create a PR from branch 'new_one_master'
```

## Explanation

The contents of each repository will be moved to a subdirectory. A new branch
will be created for each branch in each of those repositories, and branches of
equal name will be merged.

In the example above, if both repositories `one` and `two` had a branch called
`feature-XXX`, your new repository (core) would have one branch called
`feature-XXX` with two directories in it: `one/` and `two/`.

Usually, every repository will have at least a branch called `master`, so your
new monorepo will have a branch called `master` with a subdirectory for each
original repository's master branch.

A detailed explanation of this program can be found in the accompanying blog
post:

https://syslog.ravelin.com/multi-to-mono-repository-c81d004df3ce

## Further steps

Once your new repository is created, you'll need to update your CI environment.
This means merging all .travis.yml, .circle.yml and similar files into a single
file in the top level. The same holds for the Makefile, which can branch off
into the separate subdirectories to do independent work there.

Additionally, you will need to make a decision about vendoring, if applicable:
do you want to use one vendoring dir for all your code (e.g. a top-level
`vendor` for Go, or `node_modules` for node), or do you want to keep independent
vendoring directories for each project? Both solutions have their respective
pros and cons, which is best depends on your situation.
"
339,ElmerCSC/elmerfem,Fortran,":imagesdir: pics
[.text-center]
image::ElmerLogoPlain64x64.png[float=""right""]
== Elmer FEM


This is the official source code repository for the Elmer FEM software suite.



[.text-center]
image:https://img.shields.io/github/stars/ElmerCSC/elmerfem.svg?style=social&label=Stars&style=plastic[""GitHub stars""] image:https://img.shields.io/github/watchers/ElmerCSC/elmerfem.svg?style=social&label=Watch&style=plastic[""GitHub watchers""] image:https://img.shields.io/github/forks/ElmerCSC/elmerfem.svg?style=social&label=Fork&style=plastic[""GitHub forks""]

 
[.text-center]
image:https://img.shields.io/github/contributors/ElmerCSC/elmerfem.svg?style=flat[""GitHub contributors""]
 image:https://img.shields.io/github/issues-pr/ElmerCSC/elmerfem.svg?style=flat[""GitHub pull requests"", link=https://github.com/ElmerCSC/elmerfem/pulls] image:https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat[] 

[.text-center]
image:https://img.shields.io/github/last-commit/ElmerCSC/elmerfem.svg?style=flat[""GitHub last commit""] image:https://img.shields.io/github/issues-raw/ElmerCSC/elmerfem.svg?maxAge=25000[""Issues"", link=https://github.com/ElmerCSC/elmerfem/issues] image:https://img.shields.io/github/languages/count/ElmerCSC/elmerfem[GitHub language count]




[.text-justify]
Elmer is a finite element software for numerical solution of partial differential equations. Elmer is capable of handling any number of equations and is therefore ideally suited for the simulation of multiphysical problems. It includes models, for example, of structural mechanics, fluid dynamics, heat transfer and electromagnetics. Users can also write their own equations that can be dynamically linked with the main program.

Elmer consists of several parts. The most important ones are ElmerSolver the finite element solver, ElmerGUI the graphical user interface, and ElmerGrid the mesh creation and manipulation tool. Also a visualization tool, ElmerPost, is included in the package but it is no longer developed.  


=== Download binaries

You may download binaries and virtual machines from http://www.elmerfem.org/blog/binaries/[here].

=== Docker

 * nwrichmond/elmerice: https://hub.docker.com/r/nwrichmond/elmerice/[Docker Hub], https://raw.githubusercontent.com/ElmerCSC/elmerfem/release/ReleaseNotes/release_8.4.txt[more info]
 * unifem/Elmer-desktop: https://github.com/unifem/Elmer-desktop[GitHub]
 * CoSci-LLC/docker-elmerice: https://hub.docker.com/repository/docker/coscillc/elmerice[Docker Hub], https://github.com/CoSci-LLC/docker-elmerice[GitHub]

=== Documentation

You may find the PDFs for the documentation http://www.elmerfem.org/blog/documentation/[here].

=== Compiling


==== macOS

 * Download this repository either az a zip file via GitHub or using `git clone https://github.com/ElmerCSC/elmerfem.git`
 * Go to the downloaded directory `mkdir build` and `cd build`
 * Install Homebrew
 * Install GCC `brew install gcc`
 * Install CMake `brew install cmake`
 * Without MPI: 
    ** `cmake .. -D WITH_OpenMP:BOOLEAN=TRUE`
 * With MPI:
    ** Install OpenMPI `brew install open-mpi`
    ** `cmake .. -D WITH_OpenMP:BOOLEAN=TRUE -D WITH_MPI:BOOLEAN=TRUE`
 * With ElmerGUI:
    ** install qt4 with `brew install cartr/qt4/qt@4` 
    ** install qwt with `brew install cartr/qt4/qwt-qt4`
    ** `cmake .. -D WITH_OpenMP:BOOLEAN=TRUE -D WITH_MPI:BOOLEAN=TRUE -D WITH_ELMERGUI:BOOLEAN=TRUE`
 * With ElmerPost:
    ** `brew cask install xquartz`
    ** ....
 * `make`
 * `make install`

==== Ubuntu

 * Download the source code and create `build` directory as above
 * Install the dependencies `sudo apt install git cmake build-essential gfortran libopenmpi-dev libblas-dev liblapack-dev`
 * Without MPI:
    ** `cmake .. -DWITH_OpenMP:BOOLEAN=TRUE`
 * With MPI:
    ** `cmake .. -DWITH_OpenMP:BOOLEAN=TRUE -DWITH_MPI:BOOLEAN=TRUE`
 * With ElmerGUI:
    ** `sudo apt install libqt4-dev libqwt-dev`
    ** `cmake .. -DWITH_OpenMP:BOOLEAN=TRUE -DWITH_MPI:BOOLEAN=TRUE -DWITH_ELMERGUI:BOOLEAN=TRUE`
 * With Elmer/Ice (enabling netcdf and MUMPS):
    ** `sudo apt install libnetcdf-dev libnetcdff-dev libmumps-dev`
    ** `cmake .. -DWITH_OpenMP:BOOLEAN=TRUE -DWITH_MPI:BOOLEAN=TRUE -DWITH_ElmerIce:BOOLEAN=TRUE -DWITH_Mumps:BOOL=TRUE` 
 * `make`
 * `sudo make install`
 * The executables are in `/usr/local/bin` folder, you may add this to your PATH if you will

=== Licensing

image:https://img.shields.io/badge/License-GPLv2-blue.svg[""License: GPL v2"", link=https://www.gnu.org/licenses/gpl-2.0]  image:https://img.shields.io/badge/License-LGPL%20v2.1-blue.svg[""License: LGPL v2.1"", link=https://www.gnu.org/licenses/lgpl-2.1]

[.text-justify]
Elmer software is licensed under GPL except for the ElmerSolver library which is licensed under LGPL license. Elmer is mainly developed at CSC - IT Center for Science, Finland. However, there have been numerous contributions from other organizations and developers as well,
and the project is open for new contributions. More information about Elmer's licensing link:license_texts/ElmerLicensePolicy.txt[here].


=== Package managers

[.text-center]
image::https://repology.org/badge/vertical-allrepos/elmerfem.svg[""Packaging status"", link=https://repology.org/project/elmerfem/versions]

==== Chocolatey

[.text-center]
image:https://img.shields.io/chocolatey/dt/elmer-mpi[""Chocolatey"", link=https://chocolatey.org/packages/elmer-mpi]

=== Social

[.text-justify]
Here on https://discordapp.com/invite/NeZEBZn[this Discord channel] you may ask for help or dicuss different Elmer related matters:

[.text-center]
image::https://img.shields.io/discord/412182089279209474.svg[""Discord Chat"", link=https://discordapp.com/invite/NeZEBZn]

Follow ElmerFEM on Twitter:

[.text-center]
image:https://img.shields.io/twitter/follow/elmerfem.svg?style=social[""Twitter Follow"", link=https://twitter.com/elmerfem] image:https://img.shields.io/twitter/follow/ElmerIce1.svg?style=social[""Twitter Follow"", link=https://twitter.com/ElmerIce1] image:https://img.shields.io/twitter/follow/CSCfi.svg?style=social[""Twitter Follow"", link=https://twitter.com/CSCfi] 

Ask your questions on Reddit:

[.text-center]
image:https://img.shields.io/reddit/subreddit-subscribers/ElmerFEM[""Subreddit subscribers"", link=https://www.reddit.com/r/ElmerFEM/]


=== Computational Glaciology ""Elmer/Ice""

* http://elmerice.elmerfem.org[Elmer/Ice community web site]
* https://github.com/ElmerCSC/elmerfem/tree/elmerice/elmerice/[Elmer/Ice README]


=== Other links

* http://www.elmerfem.org/[Elmer Blog]
* https://www.csc.fi/web/elmer[official CSC homepage]
* http://www.elmerfem.org/forum/[Elmer forum] (preferred place for asking questions)
* https://postit.csc.fi/sympa/info/elmerupdates[Updates maling list]
* https://sourceforge.net/projects/elmerfem/[Elmer at sourceforge (deprecated)] image:https://img.shields.io/sourceforge/dt/elmerfem.svg[""Download Elmer"", link=https://sourceforge.net/projects/elmerfem/files/latest/download]
* image:https://i.stack.imgur.com/gVE0j.png[""LinkedIn badge"", link=https://www.linkedin.com/groups/3682354/] https://www.linkedin.com/groups/3682354/[LinkedIn]
* https://www.youtube.com/user/elmerfem[YouTube]
* https://launchpad.net/~elmer-csc-ubuntu/+archive/ubuntu/elmer-csc-ppa[Launchpad]
* http://www.nic.funet.fi/pub/sci/physics/elmer/bin/[VM and Windows builds]
* http://www.nic.funet.fi/pub/sci/physics/elmer/doc/[Documentation]


"
